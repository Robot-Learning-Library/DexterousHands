/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041915-2d28w7ge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandCatchAbreast_ppo_20221020041913
wandb: ⭐️ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: 🚀 View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/2d28w7ge
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:2
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=2, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=2, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:2', seed=None, sim_device='cuda:2', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandCatchAbreast', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_catch_abreast', 'numEnvs': 2048, 'envSpacing': 0.75, 'episodeLength': 150, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 50, 'transition_scale': 0.5, 'orientation_scale': 1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.65, 'fallPenalty': 0.0, 'objectType': 'egg', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandCatchAbreast', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandCatchAbreast_ppo_20221020041913', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 1859
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:2
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1096 steps/s (collection: 11.475s, learning 3.471s)
               Value function loss: 7.0282
                    Surrogate loss: 0.0632
             Mean action noise std: 0.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 14.95s
                        Total time: 14.95s
                               ETA: 1494555.9s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1420 steps/s (collection: 10.409s, learning 1.126s)
               Value function loss: 0.9575
                    Surrogate loss: -0.0266
             Mean action noise std: 0.80
                       Mean reward: 0.06
               Mean episode length: 16.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 11.54s
                        Total time: 26.48s
                               ETA: 1324054.7s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 1263 steps/s (collection: 12.754s, learning 0.209s)
               Value function loss: 0.2154
                    Surrogate loss: -0.0474
             Mean action noise std: 0.80
                       Mean reward: 0.06
               Mean episode length: 20.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 12.96s
                        Total time: 39.44s
                               ETA: 1314784.3s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 1068 steps/s (collection: 15.154s, learning 0.175s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0396
             Mean action noise std: 0.80
                       Mean reward: 0.09
               Mean episode length: 31.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 15.33s
                        Total time: 54.77s
                               ETA: 1369291.2s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 1010 steps/s (collection: 16.034s, learning 0.188s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0333
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 39.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 16.22s
                        Total time: 70.99s
                               ETA: 1419842.9s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 985 steps/s (collection: 16.414s, learning 0.206s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 46.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 16.62s
                        Total time: 87.61s
                               ETA: 1460175.7s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 950 steps/s (collection: 17.056s, learning 0.175s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0352
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 53.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 17.23s
                        Total time: 104.85s
                               ETA: 1497708.5s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 951 steps/s (collection: 17.023s, learning 0.188s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0311
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 50.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 17.21s
                        Total time: 122.06s
                               ETA: 1525606.3s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 943 steps/s (collection: 17.197s, learning 0.173s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0353
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 50.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 17.37s
                        Total time: 139.43s
                               ETA: 1549059.1s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 931 steps/s (collection: 17.416s, learning 0.165s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0425
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 52.55
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 17.58s
                        Total time: 157.01s
                               ETA: 1569929.5s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.033s, learning 0.189s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0382
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 54.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 17.22s
                        Total time: 174.23s
                               ETA: 1583744.3s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 934 steps/s (collection: 17.361s, learning 0.167s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0495
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 56.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 17.53s
                        Total time: 191.76s
                               ETA: 1597801.1s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 938 steps/s (collection: 17.221s, learning 0.241s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0311
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 59.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 17.46s
                        Total time: 209.22s
                               ETA: 1609184.6s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.206s, learning 0.188s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0373
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 58.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 17.39s
                        Total time: 226.61s
                               ETA: 1618455.7s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 936 steps/s (collection: 17.332s, learning 0.167s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0540
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 64.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 17.50s
                        Total time: 244.11s
                               ETA: 1627190.5s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 946 steps/s (collection: 17.152s, learning 0.165s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0296
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 63.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 17.32s
                        Total time: 261.43s
                               ETA: 1633691.2s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 955 steps/s (collection: 16.976s, learning 0.174s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0393
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 64.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 17.15s
                        Total time: 278.58s
                               ETA: 1638442.9s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 953 steps/s (collection: 17.007s, learning 0.184s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 57.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 17.19s
                        Total time: 295.77s
                               ETA: 1642890.1s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.457s, learning 0.182s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0338
             Mean action noise std: 0.80
                       Mean reward: 0.17
               Mean episode length: 78.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 16.64s
                        Total time: 312.41s
                               ETA: 1643966.9s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.844s, learning 0.158s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0266
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 59.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 17.00s
                        Total time: 329.41s
                               ETA: 1646744.6s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 940 steps/s (collection: 17.207s, learning 0.215s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 57.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 17.42s
                        Total time: 346.83s
                               ETA: 1651256.4s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.818s, learning 0.181s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0484
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 61.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 17.00s
                        Total time: 363.83s
                               ETA: 1653435.2s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.160s, learning 0.177s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0529
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 74.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 17.34s
                        Total time: 381.17s
                               ETA: 1656890.5s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.914s, learning 0.180s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0160
             Mean action noise std: 0.80
                       Mean reward: 0.17
               Mean episode length: 73.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 17.09s
                        Total time: 398.26s
                               ETA: 1659043.2s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 936 steps/s (collection: 17.308s, learning 0.187s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0338
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 67.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 17.49s
                        Total time: 415.76s
                               ETA: 1662628.3s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.193s, learning 0.171s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0518
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 70.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 17.36s
                        Total time: 433.12s
                               ETA: 1665436.0s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 942 steps/s (collection: 17.142s, learning 0.246s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0393
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 62.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 17.39s
                        Total time: 450.51s
                               ETA: 1668117.1s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 944 steps/s (collection: 17.092s, learning 0.249s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0439
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 56.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 17.34s
                        Total time: 467.85s
                               ETA: 1670439.3s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.742s, learning 0.172s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0414
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 57.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 16.91s
                        Total time: 484.76s
                               ETA: 1671131.5s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 944 steps/s (collection: 17.143s, learning 0.199s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0515
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 63.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 17.34s
                        Total time: 502.11s
                               ETA: 1673201.9s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 949 steps/s (collection: 17.080s, learning 0.173s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0562
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 59.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 17.25s
                        Total time: 519.36s
                               ETA: 1674847.2s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 949 steps/s (collection: 17.078s, learning 0.186s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0538
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 59.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 17.26s
                        Total time: 536.62s
                               ETA: 1676425.9s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.794s, learning 0.162s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0543
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 59.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 16.96s
                        Total time: 553.58s
                               ETA: 1676975.1s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.760s, learning 0.165s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0480
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 65.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 16.92s
                        Total time: 570.50s
                               ETA: 1677398.8s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.834s, learning 0.189s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0535
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 58.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 17.02s
                        Total time: 587.53s
                               ETA: 1678077.2s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.874s, learning 0.169s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0508
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 56.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 17.04s
                        Total time: 604.57s
                               ETA: 1678772.4s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 940 steps/s (collection: 17.220s, learning 0.204s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 54.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 17.42s
                        Total time: 621.99s
                               ETA: 1680458.2s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.921s, learning 0.174s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0335
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 58.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 13.10s
                        Total time: 635.09s
                               ETA: 1670668.6s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.919s, learning 0.223s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0387
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 55.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 9.14s
                        Total time: 644.23s
                               ETA: 1651246.8s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.617s, learning 0.185s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0474
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 53.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.80s
                        Total time: 653.03s
                               ETA: 1631946.4s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.320s, learning 0.177s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0439
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 64.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.50s
                        Total time: 661.53s
                               ETA: 1612843.8s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.882s, learning 0.175s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0522
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 55.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 9.06s
                        Total time: 670.59s
                               ETA: 1595983.1s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.184s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0519
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 53.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.66s
                        Total time: 679.25s
                               ETA: 1578977.4s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.633s, learning 0.164s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0528
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 56.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.80s
                        Total time: 688.04s
                               ETA: 1563062.4s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.845s, learning 0.236s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0593
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 55.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 9.08s
                        Total time: 697.12s
                               ETA: 1548483.6s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.482s, learning 0.172s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0506
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 58.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.65s
                        Total time: 705.78s
                               ETA: 1533611.2s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.399s, learning 0.270s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0539
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 53.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.67s
                        Total time: 714.45s
                               ETA: 1519401.1s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.624s, learning 0.183s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0529
             Mean action noise std: 0.80
                       Mean reward: 0.16
               Mean episode length: 58.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.81s
                        Total time: 723.25s
                               ETA: 1506070.6s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.510s, learning 0.189s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0362
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 53.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.70s
                        Total time: 731.95s
                               ETA: 1493064.6s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.680s, learning 0.167s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0370
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 53.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 8.85s
                        Total time: 740.80s
                               ETA: 1480873.4s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.753s, learning 0.185s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0408
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 54.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 8.94s
                        Total time: 749.74s
                               ETA: 1469340.2s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1772 steps/s (collection: 9.051s, learning 0.194s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0490
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 51.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 9.24s
                        Total time: 758.98s
                               ETA: 1458838.5s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.165s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0546
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 50.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 8.67s
                        Total time: 767.65s
                               ETA: 1447646.5s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.505s, learning 0.172s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0333
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 52.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.68s
                        Total time: 776.33s
                               ETA: 1436883.6s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.171s, learning 0.171s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 51.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.34s
                        Total time: 784.67s
                               ETA: 1425903.1s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.540s, learning 0.206s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0430
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 48.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.75s
                        Total time: 793.42s
                               ETA: 1416035.7s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.728s, learning 0.189s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0518
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 46.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.92s
                        Total time: 802.33s
                               ETA: 1406814.2s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.605s, learning 0.166s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0584
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 49.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.77s
                        Total time: 811.10s
                               ETA: 1397658.6s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.832s, learning 0.191s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0413
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 49.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 9.02s
                        Total time: 820.13s
                               ETA: 1389241.3s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.488s, learning 0.176s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0402
             Mean action noise std: 0.80
                       Mean reward: 0.15
               Mean episode length: 52.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.66s
                        Total time: 828.79s
                               ETA: 1380505.2s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.837s, learning 0.264s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0536
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 46.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 9.10s
                        Total time: 837.89s
                               ETA: 1372769.9s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.476s, learning 0.169s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0555
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 48.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.64s
                        Total time: 846.54s
                               ETA: 1364549.6s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.079s, learning 0.225s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0501
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 47.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 8.30s
                        Total time: 854.84s
                               ETA: 1356049.1s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.452s, learning 0.172s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0504
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 49.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.62s
                        Total time: 863.47s
                               ETA: 1348314.7s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.498s, learning 0.168s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 45.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.67s
                        Total time: 872.13s
                               ETA: 1340881.8s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.165s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0558
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 45.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.53s
                        Total time: 880.66s
                               ETA: 1333464.3s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.122s, learning 0.175s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0568
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 47.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.30s
                        Total time: 888.96s
                               ETA: 1325923.9s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.765s, learning 0.187s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0558
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 48.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.95s
                        Total time: 897.91s
                               ETA: 1319566.9s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.364s, learning 0.162s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0541
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 46.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.53s
                        Total time: 906.43s
                               ETA: 1312777.7s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.539s, learning 0.176s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0547
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 47.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.71s
                        Total time: 915.15s
                               ETA: 1306451.7s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.286s, learning 0.162s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0537
             Mean action noise std: 0.80
                       Mean reward: 0.14
               Mean episode length: 45.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.45s
                        Total time: 923.59s
                               ETA: 1299927.3s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.605s, learning 0.329s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0531
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 44.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.93s
                        Total time: 932.53s
                               ETA: 1294260.1s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.785s, learning 0.174s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0532
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 43.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.96s
                        Total time: 941.49s
                               ETA: 1288782.2s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.549s, learning 0.166s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0536
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 39.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.72s
                        Total time: 950.20s
                               ETA: 1283122.2s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.389s, learning 0.261s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0502
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 43.34
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.65s
                        Total time: 958.85s
                               ETA: 1277526.0s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.195s, learning 0.203s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0525
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 45.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 8.40s
                        Total time: 967.25s
                               ETA: 1271745.8s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.306s, learning 0.159s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0550
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 44.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.47s
                        Total time: 975.72s
                               ETA: 1266203.1s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.489s, learning 0.189s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0564
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 43.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.68s
                        Total time: 984.40s
                               ETA: 1261074.6s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.490s, learning 0.159s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0582
             Mean action noise std: 0.80
                       Mean reward: 0.13
               Mean episode length: 42.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.65s
                        Total time: 993.04s
                               ETA: 1256038.3s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.178s, learning 0.161s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0597
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 42.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.34s
                        Total time: 1001.38s
                               ETA: 1250741.2s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.371s, learning 0.238s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0598
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 42.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 8.61s
                        Total time: 1009.99s
                               ETA: 1245906.6s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.411s, learning 0.189s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0615
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 39.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.60s
                        Total time: 1018.59s
                               ETA: 1241180.2s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.502s, learning 0.160s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0602
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 42.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 8.66s
                        Total time: 1027.25s
                               ETA: 1236641.4s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.225s, learning 0.202s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0584
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 40.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.43s
                        Total time: 1035.68s
                               ETA: 1231931.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.402s, learning 0.177s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0575
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 37.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.58s
                        Total time: 1044.26s
                               ETA: 1227510.0s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.258s, learning 0.160s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0611
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 39.44
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.42s
                        Total time: 1052.68s
                               ETA: 1223004.6s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.203s, learning 0.171s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0617
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 39.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.37s
                        Total time: 1061.05s
                               ETA: 1218551.9s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.133s, learning 0.186s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0578
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 39.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.32s
                        Total time: 1069.37s
                               ETA: 1214138.7s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.249s, learning 0.179s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0626
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 37.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.43s
                        Total time: 1077.80s
                               ETA: 1209946.6s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.604s, learning 0.288s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0616
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 38.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.89s
                        Total time: 1086.69s
                               ETA: 1206361.7s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.522s, learning 0.220s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0574
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 36.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.74s
                        Total time: 1095.44s
                               ETA: 1202691.7s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.523s, learning 0.193s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0556
             Mean action noise std: 0.80
                       Mean reward: 0.12
               Mean episode length: 38.12
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.72s
                        Total time: 1104.15s
                               ETA: 1199071.8s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.696s, learning 0.168s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0602
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 36.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.86s
                        Total time: 1113.01s
                               ETA: 1195689.0s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.163s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0619
             Mean action noise std: 0.80
                       Mean reward: 0.11
               Mean episode length: 37.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.72s
                        Total time: 1121.73s
                               ETA: 1192223.2s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.424s, learning 0.214s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.64s
                        Total time: 1130.37s
                               ETA: 1188745.7s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.168s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0551
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 37.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.65s
                        Total time: 1139.02s
                               ETA: 1185351.3s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.276s, learning 0.176s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0553
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.45s
                        Total time: 1147.47s
                               ETA: 1181824.3s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.690s, learning 0.185s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0538
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 37.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 8.87s
                        Total time: 1156.35s
                               ETA: 1178800.1s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.605s, learning 0.168s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0529
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 37.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.77s
                        Total time: 1165.12s
                               ETA: 1175734.5s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.584s, learning 0.193s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0564
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 8.78s
                        Total time: 1173.90s
                               ETA: 1172734.0s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.455s, learning 0.188s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 37.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.64s
                        Total time: 1182.54s
                               ETA: 1169659.7s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.646s, learning 0.250s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0608
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 37.93
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 8.90s
                        Total time: 1191.43s
                               ETA: 1166893.5s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.808s, learning 0.174s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0620
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 37.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.98s
                        Total time: 1200.42s
                               ETA: 1164264.2s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.744s, learning 0.274s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 9.02s
                        Total time: 1209.43s
                               ETA: 1161719.4s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1769 steps/s (collection: 8.824s, learning 0.434s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0576
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 37.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 9.26s
                        Total time: 1218.69s
                               ETA: 1159451.3s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.390s, learning 0.193s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.58s
                        Total time: 1227.27s
                               ETA: 1156590.9s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.805s, learning 0.176s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.98s
                        Total time: 1236.26s
                               ETA: 1154154.4s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.464s, learning 0.160s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.62s
                        Total time: 1244.88s
                               ETA: 1151433.1s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.346s, learning 0.255s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.60s
                        Total time: 1253.48s
                               ETA: 1148740.5s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.356s, learning 0.206s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 8.56s
                        Total time: 1262.04s
                               ETA: 1146061.1s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.545s, learning 0.170s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.71s
                        Total time: 1270.76s
                               ETA: 1143567.5s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.482s, learning 0.172s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 8.65s
                        Total time: 1279.41s
                               ETA: 1141064.1s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.165s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 8.58s
                        Total time: 1287.99s
                               ETA: 1138538.2s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.119s, learning 0.278s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 38.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 8.40s
                        Total time: 1296.39s
                               ETA: 1135897.1s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.818s, learning 0.192s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 9.01s
                        Total time: 1305.40s
                               ETA: 1133834.3s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.764s, learning 0.267s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0558
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 9.03s
                        Total time: 1314.43s
                               ETA: 1131825.7s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.681s, learning 0.185s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0598
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 8.87s
                        Total time: 1323.30s
                               ETA: 1129709.8s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.787s, learning 0.173s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 8.96s
                        Total time: 1332.26s
                               ETA: 1127709.0s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.164s, learning 0.193s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0576
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 8.36s
                        Total time: 1340.61s
                               ETA: 1125236.2s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.168s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0612
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 8.55s
                        Total time: 1349.17s
                               ETA: 1122966.7s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.635s, learning 0.176s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0631
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.81s
                        Total time: 1357.98s
                               ETA: 1120947.8s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.263s, learning 0.157s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0614
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 36.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.42s
                        Total time: 1366.40s
                               ETA: 1118641.5s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.353s, learning 0.246s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0607
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 8.60s
                        Total time: 1375.00s
                               ETA: 1116518.7s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.338s, learning 0.176s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 37.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 8.51s
                        Total time: 1383.51s
                               ETA: 1114360.6s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.246s, learning 0.216s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 8.46s
                        Total time: 1391.97s
                               ETA: 1112196.6s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.376s, learning 0.190s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0631
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.57s
                        Total time: 1400.54s
                               ETA: 1110148.1s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.431s, learning 0.249s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 37.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 8.68s
                        Total time: 1409.22s
                               ETA: 1108221.9s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0610
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.72s
                        Total time: 1417.94s
                               ETA: 1106356.0s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.580s, learning 0.179s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0623
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.76s
                        Total time: 1426.70s
                               ETA: 1104550.2s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0634
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 8.35s
                        Total time: 1435.04s
                               ETA: 1102455.3s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.632s, learning 0.172s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0655
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 35.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 8.80s
                        Total time: 1443.85s
                               ETA: 1100740.7s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.137s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0632
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.30s
                        Total time: 1452.15s
                               ETA: 1098670.6s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.631s, learning 0.168s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0567
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 8.80s
                        Total time: 1460.95s
                               ETA: 1097006.4s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 34.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 8.49s
                        Total time: 1469.43s
                               ETA: 1095133.1s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.342s, learning 0.158s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0619
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 8.50s
                        Total time: 1477.93s
                               ETA: 1093297.8s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.427s, learning 0.162s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0559
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 8.59s
                        Total time: 1486.52s
                               ETA: 1091554.9s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.441s, learning 0.178s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.62s
                        Total time: 1495.14s
                               ETA: 1089859.1s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.499s, learning 0.208s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0629
             Mean action noise std: 0.79
                       Mean reward: 0.11
               Mean episode length: 34.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.71s
                        Total time: 1503.85s
                               ETA: 1088251.7s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.746s, learning 0.165s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0551
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.08
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 8.91s
                        Total time: 1512.76s
                               ETA: 1086813.6s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.450s, learning 0.170s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0550
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 8.62s
                        Total time: 1521.38s
                               ETA: 1085188.8s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.168s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0538
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 8.45s
                        Total time: 1529.83s
                               ETA: 1083467.8s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.166s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0568
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 8.53s
                        Total time: 1538.37s
                               ETA: 1081828.9s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.429s, learning 0.176s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0601
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 8.60s
                        Total time: 1546.97s
                               ETA: 1080261.5s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.390s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.13
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 8.56s
                        Total time: 1555.53s
                               ETA: 1078684.6s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.501s, learning 0.196s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0534
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 8.70s
                        Total time: 1564.23s
                               ETA: 1077224.2s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.274s, learning 0.160s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0539
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.43s
                        Total time: 1572.66s
                               ETA: 1075603.7s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.420s, learning 0.182s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 8.60s
                        Total time: 1581.26s
                               ETA: 1074119.1s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.558s, learning 0.174s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 37.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 8.73s
                        Total time: 1590.00s
                               ETA: 1072742.6s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.500s, learning 0.188s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 37.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.69s
                        Total time: 1598.68s
                               ETA: 1071354.5s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.172s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0625
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 8.45s
                        Total time: 1607.13s
                               ETA: 1069824.1s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.543s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0624
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 8.71s
                        Total time: 1615.84s
                               ETA: 1068486.5s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.415s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0619
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 8.58s
                        Total time: 1624.42s
                               ETA: 1067081.7s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.183s, learning 0.182s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0631
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.37s
                        Total time: 1632.78s
                               ETA: 1065555.7s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.425s, learning 0.160s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.58s
                        Total time: 1641.37s
                               ETA: 1064191.9s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.217s, learning 0.175s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0540
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 8.39s
                        Total time: 1649.76s
                               ETA: 1062721.2s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.610s, learning 0.177s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0520
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.79s
                        Total time: 1658.55s
                               ETA: 1061522.6s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.388s, learning 0.201s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0627
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.59s
                        Total time: 1667.13s
                               ETA: 1060212.8s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.641s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0541
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.80s
                        Total time: 1675.94s
                               ETA: 1059055.7s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.440s, learning 0.195s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0617
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.52
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.64s
                        Total time: 1684.57s
                               ETA: 1057806.8s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.528s, learning 0.196s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 8.72s
                        Total time: 1693.30s
                               ETA: 1056628.8s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.535s, learning 0.193s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.73s
                        Total time: 1702.03s
                               ETA: 1055468.3s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.706s, learning 0.177s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 36.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 8.88s
                        Total time: 1710.91s
                               ETA: 1054417.2s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.374s, learning 0.161s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0547
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 35.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.54s
                        Total time: 1719.45s
                               ETA: 1053166.1s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.058s, learning 0.192s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0514
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 34.86
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.25s
                        Total time: 1727.70s
                               ETA: 1051755.7s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.697s, learning 0.193s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0616
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 34.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.89s
                        Total time: 1736.59s
                               ETA: 1050750.2s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.379s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 34.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.54s
                        Total time: 1745.13s
                               ETA: 1049546.9s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.678s, learning 0.209s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0536
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 33.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.89s
                        Total time: 1754.01s
                               ETA: 1048564.3s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.873s, learning 0.195s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0534
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 9.07s
                        Total time: 1763.08s
                               ETA: 1047700.9s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.450s, learning 0.192s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0541
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 9.64s
                        Total time: 1772.72s
                               ETA: 1047186.6s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.783s, learning 0.176s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0504
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 16.96s
                        Total time: 1789.68s
                               ETA: 1050975.3s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.119s, learning 0.159s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0483
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 33.70
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 16.28s
                        Total time: 1805.96s
                               ETA: 1054321.8s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.404s, learning 0.165s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0503
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 16.57s
                        Total time: 1822.53s
                               ETA: 1057797.9s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.676s, learning 0.173s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 16.85s
                        Total time: 1839.38s
                               ETA: 1061395.2s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.487s, learning 0.170s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0524
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 37.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 16.66s
                        Total time: 1856.04s
                               ETA: 1064841.5s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.597s, learning 0.164s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0458
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 16.76s
                        Total time: 1872.80s
                               ETA: 1068307.0s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.242s, learning 0.164s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0540
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 16.41s
                        Total time: 1889.20s
                               ETA: 1071531.5s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.546s, learning 0.162s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0525
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 16.71s
                        Total time: 1905.91s
                               ETA: 1074889.7s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.032s, learning 0.173s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0506
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 34.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 16.21s
                        Total time: 1922.11s
                               ETA: 1077928.2s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.163s, learning 0.170s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0517
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 34.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 16.33s
                        Total time: 1938.45s
                               ETA: 1081003.9s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.284s, learning 0.165s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0564
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 16.45s
                        Total time: 1954.90s
                               ETA: 1084109.5s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.558s, learning 0.165s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0549
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 16.72s
                        Total time: 1971.62s
                               ETA: 1087331.7s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.099s, learning 0.162s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0594
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 16.26s
                        Total time: 1987.88s
                               ETA: 1090264.8s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.345s, learning 0.166s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 16.51s
                        Total time: 2004.39s
                               ETA: 1093302.3s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.715s, learning 0.217s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0546
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 16.93s
                        Total time: 2021.32s
                               ETA: 1096534.9s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.238s, learning 0.165s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0495
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 17.40s
                        Total time: 2038.73s
                               ETA: 1099986.4s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.387s, learning 0.194s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0533
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 34.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 16.58s
                        Total time: 2055.31s
                               ETA: 1102959.9s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.769s, learning 0.170s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 16.94s
                        Total time: 2072.25s
                               ETA: 1106092.2s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.501s, learning 0.177s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 35.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 16.68s
                        Total time: 2088.92s
                               ETA: 1109052.2s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.371s, learning 0.205s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0575
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 16.58s
                        Total time: 2105.50s
                               ETA: 1111927.2s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.236s, learning 0.166s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.25
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 16.40s
                        Total time: 2121.90s
                               ETA: 1114680.2s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.465s, learning 0.172s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0557
             Mean action noise std: 0.79
                       Mean reward: 0.13
               Mean episode length: 36.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 16.64s
                        Total time: 2138.54s
                               ETA: 1117527.0s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.219s, learning 0.196s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0535
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 16.42s
                        Total time: 2154.95s
                               ETA: 1120228.6s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.377s, learning 0.166s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0557
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 36.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 16.54s
                        Total time: 2171.50s
                               ETA: 1122968.4s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.676s, learning 0.166s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 38.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 16.84s
                        Total time: 2188.34s
                               ETA: 1125833.5s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.655s, learning 0.191s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0564
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 38.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 16.85s
                        Total time: 2205.19s
                               ETA: 1128670.9s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.567s, learning 0.194s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 16.76s
                        Total time: 2221.95s
                               ETA: 1131435.8s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.591s, learning 0.160s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0577
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 16.75s
                        Total time: 2238.70s
                               ETA: 1134167.6s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.622s, learning 0.228s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 37.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 16.85s
                        Total time: 2255.55s
                               ETA: 1136921.3s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.764s, learning 0.258s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 17.02s
                        Total time: 2272.57s
                               ETA: 1139733.3s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.842s, learning 0.182s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0470
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 17.02s
                        Total time: 2289.59s
                               ETA: 1142518.3s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.433s, learning 0.194s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 16.63s
                        Total time: 2306.22s
                               ETA: 1145078.1s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.855s, learning 0.165s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0572
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 17.02s
                        Total time: 2323.24s
                               ETA: 1147806.8s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 954 steps/s (collection: 17.003s, learning 0.168s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0567
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 17.17s
                        Total time: 2340.41s
                               ETA: 1150582.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.491s, learning 0.173s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.98
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 16.66s
                        Total time: 2357.07s
                               ETA: 1153082.9s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.005s, learning 0.196s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0610
             Mean action noise std: 0.79
                       Mean reward: 0.14
               Mean episode length: 37.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 17.20s
                        Total time: 2374.27s
                               ETA: 1155820.1s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 953 steps/s (collection: 16.988s, learning 0.189s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0525
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 17.18s
                        Total time: 2391.45s
                               ETA: 1158519.3s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.664s, learning 0.165s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 12.83s
                        Total time: 2404.28s
                               ETA: 1159095.8s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.826s, learning 0.165s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0498
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.99s
                        Total time: 2413.27s
                               ETA: 1157825.4s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.170s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0599
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.96
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.66s
                        Total time: 2421.93s
                               ETA: 1156410.2s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.510s, learning 0.272s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.09
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.78s
                        Total time: 2430.72s
                               ETA: 1155064.8s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.509s, learning 0.165s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 38.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 8.67s
                        Total time: 2439.39s
                               ETA: 1153681.7s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.163s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0559
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.72s
                        Total time: 2448.11s
                               ETA: 1152332.4s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.268s, learning 0.214s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0521
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.48s
                        Total time: 2456.59s
                               ETA: 1150884.4s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.539s, learning 0.176s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0528
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 41.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.72s
                        Total time: 2465.31s
                               ETA: 1149559.0s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.146s, learning 0.167s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0621
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.31s
                        Total time: 2473.62s
                               ETA: 1148058.7s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.697s, learning 0.162s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0587
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.86s
                        Total time: 2482.48s
                               ETA: 1146825.2s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.631s, learning 0.180s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0604
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.81s
                        Total time: 2491.29s
                               ETA: 1145580.7s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.634s, learning 0.163s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.80s
                        Total time: 2500.09s
                               ETA: 1144341.1s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.378s, learning 0.199s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0571
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 39.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.58s
                        Total time: 2508.67s
                               ETA: 1143012.3s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.284s, learning 0.196s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0600
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 38.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.48s
                        Total time: 2517.14s
                               ETA: 1141651.1s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.452s, learning 0.173s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0519
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.62s
                        Total time: 2525.77s
                               ETA: 1140368.0s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.647s, learning 0.162s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 39.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.81s
                        Total time: 2534.58s
                               ETA: 1139179.0s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.168s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 40.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.38s
                        Total time: 2542.96s
                               ETA: 1137809.3s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.861s, learning 0.170s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0527
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 41.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 9.03s
                        Total time: 2551.99s
                               ETA: 1136741.1s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.537s, learning 0.171s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0526
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 37.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.71s
                        Total time: 2560.70s
                               ETA: 1135539.3s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.375s, learning 0.172s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0516
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.77
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.55s
                        Total time: 2569.25s
                               ETA: 1134277.0s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.329s, learning 0.173s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 39.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.50s
                        Total time: 2577.75s
                               ETA: 1133005.5s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.297s, learning 0.181s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0527
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 41.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.48s
                        Total time: 2586.23s
                               ETA: 1131734.9s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.642s, learning 0.163s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 0.15
               Mean episode length: 38.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.80s
                        Total time: 2595.03s
                               ETA: 1130617.4s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.570s, learning 0.315s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0593
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 39.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.89s
                        Total time: 2603.92s
                               ETA: 1129544.8s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.613s, learning 0.162s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0569
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 40.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.78s
                        Total time: 2612.69s
                               ETA: 1128433.9s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.165s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 41.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.71s
                        Total time: 2621.41s
                               ETA: 1127306.2s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.160s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0562
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.70s
                        Total time: 2630.11s
                               ETA: 1126183.4s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.542s, learning 0.163s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0585
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.36
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.71s
                        Total time: 2638.81s
                               ETA: 1125071.1s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.555s, learning 0.185s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0457
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 8.74s
                        Total time: 2647.55s
                               ETA: 1123982.8s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.529s, learning 0.165s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 41.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 8.69s
                        Total time: 2656.25s
                               ETA: 1122884.3s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.467s, learning 0.170s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0528
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 40.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.64s
                        Total time: 2664.89s
                               ETA: 1121771.0s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.657s, learning 0.190s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0582
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 41.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.85s
                        Total time: 2673.73s
                               ETA: 1120755.0s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.511s, learning 0.192s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0514
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 41.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 8.70s
                        Total time: 2682.44s
                               ETA: 1119687.3s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.481s, learning 0.160s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0441
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 42.06
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.64s
                        Total time: 2691.08s
                               ETA: 1118602.7s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.674s, learning 0.258s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0539
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 42.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 8.93s
                        Total time: 2700.01s
                               ETA: 1117647.2s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.428s, learning 0.258s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 8.69s
                        Total time: 2708.70s
                               ETA: 1116598.5s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.166s, learning 0.259s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0588
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 8.43s
                        Total time: 2717.12s
                               ETA: 1115451.1s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.430s, learning 0.212s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0476
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 44.63
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 8.64s
                        Total time: 2725.76s
                               ETA: 1114401.3s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.817s, learning 0.159s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 44.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 8.98s
                        Total time: 2734.74s
                               ETA: 1113496.1s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.396s, learning 0.195s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0538
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 8.59s
                        Total time: 2743.33s
                               ETA: 1112442.3s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.555s, learning 0.206s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0493
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 46.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 8.76s
                        Total time: 2752.09s
                               ETA: 1111465.8s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.334s, learning 0.170s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0575
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 8.50s
                        Total time: 2760.59s
                               ETA: 1110393.7s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.489s, learning 0.178s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0584
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 44.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 8.67s
                        Total time: 2769.26s
                               ETA: 1109395.4s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.932s, learning 0.205s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 9.14s
                        Total time: 2778.40s
                               ETA: 1108592.5s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.561s, learning 0.163s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0506
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 43.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 8.72s
                        Total time: 2787.12s
                               ETA: 1107631.4s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.414s, learning 0.174s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0435
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 42.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 8.59s
                        Total time: 2795.71s
                               ETA: 1106624.4s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.485s, learning 0.158s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0442
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.82
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 8.64s
                        Total time: 2804.35s
                               ETA: 1105646.6s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.315s, learning 0.202s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0540
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 45.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 8.52s
                        Total time: 2812.87s
                               ETA: 1104627.1s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.714s, learning 0.212s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0549
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 8.93s
                        Total time: 2821.79s
                               ETA: 1103775.3s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.287s, learning 0.255s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0575
             Mean action noise std: 0.79
                       Mean reward: 0.20
               Mean episode length: 49.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 8.54s
                        Total time: 2830.34s
                               ETA: 1102780.8s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.418s, learning 0.160s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0560
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 46.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 8.58s
                        Total time: 2838.91s
                               ETA: 1101807.9s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.481s, learning 0.225s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0613
             Mean action noise std: 0.79
                       Mean reward: 0.17
               Mean episode length: 45.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 8.71s
                        Total time: 2847.62s
                               ETA: 1100892.2s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.399s, learning 0.160s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.15
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 8.56s
                        Total time: 2856.18s
                               ETA: 1099927.1s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.345s, learning 0.194s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0605
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 44.29
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 8.54s
                        Total time: 2864.72s
                               ETA: 1098961.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.324s, learning 0.186s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0542
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 8.51s
                        Total time: 2873.23s
                               ETA: 1097991.7s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.176s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0510
             Mean action noise std: 0.79
                       Mean reward: 0.18
               Mean episode length: 44.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 8.56s
                        Total time: 2881.79s
                               ETA: 1097049.5s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.549s, learning 0.197s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0480
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 8.75s
                        Total time: 2890.54s
                               ETA: 1096184.3s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.193s, learning 0.180s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0540
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 8.37s
                        Total time: 2898.91s
                               ETA: 1095184.3s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.169s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0581
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 49.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 8.72s
                        Total time: 2907.63s
                               ETA: 1094321.3s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.177s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0534
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 47.61
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 8.70s
                        Total time: 2916.33s
                               ETA: 1093457.5s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.688s, learning 0.185s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0609
             Mean action noise std: 0.79
                       Mean reward: 0.19
               Mean episode length: 48.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 8.87s
                        Total time: 2925.20s
                               ETA: 1092665.5s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.766s, learning 0.198s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0564
             Mean action noise std: 0.79
                       Mean reward: 0.20
               Mean episode length: 48.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 8.96s
                        Total time: 2934.16s
                               ETA: 1091913.4s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.684s, learning 0.263s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 0.20
               Mean episode length: 49.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 8.95s
                        Total time: 2943.11s
                               ETA: 1091160.3s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.372s, learning 0.166s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0515
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 47.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 8.54s
                        Total time: 2951.65s
                               ETA: 1090261.7s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.670s, learning 0.205s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0533
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 49.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 8.87s
                        Total time: 2960.52s
                               ETA: 1089493.7s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.508s, learning 0.186s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0492
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 50.59
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 8.69s
                        Total time: 2969.22s
                               ETA: 1088664.9s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.395s, learning 0.214s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0461
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 49.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 8.61s
                        Total time: 2977.82s
                               ETA: 1087811.2s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.719s, learning 0.194s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0527
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 49.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 8.91s
                        Total time: 2986.74s
                               ETA: 1087074.1s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.794s, learning 0.165s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0530
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 8.96s
                        Total time: 2995.70s
                               ETA: 1086359.3s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.550s, learning 0.195s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0520
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 49.60
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 8.75s
                        Total time: 3004.44s
                               ETA: 1085572.4s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.649s, learning 0.172s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0548
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 8.82s
                        Total time: 3013.26s
                               ETA: 1084818.0s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0587
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 51.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 8.45s
                        Total time: 3021.71s
                               ETA: 1083936.2s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.593s, learning 0.166s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0524
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 51.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 8.76s
                        Total time: 3030.47s
                               ETA: 1083171.2s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.397s, learning 0.173s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0556
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 49.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 8.57s
                        Total time: 3039.04s
                               ETA: 1082344.2s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.512s, learning 0.218s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0514
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 8.73s
                        Total time: 3047.77s
                               ETA: 1081579.6s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.377s, learning 0.186s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0458
             Mean action noise std: 0.78
                       Mean reward: 0.18
               Mean episode length: 46.57
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 8.56s
                        Total time: 3056.33s
                               ETA: 1080761.2s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 55.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 8.53s
                        Total time: 3064.86s
                               ETA: 1079935.5s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.438s, learning 0.190s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0574
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 50.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 8.63s
                        Total time: 3073.49s
                               ETA: 1079151.4s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.561s, learning 0.241s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0585
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 52.56
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 8.80s
                        Total time: 3082.29s
                               ETA: 1078433.9s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.355s, learning 0.194s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0585
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 52.76
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 8.55s
                        Total time: 3090.84s
                               ETA: 1077633.2s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.633s, learning 0.176s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0545
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 8.81s
                        Total time: 3099.65s
                               ETA: 1076928.2s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.405s, learning 0.221s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0497
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 51.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 8.63s
                        Total time: 3108.28s
                               ETA: 1076164.7s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.631s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0423
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 8.80s
                        Total time: 3117.08s
                               ETA: 1075466.6s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.318s, learning 0.174s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0448
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 49.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 8.49s
                        Total time: 3125.57s
                               ETA: 1074666.9s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.361s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0506
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 53.83
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.53s
                        Total time: 3134.09s
                               ETA: 1073884.3s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.018s, learning 0.223s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0458
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 8.24s
                        Total time: 3142.33s
                               ETA: 1073010.1s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.217s, learning 0.211s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0547
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 52.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 8.43s
                        Total time: 3150.76s
                               ETA: 1072205.0s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.630s, learning 0.173s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 58.97
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 8.80s
                        Total time: 3159.56s
                               ETA: 1071532.5s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.461s, learning 0.165s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 48.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 8.63s
                        Total time: 3168.19s
                               ETA: 1070804.8s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.034s, learning 0.169s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0531
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.07
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 8.20s
                        Total time: 3176.39s
                               ETA: 1069939.8s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.430s, learning 0.213s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0592
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 53.94
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 8.64s
                        Total time: 3185.04s
                               ETA: 1069228.2s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.607s, learning 0.184s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0483
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 48.33
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 8.79s
                        Total time: 3193.83s
                               ETA: 1068570.7s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.155s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.11
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 8.32s
                        Total time: 3202.15s
                               ETA: 1067760.1s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.439s, learning 0.200s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 51.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.64s
                        Total time: 3210.78s
                               ETA: 1067061.1s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.332s, learning 0.163s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0597
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 55.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.49s
                        Total time: 3219.28s
                               ETA: 1066319.1s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.467s, learning 0.201s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0578
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 8.67s
                        Total time: 3227.95s
                               ETA: 1065639.3s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.423s, learning 0.169s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0540
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 52.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.59s
                        Total time: 3236.54s
                               ETA: 1064938.7s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.267s, learning 0.190s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0476
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.46s
                        Total time: 3245.00s
                               ETA: 1064198.4s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.044s, learning 0.175s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0548
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 50.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.22s
                        Total time: 3253.21s
                               ETA: 1063384.9s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.211s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0527
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 52.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 8.37s
                        Total time: 3261.58s
                               ETA: 1062626.2s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.250s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0563
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 54.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.42s
                        Total time: 3270.00s
                               ETA: 1061888.0s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.089s, learning 0.172s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 48.87
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.26s
                        Total time: 3278.26s
                               ETA: 1061103.8s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.462s, learning 0.196s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0562
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 48.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.66s
                        Total time: 3286.92s
                               ETA: 1060452.5s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.328s, learning 0.197s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 55.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.52s
                        Total time: 3295.45s
                               ETA: 1059762.6s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.158s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 50.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.40s
                        Total time: 3303.85s
                               ETA: 1059036.5s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.074s, learning 0.195s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0536
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 51.73
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.27s
                        Total time: 3312.11s
                               ETA: 1058273.5s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.728s, learning 0.265s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0537
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 50.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.99s
                        Total time: 3321.11s
                               ETA: 1057745.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.455s, learning 0.194s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0506
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 50.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.65s
                        Total time: 3329.76s
                               ETA: 1057112.5s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.386s, learning 0.161s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0435
             Mean action noise std: 0.78
                       Mean reward: 0.23
               Mean episode length: 57.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.55s
                        Total time: 3338.30s
                               ETA: 1056450.7s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.103s, learning 0.174s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0537
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 55.20
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 8.28s
                        Total time: 3346.58s
                               ETA: 1055708.2s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.302s, learning 0.309s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0506
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 51.21
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 8.61s
                        Total time: 3355.19s
                               ETA: 1055075.0s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0455
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 48.38
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.55s
                        Total time: 3363.74s
                               ETA: 1054427.5s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.737s, learning 0.253s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 53.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 15.99s
                        Total time: 3379.73s
                               ETA: 1056108.3s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.125s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 50.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 16.29s
                        Total time: 3396.02s
                               ETA: 1057871.2s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.723s, learning 0.168s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 49.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 16.89s
                        Total time: 3412.91s
                               ETA: 1059810.2s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.179s, learning 0.162s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0501
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 52.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 16.34s
                        Total time: 3429.25s
                               ETA: 1061566.8s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.586s, learning 0.233s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0508
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 50.02
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 16.82s
                        Total time: 3446.07s
                               ETA: 1063460.0s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.365s, learning 0.166s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0524
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 49.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 16.53s
                        Total time: 3462.60s
                               ETA: 1065252.6s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.651s, learning 0.185s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 52.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 16.84s
                        Total time: 3479.44s
                               ETA: 1067127.8s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.094s, learning 0.169s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 52.79
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 16.26s
                        Total time: 3495.70s
                               ETA: 1068816.4s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.309s, learning 0.171s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0526
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 49.28
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 16.48s
                        Total time: 3512.18s
                               ETA: 1070560.3s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.236s, learning 0.171s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0456
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 52.65
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 16.41s
                        Total time: 3528.59s
                               ETA: 1072271.3s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.913s, learning 0.161s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0536
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 48.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 16.07s
                        Total time: 3544.66s
                               ETA: 1073870.9s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.297s, learning 0.167s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0559
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 51.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 16.46s
                        Total time: 3561.13s
                               ETA: 1075578.7s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.144s, learning 0.161s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0565
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.78
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 16.31s
                        Total time: 3577.43s
                               ETA: 1077228.2s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.500s, learning 0.179s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 47.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 16.68s
                        Total time: 3594.11s
                               ETA: 1078979.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.606s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 50.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 16.77s
                        Total time: 3610.88s
                               ETA: 1080748.1s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.617s, learning 0.166s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0534
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 16.78s
                        Total time: 3627.66s
                               ETA: 1082509.6s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.592s, learning 0.177s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0551
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 45.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 16.77s
                        Total time: 3644.43s
                               ETA: 1084256.3s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.260s, learning 0.185s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 16.44s
                        Total time: 3660.88s
                               ETA: 1085896.4s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.889s, learning 0.168s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0509
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 48.68
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 17.06s
                        Total time: 3677.93s
                               ETA: 1087707.8s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.189s, learning 0.176s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0572
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 44.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 16.36s
                        Total time: 3694.30s
                               ETA: 1089304.3s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.504s, learning 0.180s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0564
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 47.22
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 16.68s
                        Total time: 3710.98s
                               ETA: 1090985.2s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.321s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.84
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 16.49s
                        Total time: 3727.47s
                               ETA: 1092597.7s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.079s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 45.75
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 16.24s
                        Total time: 3743.71s
                               ETA: 1094130.0s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.427s, learning 0.232s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0536
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 47.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 16.66s
                        Total time: 3760.37s
                               ETA: 1095774.5s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.456s, learning 0.178s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0548
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 48.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 16.63s
                        Total time: 3777.01s
                               ETA: 1097401.9s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.724s, learning 0.187s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0513
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.39
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 16.91s
                        Total time: 3793.92s
                               ETA: 1099099.9s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.335s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0502
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 45.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 16.50s
                        Total time: 3810.42s
                               ETA: 1100668.9s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.787s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0487
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 45.30
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 16.95s
                        Total time: 3827.37s
                               ETA: 1102358.8s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.312s, learning 0.185s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0457
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 47.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 16.50s
                        Total time: 3843.86s
                               ETA: 1103908.8s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.146s, learning 0.193s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0549
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 47.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 16.34s
                        Total time: 3860.20s
                               ETA: 1105404.3s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.793s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0552
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 43.62
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 15.96s
                        Total time: 3876.16s
                               ETA: 1106781.7s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.997s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0568
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 16.16s
                        Total time: 3892.32s
                               ETA: 1108209.7s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.320s, learning 0.172s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0551
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 47.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 16.49s
                        Total time: 3908.81s
                               ETA: 1109723.6s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.578s, learning 0.202s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0542
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 45.49
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 16.78s
                        Total time: 3925.59s
                               ETA: 1111310.2s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.525s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0493
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 49.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 16.69s
                        Total time: 3942.28s
                               ETA: 1112862.0s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.318s, learning 0.162s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0533
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 44.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 16.48s
                        Total time: 3958.76s
                               ETA: 1114346.0s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.113s, learning 0.165s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0566
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 45.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 16.28s
                        Total time: 3975.04s
                               ETA: 1115765.0s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1095 steps/s (collection: 14.785s, learning 0.177s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 45.41
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 14.96s
                        Total time: 3990.00s
                               ETA: 1116807.5s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.505s, learning 0.172s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0503
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 45.32
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.68s
                        Total time: 3998.68s
                               ETA: 1116089.8s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.752s, learning 0.162s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.99
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.91s
                        Total time: 4007.59s
                               ETA: 1115441.8s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.561s, learning 0.161s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0577
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 46.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 8.72s
                        Total time: 4016.31s
                               ETA: 1114744.3s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.637s, learning 0.168s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 44.14
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 8.81s
                        Total time: 4025.12s
                               ETA: 1114073.7s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.363s, learning 0.194s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0547
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 48.17
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 8.56s
                        Total time: 4033.67s
                               ETA: 1113338.2s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.538s, learning 0.166s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0537
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 47.43
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 8.70s
                        Total time: 4042.38s
                               ETA: 1112647.2s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.308s, learning 0.172s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.53
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 8.48s
                        Total time: 4050.86s
                               ETA: 1111898.5s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.235s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0503
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 45.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 8.40s
                        Total time: 4059.25s
                               ETA: 1111131.7s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.787s, learning 0.195s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0542
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 43.24
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.98s
                        Total time: 4068.24s
                               ETA: 1110528.1s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0532
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 45.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 8.44s
                        Total time: 4076.67s
                               ETA: 1109779.4s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.380s, learning 0.165s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 45.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 8.55s
                        Total time: 4085.22s
                               ETA: 1109064.4s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.199s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0576
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 44.48
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.37s
                        Total time: 4093.59s
                               ETA: 1108305.1s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.213s, learning 0.189s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0601
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 44.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.40s
                        Total time: 4101.99s
                               ETA: 1107559.0s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0555
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 44.69
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 8.53s
                        Total time: 4110.52s
                               ETA: 1106851.9s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.714s, learning 0.174s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0569
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 48.42
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.89s
                        Total time: 4119.41s
                               ETA: 1106244.1s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.562s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0573
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 43.47
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.73s
                        Total time: 4128.13s
                               ETA: 1105596.0s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.389s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0562
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 47.27
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.55s
                        Total time: 4136.68s
                               ETA: 1104905.1s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.524s, learning 0.188s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0564
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 48.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.71s
                        Total time: 4145.40s
                               ETA: 1104260.3s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.163s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0540
             Mean action noise std: 0.78
                       Mean reward: 0.22
               Mean episode length: 48.51
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.72s
                        Total time: 4154.11s
                               ETA: 1103620.7s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.593s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0559
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 47.23
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 8.75s
                        Total time: 4162.87s
                               ETA: 1102993.5s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.459s, learning 0.171s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0532
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 43.10
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 8.63s
                        Total time: 4171.50s
                               ETA: 1102337.5s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.430s, learning 0.177s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.05
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 8.61s
                        Total time: 4180.10s
                               ETA: 1101678.7s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.649s, learning 0.193s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0526
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 44.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 8.84s
                        Total time: 4188.95s
                               ETA: 1101085.3s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.306s, learning 0.182s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0528
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 42.46
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 8.49s
                        Total time: 4197.44s
                               ETA: 1100401.9s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0559
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 42.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 8.39s
                        Total time: 4205.83s
                               ETA: 1099696.7s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.231s, learning 0.166s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 45.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.40s
                        Total time: 4214.22s
                               ETA: 1098996.5s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.527s, learning 0.214s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0535
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 41.90
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 8.74s
                        Total time: 4222.96s
                               ETA: 1098389.6s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.356s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0459
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 8.52s
                        Total time: 4231.48s
                               ETA: 1097728.5s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.615s, learning 0.161s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0525
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 46.72
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.78s
                        Total time: 4240.26s
                               ETA: 1097137.0s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.159s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0548
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 45.01
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 8.61s
                        Total time: 4248.87s
                               ETA: 1096505.6s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.164s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0539
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.56s
                        Total time: 4257.43s
                               ETA: 1095863.6s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.609s, learning 0.217s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0566
             Mean action noise std: 0.78
                       Mean reward: 0.18
               Mean episode length: 38.45
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.83s
                        Total time: 4266.25s
                               ETA: 1095294.3s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.179s, learning 0.194s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0493
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.03
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 8.37s
                        Total time: 4274.62s
                               ETA: 1094611.7s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.456s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0551
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 43.37
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 8.62s
                        Total time: 4283.25s
                               ETA: 1093996.2s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.232s, learning 0.183s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0543
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 42.92
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.41s
                        Total time: 4291.66s
                               ETA: 1093331.0s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.363s, learning 0.166s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0563
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 43.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.53s
                        Total time: 4300.19s
                               ETA: 1092698.3s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.788s, learning 0.159s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0575
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.81
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.95s
                        Total time: 4309.14s
                               ETA: 1092174.7s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.592s, learning 0.180s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0570
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 42.95
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 8.77s
                        Total time: 4317.91s
                               ETA: 1091609.6s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.738s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0583
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 40.04
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.90s
                        Total time: 4326.81s
                               ETA: 1091079.4s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.565s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.72s
                        Total time: 4335.54s
                               ETA: 1090507.6s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.310s, learning 0.155s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0539
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 38.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.47s
                        Total time: 4344.00s
                               ETA: 1089873.8s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.164s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0524
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.49s
                        Total time: 4352.49s
                               ETA: 1089248.1s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.350s, learning 0.221s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 40.18
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.57s
                        Total time: 4361.06s
                               ETA: 1088646.7s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.551s, learning 0.266s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0561
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 41.54
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.82s
                        Total time: 4369.87s
                               ETA: 1088109.6s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.393s, learning 0.214s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.19
               Mean episode length: 42.88
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.61s
                        Total time: 4378.48s
                               ETA: 1087523.2s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.162s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0586
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 39.80
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.61s
                        Total time: 4387.09s
                               ETA: 1086940.9s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.639s, learning 0.189s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0567
             Mean action noise std: 0.78
                       Mean reward: 0.21
               Mean episode length: 42.58
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.83s
                        Total time: 4395.92s
                               ETA: 1086414.7s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.324s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0513
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 39.64
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.48s
                        Total time: 4404.41s
                               ETA: 1085806.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.350s, learning 0.182s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0528
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 40.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.53s
                        Total time: 4412.94s
                               ETA: 1085212.5s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0553
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 40.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.60s
                        Total time: 4421.54s
                               ETA: 1084638.8s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.146s, learning 0.236s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0584
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 40.89
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.38s
                        Total time: 4429.92s
                               ETA: 1084014.0s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.247s, learning 0.208s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0594
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 40.19
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.46s
                        Total time: 4438.38s
                               ETA: 1083410.2s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.293s, learning 0.192s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0559
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 39.74
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.48s
                        Total time: 4446.86s
                               ETA: 1082816.3s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.005s, learning 0.176s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0515
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 39.16
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.18s
                        Total time: 4455.04s
                               ETA: 1082151.6s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.400s, learning 0.308s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0550
             Mean action noise std: 0.78
                       Mean reward: 0.20
               Mean episode length: 39.66
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.71s
                        Total time: 4463.75s
                               ETA: 1081617.8s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.163s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0555
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 40.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.37s
                        Total time: 4472.12s
                               ETA: 1081003.7s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.162s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0557
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 38.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.58s
                        Total time: 4480.70s
                               ETA: 1080445.1s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.612s, learning 0.194s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0579
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 39.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.81s
                        Total time: 4489.50s
                               ETA: 1079942.7s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.523s, learning 0.192s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0585
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 38.26
                  Mean reward/step: 0.00
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 8.72s
                        Total time: 4498.22s
                               ETA: 1079420.9s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0489
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 39.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.36s
                        Total time: 4506.58s
                               ETA: 1078816.6s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.333s, learning 0.249s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0561
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 38.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.58s
                        Total time: 4515.16s
                               ETA: 1078268.2s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.705s, learning 0.171s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 37.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.88s
                        Total time: 4524.04s
                               ETA: 1077792.2s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.207s, learning 0.160s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0532
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 37.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.37s
                        Total time: 4532.40s
                               ETA: 1077197.7s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.377s, learning 0.195s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0562
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 37.36
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.57s
                        Total time: 4540.98s
                               ETA: 1076654.5s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.636s, learning 0.179s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0582
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 39.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.81s
                        Total time: 4549.79s
                               ETA: 1076171.2s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.421s, learning 0.163s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0541
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 39.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.58s
                        Total time: 4558.37s
                               ETA: 1075635.7s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.092s, learning 0.161s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0528
             Mean action noise std: 0.77
                       Mean reward: 0.18
               Mean episode length: 35.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 8.25s
                        Total time: 4566.63s
                               ETA: 1075024.9s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.578s, learning 0.250s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0535
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 35.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.83s
                        Total time: 4575.45s
                               ETA: 1074552.1s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.163s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0541
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 37.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.53s
                        Total time: 4583.99s
                               ETA: 1074011.9s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.658s, learning 0.161s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0560
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 39.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 8.82s
                        Total time: 4592.81s
                               ETA: 1073541.4s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.461s, learning 0.194s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0550
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 35.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.65s
                        Total time: 4601.46s
                               ETA: 1073034.7s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.181s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0545
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 38.76
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.77s
                        Total time: 4610.23s
                               ETA: 1072556.8s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.203s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0536
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 36.45
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.37s
                        Total time: 4618.59s
                               ETA: 1071987.6s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.168s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0555
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 35.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.54s
                        Total time: 4627.14s
                               ETA: 1071462.3s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.142s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0554
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 34.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.31s
                        Total time: 4635.44s
                               ETA: 1070884.4s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.228s, learning 0.190s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0564
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 36.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.42s
                        Total time: 4643.86s
                               ETA: 1070335.1s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.760s, learning 0.168s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0586
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 36.55
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.93s
                        Total time: 4652.79s
                               ETA: 1069905.5s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.381s, learning 0.221s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 35.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.60s
                        Total time: 4661.39s
                               ETA: 1069403.2s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.823s, learning 0.276s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0578
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 36.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 9.10s
                        Total time: 4670.49s
                               ETA: 1069016.5s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.538s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 34.33
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.70s
                        Total time: 4679.19s
                               ETA: 1068540.9s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.454s, learning 0.215s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0555
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 35.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.67s
                        Total time: 4687.86s
                               ETA: 1068060.3s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.511s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0554
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 35.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.68s
                        Total time: 4696.54s
                               ETA: 1067584.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.297s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0535
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.46s
                        Total time: 4705.00s
                               ETA: 1067060.3s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0551
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 34.90
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.64s
                        Total time: 4713.64s
                               ETA: 1066579.1s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.565s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0557
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.73s
                        Total time: 4722.37s
                               ETA: 1066120.5s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.467s, learning 0.169s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.64s
                        Total time: 4731.01s
                               ETA: 1065643.0s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.552s, learning 0.188s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0528
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 34.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.74s
                        Total time: 4739.75s
                               ETA: 1065191.0s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.703s, learning 0.191s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0488
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.89s
                        Total time: 4748.64s
                               ETA: 1064775.6s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.355s, learning 0.169s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0545
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 35.63
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.52s
                        Total time: 4757.16s
                               ETA: 1064279.2s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.106s, learning 0.196s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0504
             Mean action noise std: 0.77
                       Mean reward: 0.18
               Mean episode length: 33.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.30s
                        Total time: 4765.47s
                               ETA: 1063735.5s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.531s, learning 0.200s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0483
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.73s
                        Total time: 4774.20s
                               ETA: 1063289.6s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.365s, learning 0.188s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0535
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 33.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.55s
                        Total time: 4782.75s
                               ETA: 1062806.3s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.466s, learning 0.172s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0542
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.64s
                        Total time: 4791.39s
                               ETA: 1062343.7s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.435s, learning 0.165s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0535
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 34.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.60s
                        Total time: 4799.99s
                               ETA: 1061874.7s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.593s, learning 0.197s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0525
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 34.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.79s
                        Total time: 4808.78s
                               ETA: 1061449.9s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.579s, learning 0.171s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0570
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 36.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 8.75s
                        Total time: 4817.53s
                               ETA: 1061017.9s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.161s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0549
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 35.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.57s
                        Total time: 4826.10s
                               ETA: 1060547.9s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.476s, learning 0.187s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0527
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 8.66s
                        Total time: 4834.76s
                               ETA: 1060100.8s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.219s, learning 0.172s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0562
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.39s
                        Total time: 4843.15s
                               ETA: 1059596.0s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.161s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0580
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.42s
                        Total time: 4851.57s
                               ETA: 1059100.5s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.550s, learning 0.249s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0566
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.80s
                        Total time: 4860.37s
                               ETA: 1058688.9s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.942s, learning 0.162s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0553
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 9.10s
                        Total time: 4869.48s
                               ETA: 1058345.4s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.435s, learning 0.171s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0537
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.61s
                        Total time: 4878.08s
                               ETA: 1057895.3s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.731s, learning 0.189s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0555
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 8.92s
                        Total time: 4887.00s
                               ETA: 1057515.1s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.209s, learning 0.184s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0552
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.39s
                        Total time: 4895.39s
                               ETA: 1057022.7s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.541s, learning 0.265s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0536
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 8.81s
                        Total time: 4904.20s
                               ETA: 1056621.4s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0525
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.67
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 8.63s
                        Total time: 4912.83s
                               ETA: 1056184.8s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.569s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0529
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 32.97
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.74s
                        Total time: 4921.57s
                               ETA: 1055772.2s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.707s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0545
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.87s
                        Total time: 4930.44s
                               ETA: 1055390.5s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.626s, learning 0.161s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0547
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 34.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 11.79s
                        Total time: 4942.23s
                               ETA: 1055632.9s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.528s, learning 0.188s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0542
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 16.72s
                        Total time: 4958.95s
                               ETA: 1056924.6s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.167s, learning 0.167s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0572
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 17.33s
                        Total time: 4976.28s
                               ETA: 1058342.1s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.582s, learning 0.214s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0546
             Mean action noise std: 0.77
                       Mean reward: 0.19
               Mean episode length: 32.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 16.80s
                        Total time: 4993.08s
                               ETA: 1059639.4s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.938s, learning 0.167s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0561
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 32.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 17.10s
                        Total time: 5010.18s
                               ETA: 1060996.5s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.959s, learning 0.158s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0564
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 17.12s
                        Total time: 5027.30s
                               ETA: 1062350.2s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.805s, learning 0.192s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0531
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 32.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 17.00s
                        Total time: 5044.29s
                               ETA: 1063672.9s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.200s, learning 0.173s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0548
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 16.37s
                        Total time: 5060.67s
                               ETA: 1064858.5s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.369s, learning 0.174s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0525
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 33.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 16.54s
                        Total time: 5077.21s
                               ETA: 1066074.8s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.589s, learning 0.193s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0543
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 16.78s
                        Total time: 5093.99s
                               ETA: 1067336.2s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.541s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0526
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 33.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 16.71s
                        Total time: 5110.70s
                               ETA: 1068576.1s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.760s, learning 0.192s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0554
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 32.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 16.95s
                        Total time: 5127.65s
                               ETA: 1069862.1s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.044s, learning 0.167s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0520
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 32.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 16.21s
                        Total time: 5143.86s
                               ETA: 1070988.4s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.918s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0517
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 33.04
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 17.08s
                        Total time: 5160.94s
                               ETA: 1072291.0s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.677s, learning 0.193s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0486
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 32.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 16.87s
                        Total time: 5177.81s
                               ETA: 1073544.2s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.718s, learning 0.193s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0541
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 32.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 16.91s
                        Total time: 5194.73s
                               ETA: 1074800.6s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.847s, learning 0.175s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 0.20
               Mean episode length: 32.79
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 17.02s
                        Total time: 5211.75s
                               ETA: 1076074.5s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.315s, learning 0.165s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0550
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 32.69
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 16.48s
                        Total time: 5228.23s
                               ETA: 1077231.3s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.521s, learning 0.169s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0533
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 33.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 16.69s
                        Total time: 5244.92s
                               ETA: 1078426.5s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.720s, learning 0.171s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0505
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.16
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 16.89s
                        Total time: 5261.81s
                               ETA: 1079658.0s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.719s, learning 0.174s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0538
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 32.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 16.89s
                        Total time: 5278.70s
                               ETA: 1080884.7s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 955 steps/s (collection: 16.900s, learning 0.239s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0505
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 33.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 17.14s
                        Total time: 5295.84s
                               ETA: 1082156.5s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.452s, learning 0.206s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0549
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 32.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 16.66s
                        Total time: 5312.50s
                               ETA: 1083324.9s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.524s, learning 0.174s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0540
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 16.70s
                        Total time: 5329.20s
                               ETA: 1084496.8s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.679s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0546
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 32.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 16.84s
                        Total time: 5346.04s
                               ETA: 1085693.0s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.401s, learning 0.195s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0532
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 16.60s
                        Total time: 5362.63s
                               ETA: 1086834.4s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.816s, learning 0.203s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0549
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 17.02s
                        Total time: 5379.65s
                               ETA: 1088056.7s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.951s, learning 0.186s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 34.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 16.14s
                        Total time: 5395.79s
                               ETA: 1089095.8s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.347s, learning 0.175s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0529
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 33.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 16.52s
                        Total time: 5412.31s
                               ETA: 1090208.4s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.245s, learning 0.189s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0525
             Mean action noise std: 0.77
                       Mean reward: 0.21
               Mean episode length: 32.59
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 16.43s
                        Total time: 5428.75s
                               ETA: 1091298.7s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.679s, learning 0.196s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0518
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 16.88s
                        Total time: 5445.62s
                               ETA: 1092473.1s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.545s, learning 0.184s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0513
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.65
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 16.73s
                        Total time: 5462.35s
                               ETA: 1093613.3s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.528s, learning 0.190s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0562
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 16.72s
                        Total time: 5479.07s
                               ETA: 1094746.9s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.583s, learning 0.198s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0567
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 16.78s
                        Total time: 5495.85s
                               ETA: 1095888.2s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.865s, learning 0.187s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0524
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 33.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 17.05s
                        Total time: 5512.90s
                               ETA: 1097078.7s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.511s, learning 0.199s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.22
               Mean episode length: 32.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 16.71s
                        Total time: 5529.61s
                               ETA: 1098196.5s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.598s, learning 0.194s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0482
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 32.88
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 16.79s
                        Total time: 5546.40s
                               ETA: 1099326.1s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.479s, learning 0.204s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0556
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 16.68s
                        Total time: 5563.09s
                               ETA: 1100429.7s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.680s, learning 0.219s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0540
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 10.90s
                        Total time: 5573.99s
                               ETA: 1100386.9s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.494s, learning 0.167s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0530
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 32.95
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 8.66s
                        Total time: 5582.65s
                               ETA: 1099903.3s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.165s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0530
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 32.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 8.55s
                        Total time: 5591.20s
                               ETA: 1099400.2s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.092s, learning 0.196s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0468
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 8.29s
                        Total time: 5599.49s
                               ETA: 1098847.1s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.242s, learning 0.191s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0533
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 32.75
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 8.43s
                        Total time: 5607.92s
                               ETA: 1098324.5s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.183s, learning 0.198s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0539
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 8.38s
                        Total time: 5616.30s
                               ETA: 1097794.0s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.606s, learning 0.180s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0463
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.12
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 8.79s
                        Total time: 5625.09s
                               ETA: 1097344.3s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.443s, learning 0.192s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0526
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.65
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 8.63s
                        Total time: 5633.72s
                               ETA: 1096867.1s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.424s, learning 0.191s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0511
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 8.61s
                        Total time: 5642.34s
                               ETA: 1096387.6s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.232s, learning 0.183s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0544
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 8.42s
                        Total time: 5650.75s
                               ETA: 1095871.4s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.733s, learning 0.179s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0465
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 33.46
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 8.91s
                        Total time: 5659.66s
                               ETA: 1095453.3s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.188s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0512
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 34.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 8.35s
                        Total time: 5668.02s
                               ETA: 1094928.8s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.186s, learning 0.206s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0457
             Mean action noise std: 0.77
                       Mean reward: 0.23
               Mean episode length: 32.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 8.39s
                        Total time: 5676.41s
                               ETA: 1094413.9s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.246s, learning 0.186s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0494
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 8.43s
                        Total time: 5684.84s
                               ETA: 1093908.6s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.248s, learning 0.182s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0493
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 8.43s
                        Total time: 5693.27s
                               ETA: 1093404.8s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.311s, learning 0.176s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0482
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 8.49s
                        Total time: 5701.76s
                               ETA: 1092913.8s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.275s, learning 0.168s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0514
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 8.44s
                        Total time: 5710.20s
                               ETA: 1092416.5s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.974s, learning 0.183s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0522
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 32.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 8.16s
                        Total time: 5718.36s
                               ETA: 1091866.2s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.903s, learning 0.176s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0503
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 8.08s
                        Total time: 5726.44s
                               ETA: 1091303.3s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.169s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0500
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 8.30s
                        Total time: 5734.74s
                               ETA: 1090784.2s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.377s, learning 0.199s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0454
             Mean action noise std: 0.77
                       Mean reward: 0.25
               Mean episode length: 33.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 8.58s
                        Total time: 5743.31s
                               ETA: 1090319.8s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.270s, learning 0.188s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0502
             Mean action noise std: 0.77
                       Mean reward: 0.24
               Mean episode length: 33.29
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.46s
                        Total time: 5751.77s
                               ETA: 1089834.6s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.253s, learning 0.179s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0450
             Mean action noise std: 0.76
                       Mean reward: 0.25
               Mean episode length: 33.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.43s
                        Total time: 5760.20s
                               ETA: 1089346.2s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.421s, learning 0.186s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0505
             Mean action noise std: 0.76
                       Mean reward: 0.24
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 8.61s
                        Total time: 5768.81s
                               ETA: 1088892.7s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.213s, learning 0.178s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0516
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.39s
                        Total time: 5777.20s
                               ETA: 1088400.2s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.516s, learning 0.294s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 0.25
               Mean episode length: 33.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.81s
                        Total time: 5786.01s
                               ETA: 1087988.5s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.532s, learning 0.166s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0516
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 8.70s
                        Total time: 5794.71s
                               ETA: 1087557.3s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.307s, learning 0.186s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0509
             Mean action noise std: 0.76
                       Mean reward: 0.25
               Mean episode length: 33.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.49s
                        Total time: 5803.20s
                               ETA: 1087089.3s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.226s, learning 0.190s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0504
             Mean action noise std: 0.76
                       Mean reward: 0.25
               Mean episode length: 33.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.42s
                        Total time: 5811.62s
                               ETA: 1086608.5s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.424s, learning 0.176s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0509
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.50
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 8.60s
                        Total time: 5820.22s
                               ETA: 1086163.9s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.116s, learning 0.183s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0525
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.30s
                        Total time: 5828.52s
                               ETA: 1085664.7s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.419s, learning 0.196s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 8.61s
                        Total time: 5837.13s
                               ETA: 1085226.2s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.145s, learning 0.193s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 8.34s
                        Total time: 5845.47s
                               ETA: 1084737.8s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0518
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.48s
                        Total time: 5853.95s
                               ETA: 1084277.3s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.539s, learning 0.170s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0525
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 8.71s
                        Total time: 5862.65s
                               ETA: 1083861.1s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.743s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0476
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.90
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.91s
                        Total time: 5871.57s
                               ETA: 1083483.6s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.410s, learning 0.184s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0500
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 8.59s
                        Total time: 5880.16s
                               ETA: 1083049.3s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.121s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0489
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 34.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 8.29s
                        Total time: 5888.45s
                               ETA: 1082559.7s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.552s, learning 0.196s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.75s
                        Total time: 5897.19s
                               ETA: 1082156.7s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.548s, learning 0.212s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0437
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.76s
                        Total time: 5905.95s
                               ETA: 1081757.3s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.160s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0537
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.35s
                        Total time: 5914.31s
                               ETA: 1081285.2s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.179s, learning 0.191s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0522
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.37s
                        Total time: 5922.68s
                               ETA: 1080817.8s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.398s, learning 0.170s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.04
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.57s
                        Total time: 5931.24s
                               ETA: 1080388.1s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.075s, learning 0.293s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0509
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.37s
                        Total time: 5939.61s
                               ETA: 1079923.7s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.174s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0508
             Mean action noise std: 0.76
                       Mean reward: 0.26
               Mean episode length: 33.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.42s
                        Total time: 5948.04s
                               ETA: 1079470.9s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.079s, learning 0.211s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0508
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.29s
                        Total time: 5956.33s
                               ETA: 1078995.5s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.090s, learning 0.166s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0532
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.26s
                        Total time: 5964.58s
                               ETA: 1078515.8s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.160s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0540
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.09
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.44s
                        Total time: 5973.02s
                               ETA: 1078070.5s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.580s, learning 0.164s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0528
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 32.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.74s
                        Total time: 5981.77s
                               ETA: 1077682.2s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.300s, learning 0.191s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0475
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 32.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 8.49s
                        Total time: 5990.26s
                               ETA: 1077249.5s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.178s, learning 0.192s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0540
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.88
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.37s
                        Total time: 5998.63s
                               ETA: 1076796.7s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.288s, learning 0.179s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0559
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 32.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 8.47s
                        Total time: 6007.09s
                               ETA: 1076362.9s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.175s, learning 0.293s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0523
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.47s
                        Total time: 6015.56s
                               ETA: 1075930.7s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.427s, learning 0.216s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0522
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 32.73
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.64s
                        Total time: 6024.20s
                               ETA: 1075531.2s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.171s, learning 0.193s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0492
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.36s
                        Total time: 6032.57s
                               ETA: 1075083.6s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.195s, learning 0.194s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0532
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 8.39s
                        Total time: 6040.96s
                               ETA: 1074641.9s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.335s, learning 0.251s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0541
             Mean action noise std: 0.76
                       Mean reward: 0.27
               Mean episode length: 33.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 8.59s
                        Total time: 6049.54s
                               ETA: 1074236.7s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.164s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0525
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.46
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.49s
                        Total time: 6058.03s
                               ETA: 1073815.4s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.486s, learning 0.165s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0529
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.78
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 8.65s
                        Total time: 6066.68s
                               ETA: 1073424.5s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.357s, learning 0.193s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 34.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.55s
                        Total time: 6075.23s
                               ETA: 1073017.2s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.148s, learning 0.188s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0543
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.34s
                        Total time: 6083.57s
                               ETA: 1072573.5s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0496
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 32.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.46s
                        Total time: 6092.02s
                               ETA: 1072153.1s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.944s, learning 0.191s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0547
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.16
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 9.13s
                        Total time: 6101.16s
                               ETA: 1071852.9s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.602s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0536
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.16
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.77s
                        Total time: 6109.93s
                               ETA: 1071489.5s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.647s, learning 0.167s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.30
               Mean episode length: 32.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.81s
                        Total time: 6118.74s
                               ETA: 1071135.3s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.410s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0540
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.59
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.57s
                        Total time: 6127.31s
                               ETA: 1070740.2s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.414s, learning 0.187s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0517
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 32.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.60s
                        Total time: 6135.92s
                               ETA: 1070351.4s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.511s, learning 0.188s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0523
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 32.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.70s
                        Total time: 6144.61s
                               ETA: 1069980.8s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.603s, learning 0.161s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0512
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.76s
                        Total time: 6153.38s
                               ETA: 1069622.9s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.279s, learning 0.166s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.45s
                        Total time: 6161.82s
                               ETA: 1069210.8s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.246s, learning 0.276s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.28
               Mean episode length: 33.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.52s
                        Total time: 6170.35s
                               ETA: 1068813.5s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.333s, learning 0.186s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0456
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 32.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.52s
                        Total time: 6178.86s
                               ETA: 1068417.1s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.181s, learning 0.188s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0544
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 32.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.37s
                        Total time: 6187.23s
                               ETA: 1067996.0s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.169s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0493
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 33.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.48s
                        Total time: 6195.71s
                               ETA: 1067595.7s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.486s, learning 0.161s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0509
             Mean action noise std: 0.76
                       Mean reward: 0.30
               Mean episode length: 33.04
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.65s
                        Total time: 6204.36s
                               ETA: 1067225.4s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.321s, learning 0.174s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 33.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.50s
                        Total time: 6212.86s
                               ETA: 1066830.3s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.992s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0488
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 33.59
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.16s
                        Total time: 6221.01s
                               ETA: 1066378.3s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.038s, learning 0.175s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0492
             Mean action noise std: 0.76
                       Mean reward: 0.29
               Mean episode length: 32.90
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.21s
                        Total time: 6229.23s
                               ETA: 1065937.4s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.219s, learning 0.162s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0501
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 32.79
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.38s
                        Total time: 6237.61s
                               ETA: 1065526.9s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.562s, learning 0.250s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0499
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 32.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.81s
                        Total time: 6246.42s
                               ETA: 1065191.3s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.598s, learning 0.263s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0515
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 32.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.86s
                        Total time: 6255.28s
                               ETA: 1064865.1s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.408s, learning 0.161s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.09
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.57s
                        Total time: 6263.85s
                               ETA: 1064490.3s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.330s, learning 0.183s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0530
             Mean action noise std: 0.76
                       Mean reward: 0.30
               Mean episode length: 32.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.51s
                        Total time: 6272.36s
                               ETA: 1064107.4s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.572s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0524
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 33.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.74s
                        Total time: 6281.10s
                               ETA: 1063764.0s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.161s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.57s
                        Total time: 6289.67s
                               ETA: 1063392.8s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.689s, learning 0.164s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0530
             Mean action noise std: 0.76
                       Mean reward: 0.31
               Mean episode length: 33.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.85s
                        Total time: 6298.52s
                               ETA: 1063070.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.213s, learning 0.160s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0540
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.37s
                        Total time: 6306.90s
                               ETA: 1062669.3s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.333s, learning 0.162s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0513
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.50s
                        Total time: 6315.39s
                               ETA: 1062289.4s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.116s, learning 0.160s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0550
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 12.28s
                        Total time: 6327.67s
                               ETA: 1062545.7s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.454s, learning 0.161s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0515
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 16.62s
                        Total time: 6344.28s
                               ETA: 1063528.6s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.049s, learning 0.162s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0518
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.12
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 16.21s
                        Total time: 6360.49s
                               ETA: 1064440.3s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.685s, learning 0.180s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0467
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 33.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 16.86s
                        Total time: 6377.36s
                               ETA: 1065458.2s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.344s, learning 0.201s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0515
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 16.54s
                        Total time: 6393.90s
                               ETA: 1066419.3s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.460s, learning 0.167s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0473
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 16.63s
                        Total time: 6410.53s
                               ETA: 1067390.8s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.119s, learning 0.171s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0513
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 16.29s
                        Total time: 6426.82s
                               ETA: 1068303.1s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.610s, learning 0.162s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0444
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 16.77s
                        Total time: 6443.59s
                               ETA: 1069292.2s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.576s, learning 0.165s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0500
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 33.36
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 16.74s
                        Total time: 6460.33s
                               ETA: 1070272.7s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.548s, learning 0.163s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0475
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 16.71s
                        Total time: 6477.04s
                               ETA: 1071245.0s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 950 steps/s (collection: 17.078s, learning 0.165s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0489
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 32.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 17.24s
                        Total time: 6494.29s
                               ETA: 1072301.8s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.212s, learning 0.213s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 16.42s
                        Total time: 6510.71s
                               ETA: 1073220.1s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.602s, learning 0.307s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 16.91s
                        Total time: 6527.62s
                               ETA: 1074215.1s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.381s, learning 0.166s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0512
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 16.55s
                        Total time: 6544.17s
                               ETA: 1075147.3s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.501s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0485
             Mean action noise std: 0.76
                       Mean reward: 0.32
               Mean episode length: 33.79
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 16.66s
                        Total time: 6560.83s
                               ETA: 1076095.5s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.732s, learning 0.216s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0534
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 16.95s
                        Total time: 6577.78s
                               ETA: 1077087.1s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.427s, learning 0.187s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0511
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 16.61s
                        Total time: 6594.39s
                               ETA: 1078020.6s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.581s, learning 0.171s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0561
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 16.75s
                        Total time: 6611.15s
                               ETA: 1078973.8s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 946 steps/s (collection: 17.142s, learning 0.175s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0504
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 17.32s
                        Total time: 6628.46s
                               ETA: 1080015.9s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.629s, learning 0.232s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0525
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 16.86s
                        Total time: 6645.33s
                               ETA: 1080980.2s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.777s, learning 0.186s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0512
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 33.69
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 16.96s
                        Total time: 6662.29s
                               ETA: 1081957.8s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.922s, learning 0.167s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0525
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 32.92
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 17.09s
                        Total time: 6679.38s
                               ETA: 1082952.7s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.760s, learning 0.170s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0491
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.50
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 16.93s
                        Total time: 6696.31s
                               ETA: 1083918.5s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.563s, learning 0.165s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0498
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 33.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 16.73s
                        Total time: 6713.04s
                               ETA: 1084848.5s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.619s, learning 0.200s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0496
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 16.82s
                        Total time: 6729.86s
                               ETA: 1085790.1s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.659s, learning 0.179s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0474
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.20
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 16.84s
                        Total time: 6746.69s
                               ETA: 1086731.6s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.603s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0532
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 16.77s
                        Total time: 6763.46s
                               ETA: 1087658.5s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 949 steps/s (collection: 17.046s, learning 0.213s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0501
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 17.26s
                        Total time: 6780.72s
                               ETA: 1088661.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.777s, learning 0.200s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0538
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.55
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 16.98s
                        Total time: 6797.70s
                               ETA: 1089616.0s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.515s, learning 0.172s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 16.69s
                        Total time: 6814.38s
                               ETA: 1090520.9s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.397s, learning 0.184s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0555
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 16.58s
                        Total time: 6830.97s
                               ETA: 1091405.9s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.710s, learning 0.166s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0533
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 16.88s
                        Total time: 6847.84s
                               ETA: 1092335.1s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.315s, learning 0.167s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0492
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 16.48s
                        Total time: 6864.32s
                               ETA: 1093198.5s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.561s, learning 0.202s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0462
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 16.76s
                        Total time: 6881.09s
                               ETA: 1094103.8s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.669s, learning 0.169s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0526
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 16.84s
                        Total time: 6897.93s
                               ETA: 1095018.1s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.137s, learning 0.187s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0459
             Mean action noise std: 0.76
                       Mean reward: 0.34
               Mean episode length: 33.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 17.32s
                        Total time: 6915.25s
                               ETA: 1096006.3s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.483s, learning 0.193s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0520
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 16.68s
                        Total time: 6931.93s
                               ETA: 1096889.0s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.724s, learning 0.169s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0494
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 16.89s
                        Total time: 6948.82s
                               ETA: 1097802.9s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.279s, learning 0.223s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0505
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 33.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 10.50s
                        Total time: 6959.32s
                               ETA: 1097705.9s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.797s, learning 0.166s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0555
             Mean action noise std: 0.76
                       Mean reward: 0.38
               Mean episode length: 33.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.96s
                        Total time: 6968.28s
                               ETA: 1097366.6s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.154s, learning 0.179s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0508
             Mean action noise std: 0.76
                       Mean reward: 0.38
               Mean episode length: 32.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.33s
                        Total time: 6976.62s
                               ETA: 1096929.5s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.644s, learning 0.190s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.83s
                        Total time: 6985.45s
                               ETA: 1096572.4s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.445s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0501
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 32.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 8.61s
                        Total time: 6994.06s
                               ETA: 1096181.3s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.565s, learning 0.264s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0514
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 32.97
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.83s
                        Total time: 7002.89s
                               ETA: 1095825.6s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.278s, learning 0.203s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0513
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.53
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.48s
                        Total time: 7011.37s
                               ETA: 1095416.5s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.574s, learning 0.253s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.66
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 8.83s
                        Total time: 7020.20s
                               ETA: 1095062.8s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.537s, learning 0.175s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0496
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 33.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.71s
                        Total time: 7028.91s
                               ETA: 1094692.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.076s, learning 0.175s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.40
               Mean episode length: 33.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 8.25s
                        Total time: 7037.16s
                               ETA: 1094251.1s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.344s, learning 0.169s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0495
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.51s
                        Total time: 7045.67s
                               ETA: 1093852.0s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.608s, learning 0.164s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0519
             Mean action noise std: 0.76
                       Mean reward: 0.38
               Mean episode length: 33.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.77s
                        Total time: 7054.45s
                               ETA: 1093494.3s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.255s, learning 0.187s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0503
             Mean action noise std: 0.76
                       Mean reward: 0.41
               Mean episode length: 33.72
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 8.44s
                        Total time: 7062.89s
                               ETA: 1093086.6s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.454s, learning 0.194s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0493
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.65s
                        Total time: 7071.54s
                               ETA: 1092711.9s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.169s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0514
             Mean action noise std: 0.76
                       Mean reward: 0.36
               Mean episode length: 33.81
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.38s
                        Total time: 7079.92s
                               ETA: 1092297.4s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.164s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0449
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.33
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.37s
                        Total time: 7088.29s
                               ETA: 1091882.2s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.322s, learning 0.172s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0538
             Mean action noise std: 0.76
                       Mean reward: 0.40
               Mean episode length: 33.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 8.49s
                        Total time: 7096.78s
                               ETA: 1091487.4s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.465s, learning 0.173s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0553
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.64s
                        Total time: 7105.42s
                               ETA: 1091115.8s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.671s, learning 0.188s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0534
             Mean action noise std: 0.76
                       Mean reward: 0.41
               Mean episode length: 33.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.86s
                        Total time: 7114.28s
                               ETA: 1090779.2s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.784s, learning 0.268s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0537
             Mean action noise std: 0.76
                       Mean reward: 0.43
               Mean episode length: 34.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 9.05s
                        Total time: 7123.33s
                               ETA: 1090473.3s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.312s, learning 0.193s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0505
             Mean action noise std: 0.76
                       Mean reward: 0.42
               Mean episode length: 34.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.50s
                        Total time: 7131.83s
                               ETA: 1090084.5s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.360s, learning 0.191s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0513
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.55s
                        Total time: 7140.39s
                               ETA: 1089704.1s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.118s, learning 0.213s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0496
             Mean action noise std: 0.76
                       Mean reward: 0.40
               Mean episode length: 34.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.33s
                        Total time: 7148.72s
                               ETA: 1089291.2s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.375s, learning 0.173s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0521
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.55s
                        Total time: 7157.26s
                               ETA: 1088912.6s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.490s, learning 0.196s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0463
             Mean action noise std: 0.76
                       Mean reward: 0.37
               Mean episode length: 34.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 8.69s
                        Total time: 7165.95s
                               ETA: 1088556.2s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.154s, learning 0.175s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0501
             Mean action noise std: 0.76
                       Mean reward: 0.41
               Mean episode length: 34.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.33s
                        Total time: 7174.28s
                               ETA: 1088146.7s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.386s, learning 0.166s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0483
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 34.73
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.55s
                        Total time: 7182.83s
                               ETA: 1087772.1s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.867s, learning 0.200s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0526
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 33.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 9.07s
                        Total time: 7191.90s
                               ETA: 1087476.5s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.584s, learning 0.211s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0475
             Mean action noise std: 0.76
                       Mean reward: 0.42
               Mean episode length: 33.95
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 8.80s
                        Total time: 7200.69s
                               ETA: 1087140.8s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.471s, learning 0.161s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0487
             Mean action noise std: 0.76
                       Mean reward: 0.44
               Mean episode length: 33.90
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.63s
                        Total time: 7209.33s
                               ETA: 1086781.4s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.835s, learning 0.221s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0488
             Mean action noise std: 0.76
                       Mean reward: 0.41
               Mean episode length: 33.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 9.06s
                        Total time: 7218.38s
                               ETA: 1086486.9s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.485s, learning 0.167s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0510
             Mean action noise std: 0.76
                       Mean reward: 0.40
               Mean episode length: 33.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 8.65s
                        Total time: 7227.04s
                               ETA: 1086132.6s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.162s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0508
             Mean action noise std: 0.76
                       Mean reward: 0.43
               Mean episode length: 33.76
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.46s
                        Total time: 7235.50s
                               ETA: 1085751.3s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.273s, learning 0.229s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0511
             Mean action noise std: 0.76
                       Mean reward: 0.44
               Mean episode length: 34.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.50s
                        Total time: 7244.00s
                               ETA: 1085376.6s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.712s, learning 0.159s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0471
             Mean action noise std: 0.76
                       Mean reward: 0.43
               Mean episode length: 33.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 8.87s
                        Total time: 7252.87s
                               ETA: 1085058.3s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.454s, learning 0.162s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0521
             Mean action noise std: 0.76
                       Mean reward: 0.42
               Mean episode length: 33.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.62s
                        Total time: 7261.49s
                               ETA: 1084702.7s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.160s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0489
             Mean action noise std: 0.76
                       Mean reward: 0.42
               Mean episode length: 33.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.58s
                        Total time: 7270.07s
                               ETA: 1084342.3s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.532s, learning 0.162s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0523
             Mean action noise std: 0.76
                       Mean reward: 0.44
               Mean episode length: 33.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.69s
                        Total time: 7278.76s
                               ETA: 1084000.4s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.724s, learning 0.166s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0497
             Mean action noise std: 0.76
                       Mean reward: 0.45
               Mean episode length: 34.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 8.89s
                        Total time: 7287.65s
                               ETA: 1083688.7s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.574s, learning 0.184s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0505
             Mean action noise std: 0.76
                       Mean reward: 0.46
               Mean episode length: 34.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 8.76s
                        Total time: 7296.41s
                               ETA: 1083358.3s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.334s, learning 0.188s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0484
             Mean action noise std: 0.75
                       Mean reward: 0.48
               Mean episode length: 34.26
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 8.52s
                        Total time: 7304.93s
                               ETA: 1082993.8s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.124s, learning 0.161s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0494
             Mean action noise std: 0.75
                       Mean reward: 0.44
               Mean episode length: 34.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.29s
                        Total time: 7313.21s
                               ETA: 1082595.5s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.189s, learning 0.159s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0470
             Mean action noise std: 0.75
                       Mean reward: 0.51
               Mean episode length: 34.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 8.35s
                        Total time: 7321.56s
                               ETA: 1082207.5s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.097s, learning 0.189s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0492
             Mean action noise std: 0.75
                       Mean reward: 0.44
               Mean episode length: 33.50
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.29s
                        Total time: 7329.85s
                               ETA: 1081811.6s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.363s, learning 0.184s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0469
             Mean action noise std: 0.75
                       Mean reward: 0.47
               Mean episode length: 33.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 8.55s
                        Total time: 7338.40s
                               ETA: 1081455.2s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.744s, learning 0.182s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0495
             Mean action noise std: 0.75
                       Mean reward: 0.48
               Mean episode length: 33.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 8.93s
                        Total time: 7347.32s
                               ETA: 1081155.5s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.518s, learning 0.194s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0419
             Mean action noise std: 0.75
                       Mean reward: 0.47
               Mean episode length: 33.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.71s
                        Total time: 7356.03s
                               ETA: 1080825.3s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.535s, learning 0.169s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0489
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 34.26
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.70s
                        Total time: 7364.74s
                               ETA: 1080494.9s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.173s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0457
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 33.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.75s
                        Total time: 7373.49s
                               ETA: 1080172.3s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.165s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0487
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 34.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.62s
                        Total time: 7382.11s
                               ETA: 1079831.9s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.498s, learning 0.198s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0462
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 33.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 8.70s
                        Total time: 7390.81s
                               ETA: 1079503.2s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.197s, learning 0.172s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0480
             Mean action noise std: 0.75
                       Mean reward: 0.51
               Mean episode length: 34.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 8.37s
                        Total time: 7399.17s
                               ETA: 1079127.7s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.343s, learning 0.176s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0472
             Mean action noise std: 0.75
                       Mean reward: 0.52
               Mean episode length: 34.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.52s
                        Total time: 7407.69s
                               ETA: 1078775.2s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.440s, learning 0.171s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0494
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 34.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.61s
                        Total time: 7416.30s
                               ETA: 1078437.1s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.369s, learning 0.194s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0459
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 34.09
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.56s
                        Total time: 7424.87s
                               ETA: 1078092.9s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.442s, learning 0.191s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0485
             Mean action noise std: 0.75
                       Mean reward: 0.47
               Mean episode length: 33.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 8.63s
                        Total time: 7433.50s
                               ETA: 1077759.8s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.073s, learning 0.176s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0490
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 34.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.25s
                        Total time: 7441.75s
                               ETA: 1077372.2s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.540s, learning 0.191s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0488
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 33.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.73s
                        Total time: 7450.48s
                               ETA: 1077055.4s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.551s, learning 0.177s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0465
             Mean action noise std: 0.75
                       Mean reward: 0.55
               Mean episode length: 34.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 8.73s
                        Total time: 7459.21s
                               ETA: 1076738.9s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.163s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0474
             Mean action noise std: 0.75
                       Mean reward: 0.52
               Mean episode length: 34.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.66s
                        Total time: 7467.87s
                               ETA: 1076413.7s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.167s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 34.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 8.52s
                        Total time: 7476.39s
                               ETA: 1076069.7s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.309s, learning 0.175s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0454
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 34.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 8.48s
                        Total time: 7484.88s
                               ETA: 1075720.9s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.564s, learning 0.195s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0476
             Mean action noise std: 0.75
                       Mean reward: 0.55
               Mean episode length: 34.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.76s
                        Total time: 7493.64s
                               ETA: 1075412.6s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.448s, learning 0.224s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0450
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 34.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 8.67s
                        Total time: 7502.31s
                               ETA: 1075092.5s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.476s, learning 0.167s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0478
             Mean action noise std: 0.75
                       Mean reward: 0.54
               Mean episode length: 34.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 8.64s
                        Total time: 7510.95s
                               ETA: 1074769.4s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.205s, learning 0.182s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 33.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.39s
                        Total time: 7519.34s
                               ETA: 1074410.6s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.640s, learning 0.158s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0488
             Mean action noise std: 0.75
                       Mean reward: 0.59
               Mean episode length: 34.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.80s
                        Total time: 7528.14s
                               ETA: 1074111.5s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.433s, learning 0.194s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0486
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 34.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.63s
                        Total time: 7536.76s
                               ETA: 1073788.7s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.662s, learning 0.167s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0439
             Mean action noise std: 0.75
                       Mean reward: 0.57
               Mean episode length: 34.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.83s
                        Total time: 7545.59s
                               ETA: 1073495.7s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.168s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 0.55
               Mean episode length: 34.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.55s
                        Total time: 7554.14s
                               ETA: 1073164.0s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.585s, learning 0.164s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0452
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 34.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 8.75s
                        Total time: 7562.89s
                               ETA: 1072861.3s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.578s, learning 0.188s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 34.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.77s
                        Total time: 7571.66s
                               ETA: 1072561.8s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.592s, learning 0.194s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0454
             Mean action noise std: 0.75
                       Mean reward: 0.57
               Mean episode length: 34.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 8.79s
                        Total time: 7580.45s
                               ETA: 1072265.9s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.162s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0427
             Mean action noise std: 0.75
                       Mean reward: 0.62
               Mean episode length: 34.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 8.28s
                        Total time: 7588.72s
                               ETA: 1071899.0s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.434s, learning 0.177s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 34.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 8.61s
                        Total time: 7597.33s
                               ETA: 1071580.2s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.502s, learning 0.197s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0447
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 35.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 8.70s
                        Total time: 7606.03s
                               ETA: 1071274.6s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.431s, learning 0.162s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0470
             Mean action noise std: 0.75
                       Mean reward: 0.57
               Mean episode length: 35.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.59s
                        Total time: 7614.63s
                               ETA: 1070955.0s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.299s, learning 0.241s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0450
             Mean action noise std: 0.75
                       Mean reward: 0.66
               Mean episode length: 34.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.54s
                        Total time: 7623.16s
                               ETA: 1070628.8s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.752s, learning 0.167s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0426
             Mean action noise std: 0.75
                       Mean reward: 0.61
               Mean episode length: 34.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.92s
                        Total time: 7632.08s
                               ETA: 1070356.7s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.536s, learning 0.187s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0468
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 35.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 8.72s
                        Total time: 7640.81s
                               ETA: 1070058.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.328s, learning 0.220s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0432
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 35.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.55s
                        Total time: 7649.36s
                               ETA: 1069735.4s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.575s, learning 0.168s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0477
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 35.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.74s
                        Total time: 7658.10s
                               ETA: 1069441.1s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.387s, learning 0.200s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0442
             Mean action noise std: 0.75
                       Mean reward: 0.65
               Mean episode length: 34.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 8.59s
                        Total time: 7666.69s
                               ETA: 1069125.9s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.275s, learning 0.282s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0463
             Mean action noise std: 0.75
                       Mean reward: 0.67
               Mean episode length: 35.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 8.56s
                        Total time: 7675.24s
                               ETA: 1068807.2s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.472s, learning 0.250s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 0.59
               Mean episode length: 34.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 8.72s
                        Total time: 7683.97s
                               ETA: 1068512.5s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.424s, learning 0.165s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0475
             Mean action noise std: 0.75
                       Mean reward: 0.68
               Mean episode length: 35.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 8.59s
                        Total time: 7692.55s
                               ETA: 1068199.9s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1167 steps/s (collection: 13.866s, learning 0.173s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0437
             Mean action noise std: 0.75
                       Mean reward: 0.66
               Mean episode length: 35.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 14.04s
                        Total time: 7706.59s
                               ETA: 1068643.9s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.521s, learning 0.213s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 0.68
               Mean episode length: 35.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 16.73s
                        Total time: 7723.33s
                               ETA: 1069460.0s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.400s, learning 0.199s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0472
             Mean action noise std: 0.75
                       Mean reward: 0.72
               Mean episode length: 35.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 16.60s
                        Total time: 7739.93s
                               ETA: 1070255.0s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.693s, learning 0.168s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0426
             Mean action noise std: 0.75
                       Mean reward: 0.72
               Mean episode length: 36.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 16.86s
                        Total time: 7756.79s
                               ETA: 1071083.8s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.447s, learning 0.165s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0452
             Mean action noise std: 0.75
                       Mean reward: 0.73
               Mean episode length: 35.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 16.61s
                        Total time: 7773.40s
                               ETA: 1071876.0s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.544s, learning 0.190s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0414
             Mean action noise std: 0.75
                       Mean reward: 0.81
               Mean episode length: 35.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 16.73s
                        Total time: 7790.13s
                               ETA: 1072682.8s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.601s, learning 0.248s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0471
             Mean action noise std: 0.75
                       Mean reward: 0.66
               Mean episode length: 34.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 16.85s
                        Total time: 7806.98s
                               ETA: 1073503.2s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.491s, learning 0.166s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 0.72
               Mean episode length: 35.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 16.66s
                        Total time: 7823.64s
                               ETA: 1074294.9s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.432s, learning 0.160s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0444
             Mean action noise std: 0.75
                       Mean reward: 0.73
               Mean episode length: 35.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 16.59s
                        Total time: 7840.23s
                               ETA: 1075075.4s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.763s, learning 0.172s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 0.82
               Mean episode length: 35.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 16.93s
                        Total time: 7857.17s
                               ETA: 1075900.7s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.333s, learning 0.193s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 0.73
               Mean episode length: 35.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 16.53s
                        Total time: 7873.69s
                               ETA: 1076667.6s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.775s, learning 0.196s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0457
             Mean action noise std: 0.75
                       Mean reward: 0.77
               Mean episode length: 36.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 16.97s
                        Total time: 7890.66s
                               ETA: 1077493.3s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.361s, learning 0.222s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0457
             Mean action noise std: 0.75
                       Mean reward: 0.79
               Mean episode length: 35.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 16.58s
                        Total time: 7907.25s
                               ETA: 1078263.8s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.623s, learning 0.193s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0454
             Mean action noise std: 0.75
                       Mean reward: 0.78
               Mean episode length: 35.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 16.82s
                        Total time: 7924.06s
                               ETA: 1079063.7s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.349s, learning 0.188s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0478
             Mean action noise std: 0.75
                       Mean reward: 0.77
               Mean episode length: 35.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 16.54s
                        Total time: 7940.60s
                               ETA: 1079823.4s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.315s, learning 0.164s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0446
             Mean action noise std: 0.75
                       Mean reward: 0.91
               Mean episode length: 36.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 16.48s
                        Total time: 7957.08s
                               ETA: 1080573.3s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.303s, learning 0.170s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0453
             Mean action noise std: 0.75
                       Mean reward: 0.82
               Mean episode length: 35.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 16.47s
                        Total time: 7973.55s
                               ETA: 1081320.2s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.304s, learning 0.163s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 0.81
               Mean episode length: 36.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 16.47s
                        Total time: 7990.02s
                               ETA: 1082064.3s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 946 steps/s (collection: 17.098s, learning 0.217s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0452
             Mean action noise std: 0.75
                       Mean reward: 1.00
               Mean episode length: 37.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 17.31s
                        Total time: 8007.33s
                               ETA: 1082920.8s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.582s, learning 0.165s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0389
             Mean action noise std: 0.75
                       Mean reward: 0.88
               Mean episode length: 36.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 16.75s
                        Total time: 8024.08s
                               ETA: 1083698.4s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.178s, learning 0.195s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0443
             Mean action noise std: 0.75
                       Mean reward: 0.93
               Mean episode length: 36.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 16.37s
                        Total time: 8040.45s
                               ETA: 1084423.3s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.494s, learning 0.216s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0459
             Mean action noise std: 0.75
                       Mean reward: 0.95
               Mean episode length: 37.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 16.71s
                        Total time: 8057.16s
                               ETA: 1085191.5s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.485s, learning 0.160s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0447
             Mean action noise std: 0.75
                       Mean reward: 0.89
               Mean episode length: 36.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 16.65s
                        Total time: 8073.81s
                               ETA: 1085949.0s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.439s, learning 0.213s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0429
             Mean action noise std: 0.75
                       Mean reward: 0.91
               Mean episode length: 36.59
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 16.65s
                        Total time: 8090.46s
                               ETA: 1086705.2s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.071s, learning 0.176s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 0.94
               Mean episode length: 37.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 16.25s
                        Total time: 8106.71s
                               ETA: 1087405.1s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.215s, learning 0.189s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 0.95
               Mean episode length: 37.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 16.40s
                        Total time: 8123.11s
                               ETA: 1088124.1s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.519s, learning 0.162s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 0.98
               Mean episode length: 36.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 16.68s
                        Total time: 8139.79s
                               ETA: 1088878.1s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.822s, learning 0.199s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0423
             Mean action noise std: 0.75
                       Mean reward: 0.93
               Mean episode length: 36.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 17.02s
                        Total time: 8156.81s
                               ETA: 1089675.5s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.722s, learning 0.188s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0410
             Mean action noise std: 0.75
                       Mean reward: 0.98
               Mean episode length: 37.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 16.91s
                        Total time: 8173.72s
                               ETA: 1090455.9s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.980s, learning 0.192s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 0.96
               Mean episode length: 37.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 16.17s
                        Total time: 8189.89s
                               ETA: 1091135.7s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.251s, learning 0.168s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 1.01
               Mean episode length: 37.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 16.42s
                        Total time: 8206.31s
                               ETA: 1091846.6s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.728s, learning 0.171s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 1.00
               Mean episode length: 37.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 16.90s
                        Total time: 8223.21s
                               ETA: 1092619.3s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.729s, learning 0.193s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0412
             Mean action noise std: 0.75
                       Mean reward: 1.00
               Mean episode length: 37.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 16.92s
                        Total time: 8240.13s
                               ETA: 1093393.1s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.177s, learning 0.183s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0431
             Mean action noise std: 0.75
                       Mean reward: 1.16
               Mean episode length: 37.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 16.36s
                        Total time: 8256.49s
                               ETA: 1094090.1s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.385s, learning 0.202s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0395
             Mean action noise std: 0.75
                       Mean reward: 1.01
               Mean episode length: 36.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 16.59s
                        Total time: 8273.08s
                               ETA: 1094815.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 951 steps/s (collection: 16.982s, learning 0.242s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0397
             Mean action noise std: 0.75
                       Mean reward: 1.06
               Mean episode length: 37.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 17.22s
                        Total time: 8290.30s
                               ETA: 1095622.8s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.623s, learning 0.208s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 1.17
               Mean episode length: 38.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 16.83s
                        Total time: 8307.14s
                               ETA: 1096376.3s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.467s, learning 0.201s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0404
             Mean action noise std: 0.75
                       Mean reward: 1.06
               Mean episode length: 36.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 16.67s
                        Total time: 8323.80s
                               ETA: 1097106.1s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.553s, learning 0.208s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 1.14
               Mean episode length: 37.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.76s
                        Total time: 8332.56s
                               ETA: 1096793.2s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.416s, learning 0.201s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0399
             Mean action noise std: 0.75
                       Mean reward: 1.10
               Mean episode length: 37.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.62s
                        Total time: 8341.18s
                               ETA: 1096462.1s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.349s, learning 0.167s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0413
             Mean action noise std: 0.75
                       Mean reward: 1.03
               Mean episode length: 37.96
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 8.52s
                        Total time: 8349.70s
                               ETA: 1096118.7s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.722s, learning 0.189s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 1.10
               Mean episode length: 37.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 8.91s
                        Total time: 8358.61s
                               ETA: 1095827.9s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.380s, learning 0.177s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0422
             Mean action noise std: 0.75
                       Mean reward: 0.98
               Mean episode length: 37.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.56s
                        Total time: 8367.17s
                               ETA: 1095491.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.347s, learning 0.172s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0411
             Mean action noise std: 0.75
                       Mean reward: 1.08
               Mean episode length: 37.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.52s
                        Total time: 8375.68s
                               ETA: 1095151.0s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.449s, learning 0.215s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0414
             Mean action noise std: 0.75
                       Mean reward: 1.15
               Mean episode length: 37.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.66s
                        Total time: 8384.35s
                               ETA: 1094830.3s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.159s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0409
             Mean action noise std: 0.75
                       Mean reward: 1.13
               Mean episode length: 38.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.67s
                        Total time: 8393.02s
                               ETA: 1094511.7s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.563s, learning 0.196s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0419
             Mean action noise std: 0.75
                       Mean reward: 1.56
               Mean episode length: 38.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.76s
                        Total time: 8401.78s
                               ETA: 1094205.1s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.548s, learning 0.158s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 38.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.71s
                        Total time: 8410.49s
                               ETA: 1093892.3s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.166s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0373
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 38.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.59s
                        Total time: 8419.07s
                               ETA: 1093565.0s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.164s, learning 0.164s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 39.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 8.33s
                        Total time: 8427.40s
                               ETA: 1093204.7s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.102s, learning 0.174s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0409
             Mean action noise std: 0.75
                       Mean reward: 1.11
               Mean episode length: 38.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.28s
                        Total time: 8435.68s
                               ETA: 1092838.6s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.561s, learning 0.168s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0404
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 38.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 8.73s
                        Total time: 8444.41s
                               ETA: 1092532.1s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.168s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0410
             Mean action noise std: 0.75
                       Mean reward: 1.17
               Mean episode length: 38.29
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.24s
                        Total time: 8452.64s
                               ETA: 1092162.8s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.311s, learning 0.182s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 38.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.49s
                        Total time: 8461.13s
                               ETA: 1091827.5s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.174s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0414
             Mean action noise std: 0.75
                       Mean reward: 1.17
               Mean episode length: 38.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.62s
                        Total time: 8469.76s
                               ETA: 1091509.7s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.115s, learning 0.174s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 39.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.29s
                        Total time: 8478.05s
                               ETA: 1091149.9s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.293s, learning 0.174s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0397
             Mean action noise std: 0.75
                       Mean reward: 1.14
               Mean episode length: 37.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 8.47s
                        Total time: 8486.51s
                               ETA: 1090813.8s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.455s, learning 0.161s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0427
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 39.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.62s
                        Total time: 8495.13s
                               ETA: 1090497.7s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.466s, learning 0.177s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0344
             Mean action noise std: 0.75
                       Mean reward: 1.45
               Mean episode length: 39.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.64s
                        Total time: 8503.77s
                               ETA: 1090185.9s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.452s, learning 0.188s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0306
             Mean action noise std: 0.75
                       Mean reward: 1.54
               Mean episode length: 39.02
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.64s
                        Total time: 8512.41s
                               ETA: 1089874.5s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1788 steps/s (collection: 8.967s, learning 0.192s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0441
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 38.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 9.16s
                        Total time: 8521.57s
                               ETA: 1089630.2s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.430s, learning 0.308s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 1.50
               Mean episode length: 39.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.74s
                        Total time: 8530.31s
                               ETA: 1089332.7s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.576s, learning 0.166s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0398
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 39.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.74s
                        Total time: 8539.05s
                               ETA: 1089036.5s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.019s, learning 0.159s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 1.28
               Mean episode length: 39.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.18s
                        Total time: 8547.23s
                               ETA: 1088669.2s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.452s, learning 0.190s)
               Value function loss: 46.4322
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 1.44
               Mean episode length: 39.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.64s
                        Total time: 8555.87s
                               ETA: 1088361.8s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.672s, learning 0.200s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0425
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 39.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.87s
                        Total time: 8564.74s
                               ETA: 1088084.4s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.168s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0409
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 39.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.64s
                        Total time: 8573.39s
                               ETA: 1087778.7s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.164s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0385
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 38.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.72s
                        Total time: 8582.10s
                               ETA: 1087482.9s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.540s, learning 0.267s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0378
             Mean action noise std: 0.75
                       Mean reward: 9.02
               Mean episode length: 39.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.81s
                        Total time: 8590.91s
                               ETA: 1087199.3s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.505s, learning 0.184s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 38.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.69s
                        Total time: 8599.60s
                               ETA: 1086901.6s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.837s, learning 0.195s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0365
             Mean action noise std: 0.75
                       Mean reward: 1.13
               Mean episode length: 37.85
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 9.03s
                        Total time: 8608.63s
                               ETA: 1086648.0s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.407s, learning 0.181s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 37.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.59s
                        Total time: 8617.22s
                               ETA: 1086339.0s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.511s, learning 0.173s)
               Value function loss: 3.7703
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 38.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.68s
                        Total time: 8625.90s
                               ETA: 1086042.8s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.330s, learning 0.225s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0384
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 38.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.56s
                        Total time: 8634.46s
                               ETA: 1085731.1s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.334s, learning 0.181s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0367
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 39.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.52s
                        Total time: 8642.97s
                               ETA: 1085415.3s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.611s, learning 0.162s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 1.44
               Mean episode length: 38.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.77s
                        Total time: 8651.75s
                               ETA: 1085132.4s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.645s, learning 0.157s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 38.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.80s
                        Total time: 8660.55s
                               ETA: 1084853.9s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.707s, learning 0.219s)
               Value function loss: 11.5789
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 1.19
               Mean episode length: 39.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.93s
                        Total time: 8669.47s
                               ETA: 1084591.7s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.591s, learning 0.163s)
               Value function loss: 0.1464
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 39.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.75s
                        Total time: 8678.23s
                               ETA: 1084308.5s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.164s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0385
             Mean action noise std: 0.75
                       Mean reward: 1.04
               Mean episode length: 38.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.45s
                        Total time: 8686.67s
                               ETA: 1083987.7s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.161s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0362
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 38.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.41s
                        Total time: 8695.09s
                               ETA: 1083663.6s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.429s, learning 0.167s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 39.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.60s
                        Total time: 8703.68s
                               ETA: 1083362.9s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.467s, learning 0.167s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.59
               Mean episode length: 39.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.63s
                        Total time: 8712.32s
                               ETA: 1083067.7s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.593s, learning 0.197s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 1.42
               Mean episode length: 39.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.79s
                        Total time: 8721.11s
                               ETA: 1082792.6s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.563s, learning 0.195s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0343
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 38.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.76s
                        Total time: 8729.87s
                               ETA: 1082514.2s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.302s, learning 0.177s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 39.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.48s
                        Total time: 8738.34s
                               ETA: 1082201.9s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.674s, learning 0.194s)
               Value function loss: 382.4336
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 1.15
               Mean episode length: 38.23
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.87s
                        Total time: 8747.21s
                               ETA: 1081938.5s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.619s, learning 0.183s)
               Value function loss: 331.8938
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 39.72
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 8.80s
                        Total time: 8756.01s
                               ETA: 1081667.6s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.529s, learning 0.166s)
               Value function loss: 80.4636
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 38.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.70s
                        Total time: 8764.71s
                               ETA: 1081384.1s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.701s, learning 0.222s)
               Value function loss: 80.0169
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 1.13
               Mean episode length: 38.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.92s
                        Total time: 8773.63s
                               ETA: 1081129.5s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.682s, learning 0.165s)
               Value function loss: 21.7193
                    Surrogate loss: 0.0248
             Mean action noise std: 0.75
                       Mean reward: 6.33
               Mean episode length: 39.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.85s
                        Total time: 8782.48s
                               ETA: 1080866.0s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.607s, learning 0.168s)
               Value function loss: 3.8052
                    Surrogate loss: -0.0285
             Mean action noise std: 0.75
                       Mean reward: 1.44
               Mean episode length: 39.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.77s
                        Total time: 8791.25s
                               ETA: 1080594.3s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.532s, learning 0.167s)
               Value function loss: 2.1265
                    Surrogate loss: 0.0229
             Mean action noise std: 0.75
                       Mean reward: 1.16
               Mean episode length: 38.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.70s
                        Total time: 8799.95s
                               ETA: 1080314.0s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.771s, learning 0.247s)
               Value function loss: 1.7071
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 39.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 9.02s
                        Total time: 8808.97s
                               ETA: 1080073.4s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.166s)
               Value function loss: 0.8805
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 1.43
               Mean episode length: 39.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 8.67s
                        Total time: 8817.64s
                               ETA: 1079790.2s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.562s, learning 0.188s)
               Value function loss: 0.5248
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 1.43
               Mean episode length: 39.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.75s
                        Total time: 8826.39s
                               ETA: 1079518.1s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.711s, learning 0.176s)
               Value function loss: 0.3178
                    Surrogate loss: -0.0254
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 39.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.89s
                        Total time: 8835.27s
                               ETA: 1079263.3s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.429s, learning 0.197s)
               Value function loss: 0.1950
                    Surrogate loss: -0.0273
             Mean action noise std: 0.75
                       Mean reward: 1.00
               Mean episode length: 38.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.63s
                        Total time: 8843.90s
                               ETA: 1078977.3s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.374s, learning 0.179s)
               Value function loss: 0.1425
                    Surrogate loss: -0.0391
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 39.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.55s
                        Total time: 8852.45s
                               ETA: 1078683.0s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.526s, learning 0.159s)
               Value function loss: 0.1211
                    Surrogate loss: -0.0426
             Mean action noise std: 0.75
                       Mean reward: 1.14
               Mean episode length: 38.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.69s
                        Total time: 8861.14s
                               ETA: 1078405.7s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.470s, learning 0.177s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 38.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.65s
                        Total time: 8869.78s
                               ETA: 1078124.2s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.335s, learning 0.174s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0453
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 38.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.51s
                        Total time: 8878.29s
                               ETA: 1077826.7s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.478s, learning 0.196s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 38.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.67s
                        Total time: 8886.97s
                               ETA: 1077550.0s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.870s, learning 0.169s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 39.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 9.04s
                        Total time: 8896.00s
                               ETA: 1077318.1s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.197s, learning 0.176s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0441
             Mean action noise std: 0.75
                       Mean reward: 1.46
               Mean episode length: 39.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.37s
                        Total time: 8904.38s
                               ETA: 1077006.1s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.563s, learning 0.159s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0491
             Mean action noise std: 0.75
                       Mean reward: 1.12
               Mean episode length: 38.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.72s
                        Total time: 8913.10s
                               ETA: 1076737.1s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.328s, learning 0.171s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 1.43
               Mean episode length: 39.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.50s
                        Total time: 8921.60s
                               ETA: 1076441.8s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.626s, learning 0.216s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0466
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 39.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.84s
                        Total time: 8930.44s
                               ETA: 1076188.6s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.463s, learning 0.172s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0465
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 38.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.64s
                        Total time: 8939.08s
                               ETA: 1075911.0s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.545s, learning 0.191s)
               Value function loss: 47.0602
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 39.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.74s
                        Total time: 8947.81s
                               ETA: 1075646.2s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.654s, learning 0.161s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0459
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 39.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.82s
                        Total time: 8956.63s
                               ETA: 1075391.6s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.769s, learning 0.160s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0447
             Mean action noise std: 0.75
                       Mean reward: 1.16
               Mean episode length: 39.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.93s
                        Total time: 8965.56s
                               ETA: 1075151.1s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.401s, learning 0.285s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0416
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 40.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 8.69s
                        Total time: 8974.24s
                               ETA: 1074882.1s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.656s, learning 0.215s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 41.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.87s
                        Total time: 8983.11s
                               ETA: 1074635.8s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1770 steps/s (collection: 9.043s, learning 0.209s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0470
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 40.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 9.25s
                        Total time: 8992.36s
                               ETA: 1074435.7s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.658s, learning 0.166s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0452
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 40.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 8.82s
                        Total time: 9001.19s
                               ETA: 1074185.0s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.162s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0461
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 39.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.61s
                        Total time: 9009.79s
                               ETA: 1073908.8s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.705s, learning 0.167s)
               Value function loss: 54.3312
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 1.53
               Mean episode length: 41.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.87s
                        Total time: 9018.66s
                               ETA: 1073665.0s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.360s, learning 0.203s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0470
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 41.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.56s
                        Total time: 9027.23s
                               ETA: 1073384.9s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.365s, learning 0.178s)
               Value function loss: 71.0550
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 40.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.54s
                        Total time: 9035.77s
                               ETA: 1073103.2s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.344s, learning 0.169s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0404
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 41.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.51s
                        Total time: 9044.28s
                               ETA: 1072818.5s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.747s, learning 0.271s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0436
             Mean action noise std: 0.75
                       Mean reward: 1.16
               Mean episode length: 40.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 9.02s
                        Total time: 9053.30s
                               ETA: 1072594.4s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.511s, learning 0.160s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0292
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 39.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.67s
                        Total time: 9061.97s
                               ETA: 1072329.8s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.488s, learning 0.194s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.01
               Mean episode length: 40.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.68s
                        Total time: 9070.65s
                               ETA: 1072067.1s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.393s, learning 0.164s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0460
             Mean action noise std: 0.75
                       Mean reward: 1.02
               Mean episode length: 39.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.56s
                        Total time: 9079.21s
                               ETA: 1071790.1s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.053s, learning 0.194s)
               Value function loss: 0.0481
                    Surrogate loss: -0.0431
             Mean action noise std: 0.75
                       Mean reward: 1.18
               Mean episode length: 40.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.25s
                        Total time: 9087.46s
                               ETA: 1071477.3s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.457s, learning 0.186s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 1.13
               Mean episode length: 40.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 8.64s
                        Total time: 9096.10s
                               ETA: 1071211.8s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.397s, learning 0.174s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0436
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 40.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.57s
                        Total time: 9104.67s
                               ETA: 1070938.5s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.481s, learning 0.182s)
               Value function loss: 0.0491
                    Surrogate loss: -0.0422
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 41.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.66s
                        Total time: 9113.34s
                               ETA: 1070676.7s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.161s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0437
             Mean action noise std: 0.75
                       Mean reward: 1.10
               Mean episode length: 41.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.39s
                        Total time: 9121.72s
                               ETA: 1070383.1s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.246s, learning 0.166s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0432
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 42.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 8.41s
                        Total time: 9130.14s
                               ETA: 1070093.1s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.568s, learning 0.222s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0441
             Mean action noise std: 0.75
                       Mean reward: 0.92
               Mean episode length: 41.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.79s
                        Total time: 9138.93s
                               ETA: 1069847.8s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.170s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0439
             Mean action noise std: 0.75
                       Mean reward: 0.99
               Mean episode length: 41.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.51s
                        Total time: 9147.44s
                               ETA: 1069570.4s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.620s, learning 0.186s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0431
             Mean action noise std: 0.75
                       Mean reward: 1.28
               Mean episode length: 41.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.81s
                        Total time: 9156.24s
                               ETA: 1069328.3s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.319s, learning 0.207s)
               Value function loss: 93.9980
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 1.07
               Mean episode length: 41.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.53s
                        Total time: 9164.77s
                               ETA: 1069054.1s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.459s, learning 0.191s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 0.87
               Mean episode length: 40.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.65s
                        Total time: 9173.42s
                               ETA: 1068794.8s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.440s, learning 0.295s)
               Value function loss: 3.9427
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 0.77
               Mean episode length: 41.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.73s
                        Total time: 9182.15s
                               ETA: 1068546.1s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.349s, learning 0.202s)
               Value function loss: 60.1035
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 0.81
               Mean episode length: 41.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.55s
                        Total time: 9190.70s
                               ETA: 1068276.5s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.139s, learning 0.172s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0509
             Mean action noise std: 0.75
                       Mean reward: 0.95
               Mean episode length: 41.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.31s
                        Total time: 9199.01s
                               ETA: 1067979.7s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.466s, learning 0.173s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0495
             Mean action noise std: 0.75
                       Mean reward: 0.76
               Mean episode length: 40.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.64s
                        Total time: 9207.65s
                               ETA: 1067721.6s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.337s, learning 0.189s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0507
             Mean action noise std: 0.75
                       Mean reward: 0.69
               Mean episode length: 41.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.53s
                        Total time: 9216.18s
                               ETA: 1067451.0s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.383s, learning 0.264s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 0.75
               Mean episode length: 41.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.65s
                        Total time: 9224.83s
                               ETA: 1067195.1s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.302s, learning 0.168s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0462
             Mean action noise std: 0.75
                       Mean reward: 1.02
               Mean episode length: 42.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.47s
                        Total time: 9233.30s
                               ETA: 1066919.3s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.663s, learning 0.278s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0457
             Mean action noise std: 0.75
                       Mean reward: 0.72
               Mean episode length: 41.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.94s
                        Total time: 9242.24s
                               ETA: 1066698.4s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.111s, learning 0.169s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0455
             Mean action noise std: 0.75
                       Mean reward: 0.77
               Mean episode length: 41.51
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.28s
                        Total time: 9250.52s
                               ETA: 1066401.8s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.030s, learning 0.255s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0447
             Mean action noise std: 0.75
                       Mean reward: 0.61
               Mean episode length: 40.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.28s
                        Total time: 9258.80s
                               ETA: 1066106.4s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.450s, learning 0.166s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0478
             Mean action noise std: 0.75
                       Mean reward: 0.72
               Mean episode length: 41.33
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.62s
                        Total time: 9267.42s
                               ETA: 1065849.9s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1211 steps/s (collection: 13.364s, learning 0.162s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0447
             Mean action noise std: 0.75
                       Mean reward: 0.73
               Mean episode length: 42.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 13.53s
                        Total time: 9280.94s
                               ETA: 1066157.9s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.265s, learning 0.177s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0407
             Mean action noise std: 0.75
                       Mean reward: 0.68
               Mean episode length: 41.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 16.44s
                        Total time: 9297.39s
                               ETA: 1066799.8s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.050s, learning 0.189s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 0.76
               Mean episode length: 42.30
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 16.24s
                        Total time: 9313.63s
                               ETA: 1067417.0s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.182s, learning 0.164s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 42.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 16.35s
                        Total time: 9329.97s
                               ETA: 1068044.7s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.661s, learning 0.169s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 0.63
               Mean episode length: 42.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 16.83s
                        Total time: 9346.80s
                               ETA: 1068726.6s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.213s, learning 0.163s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0410
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 41.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 16.38s
                        Total time: 9363.18s
                               ETA: 1069354.8s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.621s, learning 0.211s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0402
             Mean action noise std: 0.75
                       Mean reward: 0.62
               Mean episode length: 40.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 16.83s
                        Total time: 9380.01s
                               ETA: 1070033.7s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.278s, learning 0.166s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0410
             Mean action noise std: 0.75
                       Mean reward: 0.64
               Mean episode length: 41.63
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 16.44s
                        Total time: 9396.46s
                               ETA: 1070666.7s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.669s, learning 0.242s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0319
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 40.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 16.91s
                        Total time: 9413.37s
                               ETA: 1071351.5s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.365s, learning 0.172s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0418
             Mean action noise std: 0.75
                       Mean reward: 0.56
               Mean episode length: 42.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 16.54s
                        Total time: 9429.90s
                               ETA: 1071992.0s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.610s, learning 0.165s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0452
             Mean action noise std: 0.75
                       Mean reward: 0.48
               Mean episode length: 42.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 16.78s
                        Total time: 9446.68s
                               ETA: 1072658.1s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.842s, learning 0.167s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0426
             Mean action noise std: 0.75
                       Mean reward: 0.60
               Mean episode length: 41.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 17.01s
                        Total time: 9463.69s
                               ETA: 1073349.0s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.499s, learning 0.322s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0451
             Mean action noise std: 0.75
                       Mean reward: 0.63
               Mean episode length: 41.95
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 16.82s
                        Total time: 9480.51s
                               ETA: 1074017.1s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.466s, learning 0.170s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0448
             Mean action noise std: 0.75
                       Mean reward: 0.57
               Mean episode length: 42.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 16.64s
                        Total time: 9497.14s
                               ETA: 1074662.6s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.295s, learning 0.315s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0453
             Mean action noise std: 0.75
                       Mean reward: 0.52
               Mean episode length: 40.92
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 16.61s
                        Total time: 9513.75s
                               ETA: 1075303.8s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.565s, learning 0.260s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0387
             Mean action noise std: 0.75
                       Mean reward: 0.56
               Mean episode length: 41.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 16.83s
                        Total time: 9530.58s
                               ETA: 1075967.7s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.446s, learning 0.301s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0289
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 40.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 16.75s
                        Total time: 9547.33s
                               ETA: 1076621.3s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.330s, learning 0.193s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 0.65
               Mean episode length: 41.92
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 16.52s
                        Total time: 9563.85s
                               ETA: 1077248.2s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.964s, learning 0.206s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0411
             Mean action noise std: 0.75
                       Mean reward: 0.55
               Mean episode length: 42.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 17.17s
                        Total time: 9581.02s
                               ETA: 1077946.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.576s, learning 0.158s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 0.45
               Mean episode length: 42.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 16.73s
                        Total time: 9597.75s
                               ETA: 1078593.9s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.593s, learning 0.221s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0444
             Mean action noise std: 0.75
                       Mean reward: 0.58
               Mean episode length: 41.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 16.81s
                        Total time: 9614.57s
                               ETA: 1079249.0s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.523s, learning 0.207s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0436
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 41.60
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 16.73s
                        Total time: 9631.30s
                               ETA: 1079893.1s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.128s, learning 0.168s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 0.48
               Mean episode length: 43.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 16.30s
                        Total time: 9647.60s
                               ETA: 1080487.1s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.740s, learning 0.175s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 0.45
               Mean episode length: 41.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 16.91s
                        Total time: 9664.51s
                               ETA: 1081149.0s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.356s, learning 0.166s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0408
             Mean action noise std: 0.75
                       Mean reward: 0.54
               Mean episode length: 41.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 16.52s
                        Total time: 9681.03s
                               ETA: 1081765.4s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.257s, learning 0.194s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0417
             Mean action noise std: 0.75
                       Mean reward: 0.44
               Mean episode length: 41.76
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 16.45s
                        Total time: 9697.48s
                               ETA: 1082372.5s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.955s, learning 0.171s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0432
             Mean action noise std: 0.75
                       Mean reward: 0.54
               Mean episode length: 41.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 16.13s
                        Total time: 9713.61s
                               ETA: 1082941.9s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.789s, learning 0.242s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 0.56
               Mean episode length: 41.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 17.03s
                        Total time: 9730.64s
                               ETA: 1083610.8s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.909s, learning 0.197s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 0.51
               Mean episode length: 40.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 17.11s
                        Total time: 9747.75s
                               ETA: 1084286.4s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.115s, learning 0.169s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0431
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 41.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 16.28s
                        Total time: 9764.03s
                               ETA: 1084869.2s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.847s, learning 0.191s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0470
             Mean action noise std: 0.75
                       Mean reward: 0.54
               Mean episode length: 40.95
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 17.04s
                        Total time: 9781.07s
                               ETA: 1085534.4s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.480s, learning 0.175s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0471
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 41.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 16.65s
                        Total time: 9797.72s
                               ETA: 1086155.5s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.227s, learning 0.172s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 41.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 16.40s
                        Total time: 9814.12s
                               ETA: 1086747.0s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.711s, learning 0.178s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 40.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 16.89s
                        Total time: 9831.01s
                               ETA: 1087391.1s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.607s, learning 0.173s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0387
             Mean action noise std: 0.75
                       Mean reward: 0.52
               Mean episode length: 40.88
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 16.78s
                        Total time: 9847.79s
                               ETA: 1088021.8s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 948 steps/s (collection: 16.968s, learning 0.314s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0417
             Mean action noise std: 0.75
                       Mean reward: 0.45
               Mean episode length: 39.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 17.28s
                        Total time: 9865.07s
                               ETA: 1088706.5s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.354s, learning 0.194s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0417
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 40.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 16.55s
                        Total time: 9881.62s
                               ETA: 1089308.7s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.401s, learning 0.186s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 0.47
               Mean episode length: 40.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 16.59s
                        Total time: 9898.21s
                               ETA: 1089913.9s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.221s, learning 0.160s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 0.40
               Mean episode length: 41.20
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 8.38s
                        Total time: 9906.59s
                               ETA: 1089615.1s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.163s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0415
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 41.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 8.70s
                        Total time: 9915.29s
                               ETA: 1089351.9s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.327s, learning 0.172s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 40.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 8.50s
                        Total time: 9923.79s
                               ETA: 1089067.3s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.167s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0417
             Mean action noise std: 0.75
                       Mean reward: 0.43
               Mean episode length: 41.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 8.19s
                        Total time: 9931.98s
                               ETA: 1088748.9s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.224s, learning 0.216s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0440
             Mean action noise std: 0.75
                       Mean reward: 0.56
               Mean episode length: 40.03
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 8.44s
                        Total time: 9940.42s
                               ETA: 1088459.0s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.165s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0459
             Mean action noise std: 0.74
                       Mean reward: 0.52
               Mean episode length: 40.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 8.46s
                        Total time: 9948.88s
                               ETA: 1088172.1s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.297s, learning 0.197s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0423
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 41.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 8.49s
                        Total time: 9957.37s
                               ETA: 1087889.5s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.022s, learning 0.174s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 40.29
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 8.20s
                        Total time: 9965.57s
                               ETA: 1087574.8s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.632s, learning 0.167s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 0.45
               Mean episode length: 41.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 8.80s
                        Total time: 9974.37s
                               ETA: 1087326.6s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.288s, learning 0.189s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 0.59
               Mean episode length: 39.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 8.48s
                        Total time: 9982.84s
                               ETA: 1087043.9s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.166s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0491
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 40.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 8.69s
                        Total time: 9991.53s
                               ETA: 1086784.6s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.516s, learning 0.161s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0446
             Mean action noise std: 0.74
                       Mean reward: 0.45
               Mean episode length: 41.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 8.68s
                        Total time: 10000.21s
                               ETA: 1086524.7s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.794s, learning 0.235s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0434
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 40.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 9.03s
                        Total time: 10009.24s
                               ETA: 1086303.7s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.521s, learning 0.167s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0424
             Mean action noise std: 0.74
                       Mean reward: 0.46
               Mean episode length: 41.12
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 8.69s
                        Total time: 10017.92s
                               ETA: 1086046.1s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.432s, learning 0.162s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 0.34
               Mean episode length: 40.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 8.59s
                        Total time: 10026.52s
                               ETA: 1085778.8s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.296s, learning 0.191s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 40.45
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 8.49s
                        Total time: 10035.01s
                               ETA: 1085500.5s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.520s, learning 0.210s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0371
             Mean action noise std: 0.74
                       Mean reward: 0.46
               Mean episode length: 40.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 8.73s
                        Total time: 10043.74s
                               ETA: 1085249.1s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.176s, learning 0.184s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 0.48
               Mean episode length: 41.26
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 8.36s
                        Total time: 10052.10s
                               ETA: 1084958.3s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.295s, learning 0.171s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 0.42
               Mean episode length: 40.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 8.47s
                        Total time: 10060.56s
                               ETA: 1084679.6s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.395s, learning 0.167s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 41.65
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 8.56s
                        Total time: 10069.12s
                               ETA: 1084411.8s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.376s, learning 0.178s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0350
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 41.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 8.55s
                        Total time: 10077.68s
                               ETA: 1084143.7s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.365s, learning 0.208s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 0.45
               Mean episode length: 41.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 8.57s
                        Total time: 10086.25s
                               ETA: 1083878.2s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.619s, learning 0.159s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0410
             Mean action noise std: 0.74
                       Mean reward: 0.46
               Mean episode length: 40.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 8.78s
                        Total time: 10095.03s
                               ETA: 1083635.3s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.546s, learning 0.173s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0355
             Mean action noise std: 0.74
                       Mean reward: 0.43
               Mean episode length: 40.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 8.72s
                        Total time: 10103.75s
                               ETA: 1083386.5s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.161s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 40.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 8.37s
                        Total time: 10112.12s
                               ETA: 1083100.5s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.656s, learning 0.215s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.44
               Mean episode length: 39.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 8.87s
                        Total time: 10120.99s
                               ETA: 1082869.0s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.161s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 39.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 8.67s
                        Total time: 10129.66s
                               ETA: 1082616.5s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.385s, learning 0.194s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0426
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 40.20
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 8.58s
                        Total time: 10138.23s
                               ETA: 1082354.8s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.413s, learning 0.171s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 0.42
               Mean episode length: 39.78
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 8.58s
                        Total time: 10146.82s
                               ETA: 1082094.3s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.627s, learning 0.179s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.51
               Mean episode length: 40.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 8.81s
                        Total time: 10155.63s
                               ETA: 1081858.0s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.491s, learning 0.167s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0431
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 40.21
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 8.66s
                        Total time: 10164.28s
                               ETA: 1081606.3s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.371s, learning 0.191s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0410
             Mean action noise std: 0.74
                       Mean reward: 0.49
               Mean episode length: 39.60
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 8.56s
                        Total time: 10172.84s
                               ETA: 1081344.9s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.690s, learning 0.164s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 0.45
               Mean episode length: 39.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 8.85s
                        Total time: 10181.70s
                               ETA: 1081115.2s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.236s, learning 0.169s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 39.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 8.40s
                        Total time: 10190.10s
                               ETA: 1080838.2s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.622s, learning 0.283s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0428
             Mean action noise std: 0.74
                       Mean reward: 0.59
               Mean episode length: 39.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 8.91s
                        Total time: 10199.01s
                               ETA: 1080615.0s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.524s, learning 0.165s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0398
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 39.16
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 8.69s
                        Total time: 10207.70s
                               ETA: 1080369.2s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.331s, learning 0.159s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0370
             Mean action noise std: 0.74
                       Mean reward: 0.43
               Mean episode length: 39.29
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 8.49s
                        Total time: 10216.19s
                               ETA: 1080102.9s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.535s, learning 0.239s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0378
             Mean action noise std: 0.74
                       Mean reward: 0.48
               Mean episode length: 39.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 8.77s
                        Total time: 10224.96s
                               ETA: 1079867.2s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.471s, learning 0.160s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 39.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 8.63s
                        Total time: 10233.59s
                               ETA: 1079616.8s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.737s, learning 0.159s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0409
             Mean action noise std: 0.74
                       Mean reward: 0.41
               Mean episode length: 38.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 8.90s
                        Total time: 10242.49s
                               ETA: 1079394.9s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.581s, learning 0.216s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0402
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 39.82
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 8.80s
                        Total time: 10251.29s
                               ETA: 1079163.0s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.159s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0404
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 39.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 8.53s
                        Total time: 10259.82s
                               ETA: 1078903.5s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.418s, learning 0.164s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0444
             Mean action noise std: 0.74
                       Mean reward: 0.63
               Mean episode length: 39.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 8.58s
                        Total time: 10268.40s
                               ETA: 1078650.1s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.406s, learning 0.163s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0429
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 39.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 8.57s
                        Total time: 10276.97s
                               ETA: 1078395.7s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.739s, learning 0.220s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.51
               Mean episode length: 39.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 8.96s
                        Total time: 10285.93s
                               ETA: 1078182.8s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.164s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0437
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 39.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 8.64s
                        Total time: 10294.57s
                               ETA: 1077936.9s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.330s, learning 0.262s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0378
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.76
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 8.59s
                        Total time: 10303.16s
                               ETA: 1077686.5s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.428s, learning 0.166s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 0.51
               Mean episode length: 38.48
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 8.59s
                        Total time: 10311.75s
                               ETA: 1077436.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.167s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0395
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 39.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 8.26s
                        Total time: 10320.01s
                               ETA: 1077152.8s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.092s, learning 0.156s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 39.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 8.25s
                        Total time: 10328.26s
                               ETA: 1076868.0s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.598s, learning 0.162s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 0.48
               Mean episode length: 39.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 8.76s
                        Total time: 10337.02s
                               ETA: 1076637.1s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.555s, learning 0.168s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 39.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 8.72s
                        Total time: 10345.74s
                               ETA: 1076402.9s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.725s, learning 0.191s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0423
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 39.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 8.92s
                        Total time: 10354.66s
                               ETA: 1076189.2s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.451s, learning 0.187s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0410
             Mean action noise std: 0.74
                       Mean reward: 0.61
               Mean episode length: 39.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 8.64s
                        Total time: 10363.30s
                               ETA: 1075947.0s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.672s, learning 0.159s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 39.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 8.83s
                        Total time: 10372.13s
                               ETA: 1075725.5s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.584s, learning 0.238s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 8.82s
                        Total time: 10380.95s
                               ETA: 1075503.4s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.635s, learning 0.177s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 38.75
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 8.81s
                        Total time: 10389.76s
                               ETA: 1075280.6s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.594s, learning 0.188s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0466
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 39.36
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 8.78s
                        Total time: 10398.54s
                               ETA: 1075055.3s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.347s, learning 0.196s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 38.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 8.54s
                        Total time: 10407.09s
                               ETA: 1074805.7s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.787s, learning 0.160s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 37.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 8.95s
                        Total time: 10416.03s
                               ETA: 1074598.4s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.420s, learning 0.166s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 8.59s
                        Total time: 10424.62s
                               ETA: 1074354.3s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.340s, learning 0.194s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 38.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 8.53s
                        Total time: 10433.16s
                               ETA: 1074105.3s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.312s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0447
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 38.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 8.48s
                        Total time: 10441.63s
                               ETA: 1073850.7s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.084s, learning 0.223s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 8.31s
                        Total time: 10449.94s
                               ETA: 1073579.4s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.712s, learning 0.198s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 38.49
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 8.91s
                        Total time: 10458.85s
                               ETA: 1073370.5s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.532s, learning 0.175s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 37.95
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 8.71s
                        Total time: 10467.56s
                               ETA: 1073141.2s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.162s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 37.35
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 8.57s
                        Total time: 10476.12s
                               ETA: 1072897.8s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.336s, learning 0.223s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 38.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 8.56s
                        Total time: 10484.68s
                               ETA: 1072654.3s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.597s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 38.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 8.76s
                        Total time: 10493.44s
                               ETA: 1072431.8s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.551s, learning 0.192s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 0.64
               Mean episode length: 37.69
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 8.74s
                        Total time: 10502.18s
                               ETA: 1072207.9s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.586s, learning 0.182s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0416
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 38.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.77s
                        Total time: 10510.95s
                               ETA: 1071987.1s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.442s, learning 0.162s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0404
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 38.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.60s
                        Total time: 10519.55s
                               ETA: 1071750.0s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.570s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 37.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.73s
                        Total time: 10528.29s
                               ETA: 1071526.6s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.165s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 38.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.48s
                        Total time: 10536.76s
                               ETA: 1071277.4s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.626s, learning 0.192s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0395
             Mean action noise std: 0.74
                       Mean reward: 0.46
               Mean episode length: 37.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 8.82s
                        Total time: 10545.58s
                               ETA: 1071063.4s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.397s, learning 0.170s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.57s
                        Total time: 10554.15s
                               ETA: 1070824.4s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.504s, learning 0.241s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0416
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 37.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.75s
                        Total time: 10562.89s
                               ETA: 1070604.0s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.208s, learning 0.220s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0410
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 37.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.43s
                        Total time: 10571.32s
                               ETA: 1070351.8s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.140s, learning 0.219s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 38.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.36s
                        Total time: 10579.68s
                               ETA: 1070093.2s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.981s, learning 0.168s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0436
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 37.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.15s
                        Total time: 10587.83s
                               ETA: 1069814.0s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.209s, learning 0.161s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 37.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.37s
                        Total time: 10596.20s
                               ETA: 1069557.5s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.161s, learning 0.231s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.39s
                        Total time: 10604.59s
                               ETA: 1069303.7s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.167s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0392
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.54s
                        Total time: 10613.14s
                               ETA: 1069065.9s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.505s, learning 0.188s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0336
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 38.53
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.69s
                        Total time: 10621.83s
                               ETA: 1068843.4s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.163s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 0.52
               Mean episode length: 38.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.64s
                        Total time: 10630.47s
                               ETA: 1068615.7s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.784s, learning 0.163s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0377
             Mean action noise std: 0.74
                       Mean reward: 0.67
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.95s
                        Total time: 10639.42s
                               ETA: 1068419.6s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.317s, learning 0.192s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 37.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 8.51s
                        Total time: 10647.92s
                               ETA: 1068179.9s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.299s, learning 0.172s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0402
             Mean action noise std: 0.74
                       Mean reward: 0.50
               Mean episode length: 38.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.47s
                        Total time: 10656.40s
                               ETA: 1067936.9s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.527s, learning 0.190s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0432
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 38.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.72s
                        Total time: 10665.11s
                               ETA: 1067719.0s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.515s, learning 0.168s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0411
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 37.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.68s
                        Total time: 10673.80s
                               ETA: 1067498.1s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.370s, learning 0.169s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0453
             Mean action noise std: 0.74
                       Mean reward: 0.51
               Mean episode length: 37.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.54s
                        Total time: 10682.33s
                               ETA: 1067263.3s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.413s, learning 0.170s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0443
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 38.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.58s
                        Total time: 10690.92s
                               ETA: 1067033.3s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.985s, learning 0.173s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 38.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.16s
                        Total time: 10699.08s
                               ETA: 1066761.4s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.515s, learning 0.217s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0371
             Mean action noise std: 0.74
                       Mean reward: 0.47
               Mean episode length: 37.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.73s
                        Total time: 10707.81s
                               ETA: 1066547.2s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.544s, learning 0.165s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0364
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 37.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.71s
                        Total time: 10716.52s
                               ETA: 1066331.0s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.162s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0428
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 8.49s
                        Total time: 10725.00s
                               ETA: 1066093.4s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.558s, learning 0.170s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 39.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.73s
                        Total time: 10733.73s
                               ETA: 1065880.0s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.589s, learning 0.166s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 38.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.76s
                        Total time: 10742.49s
                               ETA: 1065669.7s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.479s, learning 0.197s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0432
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 37.78
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 8.68s
                        Total time: 10751.16s
                               ETA: 1065452.0s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.640s, learning 0.270s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0428
             Mean action noise std: 0.74
                       Mean reward: 0.61
               Mean episode length: 38.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.91s
                        Total time: 10760.07s
                               ETA: 1065257.8s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.523s, learning 0.189s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0409
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 37.97
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 8.71s
                        Total time: 10768.78s
                               ETA: 1065044.5s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.603s, learning 0.230s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 0.64
               Mean episode length: 39.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.83s
                        Total time: 10777.62s
                               ETA: 1064843.5s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.167s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 0.53
               Mean episode length: 38.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.64s
                        Total time: 10786.25s
                               ETA: 1064623.7s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.175s, learning 0.193s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 0.49
               Mean episode length: 37.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.37s
                        Total time: 10794.62s
                               ETA: 1064377.7s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.169s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 37.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.37s
                        Total time: 10802.99s
                               ETA: 1064132.3s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.615s, learning 0.163s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0377
             Mean action noise std: 0.74
                       Mean reward: 0.61
               Mean episode length: 38.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.78s
                        Total time: 10811.77s
                               ETA: 1063927.6s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.449s, learning 0.191s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0425
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 37.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.64s
                        Total time: 10820.41s
                               ETA: 1063709.6s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.779s, learning 0.237s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0424
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 9.02s
                        Total time: 10829.42s
                               ETA: 1063529.0s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.590s, learning 0.170s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0420
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 38.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.76s
                        Total time: 10838.18s
                               ETA: 1063323.5s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.544s, learning 0.212s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0395
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.76s
                        Total time: 10846.94s
                               ETA: 1063118.3s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.162s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.59
               Mean episode length: 37.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 8.70s
                        Total time: 10855.64s
                               ETA: 1062907.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.164s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 37.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.71s
                        Total time: 10864.35s
                               ETA: 1062698.9s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.679s, learning 0.253s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 0.52
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.93s
                        Total time: 10873.28s
                               ETA: 1062512.0s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.923s, learning 0.168s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0395
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 37.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 12.09s
                        Total time: 10885.37s
                               ETA: 1062633.7s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.788s, learning 0.205s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 0.66
               Mean episode length: 37.36
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 16.99s
                        Total time: 10902.37s
                               ETA: 1063233.2s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.575s, learning 0.174s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0416
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 16.75s
                        Total time: 10919.12s
                               ETA: 1063807.8s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.237s, learning 0.187s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 37.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 16.42s
                        Total time: 10935.54s
                               ETA: 1064349.6s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.677s, learning 0.164s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 0.67
               Mean episode length: 38.55
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 16.84s
                        Total time: 10952.38s
                               ETA: 1064930.8s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.397s, learning 0.172s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 0.67
               Mean episode length: 39.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 16.57s
                        Total time: 10968.95s
                               ETA: 1065484.4s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.591s, learning 0.167s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0376
             Mean action noise std: 0.74
                       Mean reward: 0.71
               Mean episode length: 38.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 16.76s
                        Total time: 10985.71s
                               ETA: 1066055.3s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.849s, learning 0.162s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 0.64
               Mean episode length: 39.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 17.01s
                        Total time: 11002.72s
                               ETA: 1066649.5s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.818s, learning 0.163s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 0.66
               Mean episode length: 37.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 16.98s
                        Total time: 11019.70s
                               ETA: 1067239.7s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.791s, learning 0.158s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 38.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 16.95s
                        Total time: 11036.65s
                               ETA: 1067825.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.036s, learning 0.161s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 0.56
               Mean episode length: 39.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 17.20s
                        Total time: 11053.85s
                               ETA: 1068434.1s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.704s, learning 0.159s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 0.60
               Mean episode length: 39.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 16.86s
                        Total time: 11070.71s
                               ETA: 1069009.3s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.771s, learning 0.164s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0409
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 38.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 16.94s
                        Total time: 11087.64s
                               ETA: 1069590.3s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 930 steps/s (collection: 17.407s, learning 0.194s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 38.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 17.60s
                        Total time: 11105.25s
                               ETA: 1070234.2s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.848s, learning 0.214s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0409
             Mean action noise std: 0.74
                       Mean reward: 0.63
               Mean episode length: 38.74
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 17.06s
                        Total time: 11122.31s
                               ETA: 1070825.1s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.646s, learning 0.166s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 38.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 16.81s
                        Total time: 11139.12s
                               ETA: 1071390.6s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.193s, learning 0.195s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0365
             Mean action noise std: 0.74
                       Mean reward: 0.66
               Mean episode length: 38.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 16.39s
                        Total time: 11155.51s
                               ETA: 1071914.3s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.511s, learning 0.166s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 0.55
               Mean episode length: 38.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 16.68s
                        Total time: 11172.19s
                               ETA: 1072464.8s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.522s, learning 0.192s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0422
             Mean action noise std: 0.74
                       Mean reward: 0.54
               Mean episode length: 37.51
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 16.71s
                        Total time: 11188.90s
                               ETA: 1073017.7s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.642s, learning 0.166s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0391
             Mean action noise std: 0.74
                       Mean reward: 0.59
               Mean episode length: 38.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 16.81s
                        Total time: 11205.71s
                               ETA: 1073578.4s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.810s, learning 0.165s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0225
             Mean action noise std: 0.74
                       Mean reward: 0.66
               Mean episode length: 38.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 16.97s
                        Total time: 11222.68s
                               ETA: 1074154.0s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.573s, learning 0.171s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 0.78
               Mean episode length: 39.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 16.74s
                        Total time: 11239.43s
                               ETA: 1074706.3s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.318s, learning 0.167s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 0.58
               Mean episode length: 37.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 16.48s
                        Total time: 11255.91s
                               ETA: 1075232.9s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.423s, learning 0.172s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 16.60s
                        Total time: 11272.51s
                               ETA: 1075768.9s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.429s, learning 0.188s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0388
             Mean action noise std: 0.74
                       Mean reward: 0.70
               Mean episode length: 38.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 16.62s
                        Total time: 11289.12s
                               ETA: 1076305.8s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.042s, learning 0.160s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 0.69
               Mean episode length: 38.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 16.20s
                        Total time: 11305.32s
                               ETA: 1076802.3s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.025s, learning 0.164s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 0.72
               Mean episode length: 39.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 17.19s
                        Total time: 11322.51s
                               ETA: 1077391.7s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.469s, learning 0.186s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 0.69
               Mean episode length: 38.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 16.65s
                        Total time: 11339.17s
                               ETA: 1077929.1s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.685s, learning 0.225s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0411
             Mean action noise std: 0.74
                       Mean reward: 0.57
               Mean episode length: 38.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 16.91s
                        Total time: 11356.08s
                               ETA: 1078489.6s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.798s, learning 0.191s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0399
             Mean action noise std: 0.74
                       Mean reward: 0.66
               Mean episode length: 38.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 16.99s
                        Total time: 11373.07s
                               ETA: 1079056.5s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.513s, learning 0.161s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 39.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 16.67s
                        Total time: 11389.74s
                               ETA: 1079592.5s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.646s, learning 0.194s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 39.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 16.84s
                        Total time: 11406.58s
                               ETA: 1080143.2s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.046s, learning 0.168s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0391
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 37.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 17.21s
                        Total time: 11423.79s
                               ETA: 1080728.1s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.296s, learning 0.172s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 0.65
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 16.47s
                        Total time: 11440.26s
                               ETA: 1081241.3s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.278s, learning 0.161s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0425
             Mean action noise std: 0.74
                       Mean reward: 0.74
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 16.44s
                        Total time: 11456.70s
                               ETA: 1081750.9s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.823s, learning 0.180s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 0.68
               Mean episode length: 38.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 17.00s
                        Total time: 11473.70s
                               ETA: 1082312.6s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.512s, learning 0.166s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0436
             Mean action noise std: 0.74
                       Mean reward: 0.62
               Mean episode length: 38.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 16.68s
                        Total time: 11490.38s
                               ETA: 1082842.6s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.590s, learning 0.196s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0344
             Mean action noise std: 0.74
                       Mean reward: 0.69
               Mean episode length: 38.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 16.79s
                        Total time: 11507.17s
                               ETA: 1083381.7s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.845s, learning 0.195s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.78
               Mean episode length: 37.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 11.04s
                        Total time: 11518.21s
                               ETA: 1083379.3s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.163s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0404
             Mean action noise std: 0.74
                       Mean reward: 0.72
               Mean episode length: 38.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.69s
                        Total time: 11526.90s
                               ETA: 1083156.4s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.446s, learning 0.156s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0381
             Mean action noise std: 0.74
                       Mean reward: 0.70
               Mean episode length: 38.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.60s
                        Total time: 11535.50s
                               ETA: 1082925.4s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.799s, learning 0.172s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 0.82
               Mean episode length: 39.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.97s
                        Total time: 11544.47s
                               ETA: 1082729.3s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.946s, learning 0.164s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0430
             Mean action noise std: 0.74
                       Mean reward: 0.71
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.11s
                        Total time: 11552.58s
                               ETA: 1082453.0s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.484s, learning 0.192s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0432
             Mean action noise std: 0.74
                       Mean reward: 0.72
               Mean episode length: 38.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.68s
                        Total time: 11561.26s
                               ETA: 1082230.1s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.271s, learning 0.171s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0376
             Mean action noise std: 0.74
                       Mean reward: 0.73
               Mean episode length: 38.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.44s
                        Total time: 11569.70s
                               ETA: 1081985.8s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.168s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 0.73
               Mean episode length: 37.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.62s
                        Total time: 11578.32s
                               ETA: 1081758.8s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.441s, learning 0.162s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0392
             Mean action noise std: 0.74
                       Mean reward: 0.74
               Mean episode length: 38.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.60s
                        Total time: 11586.93s
                               ETA: 1081530.4s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.169s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 0.64
               Mean episode length: 37.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.66s
                        Total time: 11595.59s
                               ETA: 1081307.8s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.402s, learning 0.166s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0338
             Mean action noise std: 0.74
                       Mean reward: 0.68
               Mean episode length: 38.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.57s
                        Total time: 11604.16s
                               ETA: 1081076.9s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.517s, learning 0.191s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0360
             Mean action noise std: 0.74
                       Mean reward: 0.70
               Mean episode length: 38.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.71s
                        Total time: 11612.86s
                               ETA: 1080859.4s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.715s, learning 0.161s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0392
             Mean action noise std: 0.74
                       Mean reward: 0.69
               Mean episode length: 38.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.88s
                        Total time: 11621.74s
                               ETA: 1080658.1s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.294s, learning 0.167s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 0.72
               Mean episode length: 37.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 8.46s
                        Total time: 11630.20s
                               ETA: 1080418.5s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.416s, learning 0.228s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 0.81
               Mean episode length: 38.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 8.64s
                        Total time: 11638.85s
                               ETA: 1080196.3s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.128s, learning 0.177s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 0.71
               Mean episode length: 38.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 8.30s
                        Total time: 11647.15s
                               ETA: 1079943.1s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.400s, learning 0.262s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0349
             Mean action noise std: 0.74
                       Mean reward: 0.81
               Mean episode length: 38.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.66s
                        Total time: 11655.81s
                               ETA: 1079723.3s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.462s, learning 0.175s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0381
             Mean action noise std: 0.74
                       Mean reward: 0.75
               Mean episode length: 37.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.64s
                        Total time: 11664.45s
                               ETA: 1079501.8s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.427s, learning 0.207s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0376
             Mean action noise std: 0.74
                       Mean reward: 0.78
               Mean episode length: 37.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.63s
                        Total time: 11673.08s
                               ETA: 1079280.2s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.560s, learning 0.228s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 0.84
               Mean episode length: 38.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.79s
                        Total time: 11681.87s
                               ETA: 1079073.3s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.753s, learning 0.163s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.77
               Mean episode length: 38.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.92s
                        Total time: 11690.79s
                               ETA: 1078878.6s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.360s, learning 0.183s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 0.76
               Mean episode length: 37.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.54s
                        Total time: 11699.33s
                               ETA: 1078649.9s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.166s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 0.77
               Mean episode length: 38.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.38s
                        Total time: 11707.71s
                               ETA: 1078406.8s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.495s, learning 0.171s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 0.79
               Mean episode length: 37.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 8.67s
                        Total time: 11716.38s
                               ETA: 1078190.2s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.470s, learning 0.169s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 0.85
               Mean episode length: 39.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.64s
                        Total time: 11725.02s
                               ETA: 1077971.5s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.167s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 0.83
               Mean episode length: 38.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.46s
                        Total time: 11733.48s
                               ETA: 1077737.1s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.186s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 0.98
               Mean episode length: 39.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.80s
                        Total time: 11742.28s
                               ETA: 1077534.1s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.481s, learning 0.173s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 0.82
               Mean episode length: 37.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.65s
                        Total time: 11750.93s
                               ETA: 1077317.9s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.161s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 0.87
               Mean episode length: 38.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.35s
                        Total time: 11759.29s
                               ETA: 1077074.6s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.476s, learning 0.167s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 0.92
               Mean episode length: 38.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.64s
                        Total time: 11767.93s
                               ETA: 1076858.2s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.616s, learning 0.199s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 0.80
               Mean episode length: 37.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.81s
                        Total time: 11776.75s
                               ETA: 1076658.0s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.163s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 0.84
               Mean episode length: 38.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.61s
                        Total time: 11785.36s
                               ETA: 1076439.6s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.162s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 0.78
               Mean episode length: 37.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.62s
                        Total time: 11793.98s
                               ETA: 1076222.6s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.348s, learning 0.188s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 0.95
               Mean episode length: 39.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.54s
                        Total time: 11802.52s
                               ETA: 1075998.0s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.497s, learning 0.202s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 0.97
               Mean episode length: 40.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.70s
                        Total time: 11811.22s
                               ETA: 1075788.7s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.414s, learning 0.169s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 0.95
               Mean episode length: 38.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.58s
                        Total time: 11819.80s
                               ETA: 1075569.1s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.299s, learning 0.226s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0370
             Mean action noise std: 0.73
                       Mean reward: 0.92
               Mean episode length: 39.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.52s
                        Total time: 11828.32s
                               ETA: 1075344.7s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.485s, learning 0.156s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0345
             Mean action noise std: 0.73
                       Mean reward: 0.77
               Mean episode length: 38.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.64s
                        Total time: 11836.97s
                               ETA: 1075131.2s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.162s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 0.88
               Mean episode length: 39.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 8.19s
                        Total time: 11845.16s
                               ETA: 1074877.3s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.693s, learning 0.169s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 0.83
               Mean episode length: 38.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.86s
                        Total time: 11854.02s
                               ETA: 1074684.7s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.635s, learning 0.215s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 0.90
               Mean episode length: 38.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.85s
                        Total time: 11862.87s
                               ETA: 1074491.3s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.507s, learning 0.236s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.06
               Mean episode length: 39.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.74s
                        Total time: 11871.61s
                               ETA: 1074288.5s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.401s, learning 0.258s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 38.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.66s
                        Total time: 11880.27s
                               ETA: 1074078.5s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.604s, learning 0.166s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 0.96
               Mean episode length: 38.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 8.77s
                        Total time: 11889.04s
                               ETA: 1073878.9s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.926s, learning 0.216s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 9.14s
                        Total time: 11898.18s
                               ETA: 1073713.2s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.258s, learning 0.183s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 38.96
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.44s
                        Total time: 11906.62s
                               ETA: 1073484.6s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.500s, learning 0.192s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 0.87
               Mean episode length: 38.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 8.69s
                        Total time: 11915.31s
                               ETA: 1073279.0s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.702s, learning 0.169s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 0.94
               Mean episode length: 38.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.87s
                        Total time: 11924.19s
                               ETA: 1073089.9s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.485s, learning 0.158s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 39.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.64s
                        Total time: 11932.83s
                               ETA: 1072880.7s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.368s, learning 0.190s)
               Value function loss: 11.8404
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 0.96
               Mean episode length: 38.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 8.56s
                        Total time: 11941.39s
                               ETA: 1072664.2s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.474s, learning 0.186s)
               Value function loss: 0.0572
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.66s
                        Total time: 11950.05s
                               ETA: 1072457.2s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.285s, learning 0.162s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 38.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.45s
                        Total time: 11958.49s
                               ETA: 1072231.4s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.741s, learning 0.191s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.14
               Mean episode length: 39.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 8.93s
                        Total time: 11967.43s
                               ETA: 1072049.5s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.591s, learning 0.168s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 38.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.76s
                        Total time: 11976.19s
                               ETA: 1071852.5s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.568s, learning 0.192s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 8.76s
                        Total time: 11984.95s
                               ETA: 1071655.7s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.537s, learning 0.158s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 38.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 8.70s
                        Total time: 11993.64s
                               ETA: 1071453.6s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.202s, learning 0.160s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.17
               Mean episode length: 39.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.36s
                        Total time: 12002.00s
                               ETA: 1071222.1s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.442s, learning 0.208s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 0.97
               Mean episode length: 38.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 8.65s
                        Total time: 12010.65s
                               ETA: 1071016.7s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.815s, learning 0.194s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 39.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 9.01s
                        Total time: 12019.66s
                               ETA: 1070843.7s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.463s, learning 0.185s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.05
               Mean episode length: 38.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.65s
                        Total time: 12028.31s
                               ETA: 1070638.8s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.575s, learning 0.165s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 8.74s
                        Total time: 12037.05s
                               ETA: 1070442.3s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.370s, learning 0.234s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 39.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.60s
                        Total time: 12045.65s
                               ETA: 1070234.3s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.388s, learning 0.158s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.06
               Mean episode length: 39.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.55s
                        Total time: 12054.20s
                               ETA: 1070021.3s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.478s, learning 0.164s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.20
               Mean episode length: 39.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.64s
                        Total time: 12062.84s
                               ETA: 1069817.3s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.585s, learning 0.180s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.76s
                        Total time: 12071.61s
                               ETA: 1069624.5s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.694s, learning 0.171s)
               Value function loss: 3.8319
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 1.22
               Mean episode length: 40.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.87s
                        Total time: 12080.47s
                               ETA: 1069440.9s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.580s, learning 0.172s)
               Value function loss: 59.5712
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 3.58
               Mean episode length: 39.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.75s
                        Total time: 12089.22s
                               ETA: 1069247.6s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.410s, learning 0.173s)
               Value function loss: 16.7055
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 1.13
               Mean episode length: 39.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 8.58s
                        Total time: 12097.81s
                               ETA: 1069039.7s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.285s, learning 0.208s)
               Value function loss: 0.1738
                    Surrogate loss: -0.0432
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 39.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.49s
                        Total time: 12106.30s
                               ETA: 1068824.3s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.628s, learning 0.171s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.80s
                        Total time: 12115.10s
                               ETA: 1068636.2s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.442s, learning 0.167s)
               Value function loss: 0.0527
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.61s
                        Total time: 12123.71s
                               ETA: 1068431.6s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.625s, learning 0.160s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 1.11
               Mean episode length: 38.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.79s
                        Total time: 12132.50s
                               ETA: 1068243.0s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.783s, learning 0.196s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 40.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.98s
                        Total time: 12141.47s
                               ETA: 1068071.6s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.164s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 40.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 8.58s
                        Total time: 12150.05s
                               ETA: 1067865.2s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.696s, learning 0.174s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 39.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.87s
                        Total time: 12158.92s
                               ETA: 1067684.9s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.544s, learning 0.213s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 39.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 8.76s
                        Total time: 12167.68s
                               ETA: 1067495.0s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.553s, learning 0.224s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.13
               Mean episode length: 39.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.78s
                        Total time: 12176.45s
                               ETA: 1067307.2s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.509s, learning 0.188s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.70s
                        Total time: 12185.15s
                               ETA: 1067112.7s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.316s, learning 0.167s)
               Value function loss: 3.8599
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 40.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.48s
                        Total time: 12193.63s
                               ETA: 1066899.8s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.237s, learning 0.168s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 1.03
               Mean episode length: 39.85
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 8.41s
                        Total time: 12202.04s
                               ETA: 1066680.5s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.334s, learning 0.247s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 39.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.58s
                        Total time: 12210.62s
                               ETA: 1066476.9s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.266s, learning 0.161s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 39.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.43s
                        Total time: 12219.05s
                               ETA: 1066260.2s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.102s, learning 0.165s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 39.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.27s
                        Total time: 12227.31s
                               ETA: 1066029.9s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.508s, learning 0.163s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.13
               Mean episode length: 39.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.67s
                        Total time: 12235.99s
                               ETA: 1065835.2s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.333s, learning 0.221s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0428
             Mean action noise std: 0.73
                       Mean reward: 0.91
               Mean episode length: 39.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.55s
                        Total time: 12244.54s
                               ETA: 1065630.6s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.671s, learning 0.164s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 1.05
               Mean episode length: 38.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.83s
                        Total time: 12253.37s
                               ETA: 1065450.8s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.163s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 39.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.63s
                        Total time: 12262.01s
                               ETA: 1065253.8s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.703s, learning 0.205s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 0.92
               Mean episode length: 38.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 8.91s
                        Total time: 12270.91s
                               ETA: 1065080.9s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.425s, learning 0.177s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 39.28
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.60s
                        Total time: 12279.52s
                               ETA: 1064881.8s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.601s, learning 0.167s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 39.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.77s
                        Total time: 12288.28s
                               ETA: 1064697.5s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.656s, learning 0.189s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.03
               Mean episode length: 38.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.84s
                        Total time: 12297.13s
                               ETA: 1064520.0s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.634s, learning 0.158s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0412
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 39.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 8.79s
                        Total time: 12305.92s
                               ETA: 1064338.4s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.165s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 39.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 8.66s
                        Total time: 12314.59s
                               ETA: 1064146.1s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.161s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 1.03
               Mean episode length: 39.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.54s
                        Total time: 12323.13s
                               ETA: 1063943.5s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.531s, learning 0.195s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 38.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.73s
                        Total time: 12331.86s
                               ETA: 1063757.1s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.480s, learning 0.198s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 38.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.68s
                        Total time: 12340.53s
                               ETA: 1063566.8s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.345s, learning 0.171s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 0.97
               Mean episode length: 38.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.52s
                        Total time: 12349.05s
                               ETA: 1063363.0s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.335s, learning 0.163s)
               Value function loss: 16.4659
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 39.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.50s
                        Total time: 12357.55s
                               ETA: 1063157.8s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.522s, learning 0.168s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 40.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.69s
                        Total time: 12366.24s
                               ETA: 1062969.6s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.822s, learning 0.168s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 39.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.99s
                        Total time: 12375.23s
                               ETA: 1062807.4s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.780s, learning 0.185s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 38.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.96s
                        Total time: 12384.19s
                               ETA: 1062643.3s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.283s, learning 0.173s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.17
               Mean episode length: 40.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 8.46s
                        Total time: 12392.65s
                               ETA: 1062435.9s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.692s, learning 0.192s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 0.99
               Mean episode length: 39.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.88s
                        Total time: 12401.53s
                               ETA: 1062265.5s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.468s, learning 0.180s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 0.95
               Mean episode length: 38.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 8.65s
                        Total time: 12410.18s
                               ETA: 1062075.1s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.235s, learning 0.178s)
               Value function loss: 11.7624
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 1.00
               Mean episode length: 38.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.41s
                        Total time: 12418.59s
                               ETA: 1061865.0s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.146s, learning 0.174s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 39.61
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.32s
                        Total time: 12426.91s
                               ETA: 1061647.3s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.996s, learning 0.165s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.11
               Mean episode length: 39.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 8.16s
                        Total time: 12435.07s
                               ETA: 1061416.3s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.554s, learning 0.219s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0420
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 39.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.77s
                        Total time: 12443.85s
                               ETA: 1061237.9s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.926s, learning 0.231s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 39.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 9.16s
                        Total time: 12453.00s
                               ETA: 1061092.6s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.388s, learning 0.180s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 0.94
               Mean episode length: 38.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 8.57s
                        Total time: 12461.57s
                               ETA: 1060897.3s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.502s, learning 0.181s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 40.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 8.68s
                        Total time: 12470.26s
                               ETA: 1060712.2s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.476s, learning 0.169s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 9.65s
                        Total time: 12479.90s
                               ETA: 1060609.2s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.443s, learning 0.175s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 39.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 16.62s
                        Total time: 12496.52s
                               ETA: 1061098.3s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.543s, learning 0.186s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 38.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 16.73s
                        Total time: 12513.25s
                               ETA: 1061596.0s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.532s, learning 0.254s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.21
               Mean episode length: 39.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 16.79s
                        Total time: 12530.03s
                               ETA: 1062097.7s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.601s, learning 0.211s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 0.95
               Mean episode length: 38.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 16.81s
                        Total time: 12546.85s
                               ETA: 1062600.7s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.008s, learning 0.168s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 38.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 17.18s
                        Total time: 12564.02s
                               ETA: 1063133.6s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.365s, learning 0.171s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 39.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 16.54s
                        Total time: 12580.56s
                               ETA: 1063611.5s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.420s, learning 0.263s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 39.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 16.68s
                        Total time: 12597.24s
                               ETA: 1064100.9s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.450s, learning 0.198s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 0.95
               Mean episode length: 38.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 16.65s
                        Total time: 12613.89s
                               ETA: 1064586.5s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.535s, learning 0.166s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 39.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 16.70s
                        Total time: 12630.59s
                               ETA: 1065075.7s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.838s, learning 0.173s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 40.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 17.01s
                        Total time: 12647.60s
                               ETA: 1065590.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.906s, learning 0.180s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 40.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 17.09s
                        Total time: 12664.69s
                               ETA: 1066110.1s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.505s, learning 0.190s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 39.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 16.69s
                        Total time: 12681.38s
                               ETA: 1066596.1s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.184s, learning 0.184s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.08
               Mean episode length: 39.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 16.37s
                        Total time: 12697.75s
                               ETA: 1067053.8s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.699s, learning 0.165s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 40.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 16.86s
                        Total time: 12714.62s
                               ETA: 1067552.4s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.609s, learning 0.174s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.01
               Mean episode length: 38.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 16.78s
                        Total time: 12731.40s
                               ETA: 1068043.3s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.245s, learning 0.175s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 40.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 16.42s
                        Total time: 12747.82s
                               ETA: 1068502.9s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.729s, learning 0.164s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 40.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 16.89s
                        Total time: 12764.71s
                               ETA: 1069001.3s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.382s, learning 0.165s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.05
               Mean episode length: 40.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 16.55s
                        Total time: 12781.26s
                               ETA: 1069469.9s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.640s, learning 0.173s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 39.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 16.81s
                        Total time: 12798.07s
                               ETA: 1069960.0s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.501s, learning 0.164s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 0.94
               Mean episode length: 38.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 16.67s
                        Total time: 12814.74s
                               ETA: 1070436.8s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.649s, learning 0.195s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 39.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 16.84s
                        Total time: 12831.58s
                               ETA: 1070927.7s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.588s, learning 0.223s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.03
               Mean episode length: 39.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 16.81s
                        Total time: 12848.39s
                               ETA: 1071415.1s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 939 steps/s (collection: 17.044s, learning 0.390s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 1.26
               Mean episode length: 39.57
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 17.43s
                        Total time: 12865.83s
                               ETA: 1071953.4s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.616s, learning 0.163s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 16.78s
                        Total time: 12882.61s
                               ETA: 1072436.3s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.758s, learning 0.188s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.23
               Mean episode length: 40.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 16.95s
                        Total time: 12899.55s
                               ETA: 1072932.2s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.671s, learning 0.199s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 39.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 16.87s
                        Total time: 12916.42s
                               ETA: 1073421.0s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.302s, learning 0.167s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 1.12
               Mean episode length: 38.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 16.47s
                        Total time: 12932.89s
                               ETA: 1073875.7s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.622s, learning 0.213s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 40.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 16.83s
                        Total time: 12949.73s
                               ETA: 1074359.8s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 928 steps/s (collection: 17.478s, learning 0.175s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.00
               Mean episode length: 39.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 17.65s
                        Total time: 12967.38s
                               ETA: 1074910.9s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.633s, learning 0.189s)
               Value function loss: 13.7322
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 38.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 16.82s
                        Total time: 12984.20s
                               ETA: 1075392.3s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.516s, learning 0.158s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 39.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 16.67s
                        Total time: 13000.88s
                               ETA: 1075860.6s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.648s, learning 0.181s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0358
             Mean action noise std: 0.73
                       Mean reward: 1.10
               Mean episode length: 38.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 16.83s
                        Total time: 13017.70s
                               ETA: 1076340.9s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.236s, learning 0.170s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 0.99
               Mean episode length: 39.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 16.41s
                        Total time: 13034.11s
                               ETA: 1076785.4s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.313s, learning 0.168s)
               Value function loss: 3.8756
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 39.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 16.48s
                        Total time: 13050.59s
                               ETA: 1077235.3s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.345s, learning 0.172s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0360
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 39.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 16.52s
                        Total time: 13067.11s
                               ETA: 1077687.4s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.589s, learning 0.206s)
               Value function loss: 3.8334
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: 1.00
               Mean episode length: 39.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 16.80s
                        Total time: 13083.90s
                               ETA: 1078161.7s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.588s, learning 0.167s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 1.21
               Mean episode length: 39.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 16.76s
                        Total time: 13100.66s
                               ETA: 1078631.9s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.696s, learning 0.197s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 39.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 12.89s
                        Total time: 13113.55s
                               ETA: 1078783.5s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.313s, learning 0.178s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 39.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 8.49s
                        Total time: 13122.04s
                               ETA: 1078573.0s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.383s, learning 0.184s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.28
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.57s
                        Total time: 13130.61s
                               ETA: 1078369.1s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.163s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.36s
                        Total time: 13138.97s
                               ETA: 1078148.7s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.276s, learning 0.170s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 1.17
               Mean episode length: 39.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 8.45s
                        Total time: 13147.42s
                               ETA: 1077935.5s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.563s, learning 0.158s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 40.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.72s
                        Total time: 13156.14s
                               ETA: 1077745.3s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.482s, learning 0.168s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 0.86
               Mean episode length: 38.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.65s
                        Total time: 13164.79s
                               ETA: 1077549.5s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.302s, learning 0.232s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.05
               Mean episode length: 39.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.53s
                        Total time: 13173.32s
                               ETA: 1077344.6s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.486s, learning 0.169s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0366
             Mean action noise std: 0.73
                       Mean reward: 1.20
               Mean episode length: 39.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.65s
                        Total time: 13181.98s
                               ETA: 1077149.8s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.493s, learning 0.265s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 39.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 8.76s
                        Total time: 13190.74s
                               ETA: 1076963.7s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.431s, learning 0.225s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 39.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.66s
                        Total time: 13199.39s
                               ETA: 1076769.6s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.510s, learning 0.168s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.11
               Mean episode length: 38.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.68s
                        Total time: 13208.07s
                               ETA: 1076577.7s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.163s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.56s
                        Total time: 13216.63s
                               ETA: 1076376.3s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.252s, learning 0.173s)
               Value function loss: 118.6483
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 40.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.43s
                        Total time: 13225.06s
                               ETA: 1076164.4s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.171s)
               Value function loss: 13.7177
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 8.46s
                        Total time: 13233.51s
                               ETA: 1075955.3s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.860s, learning 0.205s)
               Value function loss: 0.2460
                    Surrogate loss: -0.0350
             Mean action noise std: 0.73
                       Mean reward: 1.12
               Mean episode length: 39.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 9.07s
                        Total time: 13242.58s
                               ETA: 1075796.0s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.264s, learning 0.180s)
               Value function loss: 13.7178
                    Surrogate loss: 0.0021
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 40.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 8.44s
                        Total time: 13251.02s
                               ETA: 1075586.6s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.048s, learning 0.166s)
               Value function loss: 0.1626
                    Surrogate loss: -0.0356
             Mean action noise std: 0.73
                       Mean reward: 3.87
               Mean episode length: 40.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.21s
                        Total time: 13259.23s
                               ETA: 1075358.7s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.348s, learning 0.181s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 41.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.53s
                        Total time: 13267.76s
                               ETA: 1075156.8s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.320s, learning 0.190s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0360
             Mean action noise std: 0.73
                       Mean reward: 1.26
               Mean episode length: 40.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.51s
                        Total time: 13276.27s
                               ETA: 1074953.7s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.631s, learning 0.177s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 40.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.81s
                        Total time: 13285.08s
                               ETA: 1074775.0s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.416s, learning 0.182s)
               Value function loss: 54.0525
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: 1.22
               Mean episode length: 40.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.60s
                        Total time: 13293.68s
                               ETA: 1074579.6s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.481s, learning 0.248s)
               Value function loss: 0.1554
                    Surrogate loss: -0.0340
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 39.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.73s
                        Total time: 13302.41s
                               ETA: 1074395.1s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.324s, learning 0.182s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 39.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.51s
                        Total time: 13310.91s
                               ETA: 1074193.0s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.231s, learning 0.161s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.39s
                        Total time: 13319.31s
                               ETA: 1073981.8s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.343s, learning 0.209s)
               Value function loss: 17.5273
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 1.17
               Mean episode length: 40.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.55s
                        Total time: 13327.86s
                               ETA: 1073784.0s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.475s, learning 0.180s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 41.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 8.66s
                        Total time: 13336.51s
                               ETA: 1073594.7s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.466s, learning 0.255s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.68
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.72s
                        Total time: 13345.24s
                               ETA: 1073411.2s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.522s, learning 0.304s)
               Value function loss: 11.6873
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 1.06
               Mean episode length: 40.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.83s
                        Total time: 13354.06s
                               ETA: 1073236.3s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.174s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.20
               Mean episode length: 40.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.57s
                        Total time: 13362.63s
                               ETA: 1073041.0s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.745s, learning 0.184s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.04
               Mean episode length: 40.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 8.93s
                        Total time: 13371.56s
                               ETA: 1072874.9s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.161s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 40.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.69s
                        Total time: 13380.25s
                               ETA: 1072690.0s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.802s, learning 0.202s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 41.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 9.00s
                        Total time: 13389.26s
                               ETA: 1072530.5s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.149s, learning 0.262s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0347
             Mean action noise std: 0.73
                       Mean reward: 1.12
               Mean episode length: 40.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.41s
                        Total time: 13397.67s
                               ETA: 1072323.7s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.170s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.18
               Mean episode length: 41.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.43s
                        Total time: 13406.10s
                               ETA: 1072119.1s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.352s, learning 0.190s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 1.07
               Mean episode length: 39.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.54s
                        Total time: 13414.64s
                               ETA: 1071923.4s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.423s, learning 0.174s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.27
               Mean episode length: 41.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.60s
                        Total time: 13423.24s
                               ETA: 1071732.5s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.399s, learning 0.160s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 41.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.56s
                        Total time: 13431.80s
                               ETA: 1071538.8s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.526s, learning 0.163s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 40.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.69s
                        Total time: 13440.49s
                               ETA: 1071355.7s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.176s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.06
               Mean episode length: 40.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.31s
                        Total time: 13448.80s
                               ETA: 1071142.9s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.222s, learning 0.189s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 41.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.41s
                        Total time: 13457.21s
                               ETA: 1070938.3s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.354s, learning 0.205s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 40.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.56s
                        Total time: 13465.77s
                               ETA: 1070745.8s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.357s, learning 0.178s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0430
             Mean action noise std: 0.73
                       Mean reward: 1.19
               Mean episode length: 40.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 8.54s
                        Total time: 13474.31s
                               ETA: 1070551.7s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.483s, learning 0.230s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 1.09
               Mean episode length: 40.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.71s
                        Total time: 13483.02s
                               ETA: 1070372.0s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.794s, learning 0.161s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0420
             Mean action noise std: 0.73
                       Mean reward: 1.03
               Mean episode length: 40.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.96s
                        Total time: 13491.98s
                               ETA: 1070211.8s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.691s, learning 0.177s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.13
               Mean episode length: 40.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.87s
                        Total time: 13500.84s
                               ETA: 1070044.8s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.425s, learning 0.164s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.12
               Mean episode length: 39.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.59s
                        Total time: 13509.43s
                               ETA: 1069856.1s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.389s, learning 0.211s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0422
             Mean action noise std: 0.73
                       Mean reward: 0.97
               Mean episode length: 40.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.60s
                        Total time: 13518.03s
                               ETA: 1069668.5s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.193s, learning 0.173s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 1.06
               Mean episode length: 40.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.37s
                        Total time: 13526.40s
                               ETA: 1069462.8s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.520s, learning 0.187s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 40.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.71s
                        Total time: 13535.11s
                               ETA: 1069284.2s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.250s, learning 0.222s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 1.20
               Mean episode length: 40.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.47s
                        Total time: 13543.58s
                               ETA: 1069087.4s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.684s, learning 0.179s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0412
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 41.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.86s
                        Total time: 13552.44s
                               ETA: 1068921.7s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.170s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.48s
                        Total time: 13560.92s
                               ETA: 1068726.3s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.644s, learning 0.191s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.84s
                        Total time: 13569.76s
                               ETA: 1068559.0s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.229s, learning 0.159s)
               Value function loss: 3.9055
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 41.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.39s
                        Total time: 13578.15s
                               ETA: 1068356.6s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.167s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.42s
                        Total time: 13586.57s
                               ETA: 1068157.3s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.361s, learning 0.175s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.57
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.54s
                        Total time: 13595.10s
                               ETA: 1067967.3s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.751s, learning 0.162s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.23
               Mean episode length: 40.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.91s
                        Total time: 13604.02s
                               ETA: 1067807.2s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.584s, learning 0.165s)
               Value function loss: 9.6919
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.75s
                        Total time: 13612.77s
                               ETA: 1067634.4s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.601s, learning 0.172s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0448
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.77s
                        Total time: 13621.54s
                               ETA: 1067463.7s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.542s, learning 0.205s)
               Value function loss: 0.0499
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.21
               Mean episode length: 39.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.75s
                        Total time: 13630.29s
                               ETA: 1067291.3s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.253s, learning 0.171s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 40.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.42s
                        Total time: 13638.71s
                               ETA: 1067093.9s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.342s, learning 0.180s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.52s
                        Total time: 13647.23s
                               ETA: 1066904.4s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.248s, learning 0.195s)
               Value function loss: 0.0455
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 40.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.44s
                        Total time: 13655.67s
                               ETA: 1066709.1s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.513s, learning 0.167s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 40.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.68s
                        Total time: 13664.35s
                               ETA: 1066532.6s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.138s, learning 0.225s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 39.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.36s
                        Total time: 13672.72s
                               ETA: 1066331.5s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.616s, learning 0.190s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.46
               Mean episode length: 40.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.81s
                        Total time: 13681.52s
                               ETA: 1066165.3s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.169s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 1.27
               Mean episode length: 39.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.38s
                        Total time: 13689.91s
                               ETA: 1065966.4s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.167s)
               Value function loss: 0.0507
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 39.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 8.62s
                        Total time: 13698.53s
                               ETA: 1065786.4s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.191s, learning 0.164s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0426
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 40.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.36s
                        Total time: 13706.88s
                               ETA: 1065586.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.260s, learning 0.174s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.43s
                        Total time: 13715.32s
                               ETA: 1065391.9s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.509s, learning 0.282s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 39.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.79s
                        Total time: 13724.11s
                               ETA: 1065225.9s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.348s, learning 0.189s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.54s
                        Total time: 13732.64s
                               ETA: 1065040.5s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.699s, learning 0.193s)
               Value function loss: 9.5495
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 1.50
               Mean episode length: 40.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.89s
                        Total time: 13741.54s
                               ETA: 1064882.8s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.294s, learning 0.159s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.45s
                        Total time: 13749.99s
                               ETA: 1064691.4s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.344s, learning 0.197s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 39.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.54s
                        Total time: 13758.53s
                               ETA: 1064507.0s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.385s, learning 0.172s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0333
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 39.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.56s
                        Total time: 13767.09s
                               ETA: 1064324.2s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.501s, learning 0.171s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 40.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.67s
                        Total time: 13775.76s
                               ETA: 1064150.4s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.182s, learning 0.245s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 39.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.43s
                        Total time: 13784.19s
                               ETA: 1063958.1s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.429s, learning 0.192s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 40.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.62s
                        Total time: 13792.81s
                               ETA: 1063781.0s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.175s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 41.29
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.65s
                        Total time: 13801.46s
                               ETA: 1063606.8s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.384s, learning 0.192s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 40.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.58s
                        Total time: 13810.04s
                               ETA: 1063426.8s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.621s, learning 0.163s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0439
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.78s
                        Total time: 13818.82s
                               ETA: 1063263.0s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.673s, learning 0.284s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.96s
                        Total time: 13827.78s
                               ETA: 1063112.8s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.162s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.29
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.47s
                        Total time: 13836.25s
                               ETA: 1062925.5s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.396s, learning 0.228s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.62s
                        Total time: 13844.88s
                               ETA: 1062750.3s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.147s, learning 0.180s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.33s
                        Total time: 13853.20s
                               ETA: 1062552.4s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.585s, learning 0.192s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0414
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 40.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.78s
                        Total time: 13861.98s
                               ETA: 1062389.3s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.333s, learning 0.178s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 39.77
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.51s
                        Total time: 13870.49s
                               ETA: 1062206.1s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.166s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0423
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 39.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.37s
                        Total time: 13878.86s
                               ETA: 1062012.5s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.287s, learning 0.166s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 40.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.45s
                        Total time: 13887.31s
                               ETA: 1061825.4s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.158s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 40.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.73s
                        Total time: 13896.05s
                               ETA: 1061660.0s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.514s, learning 0.168s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 40.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.68s
                        Total time: 13904.73s
                               ETA: 1061491.0s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.554s, learning 0.177s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0438
             Mean action noise std: 0.73
                       Mean reward: 1.14
               Mean episode length: 40.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.73s
                        Total time: 13913.46s
                               ETA: 1061326.0s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.162s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 40.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 8.32s
                        Total time: 13921.78s
                               ETA: 1061130.1s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.394s, learning 0.282s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 40.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.68s
                        Total time: 13930.46s
                               ETA: 1060961.4s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.540s, learning 0.167s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0420
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 40.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.71s
                        Total time: 13939.17s
                               ETA: 1060795.2s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.377s, learning 0.161s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0414
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 39.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.54s
                        Total time: 13947.70s
                               ETA: 1060616.5s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.448s, learning 0.218s)
               Value function loss: 13.4873
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 8.67s
                        Total time: 13956.37s
                               ETA: 1060447.7s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.373s, learning 0.208s)
               Value function loss: 0.1525
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 3.90
               Mean episode length: 40.02
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 8.58s
                        Total time: 13964.95s
                               ETA: 1060272.8s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.447s, learning 0.190s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 40.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.64s
                        Total time: 13973.59s
                               ETA: 1060102.3s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.164s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 39.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.48s
                        Total time: 13982.06s
                               ETA: 1059919.8s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.164s)
               Value function loss: 0.0532
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.27
               Mean episode length: 41.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 8.37s
                        Total time: 13990.44s
                               ETA: 1059729.9s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.493s, learning 0.191s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 41.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.68s
                        Total time: 13999.12s
                               ETA: 1059563.8s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.403s, learning 0.165s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0370
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 40.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 8.57s
                        Total time: 14007.69s
                               ETA: 1059389.2s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.497s, learning 0.262s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.20
               Mean episode length: 41.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.76s
                        Total time: 14016.45s
                               ETA: 1059229.2s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.459s, learning 0.167s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.50
               Mean episode length: 40.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.63s
                        Total time: 14025.07s
                               ETA: 1059059.4s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.190s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 40.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.54s
                        Total time: 14033.61s
                               ETA: 1058883.1s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.553s, learning 0.187s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 41.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 8.74s
                        Total time: 14042.35s
                               ETA: 1058722.3s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.162s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 41.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.68s
                        Total time: 14051.03s
                               ETA: 1058557.6s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.516s, learning 0.187s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0350
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 41.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.70s
                        Total time: 14059.74s
                               ETA: 1058394.6s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.394s, learning 0.172s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 41.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.57s
                        Total time: 14068.30s
                               ETA: 1058221.5s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.186s, learning 0.170s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.36s
                        Total time: 14076.66s
                               ETA: 1058032.9s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.628s, learning 0.226s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 41.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 8.85s
                        Total time: 14085.51s
                               ETA: 1057882.0s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.576s, learning 0.172s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 41.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.75s
                        Total time: 14094.26s
                               ETA: 1057723.3s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.558s, learning 0.178s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 41.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.74s
                        Total time: 14103.00s
                               ETA: 1057563.9s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.687s, learning 0.170s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 41.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 15.86s
                        Total time: 14118.85s
                               ETA: 1057938.4s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.428s, learning 0.162s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 40.29
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 16.59s
                        Total time: 14135.44s
                               ETA: 1058367.2s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.322s, learning 0.203s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 40.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 16.53s
                        Total time: 14151.97s
                               ETA: 1058790.4s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.823s, learning 0.175s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 40.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 17.00s
                        Total time: 14168.97s
                               ETA: 1059248.3s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.454s, learning 0.164s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 41.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 16.62s
                        Total time: 14185.58s
                               ETA: 1059677.1s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.451s, learning 0.165s)
               Value function loss: 0.0446
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.24
               Mean episode length: 39.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 16.62s
                        Total time: 14202.20s
                               ETA: 1060105.0s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.530s, learning 0.215s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 16.74s
                        Total time: 14218.94s
                               ETA: 1060541.9s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.503s, learning 0.161s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 41.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 16.66s
                        Total time: 14235.61s
                               ETA: 1060972.2s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.458s, learning 0.173s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 16.63s
                        Total time: 14252.24s
                               ETA: 1061399.2s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.506s, learning 0.195s)
               Value function loss: 0.0466
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 41.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 16.70s
                        Total time: 14268.94s
                               ETA: 1061830.8s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.703s, learning 0.214s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.62
               Mean episode length: 41.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 16.92s
                        Total time: 14285.86s
                               ETA: 1062277.8s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.824s, learning 0.162s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0411
             Mean action noise std: 0.73
                       Mean reward: 1.54
               Mean episode length: 40.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 16.99s
                        Total time: 14302.84s
                               ETA: 1062729.2s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.761s, learning 0.197s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 40.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 16.96s
                        Total time: 14319.80s
                               ETA: 1063177.9s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.414s, learning 0.213s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 16.63s
                        Total time: 14336.43s
                               ETA: 1063601.3s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.341s, learning 0.179s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 1.66
               Mean episode length: 40.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 16.52s
                        Total time: 14352.95s
                               ETA: 1064016.0s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.167s, learning 0.281s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 1.65
               Mean episode length: 40.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 16.45s
                        Total time: 14369.40s
                               ETA: 1064424.8s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.743s, learning 0.195s)
               Value function loss: 16.4873
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 1.60
               Mean episode length: 41.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 16.94s
                        Total time: 14386.33s
                               ETA: 1064869.2s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.755s, learning 0.167s)
               Value function loss: 7.0130
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 40.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 16.92s
                        Total time: 14403.25s
                               ETA: 1065311.8s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.996s, learning 0.186s)
               Value function loss: 0.1606
                    Surrogate loss: -0.0350
             Mean action noise std: 0.73
                       Mean reward: 3.96
               Mean episode length: 40.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 17.18s
                        Total time: 14420.44s
                               ETA: 1065772.8s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.862s, learning 0.178s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 1.54
               Mean episode length: 41.36
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 17.04s
                        Total time: 14437.48s
                               ETA: 1066222.8s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.892s, learning 0.191s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 17.08s
                        Total time: 14454.56s
                               ETA: 1066675.1s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.829s, learning 0.189s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 40.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 17.02s
                        Total time: 14471.58s
                               ETA: 1067122.0s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.324s, learning 0.170s)
               Value function loss: 15.4492
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 39.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 16.49s
                        Total time: 14488.07s
                               ETA: 1067529.6s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.761s, learning 0.163s)
               Value function loss: 0.1223
                    Surrogate loss: -0.0411
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 39.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 16.92s
                        Total time: 14505.00s
                               ETA: 1067968.2s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.940s, learning 0.195s)
               Value function loss: 9.7234
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: 1.54
               Mean episode length: 39.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 16.13s
                        Total time: 14521.13s
                               ETA: 1068348.1s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.454s, learning 0.161s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 40.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 16.61s
                        Total time: 14537.75s
                               ETA: 1068762.6s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.434s, learning 0.167s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0427
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 39.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 16.60s
                        Total time: 14554.35s
                               ETA: 1069175.5s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.517s, learning 0.192s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0430
             Mean action noise std: 0.73
                       Mean reward: 1.44
               Mean episode length: 40.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 16.71s
                        Total time: 14571.05s
                               ETA: 1069595.7s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.668s, learning 0.172s)
               Value function loss: 0.0519
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 16.84s
                        Total time: 14587.89s
                               ETA: 1070024.8s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.860s, learning 0.224s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 39.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 17.08s
                        Total time: 14604.98s
                               ETA: 1070471.1s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.600s, learning 0.174s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 40.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 16.77s
                        Total time: 14621.75s
                               ETA: 1070894.1s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.256s, learning 0.198s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 40.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 16.45s
                        Total time: 14638.21s
                               ETA: 1071293.1s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.844s, learning 0.160s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 40.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 17.00s
                        Total time: 14655.21s
                               ETA: 1071731.6s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.725s, learning 0.286s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 39.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 17.01s
                        Total time: 14672.22s
                               ETA: 1072170.0s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.452s, learning 0.358s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 39.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 16.81s
                        Total time: 14689.03s
                               ETA: 1072593.0s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.364s, learning 0.205s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 16.57s
                        Total time: 14705.60s
                               ETA: 1072997.7s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.238s, learning 0.361s)
               Value function loss: 7.1816
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 1.54
               Mean episode length: 40.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 16.60s
                        Total time: 14722.20s
                               ETA: 1073404.0s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1091 steps/s (collection: 14.828s, learning 0.176s)
               Value function loss: 3.7697
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 15.00s
                        Total time: 14737.20s
                               ETA: 1073693.5s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.386s, learning 0.191s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0423
             Mean action noise std: 0.73
                       Mean reward: 4.20
               Mean episode length: 42.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 8.58s
                        Total time: 14745.78s
                               ETA: 1073514.6s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.327s, learning 0.170s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0370
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 39.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 8.50s
                        Total time: 14754.28s
                               ETA: 1073330.2s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.360s, learning 0.194s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.52
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 8.55s
                        Total time: 14762.83s
                               ETA: 1073150.1s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.520s, learning 0.184s)
               Value function loss: 9.6662
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: 1.59
               Mean episode length: 41.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 8.70s
                        Total time: 14771.54s
                               ETA: 1072981.3s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.267s, learning 0.164s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 40.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 8.43s
                        Total time: 14779.97s
                               ETA: 1072792.8s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.483s, learning 0.192s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.65
               Mean episode length: 41.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 8.67s
                        Total time: 14788.64s
                               ETA: 1072622.3s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.207s, learning 0.175s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.38s
                        Total time: 14797.02s
                               ETA: 1072430.8s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.527s, learning 0.194s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 40.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.72s
                        Total time: 14805.74s
                               ETA: 1072264.2s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.168s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.58s
                        Total time: 14814.32s
                               ETA: 1072087.6s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.164s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.28s
                        Total time: 14822.60s
                               ETA: 1071889.4s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.468s, learning 0.250s)
               Value function loss: 11.7354
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 1.55
               Mean episode length: 41.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.72s
                        Total time: 14831.32s
                               ETA: 1071723.2s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.439s, learning 0.162s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.58
               Mean episode length: 40.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.60s
                        Total time: 14839.92s
                               ETA: 1071548.9s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.622s, learning 0.190s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 39.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.81s
                        Total time: 14848.74s
                               ETA: 1071390.0s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.732s, learning 0.165s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 41.62
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.90s
                        Total time: 14857.63s
                               ETA: 1071237.5s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.212s, learning 0.226s)
               Value function loss: 11.6757
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 41.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.44s
                        Total time: 14866.07s
                               ETA: 1071052.1s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.628s, learning 0.168s)
               Value function loss: 0.1159
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 39.36
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 8.80s
                        Total time: 14874.87s
                               ETA: 1070892.8s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.417s, learning 0.162s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0436
             Mean action noise std: 0.73
                       Mean reward: 1.64
               Mean episode length: 41.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.58s
                        Total time: 14883.45s
                               ETA: 1070717.9s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.163s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0370
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 41.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.54s
                        Total time: 14891.99s
                               ETA: 1070540.9s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.820s, learning 0.163s)
               Value function loss: 14.9919
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 40.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.98s
                        Total time: 14900.97s
                               ETA: 1070395.6s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.842s, learning 0.178s)
               Value function loss: 0.1533
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 1.58
               Mean episode length: 40.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 9.02s
                        Total time: 14909.99s
                               ETA: 1070253.2s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.369s, learning 0.195s)
               Value function loss: 21.8583
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 41.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.56s
                        Total time: 14918.56s
                               ETA: 1070078.3s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.165s)
               Value function loss: 0.1861
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 4.18
               Mean episode length: 41.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.42s
                        Total time: 14926.98s
                               ETA: 1069893.5s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.172s, learning 0.231s)
               Value function loss: 0.1333
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 41.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.40s
                        Total time: 14935.38s
                               ETA: 1069707.5s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.164s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0348
             Mean action noise std: 0.73
                       Mean reward: 1.62
               Mean episode length: 41.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.45s
                        Total time: 14943.83s
                               ETA: 1069525.0s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.646s, learning 0.218s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 41.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.86s
                        Total time: 14952.69s
                               ETA: 1069372.5s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.465s, learning 0.166s)
               Value function loss: 0.0662
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.63
               Mean episode length: 41.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.63s
                        Total time: 14961.33s
                               ETA: 1069203.6s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.067s, learning 0.163s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0442
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 41.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.23s
                        Total time: 14969.56s
                               ETA: 1069006.2s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.253s, learning 0.162s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.53
               Mean episode length: 41.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.41s
                        Total time: 14977.97s
                               ETA: 1068822.3s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.154s, learning 0.173s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 1.63
               Mean episode length: 40.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.33s
                        Total time: 14986.30s
                               ETA: 1068632.5s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.069s, learning 0.157s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 41.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.23s
                        Total time: 14994.52s
                               ETA: 1068435.7s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.170s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 40.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.32s
                        Total time: 15002.84s
                               ETA: 1068245.6s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.061s, learning 0.217s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 41.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.28s
                        Total time: 15011.12s
                               ETA: 1068053.0s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.483s, learning 0.180s)
               Value function loss: 7.0559
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 1.53
               Mean episode length: 42.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.66s
                        Total time: 15019.78s
                               ETA: 1067888.1s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.521s, learning 0.189s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 39.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.71s
                        Total time: 15028.49s
                               ETA: 1067726.7s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.498s, learning 0.195s)
               Value function loss: 17.4264
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 42.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.69s
                        Total time: 15037.19s
                               ETA: 1067564.4s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.673s, learning 0.163s)
               Value function loss: 0.1567
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.55
               Mean episode length: 41.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.84s
                        Total time: 15046.02s
                               ETA: 1067412.4s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.548s, learning 0.192s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 3.94
               Mean episode length: 41.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.74s
                        Total time: 15054.76s
                               ETA: 1067253.8s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.230s, learning 0.195s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 41.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 8.43s
                        Total time: 15063.19s
                               ETA: 1067073.1s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.754s, learning 0.163s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.50
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.92s
                        Total time: 15072.10s
                               ETA: 1066927.5s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.484s, learning 0.195s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 1.63
               Mean episode length: 41.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.68s
                        Total time: 15080.78s
                               ETA: 1066765.2s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.385s, learning 0.192s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.58
               Mean episode length: 41.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.58s
                        Total time: 15089.36s
                               ETA: 1066596.0s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.476s, learning 0.187s)
               Value function loss: 13.7897
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 41.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.66s
                        Total time: 15098.02s
                               ETA: 1066433.0s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.447s, learning 0.162s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 41.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.61s
                        Total time: 15106.63s
                               ETA: 1066266.5s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.260s, learning 0.174s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.76
               Mean episode length: 41.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.43s
                        Total time: 15115.07s
                               ETA: 1066087.9s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.524s, learning 0.156s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0455
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 40.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.68s
                        Total time: 15123.75s
                               ETA: 1065926.8s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.140s, learning 0.175s)
               Value function loss: 0.0518
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 40.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.31s
                        Total time: 15132.06s
                               ETA: 1065740.3s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.954s, learning 0.184s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.17
               Mean episode length: 40.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.14s
                        Total time: 15140.20s
                               ETA: 1065541.5s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.428s, learning 0.194s)
               Value function loss: 9.7016
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 41.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.62s
                        Total time: 15148.82s
                               ETA: 1065377.1s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.474s, learning 0.205s)
               Value function loss: 13.4917
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 1.21
               Mean episode length: 40.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.68s
                        Total time: 15157.50s
                               ETA: 1065216.8s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.586s, learning 0.161s)
               Value function loss: 0.2380
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 4.07
               Mean episode length: 41.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.75s
                        Total time: 15166.25s
                               ETA: 1065061.6s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.436s, learning 0.242s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0347
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 41.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.68s
                        Total time: 15174.93s
                               ETA: 1064901.7s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.505s, learning 0.176s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.21
               Mean episode length: 40.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.68s
                        Total time: 15183.61s
                               ETA: 1064742.4s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.782s, learning 0.189s)
               Value function loss: 0.0489
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.63
               Mean episode length: 42.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 8.97s
                        Total time: 15192.58s
                               ETA: 1064603.4s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.390s, learning 0.215s)
               Value function loss: 0.0475
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 41.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 8.61s
                        Total time: 15201.18s
                               ETA: 1064439.1s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.164s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.53
               Mean episode length: 41.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.69s
                        Total time: 15209.87s
                               ETA: 1064280.6s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.331s, learning 0.164s)
               Value function loss: 3.9031
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 40.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 8.49s
                        Total time: 15218.36s
                               ETA: 1064109.0s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.510s, learning 0.169s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0348
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 40.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.68s
                        Total time: 15227.04s
                               ETA: 1063950.4s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.188s, learning 0.169s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.59
               Mean episode length: 41.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.36s
                        Total time: 15235.40s
                               ETA: 1063769.7s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.656s, learning 0.226s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 1.55
               Mean episode length: 42.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.88s
                        Total time: 15244.28s
                               ETA: 1063625.8s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.040s, learning 0.169s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 41.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 8.21s
                        Total time: 15252.49s
                               ETA: 1063435.1s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.408s, learning 0.175s)
               Value function loss: 0.0581
                    Surrogate loss: -0.0366
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 42.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.58s
                        Total time: 15261.07s
                               ETA: 1063270.8s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.264s, learning 0.322s)
               Value function loss: 9.5886
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 41.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.59s
                        Total time: 15269.66s
                               ETA: 1063106.9s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.329s, learning 0.166s)
               Value function loss: 11.9820
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: 1.81
               Mean episode length: 41.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.50s
                        Total time: 15278.15s
                               ETA: 1062936.9s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.189s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.64
               Mean episode length: 42.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.80s
                        Total time: 15286.96s
                               ETA: 1062788.6s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.506s, learning 0.165s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0446
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 41.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 8.67s
                        Total time: 15295.63s
                               ETA: 1062631.2s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.370s, learning 0.193s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.46
               Mean episode length: 40.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.56s
                        Total time: 15304.19s
                               ETA: 1062466.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.164s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 1.44
               Mean episode length: 40.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.49s
                        Total time: 15312.68s
                               ETA: 1062296.8s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.043s, learning 0.169s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 41.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.21s
                        Total time: 15320.89s
                               ETA: 1062108.3s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.540s, learning 0.245s)
               Value function loss: 16.5932
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 40.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.78s
                        Total time: 15329.67s
                               ETA: 1061959.7s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.145s, learning 0.192s)
               Value function loss: 0.1197
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.34s
                        Total time: 15338.01s
                               ETA: 1061780.3s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.583s, learning 0.164s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 1.65
               Mean episode length: 41.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.75s
                        Total time: 15346.76s
                               ETA: 1061629.5s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.644s, learning 0.199s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.65
               Mean episode length: 41.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.84s
                        Total time: 15355.60s
                               ETA: 1061485.5s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.294s, learning 0.188s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 40.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.48s
                        Total time: 15364.08s
                               ETA: 1061316.7s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.279s, learning 0.180s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.72
               Mean episode length: 41.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 8.46s
                        Total time: 15372.54s
                               ETA: 1061146.6s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.250s, learning 0.218s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.52
               Mean episode length: 41.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.47s
                        Total time: 15381.01s
                               ETA: 1060977.3s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.198s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0414
             Mean action noise std: 0.73
                       Mean reward: 1.47
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 8.74s
                        Total time: 15389.74s
                               ETA: 1060826.9s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.409s, learning 0.160s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.57s
                        Total time: 15398.31s
                               ETA: 1060665.1s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.430s, learning 0.182s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.67
               Mean episode length: 41.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.61s
                        Total time: 15406.93s
                               ETA: 1060506.5s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.323s, learning 0.209s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 1.60
               Mean episode length: 41.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.53s
                        Total time: 15415.46s
                               ETA: 1060342.6s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.414s, learning 0.187s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.27
               Mean episode length: 39.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.60s
                        Total time: 15424.06s
                               ETA: 1060183.6s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.671s, learning 0.165s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.54
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.84s
                        Total time: 15432.89s
                               ETA: 1060040.9s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.382s, learning 0.172s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.50
               Mean episode length: 40.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.55s
                        Total time: 15441.45s
                               ETA: 1059879.1s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.339s, learning 0.166s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.67
               Mean episode length: 41.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.50s
                        Total time: 15449.95s
                               ETA: 1059714.1s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.409s, learning 0.211s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 1.60
               Mean episode length: 41.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.62s
                        Total time: 15458.57s
                               ETA: 1059557.3s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.361s, learning 0.162s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0366
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 40.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.52s
                        Total time: 15467.10s
                               ETA: 1059394.0s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.179s, learning 0.238s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.61
               Mean episode length: 40.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 8.42s
                        Total time: 15475.51s
                               ETA: 1059223.7s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.549s, learning 0.173s)
               Value function loss: 64.0871
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 40.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.72s
                        Total time: 15484.24s
                               ETA: 1059074.4s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.352s, learning 0.165s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0433
             Mean action noise std: 0.73
                       Mean reward: 1.62
               Mean episode length: 40.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.52s
                        Total time: 15492.75s
                               ETA: 1058911.4s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.647s, learning 0.206s)
               Value function loss: 59.5237
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 39.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.85s
                        Total time: 15501.61s
                               ETA: 1058771.5s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.162s)
               Value function loss: 0.3569
                    Surrogate loss: -0.0335
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 40.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 8.40s
                        Total time: 15510.01s
                               ETA: 1058601.0s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.457s, learning 0.221s)
               Value function loss: 0.2306
                    Surrogate loss: -0.0329
             Mean action noise std: 0.73
                       Mean reward: 6.60
               Mean episode length: 40.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.68s
                        Total time: 15518.69s
                               ETA: 1058449.6s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.261s, learning 0.187s)
               Value function loss: 0.1326
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 40.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.45s
                        Total time: 15527.13s
                               ETA: 1058282.7s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.399s, learning 0.163s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0365
             Mean action noise std: 0.73
                       Mean reward: 1.71
               Mean episode length: 40.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.56s
                        Total time: 15535.70s
                               ETA: 1058123.7s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.474s, learning 0.189s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.69
               Mean episode length: 40.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.66s
                        Total time: 15544.36s
                               ETA: 1057971.9s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.169s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.61
               Mean episode length: 40.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.58s
                        Total time: 15552.94s
                               ETA: 1057814.6s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.459s, learning 0.195s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 40.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.65s
                        Total time: 15561.60s
                               ETA: 1057662.6s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.167s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.56
               Mean episode length: 40.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.52s
                        Total time: 15570.12s
                               ETA: 1057501.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.376s, learning 0.174s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0423
             Mean action noise std: 0.73
                       Mean reward: 1.78
               Mean episode length: 41.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.55s
                        Total time: 15578.67s
                               ETA: 1057343.2s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.528s, learning 0.199s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 39.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.73s
                        Total time: 15587.40s
                               ETA: 1057196.6s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.385s, learning 0.196s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.55
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.58s
                        Total time: 15595.98s
                               ETA: 1057040.3s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.402s, learning 0.165s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 1.57
               Mean episode length: 40.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.57s
                        Total time: 15604.54s
                               ETA: 1056883.4s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.239s, learning 0.185s)
               Value function loss: 0.0662
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 39.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.42s
                        Total time: 15612.97s
                               ETA: 1056716.9s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.427s, learning 0.167s)
               Value function loss: 39.3806
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 40.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.59s
                        Total time: 15621.56s
                               ETA: 1056562.1s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.167s)
               Value function loss: 0.0676
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.70
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 8.58s
                        Total time: 15630.14s
                               ETA: 1056406.3s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.304s, learning 0.188s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.65
               Mean episode length: 40.35
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.49s
                        Total time: 15638.63s
                               ETA: 1056245.1s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.166s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.76
               Mean episode length: 41.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.47s
                        Total time: 15647.10s
                               ETA: 1056082.9s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.274s, learning 0.169s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0354
             Mean action noise std: 0.73
                       Mean reward: 1.63
               Mean episode length: 40.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.44s
                        Total time: 15655.54s
                               ETA: 1055918.8s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.373s, learning 0.158s)
               Value function loss: 7.0085
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 40.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.53s
                        Total time: 15664.08s
                               ETA: 1055760.8s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.687s, learning 0.165s)
               Value function loss: 0.0561
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 3.90
               Mean episode length: 40.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.85s
                        Total time: 15672.93s
                               ETA: 1055624.7s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.914s, learning 0.159s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 1.61
               Mean episode length: 41.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 9.07s
                        Total time: 15682.00s
                               ETA: 1055503.6s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.465s, learning 0.206s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.67s
                        Total time: 15690.67s
                               ETA: 1055355.7s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.462s, learning 0.188s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 39.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.65s
                        Total time: 15699.32s
                               ETA: 1055206.5s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.170s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.73
               Mean episode length: 40.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.66s
                        Total time: 15707.98s
                               ETA: 1055058.3s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.640s, learning 0.170s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 40.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.81s
                        Total time: 15716.80s
                               ETA: 1054920.3s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.493s, learning 0.191s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 8.68s
                        Total time: 15725.48s
                               ETA: 1054773.9s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.162s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.66s
                        Total time: 15734.14s
                               ETA: 1054626.2s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.252s, learning 0.266s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 39.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.52s
                        Total time: 15742.66s
                               ETA: 1054469.1s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.167s)
               Value function loss: 7.2167
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.38s
                        Total time: 15751.04s
                               ETA: 1054303.0s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.647s, learning 0.173s)
               Value function loss: 17.6258
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 40.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 10.82s
                        Total time: 15761.86s
                               ETA: 1054300.3s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.090s, learning 0.163s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 40.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 16.25s
                        Total time: 15778.11s
                               ETA: 1054660.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.488s, learning 0.163s)
               Value function loss: 0.1007
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 16.65s
                        Total time: 15794.76s
                               ETA: 1055047.3s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.385s, learning 0.200s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 16.59s
                        Total time: 15811.35s
                               ETA: 1055428.9s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.934s, learning 0.163s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 16.10s
                        Total time: 15827.45s
                               ETA: 1055777.4s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.465s, learning 0.190s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 41.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 16.65s
                        Total time: 15844.10s
                               ETA: 1056162.5s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.374s, learning 0.199s)
               Value function loss: 0.0648
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 40.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 16.57s
                        Total time: 15860.67s
                               ETA: 1056541.7s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.276s, learning 0.264s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 16.54s
                        Total time: 15877.21s
                               ETA: 1056918.1s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.466s, learning 0.171s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0357
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 16.64s
                        Total time: 15893.85s
                               ETA: 1057300.5s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.181s, learning 0.196s)
               Value function loss: 7.0434
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 39.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 16.38s
                        Total time: 15910.23s
                               ETA: 1057665.0s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.948s, learning 0.164s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 16.11s
                        Total time: 15926.34s
                               ETA: 1058011.4s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.039s, learning 0.165s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 16.20s
                        Total time: 15942.54s
                               ETA: 1058363.5s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.161s, learning 0.165s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 16.33s
                        Total time: 15958.87s
                               ETA: 1058723.1s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.264s, learning 0.164s)
               Value function loss: 0.0578
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 16.43s
                        Total time: 15975.30s
                               ETA: 1059089.1s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.416s, learning 0.186s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 40.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 16.60s
                        Total time: 15991.90s
                               ETA: 1059466.0s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.275s, learning 0.166s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 16.44s
                        Total time: 16008.34s
                               ETA: 1059831.7s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.893s, learning 0.174s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 40.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 16.07s
                        Total time: 16024.41s
                               ETA: 1060172.1s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.241s, learning 0.166s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.68
               Mean episode length: 41.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 16.41s
                        Total time: 16040.81s
                               ETA: 1060534.6s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.713s, learning 0.170s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 40.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 16.88s
                        Total time: 16057.70s
                               ETA: 1060928.0s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.215s, learning 0.201s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 40.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 16.42s
                        Total time: 16074.11s
                               ETA: 1061290.1s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.302s, learning 0.188s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 16.49s
                        Total time: 16090.60s
                               ETA: 1061656.4s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.381s, learning 0.200s)
               Value function loss: 13.2736
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 16.58s
                        Total time: 16107.18s
                               ETA: 1062028.3s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.494s, learning 0.282s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 4.19
               Mean episode length: 40.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 16.78s
                        Total time: 16123.96s
                               ETA: 1062412.5s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.037s, learning 0.163s)
               Value function loss: 0.0996
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 39.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 17.20s
                        Total time: 16141.16s
                               ETA: 1062824.2s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.508s, learning 0.222s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 16.73s
                        Total time: 16157.89s
                               ETA: 1063204.3s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.449s, learning 0.166s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 41.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 16.62s
                        Total time: 16174.51s
                               ETA: 1063576.3s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.432s, learning 0.194s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 39.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 16.63s
                        Total time: 16191.13s
                               ETA: 1063948.5s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.116s, learning 0.275s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 39.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 16.39s
                        Total time: 16207.52s
                               ETA: 1064304.7s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.263s, learning 0.196s)
               Value function loss: 3.9286
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 39.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 16.46s
                        Total time: 16223.98s
                               ETA: 1064665.0s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.467s, learning 0.192s)
               Value function loss: 13.6024
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 16.66s
                        Total time: 16240.64s
                               ETA: 1065037.8s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.440s, learning 0.164s)
               Value function loss: 0.1501
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 40.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 16.60s
                        Total time: 16257.24s
                               ETA: 1065406.6s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.194s, learning 0.188s)
               Value function loss: 13.7352
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 4.22
               Mean episode length: 40.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 16.38s
                        Total time: 16273.63s
                               ETA: 1065760.2s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.482s, learning 0.199s)
               Value function loss: 0.2051
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 16.68s
                        Total time: 16290.31s
                               ETA: 1066133.0s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.961s, learning 0.187s)
               Value function loss: 13.8934
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 40.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 16.15s
                        Total time: 16306.46s
                               ETA: 1066470.4s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.294s, learning 0.185s)
               Value function loss: 0.1763
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 41.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 16.48s
                        Total time: 16322.94s
                               ETA: 1066828.9s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.467s, learning 0.180s)
               Value function loss: 0.1186
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 41.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 16.65s
                        Total time: 16339.58s
                               ETA: 1067197.9s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.513s, learning 0.195s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 40.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 16.71s
                        Total time: 16356.29s
                               ETA: 1067570.4s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.288s, learning 0.182s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 41.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 16.47s
                        Total time: 16372.76s
                               ETA: 1067926.8s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.164s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.73
               Mean episode length: 41.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 11.43s
                        Total time: 16384.19s
                               ETA: 1067954.2s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.535s, learning 0.161s)
               Value function loss: 17.8353
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 41.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 8.70s
                        Total time: 16392.89s
                               ETA: 1067803.5s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.468s, learning 0.191s)
               Value function loss: 0.1197
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 40.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 8.66s
                        Total time: 16401.54s
                               ETA: 1067650.6s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.739s, learning 0.172s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 8.91s
                        Total time: 16410.46s
                               ETA: 1067514.2s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.165s)
               Value function loss: 15.6470
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 41.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 8.43s
                        Total time: 16418.89s
                               ETA: 1067346.9s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.296s, learning 0.162s)
               Value function loss: 0.1091
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 4.15
               Mean episode length: 41.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 8.46s
                        Total time: 16427.35s
                               ETA: 1067181.5s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.431s, learning 0.209s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 40.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 8.64s
                        Total time: 16435.99s
                               ETA: 1067028.1s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.166s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 41.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 8.65s
                        Total time: 16444.63s
                               ETA: 1066875.3s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.635s, learning 0.162s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 41.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 8.80s
                        Total time: 16453.43s
                               ETA: 1066732.4s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.899s, learning 0.170s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 40.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 8.07s
                        Total time: 16461.50s
                               ETA: 1066542.6s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.381s, learning 0.160s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 8.54s
                        Total time: 16470.04s
                               ETA: 1066383.6s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.457s, learning 0.172s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.88
               Mean episode length: 42.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 8.63s
                        Total time: 16478.67s
                               ETA: 1066230.4s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.168s)
               Value function loss: 64.3468
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 41.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 8.55s
                        Total time: 16487.22s
                               ETA: 1066072.7s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.102s, learning 0.223s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0429
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 41.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 8.33s
                        Total time: 16495.55s
                               ETA: 1065900.3s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.516s, learning 0.164s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 8.68s
                        Total time: 16504.23s
                               ETA: 1065751.0s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.170s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 8.59s
                        Total time: 16512.82s
                               ETA: 1065596.2s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.318s, learning 0.175s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 40.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 8.49s
                        Total time: 16521.31s
                               ETA: 1065435.2s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.163s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 41.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 8.10s
                        Total time: 16529.42s
                               ETA: 1065249.5s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.544s, learning 0.223s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.87
               Mean episode length: 42.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 8.77s
                        Total time: 16538.18s
                               ETA: 1065106.6s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.495s, learning 0.197s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.89
               Mean episode length: 42.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 8.69s
                        Total time: 16546.88s
                               ETA: 1064959.1s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.786s, learning 0.162s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 8.95s
                        Total time: 16555.82s
                               ETA: 1064828.2s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.380s, learning 0.202s)
               Value function loss: 0.0685
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 42.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 8.58s
                        Total time: 16564.40s
                               ETA: 1064673.9s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.172s)
               Value function loss: 16.6626
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 8.38s
                        Total time: 16572.78s
                               ETA: 1064506.5s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.506s, learning 0.167s)
               Value function loss: 9.3038
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 41.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.67s
                        Total time: 16581.45s
                               ETA: 1064358.4s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.618s, learning 0.169s)
               Value function loss: 0.3166
                    Surrogate loss: -0.0243
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 41.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.79s
                        Total time: 16590.24s
                               ETA: 1064217.9s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.498s, learning 0.196s)
               Value function loss: 0.1480
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 41.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.69s
                        Total time: 16598.93s
                               ETA: 1064071.6s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.116s, learning 0.164s)
               Value function loss: 16.2566
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 42.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 8.28s
                        Total time: 16607.21s
                               ETA: 1063898.9s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.285s, learning 0.195s)
               Value function loss: 0.2075
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 41.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 8.48s
                        Total time: 16615.69s
                               ETA: 1063739.3s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.492s, learning 0.161s)
               Value function loss: 53.4762
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 8.65s
                        Total time: 16624.35s
                               ETA: 1063590.9s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.593s, learning 0.163s)
               Value function loss: 0.4970
                    Surrogate loss: -0.0264
             Mean action noise std: 0.72
                       Mean reward: 1.68
               Mean episode length: 41.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.76s
                        Total time: 16633.10s
                               ETA: 1063449.3s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.507s, learning 0.204s)
               Value function loss: 0.2325
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 41.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.71s
                        Total time: 16641.81s
                               ETA: 1063304.9s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.317s, learning 0.179s)
               Value function loss: 17.6511
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.50s
                        Total time: 16650.31s
                               ETA: 1063147.1s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.455s, learning 0.178s)
               Value function loss: 0.1888
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 8.63s
                        Total time: 16658.94s
                               ETA: 1062998.1s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.307s, learning 0.170s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 41.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.48s
                        Total time: 16667.42s
                               ETA: 1062839.4s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.032s, learning 0.194s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0292
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 42.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.23s
                        Total time: 16675.64s
                               ETA: 1062664.9s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.818s, learning 0.164s)
               Value function loss: 0.0987
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 1.68
               Mean episode length: 42.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 8.98s
                        Total time: 16684.63s
                               ETA: 1062538.8s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.446s, learning 0.178s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 41.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.62s
                        Total time: 16693.25s
                               ETA: 1062390.0s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.498s, learning 0.190s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 42.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 8.69s
                        Total time: 16701.94s
                               ETA: 1062245.5s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.390s, learning 0.159s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 42.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 8.55s
                        Total time: 16710.49s
                               ETA: 1062092.3s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.375s, learning 0.195s)
               Value function loss: 30.1204
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.57s
                        Total time: 16719.06s
                               ETA: 1061940.6s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.022s, learning 0.205s)
               Value function loss: 0.2140
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 41.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 8.23s
                        Total time: 16727.28s
                               ETA: 1061767.3s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.350s, learning 0.184s)
               Value function loss: 53.6748
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 42.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.53s
                        Total time: 16735.82s
                               ETA: 1061613.8s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.267s, learning 0.176s)
               Value function loss: 0.3020
                    Surrogate loss: -0.0349
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.44s
                        Total time: 16744.26s
                               ETA: 1061454.6s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.308s, learning 0.188s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 41.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 8.50s
                        Total time: 16752.76s
                               ETA: 1061299.0s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.307s, learning 0.226s)
               Value function loss: 3.8954
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 41.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.53s
                        Total time: 16761.29s
                               ETA: 1061145.9s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.454s, learning 0.210s)
               Value function loss: 0.1080
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 4.19
               Mean episode length: 42.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.66s
                        Total time: 16769.95s
                               ETA: 1061001.3s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.501s, learning 0.165s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 42.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.67s
                        Total time: 16778.62s
                               ETA: 1060857.1s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.417s, learning 0.194s)
               Value function loss: 63.6663
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 41.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.61s
                        Total time: 16787.23s
                               ETA: 1060709.5s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.469s, learning 0.176s)
               Value function loss: 17.4448
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 41.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.65s
                        Total time: 16795.88s
                               ETA: 1060564.3s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.158s)
               Value function loss: 52.8422
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 41.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.42s
                        Total time: 16804.30s
                               ETA: 1060405.0s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.479s, learning 0.195s)
               Value function loss: 5.8176
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 6.68
               Mean episode length: 41.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.67s
                        Total time: 16812.97s
                               ETA: 1060261.9s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.231s, learning 0.166s)
               Value function loss: 0.1846
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.40s
                        Total time: 16821.37s
                               ETA: 1060101.5s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.209s, learning 0.176s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 42.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 8.39s
                        Total time: 16829.75s
                               ETA: 1059940.6s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.355s, learning 0.167s)
               Value function loss: 53.2904
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 40.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.52s
                        Total time: 16838.28s
                               ETA: 1059788.6s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.543s, learning 0.210s)
               Value function loss: 13.8812
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 41.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.75s
                        Total time: 16847.03s
                               ETA: 1059651.2s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.168s)
               Value function loss: 41.4409
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 40.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.47s
                        Total time: 16855.50s
                               ETA: 1059496.0s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.431s, learning 0.171s)
               Value function loss: 16.2404
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 6.84
               Mean episode length: 42.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.60s
                        Total time: 16864.10s
                               ETA: 1059349.4s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.174s, learning 0.176s)
               Value function loss: 0.3960
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.35s
                        Total time: 16872.45s
                               ETA: 1059187.3s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.326s, learning 0.182s)
               Value function loss: 0.2287
                    Surrogate loss: -0.0282
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.51s
                        Total time: 16880.96s
                               ETA: 1059035.3s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.168s)
               Value function loss: 3.9002
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 39.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.41s
                        Total time: 16889.37s
                               ETA: 1058877.2s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.849s, learning 0.270s)
               Value function loss: 0.1611
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 4.17
               Mean episode length: 40.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 9.12s
                        Total time: 16898.49s
                               ETA: 1058763.8s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.323s, learning 0.190s)
               Value function loss: 22.2701
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.51s
                        Total time: 16907.00s
                               ETA: 1058612.6s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.244s, learning 0.180s)
               Value function loss: 0.1444
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 40.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.42s
                        Total time: 16915.42s
                               ETA: 1058456.0s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.360s, learning 0.219s)
               Value function loss: 31.9354
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.58s
                        Total time: 16924.00s
                               ETA: 1058309.3s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.269s, learning 0.196s)
               Value function loss: 0.1464
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 4.16
               Mean episode length: 40.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.47s
                        Total time: 16932.47s
                               ETA: 1058155.7s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.258s, learning 0.266s)
               Value function loss: 107.1720
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.52s
                        Total time: 16940.99s
                               ETA: 1058005.8s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.467s, learning 0.170s)
               Value function loss: 0.2064
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 9.11
               Mean episode length: 40.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.64s
                        Total time: 16949.63s
                               ETA: 1057863.2s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.388s, learning 0.221s)
               Value function loss: 0.1899
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 41.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.61s
                        Total time: 16958.24s
                               ETA: 1057719.1s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.398s, learning 0.169s)
               Value function loss: 0.1379
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 42.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.57s
                        Total time: 16966.80s
                               ETA: 1057572.4s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.324s, learning 0.175s)
               Value function loss: 27.1601
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 41.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.50s
                        Total time: 16975.30s
                               ETA: 1057421.7s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.433s, learning 0.159s)
               Value function loss: 0.2102
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.59s
                        Total time: 16983.90s
                               ETA: 1057277.0s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.505s, learning 0.190s)
               Value function loss: 29.4584
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.70s
                        Total time: 16992.59s
                               ETA: 1057138.9s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.235s, learning 0.174s)
               Value function loss: 16.4577
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.41s
                        Total time: 17001.00s
                               ETA: 1056983.2s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.045s, learning 0.161s)
               Value function loss: 0.2820
                    Surrogate loss: -0.0351
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 40.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.21s
                        Total time: 17009.21s
                               ETA: 1056815.0s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.006s, learning 0.207s)
               Value function loss: 36.5695
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 4.17
               Mean episode length: 41.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.21s
                        Total time: 17017.42s
                               ETA: 1056647.5s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.162s)
               Value function loss: 0.3369
                    Surrogate loss: -0.0277
             Mean action noise std: 0.72
                       Mean reward: 9.25
               Mean episode length: 41.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.51s
                        Total time: 17025.93s
                               ETA: 1056498.5s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.088s, learning 0.185s)
               Value function loss: 0.2722
                    Surrogate loss: -0.0294
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 41.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.27s
                        Total time: 17034.20s
                               ETA: 1056335.1s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.626s, learning 0.177s)
               Value function loss: 18.2103
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 41.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.80s
                        Total time: 17043.00s
                               ETA: 1056204.8s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.120s, learning 0.171s)
               Value function loss: 3.7874
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 42.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.29s
                        Total time: 17051.30s
                               ETA: 1056042.9s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.292s, learning 0.179s)
               Value function loss: 14.1514
                    Surrogate loss: 0.0024
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 41.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.47s
                        Total time: 17059.77s
                               ETA: 1055892.3s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.322s, learning 0.200s)
               Value function loss: 0.1543
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 41.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.52s
                        Total time: 17068.29s
                               ETA: 1055745.0s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.302s, learning 0.168s)
               Value function loss: 29.0899
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.47s
                        Total time: 17076.76s
                               ETA: 1055594.7s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.553s, learning 0.172s)
               Value function loss: 0.1694
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 6.65
               Mean episode length: 41.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.72s
                        Total time: 17085.48s
                               ETA: 1055460.3s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.976s, learning 0.172s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 41.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.15s
                        Total time: 17093.63s
                               ETA: 1055290.4s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.510s, learning 0.168s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 41.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.68s
                        Total time: 17102.31s
                               ETA: 1055153.5s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.305s, learning 0.166s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.47s
                        Total time: 17110.78s
                               ETA: 1055004.0s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.538s, learning 0.166s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 8.70s
                        Total time: 17119.48s
                               ETA: 1054869.0s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.422s, learning 0.258s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.93
               Mean episode length: 42.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.68s
                        Total time: 17128.16s
                               ETA: 1054732.6s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.436s, learning 0.188s)
               Value function loss: 0.0846
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.81
               Mean episode length: 41.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.62s
                        Total time: 17136.79s
                               ETA: 1054593.0s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.445s, learning 0.199s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 40.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.64s
                        Total time: 17145.43s
                               ETA: 1054454.8s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.305s, learning 0.165s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 41.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 8.47s
                        Total time: 17153.90s
                               ETA: 1054306.0s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.530s, learning 0.163s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 41.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.69s
                        Total time: 17162.60s
                               ETA: 1054171.2s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.169s)
               Value function loss: 88.8670
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.90s
                        Total time: 17171.50s
                               ETA: 1054049.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.126s, learning 0.253s)
               Value function loss: 0.1459
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 41.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.38s
                        Total time: 17179.87s
                               ETA: 1053895.3s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.598s, learning 0.166s)
               Value function loss: 0.1775
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 40.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.76s
                        Total time: 17188.64s
                               ETA: 1053765.3s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.522s, learning 0.190s)
               Value function loss: 57.8065
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.71s
                        Total time: 17197.35s
                               ETA: 1053632.2s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.641s, learning 0.164s)
               Value function loss: 96.1318
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 6.96
               Mean episode length: 41.20
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.81s
                        Total time: 17206.16s
                               ETA: 1053505.0s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.447s, learning 0.169s)
               Value function loss: 20.2525
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 41.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.62s
                        Total time: 17214.77s
                               ETA: 1053366.3s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.438s, learning 0.161s)
               Value function loss: 0.8351
                    Surrogate loss: -0.0314
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.60s
                        Total time: 17223.37s
                               ETA: 1053226.8s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.481s, learning 0.209s)
               Value function loss: 17.2213
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.99
               Mean episode length: 42.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.69s
                        Total time: 17232.06s
                               ETA: 1053092.9s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.164s)
               Value function loss: 7.2047
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 41.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.69s
                        Total time: 17240.75s
                               ETA: 1052959.0s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.651s, learning 0.164s)
               Value function loss: 17.8313
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.89
               Mean episode length: 42.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.81s
                        Total time: 17249.56s
                               ETA: 1052833.1s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.340s, learning 0.201s)
               Value function loss: 73.9377
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 41.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.54s
                        Total time: 17258.10s
                               ETA: 1052690.6s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.264s, learning 0.168s)
               Value function loss: 0.3022
                    Surrogate loss: -0.0296
             Mean action noise std: 0.72
                       Mean reward: 4.05
               Mean episode length: 41.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.43s
                        Total time: 17266.53s
                               ETA: 1052541.7s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.284s, learning 0.177s)
               Value function loss: 0.2639
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 41.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.46s
                        Total time: 17274.99s
                               ETA: 1052394.7s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.686s, learning 0.194s)
               Value function loss: 0.1770
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 42.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.88s
                        Total time: 17283.87s
                               ETA: 1052273.4s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.454s, learning 0.213s)
               Value function loss: 0.1429
                    Surrogate loss: -0.0353
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 41.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.67s
                        Total time: 17292.54s
                               ETA: 1052139.3s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.208s, learning 0.163s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 41.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.37s
                        Total time: 17300.91s
                               ETA: 1051987.3s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.430s, learning 0.228s)
               Value function loss: 0.1138
                    Surrogate loss: -0.0353
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.66s
                        Total time: 17309.57s
                               ETA: 1051853.0s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.413s, learning 0.258s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 41.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.67s
                        Total time: 17318.24s
                               ETA: 1051719.5s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.695s, learning 0.163s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 41.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.86s
                        Total time: 17327.10s
                               ETA: 1051597.6s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.563s, learning 0.188s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.75s
                        Total time: 17335.85s
                               ETA: 1051469.4s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.164s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 42.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.66s
                        Total time: 17344.51s
                               ETA: 1051335.9s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.177s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.56s
                        Total time: 17353.07s
                               ETA: 1051196.7s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.539s, learning 0.179s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0325
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.72s
                        Total time: 17361.79s
                               ETA: 1051066.9s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.779s, learning 0.204s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 42.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 8.98s
                        Total time: 17370.78s
                               ETA: 1050953.3s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.165s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0351
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 41.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 8.66s
                        Total time: 17379.44s
                               ETA: 1050820.3s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.354s, learning 0.194s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 8.55s
                        Total time: 17387.98s
                               ETA: 1050680.6s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.275s, learning 0.170s)
               Value function loss: 17.8482
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.44s
                        Total time: 17396.43s
                               ETA: 1050534.9s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.646s, learning 0.172s)
               Value function loss: 0.0846
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 41.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 9.82s
                        Total time: 17406.25s
                               ETA: 1050472.3s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.334s, learning 0.180s)
               Value function loss: 29.6075
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 4.11
               Mean episode length: 41.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 16.51s
                        Total time: 17422.76s
                               ETA: 1050813.6s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.715s, learning 0.166s)
               Value function loss: 29.5975
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 16.88s
                        Total time: 17439.64s
                               ETA: 1051176.5s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.286s, learning 0.163s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 16.45s
                        Total time: 17456.09s
                               ETA: 1051513.0s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.225s, learning 0.211s)
               Value function loss: 132.0932
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 42.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 16.44s
                        Total time: 17472.53s
                               ETA: 1051848.2s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.692s, learning 0.170s)
               Value function loss: 0.2319
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 9.03
               Mean episode length: 40.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 16.86s
                        Total time: 17489.39s
                               ETA: 1052208.7s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.720s, learning 0.176s)
               Value function loss: 0.1972
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 16.90s
                        Total time: 17506.28s
                               ETA: 1052570.7s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.343s, learning 0.244s)
               Value function loss: 0.1673
                    Surrogate loss: -0.0335
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 42.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 16.59s
                        Total time: 17522.87s
                               ETA: 1052913.7s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.548s, learning 0.198s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 16.75s
                        Total time: 17539.62s
                               ETA: 1053265.8s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.283s, learning 0.179s)
               Value function loss: 17.8097
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 41.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 16.46s
                        Total time: 17556.08s
                               ETA: 1053600.3s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.394s, learning 0.176s)
               Value function loss: 32.6681
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 42.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 16.57s
                        Total time: 17572.65s
                               ETA: 1053941.0s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.728s, learning 0.191s)
               Value function loss: 3.9656
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 9.22
               Mean episode length: 41.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 16.92s
                        Total time: 17589.57s
                               ETA: 1054302.1s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.557s, learning 0.163s)
               Value function loss: 18.7262
                    Surrogate loss: 0.0024
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 16.72s
                        Total time: 17606.29s
                               ETA: 1054650.9s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.747s, learning 0.194s)
               Value function loss: 0.2355
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 16.94s
                        Total time: 17623.23s
                               ETA: 1055012.4s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.533s, learning 0.226s)
               Value function loss: 0.1639
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 40.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 16.76s
                        Total time: 17639.99s
                               ETA: 1055362.6s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.341s, learning 0.173s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 41.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 16.51s
                        Total time: 17656.50s
                               ETA: 1055697.7s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.547s, learning 0.171s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 41.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 16.72s
                        Total time: 17673.22s
                               ETA: 1056044.6s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.590s, learning 0.167s)
               Value function loss: 0.0897
                    Surrogate loss: -0.0353
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 16.76s
                        Total time: 17689.98s
                               ETA: 1056393.3s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.469s, learning 0.231s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 16.70s
                        Total time: 17706.67s
                               ETA: 1056738.2s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.376s, learning 0.173s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 42.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 16.55s
                        Total time: 17723.22s
                               ETA: 1057073.7s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.539s, learning 0.172s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 40.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 16.71s
                        Total time: 17739.93s
                               ETA: 1057418.4s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.583s, learning 0.200s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 16.78s
                        Total time: 17756.72s
                               ETA: 1057766.9s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.666s, learning 0.223s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 16.89s
                        Total time: 17773.61s
                               ETA: 1058121.4s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.260s, learning 0.161s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 16.42s
                        Total time: 17790.03s
                               ETA: 1058447.5s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.968s, learning 0.160s)
               Value function loss: 111.4526
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 41.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 16.13s
                        Total time: 17806.16s
                               ETA: 1058755.8s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.816s, learning 0.227s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 6.47
               Mean episode length: 40.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 17.04s
                        Total time: 17823.20s
                               ETA: 1059118.1s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.443s, learning 0.162s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 16.61s
                        Total time: 17839.81s
                               ETA: 1059453.9s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.395s, learning 0.169s)
               Value function loss: 11.7673
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 40.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 16.56s
                        Total time: 17856.37s
                               ETA: 1059786.9s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.360s, learning 0.169s)
               Value function loss: 28.9412
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 16.53s
                        Total time: 17872.90s
                               ETA: 1060117.3s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.500s, learning 0.165s)
               Value function loss: 0.2257
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 40.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 16.67s
                        Total time: 17889.56s
                               ETA: 1060455.4s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.743s, learning 0.202s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 41.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 16.94s
                        Total time: 17906.51s
                               ETA: 1060809.6s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.438s, learning 0.208s)
               Value function loss: 17.2871
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 16.65s
                        Total time: 17923.15s
                               ETA: 1061145.7s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.271s, learning 0.167s)
               Value function loss: 3.9444
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 41.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 16.44s
                        Total time: 17939.59s
                               ETA: 1061469.1s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.604s, learning 0.192s)
               Value function loss: 0.1361
                    Surrogate loss: -0.0351
             Mean action noise std: 0.72
                       Mean reward: 1.79
               Mean episode length: 41.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 16.80s
                        Total time: 17956.39s
                               ETA: 1061813.2s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.439s, learning 0.192s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 41.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 16.63s
                        Total time: 17973.02s
                               ETA: 1062147.2s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.209s, learning 0.172s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 40.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 16.38s
                        Total time: 17989.40s
                               ETA: 1062465.9s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.810s, learning 0.228s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 40.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 17.04s
                        Total time: 18006.44s
                               ETA: 1062823.1s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.111s, learning 0.194s)
               Value function loss: 0.0572
                    Surrogate loss: -0.0342
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 40.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 16.30s
                        Total time: 18022.74s
                               ETA: 1063136.5s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.392s, learning 0.190s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 40.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 12.58s
                        Total time: 18035.33s
                               ETA: 1063230.1s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.546s, learning 0.266s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.81s
                        Total time: 18044.14s
                               ETA: 1063101.4s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.242s, learning 0.195s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0337
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 41.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.44s
                        Total time: 18052.58s
                               ETA: 1062950.8s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.672s, learning 0.189s)
               Value function loss: 13.6666
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 41.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 8.86s
                        Total time: 18061.44s
                               ETA: 1062825.3s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.488s, learning 0.169s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 41.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 8.66s
                        Total time: 18070.09s
                               ETA: 1062687.9s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.495s, learning 0.176s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 8.67s
                        Total time: 18078.76s
                               ETA: 1062551.6s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.188s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 39.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 8.53s
                        Total time: 18087.30s
                               ETA: 1062407.3s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.327s, learning 0.257s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 41.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.58s
                        Total time: 18095.88s
                               ETA: 1062266.0s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.595s, learning 0.198s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 41.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 8.79s
                        Total time: 18104.67s
                               ETA: 1062137.3s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.475s, learning 0.225s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 8.70s
                        Total time: 18113.37s
                               ETA: 1062003.2s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.447s, learning 0.166s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.61s
                        Total time: 18121.99s
                               ETA: 1061864.1s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.162s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 40.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.44s
                        Total time: 18130.43s
                               ETA: 1061715.2s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.112s, learning 0.187s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.30s
                        Total time: 18138.72s
                               ETA: 1061558.1s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.596s, learning 0.197s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0357
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 41.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 8.79s
                        Total time: 18147.52s
                               ETA: 1061430.1s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.169s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 41.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.66s
                        Total time: 18156.18s
                               ETA: 1061294.7s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.377s, learning 0.242s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 40.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.62s
                        Total time: 18164.80s
                               ETA: 1061156.8s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.270s, learning 0.192s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 40.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 8.46s
                        Total time: 18173.26s
                               ETA: 1061009.9s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.459s, learning 0.191s)
               Value function loss: 13.8904
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.65s
                        Total time: 18181.91s
                               ETA: 1060874.1s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.430s, learning 0.159s)
               Value function loss: 64.0784
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 4.03
               Mean episode length: 40.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 8.59s
                        Total time: 18190.50s
                               ETA: 1060735.0s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.348s, learning 0.160s)
               Value function loss: 18.3569
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 40.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.51s
                        Total time: 18199.01s
                               ETA: 1060591.2s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.172s)
               Value function loss: 3.9570
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 41.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.43s
                        Total time: 18207.44s
                               ETA: 1060443.3s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.462s, learning 0.188s)
               Value function loss: 0.1561
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 8.65s
                        Total time: 18216.09s
                               ETA: 1060308.1s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.170s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0326
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.37s
                        Total time: 18224.47s
                               ETA: 1060157.1s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.173s)
               Value function loss: 9.7666
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 39.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.61s
                        Total time: 18233.07s
                               ETA: 1060019.7s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.646s, learning 0.167s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 40.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 8.81s
                        Total time: 18241.89s
                               ETA: 1059894.5s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.359s, learning 0.165s)
               Value function loss: 11.8502
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 41.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.52s
                        Total time: 18250.41s
                               ETA: 1059752.7s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.336s, learning 0.192s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 41.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.53s
                        Total time: 18258.94s
                               ETA: 1059611.3s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.679s, learning 0.224s)
               Value function loss: 13.4624
                    Surrogate loss: 0.0025
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.90s
                        Total time: 18267.84s
                               ETA: 1059491.7s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.489s, learning 0.165s)
               Value function loss: 0.1840
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 41.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 8.65s
                        Total time: 18276.50s
                               ETA: 1059357.8s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.621s, learning 0.195s)
               Value function loss: 0.1374
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 8.82s
                        Total time: 18285.31s
                               ETA: 1059233.5s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.430s, learning 0.189s)
               Value function loss: 93.4688
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 1.24
               Mean episode length: 41.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 8.62s
                        Total time: 18293.93s
                               ETA: 1059097.8s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.575s, learning 0.171s)
               Value function loss: 0.0846
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 40.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 8.75s
                        Total time: 18302.67s
                               ETA: 1058969.7s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.533s, learning 0.198s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 41.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 8.73s
                        Total time: 18311.41s
                               ETA: 1058840.9s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.489s, learning 0.172s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 40.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 8.66s
                        Total time: 18320.07s
                               ETA: 1058708.1s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.696s, learning 0.167s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 41.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 8.86s
                        Total time: 18328.93s
                               ETA: 1058587.2s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.752s, learning 0.163s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 41.85
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 8.92s
                        Total time: 18337.84s
                               ETA: 1058469.4s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.582s, learning 0.163s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 40.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 8.75s
                        Total time: 18346.59s
                               ETA: 1058342.0s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.784s, learning 0.210s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 41.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 8.99s
                        Total time: 18355.58s
                               ETA: 1058229.0s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.757s, learning 0.198s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 41.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 8.95s
                        Total time: 18364.54s
                               ETA: 1058113.9s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.800s, learning 0.198s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 41.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 9.00s
                        Total time: 18373.54s
                               ETA: 1058001.4s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.460s, learning 0.165s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0327
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 8.63s
                        Total time: 18382.16s
                               ETA: 1057867.6s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.689s, learning 0.192s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0357
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 41.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 8.88s
                        Total time: 18391.04s
                               ETA: 1057748.7s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.281s, learning 0.160s)
               Value function loss: 0.0489
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 41.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 8.44s
                        Total time: 18399.49s
                               ETA: 1057604.6s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.166s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0342
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 41.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 8.65s
                        Total time: 18408.13s
                               ETA: 1057472.4s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.670s, learning 0.172s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 41.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 8.84s
                        Total time: 18416.97s
                               ETA: 1057351.6s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.731s, learning 0.160s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 41.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 8.89s
                        Total time: 18425.86s
                               ETA: 1057233.7s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.597s, learning 0.281s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 41.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 8.88s
                        Total time: 18434.74s
                               ETA: 1057115.2s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.164s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 41.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 8.48s
                        Total time: 18443.23s
                               ETA: 1056974.3s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.209s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 41.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 8.55s
                        Total time: 18451.78s
                               ETA: 1056837.3s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.593s, learning 0.191s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 40.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 8.78s
                        Total time: 18460.56s
                               ETA: 1056713.9s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1791 steps/s (collection: 8.975s, learning 0.172s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 41.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 9.15s
                        Total time: 18469.71s
                               ETA: 1056611.3s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.705s, learning 0.207s)
               Value function loss: 12.1222
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 41.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 8.91s
                        Total time: 18478.62s
                               ETA: 1056495.4s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.575s, learning 0.172s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 41.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 8.75s
                        Total time: 18487.37s
                               ETA: 1056370.3s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.599s, learning 0.195s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 41.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 8.79s
                        Total time: 18496.16s
                               ETA: 1056247.9s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.172s)
               Value function loss: 3.9015
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 40.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 8.62s
                        Total time: 18504.78s
                               ETA: 1056115.9s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.537s, learning 0.172s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 41.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 8.71s
                        Total time: 18513.49s
                               ETA: 1055989.0s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.119s, learning 0.156s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 41.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 8.27s
                        Total time: 18521.77s
                               ETA: 1055837.4s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.300s, learning 0.197s)
               Value function loss: 0.0459
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 42.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 8.50s
                        Total time: 18530.26s
                               ETA: 1055698.7s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.409s, learning 0.178s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0426
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 8.59s
                        Total time: 18538.85s
                               ETA: 1055565.2s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.374s, learning 0.173s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 40.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 8.55s
                        Total time: 18547.40s
                               ETA: 1055429.7s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.362s, learning 0.158s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 41.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 8.52s
                        Total time: 18555.92s
                               ETA: 1055292.7s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.547s, learning 0.168s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0344
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 40.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 8.71s
                        Total time: 18564.63s
                               ETA: 1055167.0s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.193s, learning 0.165s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 8.36s
                        Total time: 18572.99s
                               ETA: 1055021.1s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.845s, learning 0.174s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 41.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 9.02s
                        Total time: 18582.01s
                               ETA: 1054912.9s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.531s, learning 0.170s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 1.19
               Mean episode length: 41.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 8.70s
                        Total time: 18590.71s
                               ETA: 1054786.8s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.680s, learning 0.176s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.13
               Mean episode length: 40.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 8.86s
                        Total time: 18599.57s
                               ETA: 1054669.6s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.354s, learning 0.164s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 40.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.52s
                        Total time: 18608.09s
                               ETA: 1054533.4s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.819s, learning 0.211s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 41.61
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 9.03s
                        Total time: 18617.12s
                               ETA: 1054426.3s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.483s, learning 0.175s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 41.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 8.66s
                        Total time: 18625.77s
                               ETA: 1054298.3s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.129s, learning 0.165s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 41.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 8.29s
                        Total time: 18634.07s
                               ETA: 1054149.8s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.229s, learning 0.285s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.51s
                        Total time: 18642.58s
                               ETA: 1054013.9s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.205s, learning 0.173s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 40.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 8.38s
                        Total time: 18650.96s
                               ETA: 1053870.5s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.244s, learning 0.164s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 41.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.41s
                        Total time: 18659.37s
                               ETA: 1053728.9s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.162s)
               Value function loss: 17.6982
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 39.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 8.71s
                        Total time: 18668.08s
                               ETA: 1053604.3s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.295s, learning 0.171s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0434
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 41.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.47s
                        Total time: 18676.54s
                               ETA: 1053466.3s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.280s, learning 0.167s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 41.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.45s
                        Total time: 18684.99s
                               ETA: 1053327.3s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1784 steps/s (collection: 9.015s, learning 0.165s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 41.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 9.18s
                        Total time: 18694.17s
                               ETA: 1053229.8s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.624s, learning 0.168s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 41.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.79s
                        Total time: 18702.96s
                               ETA: 1053110.5s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.953s, learning 0.164s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.12s
                        Total time: 18711.08s
                               ETA: 1052953.5s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.207s, learning 0.174s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 41.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.38s
                        Total time: 18719.46s
                               ETA: 1052811.4s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.487s, learning 0.192s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 41.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.68s
                        Total time: 18728.14s
                               ETA: 1052686.2s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.598s, learning 0.203s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 40.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.80s
                        Total time: 18736.94s
                               ETA: 1052568.0s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.483s, learning 0.200s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 41.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.68s
                        Total time: 18745.62s
                               ETA: 1052443.4s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.186s, learning 0.241s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 41.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.43s
                        Total time: 18754.05s
                               ETA: 1052304.5s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.821s, learning 0.172s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 40.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.99s
                        Total time: 18763.04s
                               ETA: 1052197.4s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.165s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 41.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.47s
                        Total time: 18771.51s
                               ETA: 1052061.4s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.627s, learning 0.174s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 40.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.80s
                        Total time: 18780.31s
                               ETA: 1051943.8s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.272s, learning 0.198s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.47s
                        Total time: 18788.78s
                               ETA: 1051807.9s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.555s, learning 0.165s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 41.77
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.72s
                        Total time: 18797.50s
                               ETA: 1051686.1s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.270s, learning 0.167s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 41.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.44s
                        Total time: 18805.94s
                               ETA: 1051548.6s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.546s, learning 0.166s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.17
               Mean episode length: 39.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.71s
                        Total time: 18814.65s
                               ETA: 1051426.6s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.658s, learning 0.271s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.93s
                        Total time: 18823.58s
                               ETA: 1051316.8s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.355s, learning 0.159s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.51s
                        Total time: 18832.09s
                               ETA: 1051184.0s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.560s, learning 0.211s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 41.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.77s
                        Total time: 18840.87s
                               ETA: 1051065.7s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.238s, learning 0.188s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 41.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.43s
                        Total time: 18849.29s
                               ETA: 1050928.3s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.731s, learning 0.191s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.92s
                        Total time: 18858.21s
                               ETA: 1050818.6s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.571s, learning 0.174s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.75s
                        Total time: 18866.96s
                               ETA: 1050699.2s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.590s, learning 0.196s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 40.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 8.79s
                        Total time: 18875.74s
                               ETA: 1050582.3s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.163s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.51s
                        Total time: 18884.25s
                               ETA: 1050450.0s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.345s, learning 0.175s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 41.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 8.52s
                        Total time: 18892.77s
                               ETA: 1050318.4s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.023s, learning 0.241s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 40.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.26s
                        Total time: 18901.04s
                               ETA: 1050172.8s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.526s, learning 0.171s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 40.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 8.70s
                        Total time: 18909.73s
                               ETA: 1050051.4s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.263s, learning 0.159s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 40.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 8.42s
                        Total time: 18918.16s
                               ETA: 1049914.9s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.307s, learning 0.187s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 41.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.49s
                        Total time: 18926.65s
                               ETA: 1049782.5s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.510s, learning 0.158s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 8.67s
                        Total time: 18935.32s
                               ETA: 1049659.9s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.261s, learning 0.186s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.45s
                        Total time: 18943.77s
                               ETA: 1049525.2s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.215s, learning 0.159s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 39.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.37s
                        Total time: 18952.14s
                               ETA: 1049386.6s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.631s, learning 0.249s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 39.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.88s
                        Total time: 18961.02s
                               ETA: 1049276.2s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.539s, learning 0.163s)
               Value function loss: 59.2002
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 40.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.70s
                        Total time: 18969.72s
                               ETA: 1049156.0s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.406s, learning 0.198s)
               Value function loss: 0.1645
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 39.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.60s
                        Total time: 18978.33s
                               ETA: 1049030.5s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.450s, learning 0.193s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 8.64s
                        Total time: 18986.97s
                               ETA: 1048907.3s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.381s, learning 0.196s)
               Value function loss: 29.3746
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 42.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 8.58s
                        Total time: 18995.55s
                               ETA: 1048780.5s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.441s, learning 0.169s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.41
               Mean episode length: 40.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 9.61s
                        Total time: 19005.16s
                               ETA: 1048711.0s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.110s, learning 0.225s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 39.96
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 16.33s
                        Total time: 19021.49s
                               ETA: 1049012.3s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.900s, learning 0.171s)
               Value function loss: 0.0435
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 40.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 16.07s
                        Total time: 19037.56s
                               ETA: 1049298.8s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.244s, learning 0.178s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0402
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 39.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 16.42s
                        Total time: 19053.99s
                               ETA: 1049604.2s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.496s, learning 0.218s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 39.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 16.71s
                        Total time: 19070.70s
                               ETA: 1049925.4s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.685s, learning 0.191s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 40.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 16.88s
                        Total time: 19087.57s
                               ETA: 1050255.0s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.711s, learning 0.159s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 40.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 16.87s
                        Total time: 19104.44s
                               ETA: 1050584.0s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.727s, learning 0.195s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 16.92s
                        Total time: 19121.37s
                               ETA: 1050915.5s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.721s, learning 0.216s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 16.94s
                        Total time: 19138.30s
                               ETA: 1051247.4s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.699s, learning 0.167s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 39.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 16.87s
                        Total time: 19155.17s
                               ETA: 1051575.0s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.798s, learning 0.165s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 41.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 16.96s
                        Total time: 19172.13s
                               ETA: 1051907.5s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.796s, learning 0.255s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 41.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 17.05s
                        Total time: 19189.19s
                               ETA: 1052244.5s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.498s, learning 0.220s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 16.72s
                        Total time: 19205.90s
                               ETA: 1052562.8s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.451s, learning 0.197s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 16.65s
                        Total time: 19222.55s
                               ETA: 1052877.0s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.573s, learning 0.207s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 16.78s
                        Total time: 19239.33s
                               ETA: 1053198.0s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.551s, learning 0.196s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 40.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 16.75s
                        Total time: 19256.08s
                               ETA: 1053516.8s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.751s, learning 0.162s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 16.91s
                        Total time: 19272.99s
                               ETA: 1053844.2s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.457s, learning 0.253s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 16.71s
                        Total time: 19289.70s
                               ETA: 1054160.3s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 954 steps/s (collection: 17.008s, learning 0.165s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 40.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 17.17s
                        Total time: 19306.88s
                               ETA: 1054501.2s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.726s, learning 0.224s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 40.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 16.95s
                        Total time: 19323.83s
                               ETA: 1054829.6s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.563s, learning 0.169s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 16.73s
                        Total time: 19340.56s
                               ETA: 1055145.6s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.313s, learning 0.178s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 16.49s
                        Total time: 19357.05s
                               ETA: 1055448.2s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.891s, learning 0.164s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 40.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 17.06s
                        Total time: 19374.11s
                               ETA: 1055781.2s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.810s, learning 0.167s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 40.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 16.98s
                        Total time: 19391.08s
                               ETA: 1056109.5s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.384s, learning 0.195s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 41.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 16.58s
                        Total time: 19407.66s
                               ETA: 1056415.8s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.750s, learning 0.210s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 40.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 16.96s
                        Total time: 19424.62s
                               ETA: 1056742.5s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.762s, learning 0.160s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 40.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 16.92s
                        Total time: 19441.54s
                               ETA: 1057066.7s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.605s, learning 0.166s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 41.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 16.77s
                        Total time: 19458.31s
                               ETA: 1057382.3s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.776s, learning 0.179s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 40.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 16.95s
                        Total time: 19475.27s
                               ETA: 1057707.5s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.819s, learning 0.257s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 41.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 17.08s
                        Total time: 19492.35s
                               ETA: 1058039.0s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.052s, learning 0.184s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.68
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 17.24s
                        Total time: 19509.58s
                               ETA: 1058378.7s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.529s, learning 0.163s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.19
               Mean episode length: 40.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 16.69s
                        Total time: 19526.27s
                               ETA: 1058688.5s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.028s, learning 0.169s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 41.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 17.20s
                        Total time: 19543.47s
                               ETA: 1059025.3s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.735s, learning 0.173s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.61
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 16.91s
                        Total time: 19560.38s
                               ETA: 1059346.1s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.632s, learning 0.181s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 16.81s
                        Total time: 19577.19s
                               ETA: 1059661.4s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.667s, learning 0.326s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 40.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 16.99s
                        Total time: 19594.19s
                               ETA: 1059986.1s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.339s, learning 0.163s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 40.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 16.50s
                        Total time: 19610.69s
                               ETA: 1060283.8s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.400s, learning 0.165s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 16.56s
                        Total time: 19627.25s
                               ETA: 1060584.5s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1263 steps/s (collection: 12.804s, learning 0.168s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 40.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 12.97s
                        Total time: 19640.22s
                               ETA: 1060690.9s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.437s, learning 0.195s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 40.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.63s
                        Total time: 19648.86s
                               ETA: 1060563.0s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.686s, learning 0.169s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 40.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.85s
                        Total time: 19657.71s
                               ETA: 1060447.1s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.667s, learning 0.163s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 40.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.83s
                        Total time: 19666.54s
                               ETA: 1060330.0s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.397s, learning 0.169s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 40.68
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.57s
                        Total time: 19675.11s
                               ETA: 1060198.9s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.602s, learning 0.159s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 41.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.76s
                        Total time: 19683.87s
                               ETA: 1060078.4s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.343s, learning 0.208s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 40.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.55s
                        Total time: 19692.42s
                               ETA: 1059946.6s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.543s, learning 0.214s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.76s
                        Total time: 19701.18s
                               ETA: 1059826.1s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.664s, learning 0.171s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 39.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.83s
                        Total time: 19710.01s
                               ETA: 1059709.9s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.511s, learning 0.184s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 40.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.69s
                        Total time: 19718.71s
                               ETA: 1059586.3s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.312s, learning 0.216s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 39.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.53s
                        Total time: 19727.23s
                               ETA: 1059453.9s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.600s, learning 0.201s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.80s
                        Total time: 19736.03s
                               ETA: 1059336.3s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.448s, learning 0.171s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 40.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.62s
                        Total time: 19744.65s
                               ETA: 1059209.0s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.329s, learning 0.193s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0410
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.52s
                        Total time: 19753.18s
                               ETA: 1059076.6s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.566s, learning 0.215s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 39.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.78s
                        Total time: 19761.96s
                               ETA: 1058958.2s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.486s, learning 0.162s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 40.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.65s
                        Total time: 19770.60s
                               ETA: 1058832.9s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.777s, learning 0.197s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 39.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.97s
                        Total time: 19779.58s
                               ETA: 1058725.1s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.449s, learning 0.236s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.68s
                        Total time: 19788.26s
                               ETA: 1058602.0s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.554s, learning 0.202s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.76s
                        Total time: 19797.02s
                               ETA: 1058482.7s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.184s, learning 0.171s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.36s
                        Total time: 19805.37s
                               ETA: 1058342.2s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.411s, learning 0.200s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 40.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.61s
                        Total time: 19813.98s
                               ETA: 1058215.5s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.436s, learning 0.237s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.67s
                        Total time: 19822.66s
                               ETA: 1058092.3s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.335s, learning 0.181s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.52s
                        Total time: 19831.17s
                               ETA: 1057960.8s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.582s, learning 0.160s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.74s
                        Total time: 19839.92s
                               ETA: 1057841.5s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.915s, learning 0.186s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 40.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 9.10s
                        Total time: 19849.02s
                               ETA: 1057741.4s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.439s, learning 0.165s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.60s
                        Total time: 19857.62s
                               ETA: 1057615.0s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.381s, learning 0.236s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 40.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.62s
                        Total time: 19866.24s
                               ETA: 1057489.4s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.443s, learning 0.166s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.61s
                        Total time: 19874.85s
                               ETA: 1057363.4s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.907s, learning 0.170s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 40.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 9.08s
                        Total time: 19883.92s
                               ETA: 1057262.5s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.058s, learning 0.162s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 41.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 8.22s
                        Total time: 19892.14s
                               ETA: 1057116.1s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.847s, learning 0.189s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 40.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 9.04s
                        Total time: 19901.18s
                               ETA: 1057013.3s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.530s, learning 0.218s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 8.75s
                        Total time: 19909.93s
                               ETA: 1056895.2s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.586s, learning 0.162s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 41.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 8.75s
                        Total time: 19918.68s
                               ETA: 1056777.2s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.581s, learning 0.168s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 41.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 8.75s
                        Total time: 19927.42s
                               ETA: 1056659.5s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.419s, learning 0.176s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 8.59s
                        Total time: 19936.02s
                               ETA: 1056533.6s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.602s, learning 0.290s)
               Value function loss: 17.5068
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 4.01
               Mean episode length: 40.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 8.89s
                        Total time: 19944.91s
                               ETA: 1056423.7s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.462s, learning 0.194s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 8.66s
                        Total time: 19953.57s
                               ETA: 1056301.3s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.437s, learning 0.186s)
               Value function loss: 0.0629
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 8.62s
                        Total time: 19962.19s
                               ETA: 1056177.4s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.432s, learning 0.159s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 8.59s
                        Total time: 19970.78s
                               ETA: 1056051.8s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.168s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 41.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 8.67s
                        Total time: 19979.45s
                               ETA: 1055930.5s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.603s, learning 0.206s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 41.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 8.81s
                        Total time: 19988.26s
                               ETA: 1055816.7s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.263s, learning 0.210s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 39.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 8.47s
                        Total time: 19996.73s
                               ETA: 1055685.3s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.021s, learning 0.207s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 40.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 8.23s
                        Total time: 20004.96s
                               ETA: 1055541.2s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.582s, learning 0.213s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 8.80s
                        Total time: 20013.75s
                               ETA: 1055427.0s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.642s, learning 0.267s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0422
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 41.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 8.91s
                        Total time: 20022.66s
                               ETA: 1055319.0s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.938s, learning 0.187s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 40.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 9.12s
                        Total time: 20031.79s
                               ETA: 1055222.4s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.163s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 40.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 8.67s
                        Total time: 20040.45s
                               ETA: 1055101.9s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.423s, learning 0.230s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 40.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 8.65s
                        Total time: 20049.11s
                               ETA: 1054980.7s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.508s, learning 0.295s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 39.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 8.80s
                        Total time: 20057.91s
                               ETA: 1054867.6s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.332s, learning 0.162s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 40.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 8.49s
                        Total time: 20066.40s
                               ETA: 1054738.3s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.515s, learning 0.182s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 40.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 8.70s
                        Total time: 20075.10s
                               ETA: 1054619.8s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.255s, learning 0.200s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.79
               Mean episode length: 41.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 8.45s
                        Total time: 20083.56s
                               ETA: 1054488.8s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.535s, learning 0.173s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.68
               Mean episode length: 41.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 8.71s
                        Total time: 20092.26s
                               ETA: 1054371.1s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.163s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 39.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 8.40s
                        Total time: 20100.67s
                               ETA: 1054237.5s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.582s, learning 0.196s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 8.78s
                        Total time: 20109.45s
                               ETA: 1054123.8s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.512s, learning 0.175s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 40.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 8.69s
                        Total time: 20118.13s
                               ETA: 1054005.4s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.500s, learning 0.194s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 40.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 8.69s
                        Total time: 20126.83s
                               ETA: 1053887.5s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.364s, learning 0.167s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 41.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 8.53s
                        Total time: 20135.36s
                               ETA: 1053761.1s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.238s, learning 0.167s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 8.40s
                        Total time: 20143.76s
                               ETA: 1053628.3s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.512s, learning 0.188s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 41.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 8.70s
                        Total time: 20152.46s
                               ETA: 1053511.0s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.625s, learning 0.205s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 8.83s
                        Total time: 20161.29s
                               ETA: 1053400.7s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.341s, learning 0.168s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 40.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 8.51s
                        Total time: 20169.80s
                               ETA: 1053273.7s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.467s, learning 0.263s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 8.73s
                        Total time: 20178.53s
                               ETA: 1053158.3s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.701s, learning 0.187s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 41.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 8.89s
                        Total time: 20187.42s
                               ETA: 1053051.3s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.300s, learning 0.180s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 40.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 8.48s
                        Total time: 20195.90s
                               ETA: 1052923.1s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.449s, learning 0.190s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 41.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 8.64s
                        Total time: 20204.54s
                               ETA: 1052803.4s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.556s, learning 0.343s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 8.90s
                        Total time: 20213.44s
                               ETA: 1052697.3s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.677s, learning 0.200s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 41.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 8.88s
                        Total time: 20222.31s
                               ETA: 1052590.2s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.304s, learning 0.203s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 41.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 8.51s
                        Total time: 20230.82s
                               ETA: 1052463.9s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.629s, learning 0.168s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 39.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.80s
                        Total time: 20239.62s
                               ETA: 1052352.8s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.397s, learning 0.194s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 41.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.59s
                        Total time: 20248.21s
                               ETA: 1052231.2s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.411s, learning 0.190s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 40.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 8.60s
                        Total time: 20256.81s
                               ETA: 1052110.1s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.097s, learning 0.174s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.73
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.27s
                        Total time: 20265.08s
                               ETA: 1051972.1s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.313s, learning 0.184s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 41.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 8.50s
                        Total time: 20273.58s
                               ETA: 1051845.9s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.395s, learning 0.167s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 8.56s
                        Total time: 20282.14s
                               ETA: 1051723.3s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.410s, learning 0.171s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 40.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 8.58s
                        Total time: 20290.72s
                               ETA: 1051601.7s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.380s, learning 0.244s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 42.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 8.62s
                        Total time: 20299.35s
                               ETA: 1051482.5s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.758s, learning 0.177s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 8.93s
                        Total time: 20308.28s
                               ETA: 1051379.5s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.524s, learning 0.155s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0367
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 8.68s
                        Total time: 20316.96s
                               ETA: 1051263.3s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.599s, learning 0.158s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 41.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 8.76s
                        Total time: 20325.72s
                               ETA: 1051151.3s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.562s, learning 0.159s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 40.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 8.72s
                        Total time: 20334.44s
                               ETA: 1051037.6s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.555s, learning 0.162s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.80
               Mean episode length: 41.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 8.72s
                        Total time: 20343.15s
                               ETA: 1050923.7s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.509s, learning 0.195s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 41.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 8.70s
                        Total time: 20351.86s
                               ETA: 1050809.3s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.167s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 40.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 8.64s
                        Total time: 20360.50s
                               ETA: 1050691.8s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.162s, learning 0.184s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 41.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 8.35s
                        Total time: 20368.85s
                               ETA: 1050559.1s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.166s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0404
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 39.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 8.67s
                        Total time: 20377.51s
                               ETA: 1050443.0s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.172s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 8.46s
                        Total time: 20385.97s
                               ETA: 1050316.3s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.163s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 40.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 8.51s
                        Total time: 20394.48s
                               ETA: 1050192.6s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.783s, learning 0.194s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0347
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 40.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 8.98s
                        Total time: 20403.46s
                               ETA: 1050092.9s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.497s, learning 0.190s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 41.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 8.69s
                        Total time: 20412.14s
                               ETA: 1049978.4s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.279s, learning 0.168s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 41.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 8.45s
                        Total time: 20420.59s
                               ETA: 1049851.7s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.550s, learning 0.223s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 8.77s
                        Total time: 20429.36s
                               ETA: 1049741.8s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.169s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 42.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 8.55s
                        Total time: 20437.92s
                               ETA: 1049620.7s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.537s, learning 0.158s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 40.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 8.70s
                        Total time: 20446.61s
                               ETA: 1049507.1s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.296s, learning 0.168s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 40.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 8.46s
                        Total time: 20455.08s
                               ETA: 1049381.7s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.655s, learning 0.177s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 40.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 8.83s
                        Total time: 20463.91s
                               ETA: 1049275.4s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.661s, learning 0.187s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 40.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.85s
                        Total time: 20472.76s
                               ETA: 1049169.9s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.439s, learning 0.161s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 8.60s
                        Total time: 20481.36s
                               ETA: 1049051.8s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.373s, learning 0.186s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 8.56s
                        Total time: 20489.91s
                               ETA: 1048931.8s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.711s, learning 0.335s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 41.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 9.05s
                        Total time: 20498.96s
                               ETA: 1048836.8s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.065s, learning 0.163s)
               Value function loss: 28.9468
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 4.07
               Mean episode length: 40.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 8.23s
                        Total time: 20507.19s
                               ETA: 1048700.0s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.376s, learning 0.210s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 41.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.59s
                        Total time: 20515.77s
                               ETA: 1048581.6s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.442s, learning 0.181s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 41.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 8.62s
                        Total time: 20524.40s
                               ETA: 1048465.3s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.285s, learning 0.192s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 40.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 8.48s
                        Total time: 20532.87s
                               ETA: 1048341.6s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.085s, learning 0.178s)
               Value function loss: 0.0505
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 41.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.26s
                        Total time: 20541.14s
                               ETA: 1048207.2s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.220s, learning 0.166s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 40.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.39s
                        Total time: 20549.52s
                               ETA: 1048079.1s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.461s, learning 0.189s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 41.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 8.65s
                        Total time: 20558.17s
                               ETA: 1047964.6s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.166s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 40.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.59s
                        Total time: 20566.76s
                               ETA: 1047847.1s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.599s, learning 0.243s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 8.84s
                        Total time: 20575.60s
                               ETA: 1047742.6s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.776s, learning 0.164s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.45
               Mean episode length: 40.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 8.94s
                        Total time: 20584.54s
                               ETA: 1047643.2s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.165s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 8.63s
                        Total time: 20593.18s
                               ETA: 1047528.3s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.424s, learning 0.237s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 41.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 8.66s
                        Total time: 20601.84s
                               ETA: 1047414.9s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.410s, learning 0.203s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 40.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.61s
                        Total time: 20610.45s
                               ETA: 1047299.2s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.162s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 40.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.48s
                        Total time: 20618.93s
                               ETA: 1047176.8s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.806s, learning 0.187s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 41.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.99s
                        Total time: 20627.92s
                               ETA: 1047080.6s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.505s, learning 0.161s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 41.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 8.67s
                        Total time: 20636.59s
                               ETA: 1046967.9s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.427s, learning 0.164s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 41.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 8.59s
                        Total time: 20645.18s
                               ETA: 1046851.5s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.489s, learning 0.219s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 41.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 8.71s
                        Total time: 20653.89s
                               ETA: 1046741.2s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.481s, learning 0.193s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 40.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 8.67s
                        Total time: 20662.56s
                               ETA: 1046629.2s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.579s, learning 0.188s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 41.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 9.77s
                        Total time: 20672.33s
                               ETA: 1046572.6s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.500s, learning 0.169s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 40.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 16.67s
                        Total time: 20689.00s
                               ETA: 1046865.4s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.466s, learning 0.175s)
               Value function loss: 0.0446
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.88
               Mean episode length: 42.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 16.64s
                        Total time: 20705.64s
                               ETA: 1047156.4s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.580s, learning 0.167s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 40.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 16.75s
                        Total time: 20722.38s
                               ETA: 1047452.5s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.933s, learning 0.165s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 1.86
               Mean episode length: 42.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 16.10s
                        Total time: 20738.48s
                               ETA: 1047715.4s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.439s, learning 0.207s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 40.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 16.65s
                        Total time: 20755.13s
                               ETA: 1048005.7s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.155s, learning 0.169s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 41.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 16.32s
                        Total time: 20771.45s
                               ETA: 1048279.5s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.199s, learning 0.165s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 41.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 16.36s
                        Total time: 20787.82s
                               ETA: 1048555.0s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.612s, learning 0.178s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 41.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 16.79s
                        Total time: 20804.61s
                               ETA: 1048851.7s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.237s, learning 0.206s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0342
             Mean action noise std: 0.72
                       Mean reward: 1.86
               Mean episode length: 42.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 16.44s
                        Total time: 20821.05s
                               ETA: 1049130.5s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.796s, learning 0.190s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0353
             Mean action noise std: 0.72
                       Mean reward: 1.87
               Mean episode length: 41.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 16.99s
                        Total time: 20838.04s
                               ETA: 1049436.4s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.357s, learning 0.177s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 1.81
               Mean episode length: 41.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 16.53s
                        Total time: 20854.57s
                               ETA: 1049719.3s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.410s, learning 0.171s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 41.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 16.58s
                        Total time: 20871.15s
                               ETA: 1050004.1s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.924s, learning 0.172s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 41.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 17.10s
                        Total time: 20888.25s
                               ETA: 1050314.6s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.504s, learning 0.179s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.79
               Mean episode length: 41.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 16.68s
                        Total time: 20904.93s
                               ETA: 1050604.0s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.978s, learning 0.167s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 41.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 16.14s
                        Total time: 20921.07s
                               ETA: 1050866.0s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.516s, learning 0.256s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 41.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 16.77s
                        Total time: 20937.85s
                               ETA: 1051159.2s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.678s, learning 0.168s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 2.07
               Mean episode length: 41.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 16.85s
                        Total time: 20954.69s
                               ETA: 1051455.8s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.407s, learning 0.181s)
               Value function loss: 0.0455
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 42.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 16.59s
                        Total time: 20971.28s
                               ETA: 1051739.2s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.569s, learning 0.186s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0388
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 41.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 16.75s
                        Total time: 20988.03s
                               ETA: 1052030.6s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.730s, learning 0.171s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 1.67
               Mean episode length: 41.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 16.90s
                        Total time: 21004.93s
                               ETA: 1052329.0s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.906s, learning 0.174s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 41.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 17.08s
                        Total time: 21022.01s
                               ETA: 1052636.0s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.840s, learning 0.163s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 2.00
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 17.00s
                        Total time: 21039.02s
                               ETA: 1052938.9s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.395s, learning 0.163s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 2.03
               Mean episode length: 42.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 16.56s
                        Total time: 21055.58s
                               ETA: 1053219.2s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.293s, learning 0.224s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 16.52s
                        Total time: 21072.09s
                               ETA: 1053497.2s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.371s, learning 0.194s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 41.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 16.56s
                        Total time: 21088.66s
                               ETA: 1053777.2s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.378s, learning 0.165s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 1.52
               Mean episode length: 41.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 16.54s
                        Total time: 21105.20s
                               ETA: 1054055.8s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.505s, learning 0.188s)
               Value function loss: 0.0502
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 42.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 16.69s
                        Total time: 21121.89s
                               ETA: 1054341.7s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.504s, learning 0.175s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 16.68s
                        Total time: 21138.57s
                               ETA: 1054626.5s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.811s, learning 0.207s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 41.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 17.02s
                        Total time: 21155.59s
                               ETA: 1054927.9s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.570s, learning 0.168s)
               Value function loss: 0.0489
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 41.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 16.74s
                        Total time: 21172.33s
                               ETA: 1055215.1s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.706s, learning 0.172s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 16.88s
                        Total time: 21189.21s
                               ETA: 1055508.9s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.548s, learning 0.259s)
               Value function loss: 0.0479
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 16.81s
                        Total time: 21206.01s
                               ETA: 1055798.8s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.680s, learning 0.166s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 41.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 16.85s
                        Total time: 21222.86s
                               ETA: 1056090.4s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.416s, learning 0.170s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 16.59s
                        Total time: 21239.45s
                               ETA: 1056368.8s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.362s, learning 0.168s)
               Value function loss: 0.0484
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 16.53s
                        Total time: 21255.98s
                               ETA: 1056644.0s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.085s, learning 0.164s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 16.25s
                        Total time: 21272.22s
                               ETA: 1056905.0s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.937s, learning 0.172s)
               Value function loss: 64.1642
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 41.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 16.11s
                        Total time: 21288.33s
                               ETA: 1057158.8s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.077s, learning 0.194s)
               Value function loss: 0.1981
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 6.99
               Mean episode length: 42.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 12.27s
                        Total time: 21300.60s
                               ETA: 1057221.8s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.538s, learning 0.194s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 42.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.73s
                        Total time: 21309.34s
                               ETA: 1057109.1s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.386s, learning 0.171s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.56s
                        Total time: 21317.89s
                               ETA: 1056987.9s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.503s, learning 0.204s)
               Value function loss: 0.0629
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.71s
                        Total time: 21326.60s
                               ETA: 1056874.2s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.500s, learning 0.300s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 41.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.80s
                        Total time: 21335.40s
                               ETA: 1056765.3s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.799s, learning 0.171s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 2.43
               Mean episode length: 44.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.97s
                        Total time: 21344.37s
                               ETA: 1056664.8s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.166s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 43.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.36s
                        Total time: 21352.73s
                               ETA: 1056534.5s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.410s, learning 0.168s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 42.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 8.58s
                        Total time: 21361.31s
                               ETA: 1056414.9s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.977s, learning 0.167s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.14s
                        Total time: 21369.46s
                               ETA: 1056274.0s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.718s, learning 0.191s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.91s
                        Total time: 21378.36s
                               ETA: 1056170.9s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.108s, learning 0.198s)
               Value function loss: 0.0559
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.31s
                        Total time: 21386.67s
                               ETA: 1056038.2s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.166s)
               Value function loss: 15.3136
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 42.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.51s
                        Total time: 21395.18s
                               ETA: 1055915.8s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.486s, learning 0.197s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 42.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.68s
                        Total time: 21403.86s
                               ETA: 1055801.9s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.436s, learning 0.226s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 42.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.66s
                        Total time: 21412.53s
                               ETA: 1055687.1s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.092s, learning 0.186s)
               Value function loss: 16.6830
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.28s
                        Total time: 21420.80s
                               ETA: 1055553.4s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.195s, learning 0.173s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.37s
                        Total time: 21429.17s
                               ETA: 1055424.3s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.061s, learning 0.165s)
               Value function loss: 15.1945
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.23s
                        Total time: 21437.40s
                               ETA: 1055288.5s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.164s, learning 0.166s)
               Value function loss: 0.2677
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 41.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 8.33s
                        Total time: 21445.73s
                               ETA: 1055157.8s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.082s, learning 0.202s)
               Value function loss: 0.1358
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 4.70
               Mean episode length: 42.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.28s
                        Total time: 21454.01s
                               ETA: 1055025.0s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.631s, learning 0.165s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.80s
                        Total time: 21462.81s
                               ETA: 1054917.4s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.546s, learning 0.194s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 41.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.74s
                        Total time: 21471.55s
                               ETA: 1054807.2s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.216s, learning 0.309s)
               Value function loss: 0.0648
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.52s
                        Total time: 21480.07s
                               ETA: 1054686.6s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.471s, learning 0.189s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 41.35
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.66s
                        Total time: 21488.73s
                               ETA: 1054572.6s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.716s, learning 0.166s)
               Value function loss: 16.5134
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 41.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.88s
                        Total time: 21497.61s
                               ETA: 1054469.7s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.694s, learning 0.170s)
               Value function loss: 0.1709
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 41.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.86s
                        Total time: 21506.48s
                               ETA: 1054366.1s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.440s, learning 0.270s)
               Value function loss: 3.9273
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 42.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 8.71s
                        Total time: 21515.19s
                               ETA: 1054254.9s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.160s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.40s
                        Total time: 21523.59s
                               ETA: 1054128.6s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.441s, learning 0.184s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.63s
                        Total time: 21532.21s
                               ETA: 1054013.6s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.267s, learning 0.168s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 43.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.43s
                        Total time: 21540.65s
                               ETA: 1053889.3s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.461s, learning 0.192s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 43.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.65s
                        Total time: 21549.30s
                               ETA: 1053775.8s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.371s, learning 0.294s)
               Value function loss: 13.9034
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 41.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.66s
                        Total time: 21557.96s
                               ETA: 1053663.0s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.672s, learning 0.164s)
               Value function loss: 0.1358
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 42.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.84s
                        Total time: 21566.80s
                               ETA: 1053558.6s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.422s, learning 0.186s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 41.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.61s
                        Total time: 21575.41s
                               ETA: 1053443.2s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.040s, learning 0.177s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 8.22s
                        Total time: 21583.62s
                               ETA: 1053308.8s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.655s, learning 0.190s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.65
               Mean episode length: 42.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.85s
                        Total time: 21592.47s
                               ETA: 1053205.3s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.301s, learning 0.164s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.46s
                        Total time: 21600.94s
                               ETA: 1053083.2s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.163s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.61s
                        Total time: 21609.55s
                               ETA: 1052968.5s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.442s, learning 0.174s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 41.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.62s
                        Total time: 21618.16s
                               ETA: 1052854.0s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.180s)
               Value function loss: 47.2797
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.30s
                        Total time: 21626.46s
                               ETA: 1052724.2s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.276s, learning 0.167s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 7.32
               Mean episode length: 43.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.44s
                        Total time: 21634.91s
                               ETA: 1052601.6s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.461s, learning 0.196s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 41.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.66s
                        Total time: 21643.56s
                               ETA: 1052489.4s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.396s, learning 0.215s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 43.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.61s
                        Total time: 21652.17s
                               ETA: 1052375.2s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.112s, learning 0.275s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.39s
                        Total time: 21660.56s
                               ETA: 1052250.1s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.159s, learning 0.269s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 42.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.43s
                        Total time: 21668.99s
                               ETA: 1052127.2s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.517s, learning 0.177s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.69s
                        Total time: 21677.68s
                               ETA: 1052017.2s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.529s, learning 0.194s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 42.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.72s
                        Total time: 21686.41s
                               ETA: 1051908.8s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.176s)
               Value function loss: 59.1695
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 42.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.51s
                        Total time: 21694.92s
                               ETA: 1051790.1s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.319s, learning 0.173s)
               Value function loss: 51.7262
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 41.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.49s
                        Total time: 21703.41s
                               ETA: 1051670.7s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.171s, learning 0.195s)
               Value function loss: 4.5007
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 41.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.37s
                        Total time: 21711.77s
                               ETA: 1051545.3s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.281s, learning 0.177s)
               Value function loss: 0.6447
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.46s
                        Total time: 21720.23s
                               ETA: 1051424.5s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.983s, learning 0.174s)
               Value function loss: 0.2242
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.16s
                        Total time: 21728.39s
                               ETA: 1051289.2s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.118s, learning 0.197s)
               Value function loss: 51.0924
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 42.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 8.31s
                        Total time: 21736.70s
                               ETA: 1051161.6s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.898s, learning 0.172s)
               Value function loss: 14.9272
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.07s
                        Total time: 21744.77s
                               ETA: 1051022.4s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.613s, learning 0.206s)
               Value function loss: 0.5659
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.82s
                        Total time: 21753.59s
                               ETA: 1050919.5s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.333s, learning 0.191s)
               Value function loss: 54.3392
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 8.52s
                        Total time: 21762.12s
                               ETA: 1050802.4s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.359s, learning 0.181s)
               Value function loss: 7.4259
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 4.67
               Mean episode length: 42.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 8.54s
                        Total time: 21770.66s
                               ETA: 1050686.2s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.431s, learning 0.195s)
               Value function loss: 0.1526
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 43.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.63s
                        Total time: 21779.28s
                               ETA: 1050574.2s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.987s, learning 0.227s)
               Value function loss: 15.2121
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 43.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.21s
                        Total time: 21787.50s
                               ETA: 1050442.5s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.215s, learning 0.171s)
               Value function loss: 0.1699
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.39s
                        Total time: 21795.88s
                               ETA: 1050319.2s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.189s, learning 0.264s)
               Value function loss: 17.4321
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.45s
                        Total time: 21804.34s
                               ETA: 1050199.3s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.448s, learning 0.170s)
               Value function loss: 17.9958
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 43.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 8.62s
                        Total time: 21812.95s
                               ETA: 1050087.4s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.374s, learning 0.168s)
               Value function loss: 0.1645
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 43.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 8.54s
                        Total time: 21821.50s
                               ETA: 1049971.9s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.438s, learning 0.211s)
               Value function loss: 12.0549
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 41.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.65s
                        Total time: 21830.14s
                               ETA: 1049861.7s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.745s, learning 0.176s)
               Value function loss: 4.0429
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 42.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.92s
                        Total time: 21839.07s
                               ETA: 1049764.7s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.586s, learning 0.160s)
               Value function loss: 12.2878
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 43.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 8.75s
                        Total time: 21847.81s
                               ETA: 1049659.3s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.947s, learning 0.168s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 41.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 8.11s
                        Total time: 21855.93s
                               ETA: 1049523.7s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.456s, learning 0.167s)
               Value function loss: 0.0911
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 42.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.62s
                        Total time: 21864.55s
                               ETA: 1049412.7s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.332s, learning 0.176s)
               Value function loss: 64.2377
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 43.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 8.51s
                        Total time: 21873.06s
                               ETA: 1049296.2s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.504s, learning 0.213s)
               Value function loss: 3.9766
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 42.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.72s
                        Total time: 21881.78s
                               ETA: 1049189.9s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.390s, learning 0.192s)
               Value function loss: 3.9688
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.58s
                        Total time: 21890.36s
                               ETA: 1049077.2s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.191s, learning 0.167s)
               Value function loss: 14.1536
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 4.68
               Mean episode length: 42.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.36s
                        Total time: 21898.71s
                               ETA: 1048953.8s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.405s, learning 0.202s)
               Value function loss: 11.8119
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.61s
                        Total time: 21907.32s
                               ETA: 1048842.5s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.166s)
               Value function loss: 15.3882
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.42
               Mean episode length: 42.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 8.64s
                        Total time: 21915.96s
                               ETA: 1048732.6s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.653s, learning 0.197s)
               Value function loss: 15.3787
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 42.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.85s
                        Total time: 21924.81s
                               ETA: 1048633.2s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.121s, learning 0.167s)
               Value function loss: 11.8594
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 42.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.29s
                        Total time: 21933.10s
                               ETA: 1048506.9s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.709s, learning 0.165s)
               Value function loss: 0.3166
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.87s
                        Total time: 21941.97s
                               ETA: 1048408.7s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.891s, learning 0.190s)
               Value function loss: 439.0611
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 42.47
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 8.08s
                        Total time: 21950.05s
                               ETA: 1048272.8s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.589s, learning 0.187s)
               Value function loss: 4.0751
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 42.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.78s
                        Total time: 21958.83s
                               ETA: 1048170.2s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.165s, learning 0.244s)
               Value function loss: 4.0703
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 43.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.41s
                        Total time: 21967.24s
                               ETA: 1048050.1s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.370s, learning 0.266s)
               Value function loss: 0.1770
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 8.64s
                        Total time: 21975.87s
                               ETA: 1047941.0s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.167s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.55s
                        Total time: 21984.42s
                               ETA: 1047827.7s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.441s, learning 0.174s)
               Value function loss: 0.1395
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 8.62s
                        Total time: 21993.03s
                               ETA: 1047717.7s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.421s, learning 0.176s)
               Value function loss: 28.8527
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.86
               Mean episode length: 44.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.60s
                        Total time: 22001.63s
                               ETA: 1047607.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.512s, learning 0.208s)
               Value function loss: 3.8927
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 5.10
               Mean episode length: 44.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.72s
                        Total time: 22010.35s
                               ETA: 1047502.3s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.480s, learning 0.215s)
               Value function loss: 29.8063
                    Surrogate loss: 0.0027
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 43.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.69s
                        Total time: 22019.04s
                               ETA: 1047396.5s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.558s, learning 0.184s)
               Value function loss: 0.3392
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 4.77
               Mean episode length: 43.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 8.74s
                        Total time: 22027.79s
                               ETA: 1047293.0s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.264s, learning 0.192s)
               Value function loss: 0.2079
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 8.46s
                        Total time: 22036.24s
                               ETA: 1047176.0s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.218s, learning 0.170s)
               Value function loss: 99.2481
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 43.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.39s
                        Total time: 22044.63s
                               ETA: 1047055.8s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.912s, learning 0.203s)
               Value function loss: 0.2168
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 9.12s
                        Total time: 22053.75s
                               ETA: 1046970.3s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.467s, learning 0.185s)
               Value function loss: 16.6666
                    Surrogate loss: 0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 8.65s
                        Total time: 22062.40s
                               ETA: 1046862.9s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.590s, learning 0.212s)
               Value function loss: 0.1925
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.35
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 8.80s
                        Total time: 22071.20s
                               ETA: 1046762.7s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.516s, learning 0.166s)
               Value function loss: 17.7559
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 42.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.68s
                        Total time: 22079.88s
                               ETA: 1046657.0s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.872s, learning 0.182s)
               Value function loss: 0.2184
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 1.57
               Mean episode length: 42.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 9.05s
                        Total time: 22088.94s
                               ETA: 1046568.9s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.182s, learning 0.184s)
               Value function loss: 0.1454
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 42.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.37s
                        Total time: 22097.30s
                               ETA: 1046448.3s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.741s, learning 0.247s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 41.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.99s
                        Total time: 22106.29s
                               ETA: 1046357.3s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.308s, learning 0.188s)
               Value function loss: 0.1174
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.50s
                        Total time: 22114.79s
                               ETA: 1046243.1s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.607s, learning 0.175s)
               Value function loss: 27.3725
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 41.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.78s
                        Total time: 22123.57s
                               ETA: 1046142.5s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.310s, learning 0.191s)
               Value function loss: 0.1326
                    Surrogate loss: -0.0401
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.50s
                        Total time: 22132.07s
                               ETA: 1046028.7s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.657s, learning 0.168s)
               Value function loss: 0.0959
                    Surrogate loss: -0.0409
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 43.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.82s
                        Total time: 22140.89s
                               ETA: 1045930.3s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.628s, learning 0.161s)
               Value function loss: 47.0574
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 41.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.79s
                        Total time: 22149.68s
                               ETA: 1045830.3s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.484s, learning 0.171s)
               Value function loss: 0.1090
                    Surrogate loss: -0.0412
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.65s
                        Total time: 22158.34s
                               ETA: 1045724.1s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.504s, learning 0.273s)
               Value function loss: 17.0029
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.78s
                        Total time: 22167.12s
                               ETA: 1045623.7s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.171s)
               Value function loss: 13.9881
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 42.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.66s
                        Total time: 22175.78s
                               ETA: 1045518.0s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.124s, learning 0.165s)
               Value function loss: 0.2629
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 42.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.29s
                        Total time: 22184.07s
                               ETA: 1045394.8s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.005s, learning 0.158s)
               Value function loss: 0.1858
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 8.16s
                        Total time: 22192.23s
                               ETA: 1045265.8s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.555s, learning 0.184s)
               Value function loss: 31.1573
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.74s
                        Total time: 22200.97s
                               ETA: 1045164.1s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.643s, learning 0.182s)
               Value function loss: 9.8593
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 41.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.82s
                        Total time: 22209.80s
                               ETA: 1045066.4s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.658s, learning 0.158s)
               Value function loss: 0.2134
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.82s
                        Total time: 22218.61s
                               ETA: 1044968.4s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.164s)
               Value function loss: 11.8485
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.58s
                        Total time: 22227.19s
                               ETA: 1044859.3s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.462s, learning 0.277s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 41.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.74s
                        Total time: 22235.93s
                               ETA: 1044757.8s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.181s, learning 0.164s)
               Value function loss: 106.3560
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 8.35s
                        Total time: 22244.27s
                               ETA: 1044638.0s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.704s, learning 0.165s)
               Value function loss: 0.3439
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 9.84
               Mean episode length: 42.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.87s
                        Total time: 22253.14s
                               ETA: 1044542.8s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.402s, learning 0.195s)
               Value function loss: 0.2023
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 8.60s
                        Total time: 22261.74s
                               ETA: 1044435.0s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.572s, learning 0.169s)
               Value function loss: 0.1568
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 8.74s
                        Total time: 22270.48s
                               ETA: 1044334.1s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.012s, learning 0.167s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 41.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 8.18s
                        Total time: 22278.66s
                               ETA: 1044206.8s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.507s, learning 0.160s)
               Value function loss: 0.1088
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 8.67s
                        Total time: 22287.33s
                               ETA: 1044102.6s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.430s, learning 0.163s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.59s
                        Total time: 22295.92s
                               ETA: 1043994.9s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.352s, learning 0.214s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 41.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 8.57s
                        Total time: 22304.49s
                               ETA: 1043886.2s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.443s, learning 0.207s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 41.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.65s
                        Total time: 22313.14s
                               ETA: 1043781.4s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.160s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 8.61s
                        Total time: 22321.74s
                               ETA: 1043674.8s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.450s, learning 0.171s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 41.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.62s
                        Total time: 22330.37s
                               ETA: 1043568.9s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.221s, learning 0.165s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 41.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 8.39s
                        Total time: 22338.75s
                               ETA: 1043452.1s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.650s, learning 0.175s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 41.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 9.82s
                        Total time: 22348.58s
                               ETA: 1043402.5s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.066s, learning 0.165s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 40.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 16.23s
                        Total time: 22364.81s
                               ETA: 1043652.0s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.357s, learning 0.162s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 16.52s
                        Total time: 22381.33s
                               ETA: 1043914.6s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.287s, learning 0.200s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 16.49s
                        Total time: 22397.81s
                               ETA: 1044175.4s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.545s, learning 0.272s)
               Value function loss: 17.5433
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 41.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 16.82s
                        Total time: 22414.63s
                               ETA: 1044451.4s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.042s, learning 0.183s)
               Value function loss: 0.1423
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 16.22s
                        Total time: 22430.86s
                               ETA: 1044699.5s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.474s, learning 0.294s)
               Value function loss: 3.9243
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.66
               Mean episode length: 42.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 16.77s
                        Total time: 22447.62s
                               ETA: 1044972.7s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.610s, learning 0.179s)
               Value function loss: 0.0964
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 41.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 16.79s
                        Total time: 22464.41s
                               ETA: 1045246.6s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.074s, learning 0.181s)
               Value function loss: 13.8112
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 16.25s
                        Total time: 22480.67s
                               ETA: 1045495.3s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.561s, learning 0.281s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 4.47
               Mean episode length: 41.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 16.84s
                        Total time: 22497.51s
                               ETA: 1045771.1s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.527s, learning 0.172s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 1.58
               Mean episode length: 41.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 16.70s
                        Total time: 22514.21s
                               ETA: 1046039.9s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.532s, learning 0.222s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 16.75s
                        Total time: 22530.96s
                               ETA: 1046311.0s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.784s, learning 0.163s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 41.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 16.95s
                        Total time: 22547.91s
                               ETA: 1046590.9s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.810s, learning 0.170s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0375
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 41.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 16.98s
                        Total time: 22564.89s
                               ETA: 1046871.9s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.198s, learning 0.181s)
               Value function loss: 0.0667
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 40.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 16.38s
                        Total time: 22581.27s
                               ETA: 1047124.9s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.751s, learning 0.228s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 41.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 16.98s
                        Total time: 22598.25s
                               ETA: 1047405.3s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.455s, learning 0.249s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 42.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 16.70s
                        Total time: 22614.95s
                               ETA: 1047672.8s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.249s, learning 0.195s)
               Value function loss: 0.0532
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 42.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 16.44s
                        Total time: 22631.40s
                               ETA: 1047927.9s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.580s, learning 0.195s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 41.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 16.78s
                        Total time: 22648.17s
                               ETA: 1048198.1s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.040s, learning 0.164s)
               Value function loss: 15.4020
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 41.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 16.20s
                        Total time: 22664.38s
                               ETA: 1048441.6s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.342s, learning 0.252s)
               Value function loss: 16.4070
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 41.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 16.59s
                        Total time: 22680.97s
                               ETA: 1048702.9s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.695s, learning 0.162s)
               Value function loss: 0.2226
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 16.86s
                        Total time: 22697.83s
                               ETA: 1048976.2s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.554s, learning 0.160s)
               Value function loss: 0.1451
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 4.64
               Mean episode length: 41.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 16.71s
                        Total time: 22714.54s
                               ETA: 1049242.5s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.086s, learning 0.190s)
               Value function loss: 0.1196
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 40.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 16.28s
                        Total time: 22730.82s
                               ETA: 1049488.3s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.164s, learning 0.162s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 16.33s
                        Total time: 22747.14s
                               ETA: 1049736.2s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.475s, learning 0.195s)
               Value function loss: 64.7255
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 16.67s
                        Total time: 22763.81s
                               ETA: 1049999.7s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.207s, learning 0.166s)
               Value function loss: 21.2642
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 42.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 16.37s
                        Total time: 22780.19s
                               ETA: 1050249.3s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.415s, learning 0.184s)
               Value function loss: 0.2877
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 41.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 16.60s
                        Total time: 22796.79s
                               ETA: 1050509.0s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.416s, learning 0.172s)
               Value function loss: 0.1570
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 16.59s
                        Total time: 22813.38s
                               ETA: 1050768.0s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.645s, learning 0.178s)
               Value function loss: 13.7882
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 16.82s
                        Total time: 22830.20s
                               ETA: 1051037.5s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.383s, learning 0.233s)
               Value function loss: 0.1501
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 16.62s
                        Total time: 22846.81s
                               ETA: 1051297.2s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.829s, learning 0.161s)
               Value function loss: 36.2768
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 41.78
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 16.99s
                        Total time: 22863.80s
                               ETA: 1051573.9s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.234s, learning 0.195s)
               Value function loss: 0.1102
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 17.43s
                        Total time: 22881.23s
                               ETA: 1051870.4s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.665s, learning 0.160s)
               Value function loss: 15.4225
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 43.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 16.83s
                        Total time: 22898.06s
                               ETA: 1052138.9s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.590s, learning 0.193s)
               Value function loss: 11.6722
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 1.62
               Mean episode length: 40.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 16.78s
                        Total time: 22914.84s
                               ETA: 1052405.2s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.711s, learning 0.172s)
               Value function loss: 17.7894
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.94
               Mean episode length: 43.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 16.88s
                        Total time: 22931.72s
                               ETA: 1052675.9s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.516s, learning 0.164s)
               Value function loss: 0.1596
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 16.68s
                        Total time: 22948.40s
                               ETA: 1052936.9s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.209s, learning 0.163s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0410
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 16.37s
                        Total time: 22964.78s
                               ETA: 1053183.6s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.836s, learning 0.161s)
               Value function loss: 63.8454
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 42.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 13.00s
                        Total time: 22977.77s
                               ETA: 1053275.3s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.594s, learning 0.161s)
               Value function loss: 15.1229
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 8.75s
                        Total time: 22986.53s
                               ETA: 1053172.6s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.162s)
               Value function loss: 11.7700
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 41.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 8.58s
                        Total time: 22995.11s
                               ETA: 1053062.1s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.048s, learning 0.175s)
               Value function loss: 0.2808
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 8.22s
                        Total time: 23003.34s
                               ETA: 1052935.2s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.733s, learning 0.181s)
               Value function loss: 9.7917
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 42.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 8.91s
                        Total time: 23012.25s
                               ETA: 1052840.0s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.521s, learning 0.168s)
               Value function loss: 11.9365
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 42.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 8.69s
                        Total time: 23020.94s
                               ETA: 1052734.6s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.000s, learning 0.196s)
               Value function loss: 0.1573
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 8.20s
                        Total time: 23029.13s
                               ETA: 1052606.8s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.457s, learning 0.164s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 8.62s
                        Total time: 23037.76s
                               ETA: 1052498.5s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.201s, learning 0.193s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 41.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 8.39s
                        Total time: 23046.15s
                               ETA: 1052379.9s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.171s)
               Value function loss: 15.3422
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 43.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 8.59s
                        Total time: 23054.74s
                               ETA: 1052270.3s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.586s, learning 0.162s)
               Value function loss: 120.7226
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 42.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.75s
                        Total time: 23063.49s
                               ETA: 1052168.1s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.611s, learning 0.175s)
               Value function loss: 16.8856
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 42.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 8.79s
                        Total time: 23072.27s
                               ETA: 1052067.6s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.371s, learning 0.186s)
               Value function loss: 121.0084
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 1.60
               Mean episode length: 40.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 8.56s
                        Total time: 23080.83s
                               ETA: 1051956.9s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.414s, learning 0.161s)
               Value function loss: 1.5121
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.57s
                        Total time: 23089.40s
                               ETA: 1051847.0s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.526s, learning 0.163s)
               Value function loss: 0.5587
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 8.69s
                        Total time: 23098.09s
                               ETA: 1051742.4s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.160s)
               Value function loss: 0.4171
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 41.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.27s
                        Total time: 23106.36s
                               ETA: 1051619.0s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.609s, learning 0.296s)
               Value function loss: 75.5117
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 42.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.90s
                        Total time: 23115.27s
                               ETA: 1051524.5s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.588s, learning 0.173s)
               Value function loss: 18.7734
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 42.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.76s
                        Total time: 23124.03s
                               ETA: 1051423.5s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.100s, learning 0.165s)
               Value function loss: 0.6796
                    Surrogate loss: -0.0271
             Mean action noise std: 0.71
                       Mean reward: 9.16
               Mean episode length: 41.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 8.27s
                        Total time: 23132.30s
                               ETA: 1051300.0s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.785s, learning 0.170s)
               Value function loss: 7.2597
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 6.97
               Mean episode length: 42.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 7.96s
                        Total time: 23140.25s
                               ETA: 1051162.6s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.477s, learning 0.160s)
               Value function loss: 0.2011
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.64s
                        Total time: 23148.89s
                               ETA: 1051056.2s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.711s, learning 0.219s)
               Value function loss: 15.4558
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.93s
                        Total time: 23157.82s
                               ETA: 1050963.2s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.424s, learning 0.164s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.59s
                        Total time: 23166.41s
                               ETA: 1050854.8s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.529s, learning 0.171s)
               Value function loss: 20.3759
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.70s
                        Total time: 23175.11s
                               ETA: 1050751.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.072s, learning 0.167s)
               Value function loss: 0.2018
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 42.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.24s
                        Total time: 23183.34s
                               ETA: 1050627.5s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.340s, learning 0.166s)
               Value function loss: 0.1595
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 41.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 8.51s
                        Total time: 23191.85s
                               ETA: 1050515.7s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.303s, learning 0.164s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 40.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 8.47s
                        Total time: 23200.32s
                               ETA: 1050402.2s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.438s, learning 0.172s)
               Value function loss: 15.2304
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.61s
                        Total time: 23208.93s
                               ETA: 1050295.2s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.768s, learning 0.177s)
               Value function loss: 16.6163
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 40.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.94s
                        Total time: 23217.87s
                               ETA: 1050203.5s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.632s, learning 0.189s)
               Value function loss: 14.9496
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.82s
                        Total time: 23226.69s
                               ETA: 1050106.3s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.613s, learning 0.164s)
               Value function loss: 0.3873
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 41.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.78s
                        Total time: 23235.47s
                               ETA: 1050007.1s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.236s, learning 0.169s)
               Value function loss: 64.5025
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 40.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 8.40s
                        Total time: 23243.87s
                               ETA: 1049891.3s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.834s, learning 0.180s)
               Value function loss: 0.2209
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 9.01s
                        Total time: 23252.89s
                               ETA: 1049803.0s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.551s, learning 0.230s)
               Value function loss: 0.1533
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 7.00
               Mean episode length: 41.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 8.78s
                        Total time: 23261.67s
                               ETA: 1049704.3s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.449s, learning 0.236s)
               Value function loss: 0.1223
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.68s
                        Total time: 23270.36s
                               ETA: 1049601.4s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.398s, learning 0.203s)
               Value function loss: 0.1246
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 41.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.60s
                        Total time: 23278.96s
                               ETA: 1049494.8s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.454s, learning 0.295s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 41.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.75s
                        Total time: 23287.71s
                               ETA: 1049394.9s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.524s, learning 0.199s)
               Value function loss: 35.2424
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 41.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 8.72s
                        Total time: 23296.43s
                               ETA: 1049293.9s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.443s, learning 0.166s)
               Value function loss: 17.9726
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 41.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.61s
                        Total time: 23305.04s
                               ETA: 1049187.8s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.569s, learning 0.191s)
               Value function loss: 3.8442
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.76s
                        Total time: 23313.80s
                               ETA: 1049088.7s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.718s, learning 0.160s)
               Value function loss: 9.7837
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 7.05
               Mean episode length: 42.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 8.88s
                        Total time: 23322.68s
                               ETA: 1048995.0s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.220s, learning 0.188s)
               Value function loss: 0.1476
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 8.41s
                        Total time: 23331.08s
                               ETA: 1048880.2s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.210s, learning 0.186s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.40s
                        Total time: 23339.48s
                               ETA: 1048764.9s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.390s, learning 0.158s)
               Value function loss: 0.1142
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.55s
                        Total time: 23348.03s
                               ETA: 1048656.6s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.187s, learning 0.191s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0389
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 41.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.38s
                        Total time: 23356.41s
                               ETA: 1048540.8s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.042s, learning 0.164s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0409
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.21s
                        Total time: 23364.61s
                               ETA: 1048417.3s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.306s, learning 0.160s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0398
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 40.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.47s
                        Total time: 23373.08s
                               ETA: 1048305.5s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.465s, learning 0.210s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0403
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.67s
                        Total time: 23381.75s
                               ETA: 1048203.3s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.778s, learning 0.162s)
               Value function loss: 29.8352
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 41.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.94s
                        Total time: 23390.69s
                               ETA: 1048113.0s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.366s, learning 0.183s)
               Value function loss: 17.6074
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.55s
                        Total time: 23399.24s
                               ETA: 1048005.2s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.481s, learning 0.192s)
               Value function loss: 0.1603
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 41.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.67s
                        Total time: 23407.91s
                               ETA: 1047903.1s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.470s, learning 0.182s)
               Value function loss: 0.1653
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 43.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.65s
                        Total time: 23416.56s
                               ETA: 1047800.2s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.544s, learning 0.160s)
               Value function loss: 0.1031
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.70s
                        Total time: 23425.27s
                               ETA: 1047699.6s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.467s, learning 0.183s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0383
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 43.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.65s
                        Total time: 23433.92s
                               ETA: 1047596.8s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.525s, learning 0.170s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0394
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 41.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.69s
                        Total time: 23442.61s
                               ETA: 1047496.0s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.985s, learning 0.159s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.14s
                        Total time: 23450.76s
                               ETA: 1047370.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.492s, learning 0.172s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 1.77
               Mean episode length: 42.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.66s
                        Total time: 23459.42s
                               ETA: 1047268.8s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.258s, learning 0.202s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.46s
                        Total time: 23467.88s
                               ETA: 1047157.8s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.467s, learning 0.261s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0390
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 8.73s
                        Total time: 23476.61s
                               ETA: 1047058.9s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.543s, learning 0.173s)
               Value function loss: 15.4673
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.72s
                        Total time: 23485.32s
                               ETA: 1046959.5s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.178s)
               Value function loss: 7.0305
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 42.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.30s
                        Total time: 23493.62s
                               ETA: 1046841.5s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.170s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 42.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.45s
                        Total time: 23502.07s
                               ETA: 1046730.6s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.216s, learning 0.161s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.38s
                        Total time: 23510.45s
                               ETA: 1046616.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.918s, learning 0.161s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0401
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 41.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 9.08s
                        Total time: 23519.53s
                               ETA: 1046533.5s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.291s, learning 0.170s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.46s
                        Total time: 23527.99s
                               ETA: 1046423.2s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.437s, learning 0.219s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0383
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.66s
                        Total time: 23536.65s
                               ETA: 1046321.7s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.376s, learning 0.209s)
               Value function loss: 0.0935
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 41.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.59s
                        Total time: 23545.23s
                               ETA: 1046217.1s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.633s, learning 0.186s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 8.82s
                        Total time: 23554.05s
                               ETA: 1046123.0s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.378s, learning 0.174s)
               Value function loss: 7.1003
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 8.55s
                        Total time: 23562.61s
                               ETA: 1046017.1s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.171s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 4.73
               Mean episode length: 42.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.37s
                        Total time: 23570.98s
                               ETA: 1045903.3s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.153s, learning 0.182s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 41.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.33s
                        Total time: 23579.31s
                               ETA: 1045787.9s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.221s, learning 0.180s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.40s
                        Total time: 23587.71s
                               ETA: 1045675.6s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.313s, learning 0.173s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.49s
                        Total time: 23596.20s
                               ETA: 1045567.2s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.553s, learning 0.192s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0380
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 42.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.74s
                        Total time: 23604.94s
                               ETA: 1045470.3s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.593s, learning 0.192s)
               Value function loss: 71.7183
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 41.62
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 8.78s
                        Total time: 23613.73s
                               ETA: 1045375.2s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.343s, learning 0.171s)
               Value function loss: 17.4768
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 42.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 8.51s
                        Total time: 23622.24s
                               ETA: 1045268.2s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.353s, learning 0.179s)
               Value function loss: 0.2888
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 43.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.53s
                        Total time: 23630.78s
                               ETA: 1045162.2s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.359s, learning 0.182s)
               Value function loss: 0.2163
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.54s
                        Total time: 23639.32s
                               ETA: 1045056.6s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.485s, learning 0.172s)
               Value function loss: 0.1463
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.66s
                        Total time: 23647.97s
                               ETA: 1044956.2s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.334s, learning 0.195s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.53s
                        Total time: 23656.50s
                               ETA: 1044850.3s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.144s, learning 0.257s)
               Value function loss: 12.0463
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 42.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.40s
                        Total time: 23664.90s
                               ETA: 1044738.7s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.427s, learning 0.195s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0419
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 43.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.62s
                        Total time: 23673.52s
                               ETA: 1044637.0s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.515s, learning 0.166s)
               Value function loss: 0.0897
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 2.41
               Mean episode length: 43.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.68s
                        Total time: 23682.21s
                               ETA: 1044538.1s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.649s, learning 0.176s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 2.84
               Mean episode length: 44.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.83s
                        Total time: 23691.03s
                               ETA: 1044445.5s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.491s, learning 0.163s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.65s
                        Total time: 23699.69s
                               ETA: 1044345.5s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.473s, learning 0.174s)
               Value function loss: 16.4985
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 8.65s
                        Total time: 23708.33s
                               ETA: 1044245.2s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.268s, learning 0.187s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0416
             Mean action noise std: 0.71
                       Mean reward: 2.28
               Mean episode length: 43.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.46s
                        Total time: 23716.79s
                               ETA: 1044136.7s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.364s, learning 0.167s)
               Value function loss: 17.8992
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 8.53s
                        Total time: 23725.32s
                               ETA: 1044031.5s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.383s, learning 0.166s)
               Value function loss: 3.8647
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 43.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 8.55s
                        Total time: 23733.87s
                               ETA: 1043927.1s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.888s, learning 0.237s)
               Value function loss: 0.1741
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 6.97
               Mean episode length: 42.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 9.13s
                        Total time: 23742.99s
                               ETA: 1043848.3s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.273s, learning 0.196s)
               Value function loss: 81.5701
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 43.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 8.47s
                        Total time: 23751.46s
                               ETA: 1043740.6s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.173s)
               Value function loss: 4.0259
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 8.46s
                        Total time: 23759.92s
                               ETA: 1043632.5s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.222s, learning 0.249s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0417
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 8.47s
                        Total time: 23768.39s
                               ETA: 1043525.0s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.113s, learning 0.178s)
               Value function loss: 17.7835
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 42.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 8.29s
                        Total time: 23776.68s
                               ETA: 1043409.9s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.363s, learning 0.200s)
               Value function loss: 3.9636
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 2.41
               Mean episode length: 43.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.56s
                        Total time: 23785.24s
                               ETA: 1043306.7s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.547s, learning 0.181s)
               Value function loss: 0.1529
                    Surrogate loss: -0.0405
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 8.73s
                        Total time: 23793.97s
                               ETA: 1043210.9s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.038s, learning 0.189s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0380
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.23s
                        Total time: 23802.20s
                               ETA: 1043093.1s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.163s, learning 0.182s)
               Value function loss: 13.7131
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 8.34s
                        Total time: 23810.54s
                               ETA: 1042980.6s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.206s, learning 0.167s)
               Value function loss: 7.1066
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 41.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 8.37s
                        Total time: 23818.91s
                               ETA: 1042869.5s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.456s, learning 0.232s)
               Value function loss: 3.8760
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 43.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 8.69s
                        Total time: 23827.60s
                               ETA: 1042772.2s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.410s, learning 0.268s)
               Value function loss: 17.5811
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 4.74
               Mean episode length: 43.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.68s
                        Total time: 23836.28s
                               ETA: 1042674.6s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.715s, learning 0.163s)
               Value function loss: 0.1829
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 4.56
               Mean episode length: 42.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.88s
                        Total time: 23845.16s
                               ETA: 1042585.8s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.519s, learning 0.167s)
               Value function loss: 0.1445
                    Surrogate loss: -0.0383
             Mean action noise std: 0.71
                       Mean reward: 2.44
               Mean episode length: 43.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 8.69s
                        Total time: 23853.84s
                               ETA: 1042488.7s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.380s, learning 0.180s)
               Value function loss: 230.6838
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.40
               Mean episode length: 43.13
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 8.56s
                        Total time: 23862.40s
                               ETA: 1042386.1s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.142s, learning 0.199s)
               Value function loss: 0.1978
                    Surrogate loss: -0.0402
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 8.34s
                        Total time: 23870.74s
                               ETA: 1042274.1s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.404s, learning 0.175s)
               Value function loss: 4.0100
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 12.43
               Mean episode length: 43.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 8.58s
                        Total time: 23879.32s
                               ETA: 1042172.5s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.307s, learning 0.159s)
               Value function loss: 17.0658
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.69
               Mean episode length: 42.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 8.47s
                        Total time: 23887.79s
                               ETA: 1042066.2s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.600s, learning 0.169s)
               Value function loss: 59.6104
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.77s
                        Total time: 23896.56s
                               ETA: 1041973.1s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.744s, learning 0.164s)
               Value function loss: 30.7219
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 8.91s
                        Total time: 23905.47s
                               ETA: 1041886.1s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.178s, learning 0.193s)
               Value function loss: 4.5927
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 43.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.37s
                        Total time: 23913.84s
                               ETA: 1041775.9s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.411s, learning 0.240s)
               Value function loss: 0.3538
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 42.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.65s
                        Total time: 23922.49s
                               ETA: 1041677.9s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.690s, learning 0.162s)
               Value function loss: 0.1960
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 2.83
               Mean episode length: 44.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.85s
                        Total time: 23931.34s
                               ETA: 1041588.7s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.184s, learning 0.189s)
               Value function loss: 34.3347
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 41.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.37s
                        Total time: 23939.71s
                               ETA: 1041478.8s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.121s, learning 0.182s)
               Value function loss: 214.1233
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 2.48
               Mean episode length: 44.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.30s
                        Total time: 23948.02s
                               ETA: 1041365.9s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.626s, learning 0.173s)
               Value function loss: 80.4444
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 7.31
               Mean episode length: 43.10
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.80s
                        Total time: 23956.82s
                               ETA: 1041274.6s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.286s, learning 0.222s)
               Value function loss: 28.6057
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 4.91
               Mean episode length: 44.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.51s
                        Total time: 23965.32s
                               ETA: 1041170.8s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.167s)
               Value function loss: 57.2263
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.59
               Mean episode length: 43.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.55s
                        Total time: 23973.88s
                               ETA: 1041069.0s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.445s, learning 0.173s)
               Value function loss: 2.8713
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 4.98
               Mean episode length: 44.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.62s
                        Total time: 23982.49s
                               ETA: 1040970.1s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.174s)
               Value function loss: 0.2794
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 7.26
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.65s
                        Total time: 23991.15s
                               ETA: 1040872.8s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.423s, learning 0.159s)
               Value function loss: 54.1176
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 2.58
               Mean episode length: 44.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.58s
                        Total time: 23999.73s
                               ETA: 1040772.6s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.079s, learning 0.166s)
               Value function loss: 67.9552
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.42
               Mean episode length: 42.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.24s
                        Total time: 24007.97s
                               ETA: 1040657.8s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.394s, learning 0.162s)
               Value function loss: 0.2569
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 4.97
               Mean episode length: 43.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.56s
                        Total time: 24016.53s
                               ETA: 1040556.5s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.256s, learning 0.169s)
               Value function loss: 0.2178
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 7.73
               Mean episode length: 44.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 8.42s
                        Total time: 24024.95s
                               ETA: 1040449.7s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.317s, learning 0.158s)
               Value function loss: 73.3596
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 43.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.47s
                        Total time: 24033.43s
                               ETA: 1040345.1s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.468s, learning 0.169s)
               Value function loss: 66.7280
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.86
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.64s
                        Total time: 24042.06s
                               ETA: 1040247.6s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.581s, learning 0.188s)
               Value function loss: 17.6296
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 4.95
               Mean episode length: 43.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 10.77s
                        Total time: 24052.83s
                               ETA: 1040242.5s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.442s, learning 0.165s)
               Value function loss: 29.5507
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 43.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 16.61s
                        Total time: 24069.44s
                               ETA: 1040489.6s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.582s, learning 0.174s)
               Value function loss: 0.3560
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 16.76s
                        Total time: 24086.20s
                               ETA: 1040743.0s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.353s, learning 0.191s)
               Value function loss: 0.2670
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 16.54s
                        Total time: 24102.74s
                               ETA: 1040987.0s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.335s, learning 0.188s)
               Value function loss: 0.2095
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.51
               Mean episode length: 43.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 16.52s
                        Total time: 24119.26s
                               ETA: 1041229.9s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.623s, learning 0.231s)
               Value function loss: 0.1982
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 16.85s
                        Total time: 24136.12s
                               ETA: 1041486.8s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.668s, learning 0.163s)
               Value function loss: 37.3025
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 43.78
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 16.83s
                        Total time: 24152.95s
                               ETA: 1041742.5s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.273s, learning 0.233s)
               Value function loss: 14.1310
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.57
               Mean episode length: 43.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 16.51s
                        Total time: 24169.45s
                               ETA: 1041983.9s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.386s, learning 0.166s)
               Value function loss: 0.2758
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 43.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 16.55s
                        Total time: 24186.01s
                               ETA: 1042227.1s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.593s, learning 0.235s)
               Value function loss: 3.9631
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 16.83s
                        Total time: 24202.83s
                               ETA: 1042481.9s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.464s, learning 0.216s)
               Value function loss: 0.1486
                    Surrogate loss: -0.0375
             Mean action noise std: 0.71
                       Mean reward: 4.51
               Mean episode length: 42.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 16.68s
                        Total time: 24219.52s
                               ETA: 1042730.1s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.538s, learning 0.176s)
               Value function loss: 9.6441
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 42.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 16.71s
                        Total time: 24236.23s
                               ETA: 1042979.6s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.167s, learning 0.229s)
               Value function loss: 64.1820
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.84
               Mean episode length: 43.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 16.40s
                        Total time: 24252.62s
                               ETA: 1043215.1s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 942 steps/s (collection: 17.190s, learning 0.191s)
               Value function loss: 17.7364
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.67
               Mean episode length: 44.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 17.38s
                        Total time: 24270.01s
                               ETA: 1043492.8s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.646s, learning 0.161s)
               Value function loss: 0.2612
                    Surrogate loss: -0.0400
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 43.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 16.81s
                        Total time: 24286.81s
                               ETA: 1043745.5s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.668s, learning 0.189s)
               Value function loss: 0.1970
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 4.57
               Mean episode length: 42.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 16.86s
                        Total time: 24303.67s
                               ETA: 1044000.2s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.603s, learning 0.163s)
               Value function loss: 39.1062
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 16.77s
                        Total time: 24320.44s
                               ETA: 1044250.7s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.561s, learning 0.237s)
               Value function loss: 53.8090
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 16.80s
                        Total time: 24337.23s
                               ETA: 1044502.3s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.453s, learning 0.169s)
               Value function loss: 15.5099
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 16.62s
                        Total time: 24353.86s
                               ETA: 1044746.2s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.973s, learning 0.158s)
               Value function loss: 0.3831
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 43.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 16.13s
                        Total time: 24369.99s
                               ETA: 1044968.8s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.396s, learning 0.176s)
               Value function loss: 0.2105
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 42.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 16.57s
                        Total time: 24386.56s
                               ETA: 1045210.0s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.763s, learning 0.257s)
               Value function loss: 0.1588
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 43.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 17.02s
                        Total time: 24403.58s
                               ETA: 1045470.2s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.186s, learning 0.171s)
               Value function loss: 81.9275
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.44
               Mean episode length: 44.15
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 16.36s
                        Total time: 24419.94s
                               ETA: 1045701.9s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.855s, learning 0.190s)
               Value function loss: 35.7570
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 17.05s
                        Total time: 24436.98s
                               ETA: 1045962.7s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.729s, learning 0.164s)
               Value function loss: 7.3960
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 43.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 16.89s
                        Total time: 24453.87s
                               ETA: 1046216.8s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.482s, learning 0.163s)
               Value function loss: 111.8208
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 7.21
               Mean episode length: 42.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 16.65s
                        Total time: 24470.52s
                               ETA: 1046460.0s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.428s, learning 0.170s)
               Value function loss: 15.3138
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 16.60s
                        Total time: 24487.12s
                               ETA: 1046701.1s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.177s, learning 0.177s)
               Value function loss: 16.9181
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 16.35s
                        Total time: 24503.47s
                               ETA: 1046931.4s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.147s, learning 0.170s)
               Value function loss: 22.6206
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 43.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 17.32s
                        Total time: 24520.79s
                               ETA: 1047202.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.436s, learning 0.177s)
               Value function loss: 0.3462
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 44.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 16.61s
                        Total time: 24537.40s
                               ETA: 1047443.7s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.109s, learning 0.261s)
               Value function loss: 0.2512
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 2.41
               Mean episode length: 43.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 16.37s
                        Total time: 24553.77s
                               ETA: 1047674.1s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.088s, learning 0.190s)
               Value function loss: 59.0969
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 42.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 16.28s
                        Total time: 24570.05s
                               ETA: 1047900.3s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.264s, learning 0.175s)
               Value function loss: 0.2876
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 16.44s
                        Total time: 24586.49s
                               ETA: 1048133.2s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.983s, learning 0.205s)
               Value function loss: 17.7219
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 7.21
               Mean episode length: 43.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 16.19s
                        Total time: 24602.68s
                               ETA: 1048355.1s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.474s, learning 0.185s)
               Value function loss: 78.6212
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 44.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 16.66s
                        Total time: 24619.34s
                               ETA: 1048597.0s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.255s, learning 0.183s)
               Value function loss: 18.0599
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 42.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 16.44s
                        Total time: 24635.77s
                               ETA: 1048829.2s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.919s, learning 0.251s)
               Value function loss: 12.0780
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 12.08
               Mean episode length: 42.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 17.17s
                        Total time: 24652.95s
                               ETA: 1049092.3s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.022s, learning 0.205s)
               Value function loss: 0.2873
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 4.84
               Mean episode length: 44.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 17.23s
                        Total time: 24670.17s
                               ETA: 1049357.6s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.930s, learning 0.178s)
               Value function loss: 64.0114
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 43.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 12.11s
                        Total time: 24682.28s
                               ETA: 1049405.1s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.540s, learning 0.207s)
               Value function loss: 93.3644
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 8.75s
                        Total time: 24691.03s
                               ETA: 1049309.6s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.587s, learning 0.223s)
               Value function loss: 7.0634
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 43.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 8.81s
                        Total time: 24699.84s
                               ETA: 1049216.9s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.180s)
               Value function loss: 254.6109
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 5.06
               Mean episode length: 44.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 8.87s
                        Total time: 24708.71s
                               ETA: 1049127.0s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.353s, learning 0.217s)
               Value function loss: 1.9043
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 44.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 8.57s
                        Total time: 24717.28s
                               ETA: 1049024.2s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.558s, learning 0.162s)
               Value function loss: 0.4193
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 43.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 8.72s
                        Total time: 24726.00s
                               ETA: 1048927.9s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.171s)
               Value function loss: 0.2858
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 8.41s
                        Total time: 24734.41s
                               ETA: 1048818.4s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.163s)
               Value function loss: 39.5044
                    Surrogate loss: 0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 8.64s
                        Total time: 24743.05s
                               ETA: 1048718.7s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.266s, learning 0.160s)
               Value function loss: 0.2293
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 8.43s
                        Total time: 24751.47s
                               ETA: 1048610.2s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.160s, learning 0.179s)
               Value function loss: 0.2376
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 42.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 8.34s
                        Total time: 24759.81s
                               ETA: 1048498.0s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.398s, learning 0.181s)
               Value function loss: 15.2551
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.28
               Mean episode length: 42.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 8.58s
                        Total time: 24768.39s
                               ETA: 1048396.1s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.956s, learning 0.201s)
               Value function loss: 0.2070
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 44.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 9.16s
                        Total time: 24777.55s
                               ETA: 1048318.8s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.415s, learning 0.182s)
               Value function loss: 0.1794
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 43.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 8.60s
                        Total time: 24786.14s
                               ETA: 1048217.8s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.330s, learning 0.176s)
               Value function loss: 0.1355
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 43.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 8.51s
                        Total time: 24794.65s
                               ETA: 1048113.0s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.856s, learning 0.209s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 9.06s
                        Total time: 24803.71s
                               ETA: 1048032.0s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.340s, learning 0.197s)
               Value function loss: 46.4587
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 43.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.54s
                        Total time: 24812.25s
                               ETA: 1047928.7s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.454s, learning 0.186s)
               Value function loss: 3.9357
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 42.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.64s
                        Total time: 24820.89s
                               ETA: 1047829.8s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.539s, learning 0.223s)
               Value function loss: 0.1318
                    Surrogate loss: -0.0380
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 41.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 8.76s
                        Total time: 24829.65s
                               ETA: 1047736.2s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.325s, learning 0.195s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 43.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.52s
                        Total time: 24838.17s
                               ETA: 1047632.5s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.361s, learning 0.170s)
               Value function loss: 16.3744
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 8.53s
                        Total time: 24846.70s
                               ETA: 1047529.3s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.670s, learning 0.195s)
               Value function loss: 0.1501
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.86s
                        Total time: 24855.57s
                               ETA: 1047440.2s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.767s, learning 0.172s)
               Value function loss: 0.1368
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 8.94s
                        Total time: 24864.51s
                               ETA: 1047354.4s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.812s, learning 0.198s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 43.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 9.01s
                        Total time: 24873.52s
                               ETA: 1047271.6s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.414s, learning 0.182s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 43.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 8.60s
                        Total time: 24882.11s
                               ETA: 1047171.4s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.550s, learning 0.198s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 8.75s
                        Total time: 24890.86s
                               ETA: 1047077.7s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1770 steps/s (collection: 8.986s, learning 0.269s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 2.75
               Mean episode length: 45.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 9.26s
                        Total time: 24900.12s
                               ETA: 1047005.4s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.706s, learning 0.190s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 43.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.90s
                        Total time: 24909.01s
                               ETA: 1046918.1s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.212s, learning 0.184s)
               Value function loss: 46.8681
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 43.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.40s
                        Total time: 24917.41s
                               ETA: 1046809.8s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.261s, learning 0.193s)
               Value function loss: 0.1411
                    Surrogate loss: -0.0409
             Mean action noise std: 0.71
                       Mean reward: 7.18
               Mean episode length: 43.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.45s
                        Total time: 24925.86s
                               ETA: 1046704.0s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.305s, learning 0.161s)
               Value function loss: 9.8178
                    Surrogate loss: 0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.74
               Mean episode length: 43.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.47s
                        Total time: 24934.33s
                               ETA: 1046598.9s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.317s, learning 0.187s)
               Value function loss: 0.1244
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 44.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.50s
                        Total time: 24942.83s
                               ETA: 1046495.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.312s, learning 0.167s)
               Value function loss: 39.0378
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 43.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.48s
                        Total time: 24951.31s
                               ETA: 1046391.0s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.335s, learning 0.172s)
               Value function loss: 45.0168
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 4.80
               Mean episode length: 43.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.51s
                        Total time: 24959.82s
                               ETA: 1046287.8s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.432s, learning 0.161s)
               Value function loss: 0.9209
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 9.95
               Mean episode length: 44.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.59s
                        Total time: 24968.41s
                               ETA: 1046188.3s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.576s, learning 0.204s)
               Value function loss: 0.3067
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 43.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.78s
                        Total time: 24977.19s
                               ETA: 1046096.7s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.542s, learning 0.173s)
               Value function loss: 32.6990
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 43.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.72s
                        Total time: 24985.91s
                               ETA: 1046002.4s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.279s, learning 0.211s)
               Value function loss: 0.1942
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.49s
                        Total time: 24994.40s
                               ETA: 1045898.8s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.164s)
               Value function loss: 0.1372
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 43.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.49s
                        Total time: 25002.89s
                               ETA: 1045795.3s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.332s, learning 0.167s)
               Value function loss: 0.1372
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.50s
                        Total time: 25011.39s
                               ETA: 1045692.2s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.714s, learning 0.175s)
               Value function loss: 0.1009
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 43.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.89s
                        Total time: 25020.28s
                               ETA: 1045605.5s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.161s)
               Value function loss: 14.0359
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 43.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.52s
                        Total time: 25028.80s
                               ETA: 1045503.5s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.626s, learning 0.171s)
               Value function loss: 3.9369
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 42.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.80s
                        Total time: 25037.59s
                               ETA: 1045413.2s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.038s, learning 0.165s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0432
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 43.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.20s
                        Total time: 25045.80s
                               ETA: 1045298.1s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.375s, learning 0.172s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 42.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.55s
                        Total time: 25054.34s
                               ETA: 1045197.4s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.662s, learning 0.177s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 43.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.84s
                        Total time: 25063.18s
                               ETA: 1045109.0s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.354s, learning 0.165s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.52s
                        Total time: 25071.70s
                               ETA: 1045007.4s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.415s, learning 0.168s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0396
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 42.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 8.58s
                        Total time: 25080.29s
                               ETA: 1044908.4s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.565s, learning 0.193s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 8.76s
                        Total time: 25089.04s
                               ETA: 1044816.9s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.162s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 42.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 8.57s
                        Total time: 25097.62s
                               ETA: 1044717.7s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.188s, learning 0.189s)
               Value function loss: 54.0816
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 41.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.38s
                        Total time: 25106.00s
                               ETA: 1044610.5s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.446s, learning 0.195s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0394
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.64s
                        Total time: 25114.64s
                               ETA: 1044514.3s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.373s, learning 0.220s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0414
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 42.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 8.59s
                        Total time: 25123.23s
                               ETA: 1044416.2s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.517s, learning 0.194s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 43.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.71s
                        Total time: 25131.94s
                               ETA: 1044323.0s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.480s, learning 0.166s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 41.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.65s
                        Total time: 25140.59s
                               ETA: 1044227.2s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.161s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.59s
                        Total time: 25149.17s
                               ETA: 1044129.1s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.322s, learning 0.189s)
               Value function loss: 13.6145
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.51s
                        Total time: 25157.69s
                               ETA: 1044027.9s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.487s, learning 0.196s)
               Value function loss: 0.1325
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 43.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.68s
                        Total time: 25166.37s
                               ETA: 1043933.9s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.613s, learning 0.175s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 41.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.79s
                        Total time: 25175.16s
                               ETA: 1043844.3s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.847s, learning 0.161s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.01s
                        Total time: 25183.17s
                               ETA: 1043722.5s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.553s, learning 0.174s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 8.73s
                        Total time: 25191.89s
                               ETA: 1043630.5s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.484s, learning 0.171s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 41.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 8.66s
                        Total time: 25200.55s
                               ETA: 1043535.6s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.534s, learning 0.197s)
               Value function loss: 0.0978
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 41.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 8.73s
                        Total time: 25209.28s
                               ETA: 1043443.9s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.164s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 41.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 8.44s
                        Total time: 25217.72s
                               ETA: 1043340.3s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.641s, learning 0.161s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0393
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 8.80s
                        Total time: 25226.52s
                               ETA: 1043251.8s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.528s, learning 0.161s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 43.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 8.69s
                        Total time: 25235.21s
                               ETA: 1043158.6s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.719s, learning 0.163s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 43.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 8.88s
                        Total time: 25244.09s
                               ETA: 1043073.5s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.375s, learning 0.164s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 41.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 8.54s
                        Total time: 25252.63s
                               ETA: 1042974.3s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.353s, learning 0.160s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 8.51s
                        Total time: 25261.14s
                               ETA: 1042874.1s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.508s, learning 0.167s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 8.67s
                        Total time: 25269.82s
                               ETA: 1042780.6s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.557s, learning 0.172s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 8.73s
                        Total time: 25278.55s
                               ETA: 1042689.4s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.624s, learning 0.164s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 8.79s
                        Total time: 25287.34s
                               ETA: 1042600.8s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.172s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 44.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 8.63s
                        Total time: 25295.97s
                               ETA: 1042505.6s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.167s)
               Value function loss: 7.0939
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 8.46s
                        Total time: 25304.42s
                               ETA: 1042403.4s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.419s, learning 0.160s)
               Value function loss: 13.6502
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 8.58s
                        Total time: 25313.00s
                               ETA: 1042306.4s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.323s, learning 0.156s)
               Value function loss: 0.1313
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 41.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 8.48s
                        Total time: 25321.48s
                               ETA: 1042205.3s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.296s, learning 0.174s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 8.47s
                        Total time: 25329.95s
                               ETA: 1042103.9s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.652s, learning 0.275s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 8.93s
                        Total time: 25338.88s
                               ETA: 1042021.4s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.901s, learning 0.181s)
               Value function loss: 17.3612
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 42.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 9.08s
                        Total time: 25347.96s
                               ETA: 1041945.3s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.249s, learning 0.165s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0405
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 8.41s
                        Total time: 25356.37s
                               ETA: 1041841.8s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.126s, learning 0.163s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 8.29s
                        Total time: 25364.66s
                               ETA: 1041733.2s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.550s, learning 0.189s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 8.74s
                        Total time: 25373.40s
                               ETA: 1041643.2s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.477s, learning 0.165s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 42.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 8.64s
                        Total time: 25382.04s
                               ETA: 1041549.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.544s, learning 0.180s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 8.72s
                        Total time: 25390.77s
                               ETA: 1041458.9s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.162s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 41.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 8.66s
                        Total time: 25399.43s
                               ETA: 1041365.9s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.403s, learning 0.164s)
               Value function loss: 0.0676
                    Surrogate loss: -0.0384
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 8.57s
                        Total time: 25407.99s
                               ETA: 1041269.1s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.672s, learning 0.164s)
               Value function loss: 13.7780
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 8.84s
                        Total time: 25416.83s
                               ETA: 1041183.5s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.492s, learning 0.199s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0413
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 8.69s
                        Total time: 25425.52s
                               ETA: 1041091.9s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.465s, learning 0.202s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 8.67s
                        Total time: 25434.19s
                               ETA: 1040999.5s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.646s, learning 0.186s)
               Value function loss: 64.1056
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 43.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 8.83s
                        Total time: 25443.02s
                               ETA: 1040913.8s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.572s, learning 0.214s)
               Value function loss: 3.9133
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 41.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 8.79s
                        Total time: 25451.81s
                               ETA: 1040826.4s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.324s, learning 0.172s)
               Value function loss: 21.4201
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 42.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 8.50s
                        Total time: 25460.30s
                               ETA: 1040727.1s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.833s, learning 0.167s)
               Value function loss: 0.1313
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 9.00s
                        Total time: 25469.30s
                               ETA: 1040648.6s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.320s, learning 0.198s)
               Value function loss: 6.9792
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 42.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 8.52s
                        Total time: 25477.82s
                               ETA: 1040550.4s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.423s, learning 0.273s)
               Value function loss: 0.1090
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 41.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 8.70s
                        Total time: 25486.52s
                               ETA: 1040459.5s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.093s, learning 0.249s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 8.34s
                        Total time: 25494.86s
                               ETA: 1040354.3s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.290s, learning 0.178s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 43.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 8.47s
                        Total time: 25503.33s
                               ETA: 1040254.3s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.427s, learning 0.221s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 41.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 8.65s
                        Total time: 25511.97s
                               ETA: 1040161.7s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.485s, learning 0.181s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.67s
                        Total time: 25520.64s
                               ETA: 1040069.9s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.160s, learning 0.191s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 8.35s
                        Total time: 25528.99s
                               ETA: 1039965.4s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.490s, learning 0.234s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 41.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.72s
                        Total time: 25537.72s
                               ETA: 1039876.2s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.216s, learning 0.166s)
               Value function loss: 29.6993
                    Surrogate loss: 0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 8.38s
                        Total time: 25546.10s
                               ETA: 1039773.0s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.513s, learning 0.165s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 41.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 8.68s
                        Total time: 25554.78s
                               ETA: 1039682.0s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.389s, learning 0.167s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.56s
                        Total time: 25563.33s
                               ETA: 1039586.1s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.570s, learning 0.174s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 8.74s
                        Total time: 25572.07s
                               ETA: 1039497.9s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.278s, learning 0.234s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 40.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.51s
                        Total time: 25580.59s
                               ETA: 1039400.4s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.194s, learning 0.190s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.38s
                        Total time: 25588.97s
                               ETA: 1039297.7s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.960s, learning 0.168s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 9.13s
                        Total time: 25598.10s
                               ETA: 1039225.3s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.483s, learning 0.218s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.70s
                        Total time: 25606.80s
                               ETA: 1039135.6s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.352s, learning 0.164s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 42.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 8.52s
                        Total time: 25615.32s
                               ETA: 1039038.5s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.429s, learning 0.201s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.63s
                        Total time: 25623.94s
                               ETA: 1038946.1s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.573s, learning 0.164s)
               Value function loss: 0.0646
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 8.74s
                        Total time: 25632.68s
                               ETA: 1038858.1s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.368s, learning 0.254s)
               Value function loss: 15.3820
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 41.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 8.62s
                        Total time: 25641.30s
                               ETA: 1038765.5s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.161s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 41.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 8.54s
                        Total time: 25649.85s
                               ETA: 1038669.8s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.281s, learning 0.199s)
               Value function loss: 13.7873
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.48s
                        Total time: 25658.33s
                               ETA: 1038571.6s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.165s)
               Value function loss: 3.9023
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.39s
                        Total time: 25666.72s
                               ETA: 1038469.9s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.388s, learning 0.273s)
               Value function loss: 0.0997
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 42.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 8.66s
                        Total time: 25675.38s
                               ETA: 1038379.2s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.486s, learning 0.165s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 41.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.65s
                        Total time: 25684.03s
                               ETA: 1038288.2s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.537s, learning 0.195s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 8.73s
                        Total time: 25692.76s
                               ETA: 1038200.5s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.286s, learning 0.219s)
               Value function loss: 64.1661
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 8.51s
                        Total time: 25701.27s
                               ETA: 1038103.6s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.264s, learning 0.225s)
               Value function loss: 3.9601
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 9.49
               Mean episode length: 41.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 8.49s
                        Total time: 25709.76s
                               ETA: 1038006.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.371s, learning 0.161s)
               Value function loss: 17.8386
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 42.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.53s
                        Total time: 25718.29s
                               ETA: 1037910.6s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.861s, learning 0.161s)
               Value function loss: 0.1246
                    Surrogate loss: -0.0396
             Mean action noise std: 0.71
                       Mean reward: 7.12
               Mean episode length: 42.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 9.02s
                        Total time: 25727.31s
                               ETA: 1037834.9s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.290s, learning 0.179s)
               Value function loss: 17.4501
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.47s
                        Total time: 25735.78s
                               ETA: 1037736.9s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.686s, learning 0.191s)
               Value function loss: 0.1370
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.88s
                        Total time: 25744.66s
                               ETA: 1037655.4s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.905s, learning 0.191s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 17.10s
                        Total time: 25761.75s
                               ETA: 1037905.1s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.299s, learning 0.197s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 43.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 16.50s
                        Total time: 25778.25s
                               ETA: 1038130.5s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.810s, learning 0.215s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 1.71
               Mean episode length: 41.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 17.02s
                        Total time: 25795.28s
                               ETA: 1038376.9s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.774s, learning 0.173s)
               Value function loss: 53.4781
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 16.95s
                        Total time: 25812.22s
                               ETA: 1038619.9s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.675s, learning 0.192s)
               Value function loss: 0.1494
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 41.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 16.87s
                        Total time: 25829.09s
                               ETA: 1038859.6s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.417s, learning 0.205s)
               Value function loss: 63.9280
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 42.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 16.62s
                        Total time: 25845.71s
                               ETA: 1039089.2s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.433s, learning 0.200s)
               Value function loss: 0.1774
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 42.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 16.63s
                        Total time: 25862.34s
                               ETA: 1039319.0s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.546s, learning 0.180s)
               Value function loss: 7.1687
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 16.73s
                        Total time: 25879.07s
                               ETA: 1039552.3s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.672s, learning 0.358s)
               Value function loss: 29.2088
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 17.03s
                        Total time: 25896.10s
                               ETA: 1039797.7s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.499s, learning 0.231s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 16.73s
                        Total time: 25912.83s
                               ETA: 1040030.8s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.068s, learning 0.163s)
               Value function loss: 0.0972
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 43.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 16.23s
                        Total time: 25929.06s
                               ETA: 1040243.6s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.513s, learning 0.170s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 16.68s
                        Total time: 25945.74s
                               ETA: 1040474.4s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.778s, learning 0.288s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 40.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 17.07s
                        Total time: 25962.81s
                               ETA: 1040720.4s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.743s, learning 0.227s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 41.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 16.97s
                        Total time: 25979.78s
                               ETA: 1040962.3s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.404s, learning 0.223s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 16.63s
                        Total time: 25996.41s
                               ETA: 1041190.2s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.785s, learning 0.231s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 17.02s
                        Total time: 26013.42s
                               ETA: 1041433.5s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.140s, learning 0.170s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 42.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 16.31s
                        Total time: 26029.73s
                               ETA: 1041648.4s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.555s, learning 0.172s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 16.73s
                        Total time: 26046.46s
                               ETA: 1041879.7s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.723s, learning 0.177s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 42.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 16.90s
                        Total time: 26063.36s
                               ETA: 1042117.8s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.945s, learning 0.199s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 41.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 17.14s
                        Total time: 26080.50s
                               ETA: 1042365.4s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.345s, learning 0.171s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 42.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 16.52s
                        Total time: 26097.02s
                               ETA: 1042587.7s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.871s, learning 0.176s)
               Value function loss: 17.5312
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 41.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 17.05s
                        Total time: 26114.07s
                               ETA: 1042831.0s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.576s, learning 0.173s)
               Value function loss: 50.3595
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 16.75s
                        Total time: 26130.82s
                               ETA: 1043062.2s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.345s, learning 0.285s)
               Value function loss: 0.2663
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 4.43
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 16.63s
                        Total time: 26147.44s
                               ETA: 1043288.4s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.233s, learning 0.163s)
               Value function loss: 0.1538
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 16.40s
                        Total time: 26163.84s
                               ETA: 1043505.1s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.372s, learning 0.169s)
               Value function loss: 0.1131
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 16.54s
                        Total time: 26180.38s
                               ETA: 1043727.4s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.388s, learning 0.162s)
               Value function loss: 0.0963
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 16.55s
                        Total time: 26196.93s
                               ETA: 1043949.9s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.424s, learning 0.195s)
               Value function loss: 63.8134
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 41.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 16.62s
                        Total time: 26213.55s
                               ETA: 1044174.9s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.754s, learning 0.198s)
               Value function loss: 16.5111
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 16.95s
                        Total time: 26230.50s
                               ETA: 1044413.0s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.830s, learning 0.167s)
               Value function loss: 0.1837
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 41.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 17.00s
                        Total time: 26247.50s
                               ETA: 1044652.6s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.257s, learning 0.261s)
               Value function loss: 0.1467
                    Surrogate loss: -0.0278
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 41.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 16.52s
                        Total time: 26264.02s
                               ETA: 1044873.0s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.938s, learning 0.163s)
               Value function loss: 0.1325
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 17.10s
                        Total time: 26281.12s
                               ETA: 1045116.4s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.024s, learning 0.200s)
               Value function loss: 0.0963
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 43.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 17.22s
                        Total time: 26298.34s
                               ETA: 1045364.4s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.590s, learning 0.172s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 41.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 16.76s
                        Total time: 26315.10s
                               ETA: 1045594.0s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.691s, learning 0.197s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 16.89s
                        Total time: 26331.99s
                               ETA: 1045828.3s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.514s, learning 0.169s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 42.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 16.68s
                        Total time: 26348.68s
                               ETA: 1046054.2s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.948s, learning 0.194s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 17.14s
                        Total time: 26365.82s
                               ETA: 1046298.2s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1168 steps/s (collection: 13.837s, learning 0.183s)
               Value function loss: 61.2376
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.91
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 14.02s
                        Total time: 26379.84s
                               ETA: 1046418.1s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.441s, learning 0.161s)
               Value function loss: 0.1471
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 4.98
               Mean episode length: 43.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.60s
                        Total time: 26388.44s
                               ETA: 1046323.0s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.424s, learning 0.173s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 8.60s
                        Total time: 26397.04s
                               ETA: 1046227.9s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.542s, learning 0.165s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 42.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.71s
                        Total time: 26405.74s
                               ETA: 1046137.2s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.325s, learning 0.187s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 41.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.51s
                        Total time: 26414.25s
                               ETA: 1046038.8s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.330s, learning 0.259s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 8.59s
                        Total time: 26422.84s
                               ETA: 1045943.5s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.653s, learning 0.226s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 8.88s
                        Total time: 26431.72s
                               ETA: 1045859.8s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.546s, learning 0.174s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.72s
                        Total time: 26440.44s
                               ETA: 1045769.9s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.594s, learning 0.233s)
               Value function loss: 16.7373
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 8.83s
                        Total time: 26449.27s
                               ETA: 1045684.2s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.651s, learning 0.165s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 4.61
               Mean episode length: 42.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 8.82s
                        Total time: 26458.08s
                               ETA: 1045598.2s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.328s, learning 0.182s)
               Value function loss: 11.8123
                    Surrogate loss: 0.0028
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.51s
                        Total time: 26466.59s
                               ETA: 1045500.2s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.841s, learning 0.176s)
               Value function loss: 62.3647
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 9.02s
                        Total time: 26475.61s
                               ETA: 1045422.2s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.179s)
               Value function loss: 0.3335
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 8.30s
                        Total time: 26483.91s
                               ETA: 1045315.9s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.546s, learning 0.202s)
               Value function loss: 0.1913
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 43.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 8.75s
                        Total time: 26492.66s
                               ETA: 1045227.5s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.270s, learning 0.178s)
               Value function loss: 0.1279
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 42.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 8.45s
                        Total time: 26501.10s
                               ETA: 1045127.3s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.466s, learning 0.173s)
               Value function loss: 17.9088
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 8.64s
                        Total time: 26509.74s
                               ETA: 1045034.7s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.563s, learning 0.177s)
               Value function loss: 0.1468
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 43.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 8.74s
                        Total time: 26518.48s
                               ETA: 1044946.1s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.124s, learning 0.185s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 8.31s
                        Total time: 26526.79s
                               ETA: 1044840.7s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.340s, learning 0.177s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 8.52s
                        Total time: 26535.31s
                               ETA: 1044743.5s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.433s, learning 0.257s)
               Value function loss: 53.7103
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 42.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 8.69s
                        Total time: 26544.00s
                               ETA: 1044653.1s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.464s, learning 0.183s)
               Value function loss: 0.1825
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 7.18
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 8.65s
                        Total time: 26552.65s
                               ETA: 1044561.2s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.283s, learning 0.233s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 8.52s
                        Total time: 26561.16s
                               ETA: 1044464.2s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.306s, learning 0.175s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 43.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 8.48s
                        Total time: 26569.64s
                               ETA: 1044365.9s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.420s, learning 0.271s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 43.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 8.69s
                        Total time: 26578.33s
                               ETA: 1044275.8s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.168s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 8.72s
                        Total time: 26587.05s
                               ETA: 1044186.9s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.135s, learning 0.180s)
               Value function loss: 3.9236
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.43
               Mean episode length: 44.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 8.31s
                        Total time: 26595.36s
                               ETA: 1044082.2s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.163s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 8.54s
                        Total time: 26603.91s
                               ETA: 1043986.6s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.634s, learning 0.228s)
               Value function loss: 73.6564
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.87
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 8.86s
                        Total time: 26612.77s
                               ETA: 1043903.6s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.669s, learning 0.162s)
               Value function loss: 0.1565
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 43.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 8.83s
                        Total time: 26621.60s
                               ETA: 1043819.4s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.137s, learning 0.157s)
               Value function loss: 0.1016
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 42.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 8.29s
                        Total time: 26629.90s
                               ETA: 1043714.2s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.175s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 8.65s
                        Total time: 26638.55s
                               ETA: 1043623.2s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.691s, learning 0.198s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 8.89s
                        Total time: 26647.44s
                               ETA: 1043541.5s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.273s, learning 0.182s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 8.45s
                        Total time: 26655.89s
                               ETA: 1043442.9s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.398s, learning 0.191s)
               Value function loss: 3.9286
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 8.59s
                        Total time: 26664.48s
                               ETA: 1043349.5s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.541s, learning 0.170s)
               Value function loss: 100.9643
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.82
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 8.71s
                        Total time: 26673.19s
                               ETA: 1043261.0s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.587s, learning 0.173s)
               Value function loss: 17.5007
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 7.31
               Mean episode length: 41.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 8.76s
                        Total time: 26681.95s
                               ETA: 1043174.5s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.135s, learning 0.326s)
               Value function loss: 0.2934
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 8.46s
                        Total time: 26690.41s
                               ETA: 1043076.4s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.545s, learning 0.281s)
               Value function loss: 63.2103
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.83s
                        Total time: 26699.24s
                               ETA: 1042992.6s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.658s, learning 0.167s)
               Value function loss: 0.3243
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 43.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.83s
                        Total time: 26708.07s
                               ETA: 1042908.8s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.333s, learning 0.168s)
               Value function loss: 13.2622
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 43.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.50s
                        Total time: 26716.57s
                               ETA: 1042812.4s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.497s, learning 0.201s)
               Value function loss: 11.9746
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 8.70s
                        Total time: 26725.26s
                               ETA: 1042723.8s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.287s, learning 0.170s)
               Value function loss: 54.2003
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 43.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 8.46s
                        Total time: 26733.72s
                               ETA: 1042625.8s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.288s, learning 0.165s)
               Value function loss: 13.7389
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.45s
                        Total time: 26742.17s
                               ETA: 1042527.7s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.105s, learning 0.174s)
               Value function loss: 38.0432
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 7.01
               Mean episode length: 42.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.28s
                        Total time: 26750.45s
                               ETA: 1042423.0s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.174s)
               Value function loss: 0.6041
                    Surrogate loss: -0.0245
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 42.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 8.41s
                        Total time: 26758.87s
                               ETA: 1042323.6s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.486s, learning 0.175s)
               Value function loss: 0.2428
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 8.66s
                        Total time: 26767.53s
                               ETA: 1042233.9s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.406s, learning 0.168s)
               Value function loss: 0.1407
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 44.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.57s
                        Total time: 26776.10s
                               ETA: 1042140.9s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.169s)
               Value function loss: 0.1395
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 8.22s
                        Total time: 26784.32s
                               ETA: 1042034.0s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.455s, learning 0.182s)
               Value function loss: 3.9045
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 8.64s
                        Total time: 26792.96s
                               ETA: 1041943.6s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.258s, learning 0.183s)
               Value function loss: 13.7593
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.44s
                        Total time: 26801.40s
                               ETA: 1041845.6s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.659s, learning 0.164s)
               Value function loss: 0.1826
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.82s
                        Total time: 26810.22s
                               ETA: 1041762.5s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.208s, learning 0.223s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 43.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 8.43s
                        Total time: 26818.65s
                               ETA: 1041664.2s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.394s, learning 0.174s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.57s
                        Total time: 26827.22s
                               ETA: 1041571.4s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.226s, learning 0.233s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 41.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 8.46s
                        Total time: 26835.68s
                               ETA: 1041474.4s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.822s, learning 0.160s)
               Value function loss: 94.0282
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 42.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.98s
                        Total time: 26844.66s
                               ETA: 1041397.7s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.223s, learning 0.167s)
               Value function loss: 16.9029
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.39s
                        Total time: 26853.05s
                               ETA: 1041298.2s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.811s, learning 0.184s)
               Value function loss: 18.6659
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 42.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.99s
                        Total time: 26862.05s
                               ETA: 1041222.1s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.422s, learning 0.182s)
               Value function loss: 0.3213
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.60s
                        Total time: 26870.65s
                               ETA: 1041130.9s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.352s, learning 0.251s)
               Value function loss: 17.8148
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 7.03
               Mean episode length: 43.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.60s
                        Total time: 26879.25s
                               ETA: 1041039.8s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.368s, learning 0.162s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 43.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.53s
                        Total time: 26887.78s
                               ETA: 1040945.9s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.244s, learning 0.198s)
               Value function loss: 0.0958
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.44s
                        Total time: 26896.23s
                               ETA: 1040848.7s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.166s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.53s
                        Total time: 26904.76s
                               ETA: 1040755.0s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.279s, learning 0.194s)
               Value function loss: 0.0938
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 42.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.47s
                        Total time: 26913.23s
                               ETA: 1040659.1s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.520s, learning 0.206s)
               Value function loss: 45.7689
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 43.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.73s
                        Total time: 26921.96s
                               ETA: 1040573.1s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.244s, learning 0.165s)
               Value function loss: 21.3489
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 7.29
               Mean episode length: 43.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.41s
                        Total time: 26930.36s
                               ETA: 1040474.9s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.162s)
               Value function loss: 17.7436
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 8.60s
                        Total time: 26938.97s
                               ETA: 1040384.2s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.322s, learning 0.190s)
               Value function loss: 7.3032
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.51s
                        Total time: 26947.48s
                               ETA: 1040290.1s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.382s, learning 0.174s)
               Value function loss: 15.4786
                    Surrogate loss: 0.0071
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 43.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.56s
                        Total time: 26956.04s
                               ETA: 1040197.7s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.055s, learning 0.200s)
               Value function loss: 0.2032
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 4.87
               Mean episode length: 43.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.25s
                        Total time: 26964.29s
                               ETA: 1040093.9s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.687s, learning 0.158s)
               Value function loss: 0.1017
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.85s
                        Total time: 26973.13s
                               ETA: 1040012.8s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.454s, learning 0.180s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 41.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.63s
                        Total time: 26981.77s
                               ETA: 1039923.6s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.281s, learning 0.188s)
               Value function loss: 7.0489
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 41.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.47s
                        Total time: 26990.24s
                               ETA: 1039828.2s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.422s, learning 0.178s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.60s
                        Total time: 26998.84s
                               ETA: 1039737.9s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.613s, learning 0.163s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 43.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.78s
                        Total time: 27007.61s
                               ETA: 1039654.4s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.668s, learning 0.249s)
               Value function loss: 0.0910
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 41.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 8.92s
                        Total time: 27016.53s
                               ETA: 1039576.5s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.138s, learning 0.173s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.31s
                        Total time: 27024.84s
                               ETA: 1039475.2s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.755s, learning 0.160s)
               Value function loss: 7.0064
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.92s
                        Total time: 27033.76s
                               ETA: 1039397.3s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.750s, learning 0.206s)
               Value function loss: 13.7490
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.96s
                        Total time: 27042.71s
                               ETA: 1039321.0s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.453s, learning 0.197s)
               Value function loss: 0.1232
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 4.68
               Mean episode length: 43.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 8.65s
                        Total time: 27051.36s
                               ETA: 1039233.0s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.347s, learning 0.195s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 43.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.54s
                        Total time: 27059.91s
                               ETA: 1039140.9s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.168s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 43.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.47s
                        Total time: 27068.37s
                               ETA: 1039045.9s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.082s, learning 0.158s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.24s
                        Total time: 27076.61s
                               ETA: 1038942.4s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.480s, learning 0.162s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 43.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.64s
                        Total time: 27085.25s
                               ETA: 1038854.3s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.135s, learning 0.235s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0375
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.37s
                        Total time: 27093.62s
                               ETA: 1038755.9s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.166s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 42.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.25s
                        Total time: 27101.87s
                               ETA: 1038652.9s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.453s, learning 0.164s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 8.62s
                        Total time: 27110.49s
                               ETA: 1038564.0s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.425s, learning 0.168s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.59s
                        Total time: 27119.08s
                               ETA: 1038474.4s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.349s, learning 0.301s)
               Value function loss: 3.8538
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 42.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.65s
                        Total time: 27127.73s
                               ETA: 1038387.0s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.369s, learning 0.223s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 43.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.59s
                        Total time: 27136.32s
                               ETA: 1038297.4s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.590s, learning 0.160s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 42.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.75s
                        Total time: 27145.08s
                               ETA: 1038213.9s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.164s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.57s
                        Total time: 27153.64s
                               ETA: 1038123.5s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.368s, learning 0.228s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 42.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.60s
                        Total time: 27162.24s
                               ETA: 1038034.3s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.211s, learning 0.200s)
               Value function loss: 54.1159
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.41s
                        Total time: 27170.65s
                               ETA: 1037938.0s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.444s, learning 0.159s)
               Value function loss: 33.7722
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 43.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 8.60s
                        Total time: 27179.25s
                               ETA: 1037849.2s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.592s, learning 0.187s)
               Value function loss: 59.7800
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 7.15
               Mean episode length: 43.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.78s
                        Total time: 27188.03s
                               ETA: 1037767.1s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.601s, learning 0.166s)
               Value function loss: 0.5572
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 42.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.77s
                        Total time: 27196.80s
                               ETA: 1037684.6s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.547s, learning 0.167s)
               Value function loss: 0.2156
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 42.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.71s
                        Total time: 27205.51s
                               ETA: 1037600.2s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.166s)
               Value function loss: 12.1134
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 5.03
               Mean episode length: 43.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.45s
                        Total time: 27213.96s
                               ETA: 1037505.8s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.371s, learning 0.168s)
               Value function loss: 17.5481
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 8.54s
                        Total time: 27222.50s
                               ETA: 1037414.8s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.631s, learning 0.164s)
               Value function loss: 0.2021
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.80s
                        Total time: 27231.30s
                               ETA: 1037333.6s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.167s)
               Value function loss: 11.9135
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.99
               Mean episode length: 42.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.44s
                        Total time: 27239.73s
                               ETA: 1037238.8s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.667s, learning 0.159s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.83s
                        Total time: 27248.56s
                               ETA: 1037159.0s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.259s, learning 0.196s)
               Value function loss: 0.1080
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 42.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.46s
                        Total time: 27257.02s
                               ETA: 1037065.1s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.600s, learning 0.175s)
               Value function loss: 29.8244
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.55
               Mean episode length: 43.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.78s
                        Total time: 27265.79s
                               ETA: 1036983.4s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.113s, learning 0.187s)
               Value function loss: 142.9006
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 43.34
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.30s
                        Total time: 27274.09s
                               ETA: 1036883.7s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.685s, learning 0.178s)
               Value function loss: 2.3403
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.86s
                        Total time: 27282.95s
                               ETA: 1036805.5s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.260s, learning 0.264s)
               Value function loss: 0.5308
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 41.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.52s
                        Total time: 27291.48s
                               ETA: 1036714.4s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.420s, learning 0.219s)
               Value function loss: 0.2537
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.64s
                        Total time: 27300.12s
                               ETA: 1036627.8s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.433s, learning 0.246s)
               Value function loss: 0.1621
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.68s
                        Total time: 27308.80s
                               ETA: 1036542.8s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.361s, learning 0.173s)
               Value function loss: 0.1502
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 8.53s
                        Total time: 27317.33s
                               ETA: 1036452.3s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.512s, learning 0.176s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.69s
                        Total time: 27326.02s
                               ETA: 1036367.8s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.352s, learning 0.170s)
               Value function loss: 0.0978
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 8.52s
                        Total time: 27334.54s
                               ETA: 1036276.9s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.681s, learning 0.187s)
               Value function loss: 17.8031
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.87s
                        Total time: 27343.41s
                               ETA: 1036199.3s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.488s, learning 0.230s)
               Value function loss: 0.1115
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 43.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 8.72s
                        Total time: 27352.13s
                               ETA: 1036116.0s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.528s, learning 0.166s)
               Value function loss: 29.6001
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 7.40
               Mean episode length: 43.67
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.69s
                        Total time: 27360.82s
                               ETA: 1036031.9s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.172s)
               Value function loss: 9.5649
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 7.03
               Mean episode length: 42.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.45s
                        Total time: 27369.27s
                               ETA: 1035938.5s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.162s)
               Value function loss: 12.1232
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 43.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.40s
                        Total time: 27377.66s
                               ETA: 1035843.2s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.163s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 1.55
               Mean episode length: 41.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.66s
                        Total time: 27386.32s
                               ETA: 1035757.8s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.406s, learning 0.170s)
               Value function loss: 17.6169
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 42.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 8.58s
                        Total time: 27394.90s
                               ETA: 1035669.5s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.698s, learning 0.161s)
               Value function loss: 13.9096
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 8.86s
                        Total time: 27403.75s
                               ETA: 1035591.9s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.446s, learning 0.195s)
               Value function loss: 29.3790
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 41.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.64s
                        Total time: 27412.40s
                               ETA: 1035506.2s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.434s, learning 0.289s)
               Value function loss: 24.7954
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.72s
                        Total time: 27421.12s
                               ETA: 1035423.6s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.533s, learning 0.164s)
               Value function loss: 67.1305
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 41.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 8.70s
                        Total time: 27429.82s
                               ETA: 1035340.0s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.516s, learning 0.165s)
               Value function loss: 0.4343
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 8.68s
                        Total time: 27438.50s
                               ETA: 1035255.9s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.591s, learning 0.168s)
               Value function loss: 0.2688
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 42.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.76s
                        Total time: 27447.25s
                               ETA: 1035174.8s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.594s, learning 0.191s)
               Value function loss: 62.3970
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.51
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.79s
                        Total time: 27456.04s
                               ETA: 1035094.8s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.429s, learning 0.164s)
               Value function loss: 24.6065
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 42.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.59s
                        Total time: 27464.63s
                               ETA: 1035007.6s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.280s, learning 0.161s)
               Value function loss: 0.5013
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.44s
                        Total time: 27473.07s
                               ETA: 1034914.7s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.523s, learning 0.201s)
               Value function loss: 0.2321
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.72s
                        Total time: 27481.80s
                               ETA: 1034832.6s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.169s)
               Value function loss: 13.6607
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 42.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 8.53s
                        Total time: 27490.32s
                               ETA: 1034743.0s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.126s, learning 0.191s)
               Value function loss: 15.3211
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 42.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 16.32s
                        Total time: 27506.64s
                               ETA: 1034946.7s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.996s, learning 0.204s)
               Value function loss: 32.8099
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 41.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 16.20s
                        Total time: 27522.84s
                               ETA: 1035145.8s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 931 steps/s (collection: 17.417s, learning 0.175s)
               Value function loss: 20.6117
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.59
               Mean episode length: 41.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 17.59s
                        Total time: 27540.43s
                               ETA: 1035397.0s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.743s, learning 0.227s)
               Value function loss: 0.2871
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 41.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 16.97s
                        Total time: 27557.40s
                               ETA: 1035624.6s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.716s, learning 0.174s)
               Value function loss: 0.2051
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 2.42
               Mean episode length: 42.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 16.89s
                        Total time: 27574.29s
                               ETA: 1035849.1s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.251s, learning 0.200s)
               Value function loss: 0.1634
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 2.63
               Mean episode length: 43.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 16.45s
                        Total time: 27590.74s
                               ETA: 1036056.9s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.267s, learning 0.164s)
               Value function loss: 38.6734
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 16.43s
                        Total time: 27607.17s
                               ETA: 1036263.7s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.462s, learning 0.202s)
               Value function loss: 16.5672
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.88
               Mean episode length: 43.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 16.66s
                        Total time: 27623.84s
                               ETA: 1036479.2s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.769s, learning 0.175s)
               Value function loss: 147.4848
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 2.47
               Mean episode length: 42.83
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 16.94s
                        Total time: 27640.78s
                               ETA: 1036704.9s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.582s, learning 0.227s)
               Value function loss: 0.4979
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 2.40
               Mean episode length: 42.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 16.81s
                        Total time: 27657.59s
                               ETA: 1036925.4s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.592s, learning 0.191s)
               Value function loss: 17.9933
                    Surrogate loss: 0.0041
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 43.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 16.78s
                        Total time: 27674.37s
                               ETA: 1037144.8s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.380s, learning 0.199s)
               Value function loss: 4.0310
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 42.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 16.58s
                        Total time: 27690.95s
                               ETA: 1037356.3s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.939s, learning 0.176s)
               Value function loss: 17.7134
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 17.12s
                        Total time: 27708.07s
                               ETA: 1037587.7s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.672s, learning 0.177s)
               Value function loss: 4.0216
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 2.52
               Mean episode length: 43.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 16.85s
                        Total time: 27724.92s
                               ETA: 1037809.0s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.379s, learning 0.256s)
               Value function loss: 13.5196
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.55
               Mean episode length: 43.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 16.64s
                        Total time: 27741.55s
                               ETA: 1038022.1s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.619s, learning 0.188s)
               Value function loss: 0.1878
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 4.72
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 16.81s
                        Total time: 27758.36s
                               ETA: 1038241.4s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.335s, learning 0.164s)
               Value function loss: 27.0323
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.43
               Mean episode length: 42.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 16.50s
                        Total time: 27774.86s
                               ETA: 1038449.1s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.184s, learning 0.193s)
               Value function loss: 34.7994
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 16.38s
                        Total time: 27791.23s
                               ETA: 1038652.0s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.889s, learning 0.168s)
               Value function loss: 15.2705
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 17.06s
                        Total time: 27808.29s
                               ETA: 1038880.2s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.182s)
               Value function loss: 111.8045
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 9.72
               Mean episode length: 42.03
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 16.36s
                        Total time: 27824.65s
                               ETA: 1039082.3s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.207s, learning 0.180s)
               Value function loss: 0.4949
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 16.39s
                        Total time: 27841.04s
                               ETA: 1039285.0s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.218s, learning 0.181s)
               Value function loss: 0.3984
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 42.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 16.40s
                        Total time: 27857.44s
                               ETA: 1039488.1s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.368s, learning 0.189s)
               Value function loss: 15.5057
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 16.56s
                        Total time: 27874.00s
                               ETA: 1039696.9s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.336s, learning 0.204s)
               Value function loss: 0.3302
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 42.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 16.54s
                        Total time: 27890.54s
                               ETA: 1039904.8s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.383s, learning 0.202s)
               Value function loss: 68.3033
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 7.44
               Mean episode length: 43.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 16.59s
                        Total time: 27907.12s
                               ETA: 1040114.3s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.174s, learning 0.202s)
               Value function loss: 0.3275
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 2.28
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 16.38s
                        Total time: 27923.50s
                               ETA: 1040315.9s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.192s, learning 0.186s)
               Value function loss: 0.2484
                    Surrogate loss: -0.0270
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 16.38s
                        Total time: 27939.88s
                               ETA: 1040517.3s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.476s, learning 0.160s)
               Value function loss: 17.6278
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.52
               Mean episode length: 43.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 16.64s
                        Total time: 27956.51s
                               ETA: 1040728.2s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.492s, learning 0.162s)
               Value function loss: 18.2336
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 42.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 16.65s
                        Total time: 27973.17s
                               ETA: 1040939.6s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.372s, learning 0.167s)
               Value function loss: 13.6679
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 16.54s
                        Total time: 27989.71s
                               ETA: 1041146.5s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.806s, learning 0.162s)
               Value function loss: 0.3097
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 40.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 16.97s
                        Total time: 28006.67s
                               ETA: 1041369.2s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.478s, learning 0.196s)
               Value function loss: 0.1786
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 42.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 16.67s
                        Total time: 28023.35s
                               ETA: 1041580.8s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.402s, learning 0.166s)
               Value function loss: 0.1217
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 41.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 16.57s
                        Total time: 28039.92s
                               ETA: 1041788.2s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.451s, learning 0.176s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 41.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 16.63s
                        Total time: 28056.54s
                               ETA: 1041997.7s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.436s, learning 0.273s)
               Value function loss: 13.7243
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 16.71s
                        Total time: 28073.25s
                               ETA: 1042210.1s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.285s, learning 0.242s)
               Value function loss: 0.1861
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 16.53s
                        Total time: 28089.78s
                               ETA: 1042415.6s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.586s, learning 0.168s)
               Value function loss: 63.4586
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 42.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 16.75s
                        Total time: 28106.53s
                               ETA: 1042629.3s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1135 steps/s (collection: 14.259s, learning 0.169s)
               Value function loss: 17.4916
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 43.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 14.43s
                        Total time: 28120.96s
                               ETA: 1042756.5s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.597s, learning 0.199s)
               Value function loss: 15.6105
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 41.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 8.80s
                        Total time: 28129.76s
                               ETA: 1042674.9s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.224s, learning 0.160s)
               Value function loss: 17.6759
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 42.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.38s
                        Total time: 28138.14s
                               ETA: 1042578.1s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.607s, learning 0.183s)
               Value function loss: 0.3875
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 41.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 8.79s
                        Total time: 28146.93s
                               ETA: 1042496.4s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.102s, learning 0.225s)
               Value function loss: 21.2487
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.30
               Mean episode length: 42.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.33s
                        Total time: 28155.26s
                               ETA: 1042397.6s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.196s, learning 0.226s)
               Value function loss: 17.7389
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.73
               Mean episode length: 43.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 8.42s
                        Total time: 28163.68s
                               ETA: 1042302.4s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.338s, learning 0.186s)
               Value function loss: 0.1850
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.52s
                        Total time: 28172.20s
                               ETA: 1042211.0s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.556s, learning 0.186s)
               Value function loss: 0.1357
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 42.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 8.74s
                        Total time: 28180.95s
                               ETA: 1042127.7s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.173s)
               Value function loss: 0.1103
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 40.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.68s
                        Total time: 28189.62s
                               ETA: 1042042.2s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.417s, learning 0.224s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 41.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.64s
                        Total time: 28198.27s
                               ETA: 1041955.4s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.243s, learning 0.161s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 8.40s
                        Total time: 28206.67s
                               ETA: 1041859.8s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.386s, learning 0.159s)
               Value function loss: 30.5810
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 42.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.55s
                        Total time: 28215.22s
                               ETA: 1041769.5s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.169s)
               Value function loss: 0.1463
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 8.68s
                        Total time: 28223.90s
                               ETA: 1041684.3s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.311s, learning 0.166s)
               Value function loss: 42.4479
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.50
               Mean episode length: 42.65
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.48s
                        Total time: 28232.37s
                               ETA: 1041591.6s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.433s, learning 0.226s)
               Value function loss: 61.0011
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 42.83
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.66s
                        Total time: 28241.03s
                               ETA: 1041505.7s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.633s, learning 0.173s)
               Value function loss: 0.8895
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 43.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.81s
                        Total time: 28249.84s
                               ETA: 1041425.3s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.379s, learning 0.239s)
               Value function loss: 0.3618
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 1.71
               Mean episode length: 40.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.62s
                        Total time: 28258.46s
                               ETA: 1041338.0s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.250s, learning 0.185s)
               Value function loss: 0.2095
                    Surrogate loss: -0.0294
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.44s
                        Total time: 28266.89s
                               ETA: 1041244.0s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.735s, learning 0.165s)
               Value function loss: 0.1843
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 43.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.90s
                        Total time: 28275.79s
                               ETA: 1041167.2s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.645s, learning 0.202s)
               Value function loss: 16.6797
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 42.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.85s
                        Total time: 28284.64s
                               ETA: 1041088.6s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.540s, learning 0.186s)
               Value function loss: 70.0572
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.73s
                        Total time: 28293.36s
                               ETA: 1041005.5s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.275s, learning 0.164s)
               Value function loss: 0.4857
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 41.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.44s
                        Total time: 28301.80s
                               ETA: 1040911.9s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.230s, learning 0.198s)
               Value function loss: 0.3389
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.43s
                        Total time: 28310.23s
                               ETA: 1040818.0s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.139s, learning 0.240s)
               Value function loss: 0.1732
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 41.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 8.38s
                        Total time: 28318.61s
                               ETA: 1040722.3s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.427s, learning 0.163s)
               Value function loss: 0.1433
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 42.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 8.59s
                        Total time: 28327.20s
                               ETA: 1040634.5s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.624s, learning 0.230s)
               Value function loss: 0.1244
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 8.85s
                        Total time: 28336.06s
                               ETA: 1040556.4s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.453s, learning 0.157s)
               Value function loss: 0.1314
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 8.61s
                        Total time: 28344.67s
                               ETA: 1040469.4s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.641s, learning 0.197s)
               Value function loss: 71.0023
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.49
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 8.84s
                        Total time: 28353.50s
                               ETA: 1040390.8s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.755s, learning 0.176s)
               Value function loss: 0.1511
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 43.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 8.93s
                        Total time: 28362.43s
                               ETA: 1040315.7s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.385s, learning 0.180s)
               Value function loss: 13.9421
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.55
               Mean episode length: 43.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 8.57s
                        Total time: 28371.00s
                               ETA: 1040227.2s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.362s, learning 0.213s)
               Value function loss: 0.1553
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 43.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 8.58s
                        Total time: 28379.57s
                               ETA: 1040139.2s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.441s, learning 0.162s)
               Value function loss: 72.5329
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.47
               Mean episode length: 44.12
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 8.60s
                        Total time: 28388.18s
                               ETA: 1040052.2s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.165s)
               Value function loss: 0.2376
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 43.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 8.62s
                        Total time: 28396.80s
                               ETA: 1039965.9s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.161s)
               Value function loss: 0.1827
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 42.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 8.53s
                        Total time: 28405.33s
                               ETA: 1039876.4s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.167s)
               Value function loss: 0.1539
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 43.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 8.42s
                        Total time: 28413.75s
                               ETA: 1039782.9s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.164s)
               Value function loss: 12.1269
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 43.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 8.60s
                        Total time: 28422.35s
                               ETA: 1039696.2s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.437s, learning 0.179s)
               Value function loss: 0.1340
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 8.62s
                        Total time: 28430.97s
                               ETA: 1039610.0s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.479s, learning 0.213s)
               Value function loss: 7.0245
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.92
               Mean episode length: 43.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 8.69s
                        Total time: 28439.66s
                               ETA: 1039526.6s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.172s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0389
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 8.65s
                        Total time: 28448.31s
                               ETA: 1039441.9s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.314s, learning 0.167s)
               Value function loss: 39.1380
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 7.21
               Mean episode length: 42.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 8.48s
                        Total time: 28456.79s
                               ETA: 1039350.9s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.986s, learning 0.173s)
               Value function loss: 21.3476
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 42.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 8.16s
                        Total time: 28464.95s
                               ETA: 1039248.3s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.502s, learning 0.187s)
               Value function loss: 0.2066
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 43.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 8.69s
                        Total time: 28473.64s
                               ETA: 1039165.0s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.607s, learning 0.166s)
               Value function loss: 3.8716
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 41.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 8.77s
                        Total time: 28482.41s
                               ETA: 1039084.9s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.419s, learning 0.191s)
               Value function loss: 6.7458
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 43.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 8.61s
                        Total time: 28491.02s
                               ETA: 1038998.9s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.460s, learning 0.182s)
               Value function loss: 0.1216
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 8.64s
                        Total time: 28499.66s
                               ETA: 1038914.2s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.445s, learning 0.163s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 8.61s
                        Total time: 28508.27s
                               ETA: 1038828.2s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.334s, learning 0.159s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 8.49s
                        Total time: 28516.77s
                               ETA: 1038738.1s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.407s, learning 0.173s)
               Value function loss: 23.3603
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 42.68
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 8.58s
                        Total time: 28525.35s
                               ETA: 1038651.3s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.653s, learning 0.174s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 43.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 8.83s
                        Total time: 28534.17s
                               ETA: 1038573.4s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.166s)
               Value function loss: 16.3401
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 42.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 8.64s
                        Total time: 28542.81s
                               ETA: 1038488.7s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.639s, learning 0.199s)
               Value function loss: 0.1889
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 4.94
               Mean episode length: 43.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 8.84s
                        Total time: 28551.65s
                               ETA: 1038411.4s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.250s, learning 0.163s)
               Value function loss: 13.7089
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.80
               Mean episode length: 42.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 8.41s
                        Total time: 28560.06s
                               ETA: 1038318.7s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.499s, learning 0.184s)
               Value function loss: 63.2243
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.74
               Mean episode length: 43.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 8.68s
                        Total time: 28568.74s
                               ETA: 1038235.9s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.557s, learning 0.167s)
               Value function loss: 0.1888
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 2.44
               Mean episode length: 43.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 8.72s
                        Total time: 28577.47s
                               ETA: 1038154.6s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.168s)
               Value function loss: 0.1570
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 8.68s
                        Total time: 28586.15s
                               ETA: 1038071.8s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.164s)
               Value function loss: 16.6209
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.87
               Mean episode length: 41.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 8.57s
                        Total time: 28594.71s
                               ETA: 1037984.9s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.208s, learning 0.161s)
               Value function loss: 17.7382
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 42.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 8.37s
                        Total time: 28603.08s
                               ETA: 1037890.9s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.169s)
               Value function loss: 3.9099
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 42.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 8.62s
                        Total time: 28611.71s
                               ETA: 1037806.2s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.610s, learning 0.169s)
               Value function loss: 13.7405
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 40.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 8.78s
                        Total time: 28620.49s
                               ETA: 1037727.2s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.509s, learning 0.184s)
               Value function loss: 16.6495
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 43.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.69s
                        Total time: 28629.18s
                               ETA: 1037645.1s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.054s, learning 0.162s)
               Value function loss: 11.8656
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 42.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 8.22s
                        Total time: 28637.39s
                               ETA: 1037545.8s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.545s, learning 0.189s)
               Value function loss: 250.1008
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.20
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.73s
                        Total time: 28646.13s
                               ETA: 1037465.3s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.268s, learning 0.159s)
               Value function loss: 71.1755
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 4.95
               Mean episode length: 43.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.43s
                        Total time: 28654.56s
                               ETA: 1037373.8s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.725s, learning 0.179s)
               Value function loss: 1.3888
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 2.28
               Mean episode length: 41.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.90s
                        Total time: 28663.46s
                               ETA: 1037299.5s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.166s)
               Value function loss: 0.6106
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 2.60
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.57s
                        Total time: 28672.03s
                               ETA: 1037213.4s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.555s, learning 0.167s)
               Value function loss: 85.6600
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 42.57
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.72s
                        Total time: 28680.75s
                               ETA: 1037132.7s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.251s, learning 0.198s)
               Value function loss: 7.3624
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 10.06
               Mean episode length: 43.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 8.45s
                        Total time: 28689.20s
                               ETA: 1037042.2s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.344s, learning 0.163s)
               Value function loss: 0.4036
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 2.46
               Mean episode length: 43.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.51s
                        Total time: 28697.71s
                               ETA: 1036953.8s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.515s, learning 0.167s)
               Value function loss: 0.2011
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 42.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.68s
                        Total time: 28706.39s
                               ETA: 1036871.9s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.409s, learning 0.195s)
               Value function loss: 0.1738
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 2.40
               Mean episode length: 43.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 8.60s
                        Total time: 28714.99s
                               ETA: 1036787.1s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.406s, learning 0.172s)
               Value function loss: 0.1609
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.58s
                        Total time: 28723.57s
                               ETA: 1036701.5s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.167s)
               Value function loss: 0.1430
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 2.46
               Mean episode length: 43.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.80s
                        Total time: 28732.37s
                               ETA: 1036623.8s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.263s, learning 0.189s)
               Value function loss: 32.7895
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 44.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.45s
                        Total time: 28740.82s
                               ETA: 1036533.8s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.508s, learning 0.166s)
               Value function loss: 11.7425
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.93
               Mean episode length: 43.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.67s
                        Total time: 28749.50s
                               ETA: 1036451.8s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.165s)
               Value function loss: 0.2939
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.45s
                        Total time: 28757.95s
                               ETA: 1036362.0s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.401s, learning 0.172s)
               Value function loss: 0.1805
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 42.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.57s
                        Total time: 28766.52s
                               ETA: 1036276.5s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.678s, learning 0.251s)
               Value function loss: 79.5710
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 43.10
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.93s
                        Total time: 28775.45s
                               ETA: 1036203.8s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.161s)
               Value function loss: 23.6815
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 42.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.71s
                        Total time: 28784.16s
                               ETA: 1036123.3s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.942s, learning 0.210s)
               Value function loss: 0.2980
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 2.46
               Mean episode length: 43.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 9.15s
                        Total time: 28793.31s
                               ETA: 1036058.8s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.384s, learning 0.182s)
               Value function loss: 9.3893
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 43.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.57s
                        Total time: 28801.88s
                               ETA: 1035973.2s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.269s, learning 0.161s)
               Value function loss: 297.6283
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 40.85
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.43s
                        Total time: 28810.31s
                               ETA: 1035882.8s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.169s)
               Value function loss: 126.5872
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 2.55
               Mean episode length: 42.57
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.63s
                        Total time: 28818.94s
                               ETA: 1035799.7s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.322s, learning 0.167s)
               Value function loss: 2.2489
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 2.68
               Mean episode length: 44.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.49s
                        Total time: 28827.43s
                               ETA: 1035711.5s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.167s)
               Value function loss: 0.5921
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 7.28
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.46s
                        Total time: 28835.88s
                               ETA: 1035622.3s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.166s)
               Value function loss: 0.2915
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 2.49
               Mean episode length: 43.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.63s
                        Total time: 28844.51s
                               ETA: 1035539.3s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.511s, learning 0.161s)
               Value function loss: 0.1989
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.67s
                        Total time: 28853.18s
                               ETA: 1035457.9s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.718s, learning 0.168s)
               Value function loss: 0.1697
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 42.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.89s
                        Total time: 28862.07s
                               ETA: 1035384.2s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.440s, learning 0.178s)
               Value function loss: 0.1486
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 42.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.62s
                        Total time: 28870.69s
                               ETA: 1035301.0s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.591s, learning 0.170s)
               Value function loss: 0.1444
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 43.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.76s
                        Total time: 28879.45s
                               ETA: 1035222.9s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.247s, learning 0.184s)
               Value function loss: 0.1344
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 42.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.43s
                        Total time: 28887.88s
                               ETA: 1035133.1s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.525s, learning 0.198s)
               Value function loss: 15.9074
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.71
               Mean episode length: 43.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 8.72s
                        Total time: 28896.60s
                               ETA: 1035053.8s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.856s, learning 0.170s)
               Value function loss: 0.1342
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 4.69
               Mean episode length: 42.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 9.03s
                        Total time: 28905.63s
                               ETA: 1034985.4s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.211s, learning 0.174s)
               Value function loss: 7.2165
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 41.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.39s
                        Total time: 28914.02s
                               ETA: 1034894.1s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.817s, learning 0.166s)
               Value function loss: 0.1074
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 41.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 8.98s
                        Total time: 28923.00s
                               ETA: 1034824.3s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.175s)
               Value function loss: 73.1829
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 2.53
               Mean episode length: 43.35
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 8.47s
                        Total time: 28931.46s
                               ETA: 1034736.0s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.638s, learning 0.274s)
               Value function loss: 0.1694
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 42.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.91s
                        Total time: 28940.38s
                               ETA: 1034663.7s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.489s, learning 0.232s)
               Value function loss: 0.1471
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 2.45
               Mean episode length: 42.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 8.72s
                        Total time: 28949.10s
                               ETA: 1034584.6s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.766s, learning 0.204s)
               Value function loss: 9.5450
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.81
               Mean episode length: 43.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.97s
                        Total time: 28958.07s
                               ETA: 1034514.5s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.170s)
               Value function loss: 71.0851
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.84
               Mean episode length: 42.49
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 8.62s
                        Total time: 28966.69s
                               ETA: 1034431.9s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.533s, learning 0.157s)
               Value function loss: 13.8782
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 41.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 8.69s
                        Total time: 28975.38s
                               ETA: 1034351.9s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.163s)
               Value function loss: 79.9611
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 41.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 8.33s
                        Total time: 28983.71s
                               ETA: 1034259.2s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.162s)
               Value function loss: 16.9217
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.93
               Mean episode length: 42.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.36s
                        Total time: 28992.08s
                               ETA: 1034167.7s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.526s, learning 0.194s)
               Value function loss: 0.8043
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 4.66
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 8.72s
                        Total time: 29000.80s
                               ETA: 1034088.9s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.269s, learning 0.170s)
               Value function loss: 22.7321
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 41.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 8.44s
                        Total time: 29009.23s
                               ETA: 1034000.1s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.732s, learning 0.164s)
               Value function loss: 9.4473
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 4.67
               Mean episode length: 42.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.90s
                        Total time: 29018.13s
                               ETA: 1033927.7s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.164s)
               Value function loss: 50.4464
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 43.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.40s
                        Total time: 29026.53s
                               ETA: 1033837.5s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.160s)
               Value function loss: 2.2196
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.48s
                        Total time: 29035.01s
                               ETA: 1033750.4s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.039s, learning 0.198s)
               Value function loss: 0.6568
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 42.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.24s
                        Total time: 29043.25s
                               ETA: 1033654.7s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.558s, learning 0.296s)
               Value function loss: 0.2739
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 2.65
               Mean episode length: 44.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.85s
                        Total time: 29052.10s
                               ETA: 1033581.0s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.871s, learning 0.193s)
               Value function loss: 0.1758
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 2.51
               Mean episode length: 44.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 9.06s
                        Total time: 29061.16s
                               ETA: 1033514.8s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.160s)
               Value function loss: 3.9637
                    Surrogate loss: 0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 42.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 8.72s
                        Total time: 29069.88s
                               ETA: 1033436.3s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.161s)
               Value function loss: 0.1507
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 2.61
               Mean episode length: 43.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 8.36s
                        Total time: 29078.24s
                               ETA: 1033345.1s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.162s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 42.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 8.58s
                        Total time: 29086.81s
                               ETA: 1033261.8s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.515s, learning 0.161s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 2.61
               Mean episode length: 43.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 8.68s
                        Total time: 29095.49s
                               ETA: 1033182.1s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.595s, learning 0.164s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 43.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 8.76s
                        Total time: 29104.25s
                               ETA: 1033105.3s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.842s, learning 0.159s)
               Value function loss: 3.8687
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 42.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 9.00s
                        Total time: 29113.25s
                               ETA: 1033037.1s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.288s, learning 0.157s)
               Value function loss: 0.1004
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 4.99
               Mean episode length: 44.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 8.44s
                        Total time: 29121.70s
                               ETA: 1032949.3s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.310s, learning 0.168s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 8.48s
                        Total time: 29130.17s
                               ETA: 1032862.7s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.287s, learning 0.187s)
               Value function loss: 0.1204
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 42.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 8.47s
                        Total time: 29138.65s
                               ETA: 1032776.0s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.870s, learning 0.195s)
               Value function loss: 16.5324
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 42.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 9.06s
                        Total time: 29147.71s
                               ETA: 1032710.3s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.127s, learning 0.170s)
               Value function loss: 38.6892
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 43.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 11.30s
                        Total time: 29159.01s
                               ETA: 1032723.7s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.405s, learning 0.190s)
               Value function loss: 0.1598
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 16.59s
                        Total time: 29175.60s
                               ETA: 1032924.7s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.407s, learning 0.165s)
               Value function loss: 12.1173
                    Surrogate loss: 0.0022
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 16.57s
                        Total time: 29192.18s
                               ETA: 1033124.7s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.729s, learning 0.258s)
               Value function loss: 0.1716
                    Surrogate loss: -0.0345
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 16.99s
                        Total time: 29209.16s
                               ETA: 1033339.3s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.538s, learning 0.168s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0351
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 42.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 16.71s
                        Total time: 29225.87s
                               ETA: 1033543.7s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.776s, learning 0.217s)
               Value function loss: 0.0996
                    Surrogate loss: -0.0364
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 43.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 16.99s
                        Total time: 29242.86s
                               ETA: 1033758.0s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.485s, learning 0.167s)
               Value function loss: 11.8246
                    Surrogate loss: 0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 43.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 16.65s
                        Total time: 29259.51s
                               ETA: 1033960.2s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.595s, learning 0.314s)
               Value function loss: 0.1036
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 43.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 16.91s
                        Total time: 29276.42s
                               ETA: 1034171.3s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.830s, learning 0.193s)
               Value function loss: 38.5504
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 17.02s
                        Total time: 29293.45s
                               ETA: 1034386.3s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.733s, learning 0.173s)
               Value function loss: 16.2439
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 42.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 16.91s
                        Total time: 29310.35s
                               ETA: 1034596.9s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.665s, learning 0.164s)
               Value function loss: 0.2205
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 42.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 16.83s
                        Total time: 29327.18s
                               ETA: 1034804.7s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.548s, learning 0.171s)
               Value function loss: 52.0843
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 42.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 16.72s
                        Total time: 29343.90s
                               ETA: 1035008.4s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.485s, learning 0.173s)
               Value function loss: 11.7850
                    Surrogate loss: -0.0032
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 16.66s
                        Total time: 29360.56s
                               ETA: 1035209.8s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.302s, learning 0.204s)
               Value function loss: 0.2366
                    Surrogate loss: -0.0305
             Mean action noise std: 0.70
                       Mean reward: 1.86
               Mean episode length: 41.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 16.51s
                        Total time: 29377.06s
                               ETA: 1035405.8s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.731s, learning 0.229s)
               Value function loss: 13.4583
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 42.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 16.96s
                        Total time: 29394.02s
                               ETA: 1035617.5s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.667s, learning 0.198s)
               Value function loss: 17.8019
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 43.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 16.86s
                        Total time: 29410.89s
                               ETA: 1035825.7s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.180s, learning 0.170s)
               Value function loss: 0.6236
                    Surrogate loss: -0.0249
             Mean action noise std: 0.70
                       Mean reward: 4.75
               Mean episode length: 43.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 16.35s
                        Total time: 29427.24s
                               ETA: 1036015.6s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.771s, learning 0.170s)
               Value function loss: 0.2134
                    Surrogate loss: -0.0303
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 16.94s
                        Total time: 29444.18s
                               ETA: 1036226.2s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.538s, learning 0.162s)
               Value function loss: 0.1435
                    Surrogate loss: -0.0370
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 16.70s
                        Total time: 29460.88s
                               ETA: 1036428.2s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.438s, learning 0.165s)
               Value function loss: 82.0175
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 43.99
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 16.60s
                        Total time: 29477.48s
                               ETA: 1036626.5s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.449s, learning 0.204s)
               Value function loss: 40.3592
                    Surrogate loss: -0.0033
             Mean action noise std: 0.70
                       Mean reward: 7.66
               Mean episode length: 43.76
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 16.65s
                        Total time: 29494.13s
                               ETA: 1036826.5s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.466s, learning 0.187s)
               Value function loss: 0.2967
                    Surrogate loss: -0.0382
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 42.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 16.65s
                        Total time: 29510.79s
                               ETA: 1037026.3s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.564s, learning 0.174s)
               Value function loss: 0.2098
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 42.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 16.74s
                        Total time: 29527.52s
                               ETA: 1037229.0s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.360s, learning 0.197s)
               Value function loss: 0.1584
                    Surrogate loss: -0.0326
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 16.56s
                        Total time: 29544.08s
                               ETA: 1037425.1s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.558s, learning 0.211s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0326
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 41.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 16.77s
                        Total time: 29560.85s
                               ETA: 1037628.6s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.351s, learning 0.202s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0353
             Mean action noise std: 0.70
                       Mean reward: 1.96
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 16.55s
                        Total time: 29577.40s
                               ETA: 1037824.2s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.053s, learning 0.170s)
               Value function loss: 29.4764
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 17.22s
                        Total time: 29594.63s
                               ETA: 1038043.3s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.518s, learning 0.175s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 43.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 16.69s
                        Total time: 29611.32s
                               ETA: 1038243.6s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.800s, learning 0.171s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0351
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 16.97s
                        Total time: 29628.29s
                               ETA: 1038453.5s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.464s, learning 0.277s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0361
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 42.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 16.74s
                        Total time: 29645.03s
                               ETA: 1038655.1s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.856s, learning 0.167s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0365
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 42.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 17.02s
                        Total time: 29662.06s
                               ETA: 1038866.5s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.838s, learning 0.171s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0348
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 41.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 17.01s
                        Total time: 29679.06s
                               ETA: 1039077.2s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.480s, learning 0.172s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0346
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 16.65s
                        Total time: 29695.72s
                               ETA: 1039275.2s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.675s, learning 0.159s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 41.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 16.83s
                        Total time: 29712.55s
                               ETA: 1039479.5s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.403s, learning 0.159s)
               Value function loss: 64.0378
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 42.90
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 16.56s
                        Total time: 29729.11s
                               ETA: 1039674.1s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.438s, learning 0.213s)
               Value function loss: 3.8391
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 16.65s
                        Total time: 29745.76s
                               ETA: 1039871.6s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.647s, learning 0.157s)
               Value function loss: 0.1653
                    Surrogate loss: -0.0324
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 42.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 16.80s
                        Total time: 29762.57s
                               ETA: 1040074.4s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.697s, learning 0.190s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0324
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 42.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 16.89s
                        Total time: 29779.45s
                               ETA: 1040279.9s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.317s, learning 0.177s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0370
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 11.49s
                        Total time: 29790.95s
                               ETA: 1040296.9s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.712s, learning 0.186s)
               Value function loss: 0.0982
                    Surrogate loss: -0.0353
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 42.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.90s
                        Total time: 29799.85s
                               ETA: 1040223.3s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.487s, learning 0.157s)
               Value function loss: 58.5218
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.64s
                        Total time: 29808.49s
                               ETA: 1040140.8s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.443s, learning 0.167s)
               Value function loss: 0.1821
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 41.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.61s
                        Total time: 29817.10s
                               ETA: 1040057.2s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.535s, learning 0.161s)
               Value function loss: 3.8155
                    Surrogate loss: -0.0035
             Mean action noise std: 0.70
                       Mean reward: 1.69
               Mean episode length: 40.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.70s
                        Total time: 29825.79s
                               ETA: 1039976.7s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.159s)
               Value function loss: 17.4960
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 42.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.47s
                        Total time: 29834.26s
                               ETA: 1039888.2s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.167s, learning 0.218s)
               Value function loss: 3.8900
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 7.02
               Mean episode length: 40.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.39s
                        Total time: 29842.65s
                               ETA: 1039797.0s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.402s, learning 0.164s)
               Value function loss: 0.2016
                    Surrogate loss: -0.0353
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 43.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.57s
                        Total time: 29851.21s
                               ETA: 1039712.1s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.425s, learning 0.158s)
               Value function loss: 20.9857
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 41.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.58s
                        Total time: 29859.80s
                               ETA: 1039627.9s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.160s)
               Value function loss: 0.2122
                    Surrogate loss: -0.0318
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 43.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.67s
                        Total time: 29868.47s
                               ETA: 1039546.9s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.254s, learning 0.162s)
               Value function loss: 0.1773
                    Surrogate loss: -0.0312
             Mean action noise std: 0.70
                       Mean reward: 1.82
               Mean episode length: 40.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.42s
                        Total time: 29876.89s
                               ETA: 1039456.9s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.351s, learning 0.183s)
               Value function loss: 9.8458
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 41.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.53s
                        Total time: 29885.42s
                               ETA: 1039371.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.237s, learning 0.175s)
               Value function loss: 0.1212
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.41s
                        Total time: 29893.83s
                               ETA: 1039281.1s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.193s, learning 0.199s)
               Value function loss: 0.1143
                    Surrogate loss: -0.0317
             Mean action noise std: 0.70
                       Mean reward: 1.74
               Mean episode length: 41.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.39s
                        Total time: 29902.22s
                               ETA: 1039190.5s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.516s, learning 0.193s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0362
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.71s
                        Total time: 29910.93s
                               ETA: 1039111.0s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.550s, learning 0.168s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0369
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 42.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.72s
                        Total time: 29919.65s
                               ETA: 1039031.8s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.524s, learning 0.178s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0364
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.70s
                        Total time: 29928.35s
                               ETA: 1038952.1s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.375s, learning 0.169s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 41.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.54s
                        Total time: 29936.90s
                               ETA: 1038867.0s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.705s, learning 0.205s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 40.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.91s
                        Total time: 29945.81s
                               ETA: 1038794.6s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.271s, learning 0.175s)
               Value function loss: 15.2991
                    Surrogate loss: 0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.45s
                        Total time: 29954.25s
                               ETA: 1038706.2s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.288s, learning 0.193s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0405
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 42.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.48s
                        Total time: 29962.73s
                               ETA: 1038619.1s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.318s, learning 0.190s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0397
             Mean action noise std: 0.70
                       Mean reward: 1.95
               Mean episode length: 41.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.51s
                        Total time: 29971.24s
                               ETA: 1038533.0s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.486s, learning 0.201s)
               Value function loss: 13.9163
                    Surrogate loss: 0.0013
             Mean action noise std: 0.70
                       Mean reward: 1.78
               Mean episode length: 40.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.69s
                        Total time: 29979.93s
                               ETA: 1038453.1s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.534s, learning 0.216s)
               Value function loss: 0.1249
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.75s
                        Total time: 29988.68s
                               ETA: 1038375.4s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.609s, learning 0.163s)
               Value function loss: 0.0970
                    Surrogate loss: -0.0335
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 42.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.77s
                        Total time: 29997.45s
                               ETA: 1038298.5s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.160s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0379
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 8.62s
                        Total time: 30006.07s
                               ETA: 1038216.5s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.494s, learning 0.177s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0418
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.67s
                        Total time: 30014.74s
                               ETA: 1038136.3s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.433s, learning 0.166s)
               Value function loss: 47.0408
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.60s
                        Total time: 30023.34s
                               ETA: 1038053.6s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.109s, learning 0.173s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0421
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 8.28s
                        Total time: 30031.62s
                               ETA: 1037960.0s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.336s, learning 0.169s)
               Value function loss: 120.4233
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 42.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 8.51s
                        Total time: 30040.13s
                               ETA: 1037874.2s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.390s, learning 0.202s)
               Value function loss: 0.2171
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 41.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.59s
                        Total time: 30048.72s
                               ETA: 1037791.4s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.485s, learning 0.173s)
               Value function loss: 13.6138
                    Surrogate loss: 0.0035
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 42.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.66s
                        Total time: 30057.38s
                               ETA: 1037711.0s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.281s, learning 0.166s)
               Value function loss: 0.1816
                    Surrogate loss: -0.0363
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 42.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.45s
                        Total time: 30065.83s
                               ETA: 1037623.3s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.407s, learning 0.183s)
               Value function loss: 0.1309
                    Surrogate loss: -0.0332
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 41.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 8.59s
                        Total time: 30074.42s
                               ETA: 1037540.6s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.248s, learning 0.183s)
               Value function loss: 0.1026
                    Surrogate loss: -0.0390
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.43s
                        Total time: 30082.85s
                               ETA: 1037452.5s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.289s, learning 0.160s)
               Value function loss: 0.1024
                    Surrogate loss: -0.0341
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 8.45s
                        Total time: 30091.30s
                               ETA: 1037365.1s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.718s, learning 0.192s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0356
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 41.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 8.91s
                        Total time: 30100.21s
                               ETA: 1037293.7s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.582s, learning 0.195s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0343
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 41.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 8.78s
                        Total time: 30108.98s
                               ETA: 1037217.6s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.600s, learning 0.183s)
               Value function loss: 53.7841
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 42.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.78s
                        Total time: 30117.77s
                               ETA: 1037141.9s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.033s, learning 0.215s)
               Value function loss: 38.9061
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 8.25s
                        Total time: 30126.01s
                               ETA: 1037047.7s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.572s, learning 0.194s)
               Value function loss: 7.0542
                    Surrogate loss: -0.0050
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 42.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.77s
                        Total time: 30134.78s
                               ETA: 1036971.5s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.195s)
               Value function loss: 0.2286
                    Surrogate loss: -0.0317
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.74s
                        Total time: 30143.51s
                               ETA: 1036894.2s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.416s, learning 0.192s)
               Value function loss: 52.5671
                    Surrogate loss: -0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.61s
                        Total time: 30152.12s
                               ETA: 1036812.6s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.571s, learning 0.174s)
               Value function loss: 0.2923
                    Surrogate loss: -0.0306
             Mean action noise std: 0.70
                       Mean reward: 2.39
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 8.74s
                        Total time: 30160.87s
                               ETA: 1036735.8s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.366s, learning 0.167s)
               Value function loss: 0.1941
                    Surrogate loss: -0.0280
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.53s
                        Total time: 30169.40s
                               ETA: 1036651.7s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.415s, learning 0.164s)
               Value function loss: 34.0128
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 1.84
               Mean episode length: 40.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.58s
                        Total time: 30177.98s
                               ETA: 1036569.3s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.138s, learning 0.171s)
               Value function loss: 151.6435
                    Surrogate loss: -0.0034
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 41.18
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.31s
                        Total time: 30186.29s
                               ETA: 1036477.7s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.165s)
               Value function loss: 4.2898
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 1.85
               Mean episode length: 40.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.36s
                        Total time: 30194.65s
                               ETA: 1036387.8s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.951s, learning 0.171s)
               Value function loss: 0.7065
                    Surrogate loss: -0.0258
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.12s
                        Total time: 30202.77s
                               ETA: 1036289.9s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.472s, learning 0.183s)
               Value function loss: 0.2353
                    Surrogate loss: -0.0260
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.65s
                        Total time: 30211.42s
                               ETA: 1036210.3s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.562s, learning 0.278s)
               Value function loss: 0.1759
                    Surrogate loss: -0.0327
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 41.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.84s
                        Total time: 30220.26s
                               ETA: 1036137.1s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.458s, learning 0.165s)
               Value function loss: 26.1790
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 4.79
               Mean episode length: 42.48
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 8.62s
                        Total time: 30228.89s
                               ETA: 1036056.5s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.743s, learning 0.277s)
               Value function loss: 9.5605
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 42.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.02s
                        Total time: 30236.91s
                               ETA: 1035955.3s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.361s, learning 0.166s)
               Value function loss: 0.1642
                    Surrogate loss: -0.0383
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 41.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 8.53s
                        Total time: 30245.44s
                               ETA: 1035871.5s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.320s, learning 0.196s)
               Value function loss: 0.1529
                    Surrogate loss: -0.0341
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 41.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 8.52s
                        Total time: 30253.95s
                               ETA: 1035787.4s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.299s, learning 0.195s)
               Value function loss: 0.1328
                    Surrogate loss: -0.0359
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 42.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 8.49s
                        Total time: 30262.45s
                               ETA: 1035702.6s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.408s, learning 0.161s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 43.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 8.57s
                        Total time: 30271.02s
                               ETA: 1035620.5s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.563s, learning 0.165s)
               Value function loss: 17.9030
                    Surrogate loss: -0.0009
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 8.73s
                        Total time: 30279.74s
                               ETA: 1035543.8s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.591s, learning 0.162s)
               Value function loss: 9.6391
                    Surrogate loss: -0.0020
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 41.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 8.75s
                        Total time: 30288.50s
                               ETA: 1035468.0s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.367s, learning 0.171s)
               Value function loss: 120.5193
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 42.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 8.54s
                        Total time: 30297.03s
                               ETA: 1035384.9s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.693s, learning 0.165s)
               Value function loss: 96.0166
                    Surrogate loss: -0.0034
             Mean action noise std: 0.70
                       Mean reward: 9.81
               Mean episode length: 42.47
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 8.86s
                        Total time: 30305.89s
                               ETA: 1035312.8s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.465s, learning 0.163s)
               Value function loss: 1.3334
                    Surrogate loss: -0.0278
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 41.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 8.63s
                        Total time: 30314.52s
                               ETA: 1035232.9s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.782s, learning 0.195s)
               Value function loss: 0.5494
                    Surrogate loss: -0.0276
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 8.98s
                        Total time: 30323.50s
                               ETA: 1035164.9s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.389s, learning 0.160s)
               Value function loss: 4.0071
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 41.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 8.55s
                        Total time: 30332.05s
                               ETA: 1035082.4s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.592s, learning 0.169s)
               Value function loss: 0.2069
                    Surrogate loss: -0.0362
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 41.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 8.76s
                        Total time: 30340.81s
                               ETA: 1035007.2s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.592s, learning 0.193s)
               Value function loss: 15.2489
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 43.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 8.79s
                        Total time: 30349.59s
                               ETA: 1034932.8s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.616s, learning 0.162s)
               Value function loss: 46.9842
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 4.87
               Mean episode length: 43.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 8.78s
                        Total time: 30358.37s
                               ETA: 1034858.3s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.407s, learning 0.187s)
               Value function loss: 0.3806
                    Surrogate loss: -0.0340
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 8.59s
                        Total time: 30366.96s
                               ETA: 1034777.5s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.738s, learning 0.271s)
               Value function loss: 0.2656
                    Surrogate loss: -0.0311
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 9.01s
                        Total time: 30375.97s
                               ETA: 1034710.9s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.163s)
               Value function loss: 9.8955
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 42.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 8.59s
                        Total time: 30384.56s
                               ETA: 1034630.0s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.574s, learning 0.243s)
               Value function loss: 0.1769
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 41.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 8.82s
                        Total time: 30393.38s
                               ETA: 1034557.0s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.272s, learning 0.172s)
               Value function loss: 215.8510
                    Surrogate loss: -0.0009
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 43.30
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 8.44s
                        Total time: 30401.82s
                               ETA: 1034471.3s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.645s, learning 0.160s)
               Value function loss: 7.1355
                    Surrogate loss: -0.0049
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 8.80s
                        Total time: 30410.63s
                               ETA: 1034397.9s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.347s, learning 0.178s)
               Value function loss: 0.1635
                    Surrogate loss: -0.0390
             Mean action noise std: 0.70
                       Mean reward: 1.95
               Mean episode length: 41.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 8.53s
                        Total time: 30419.15s
                               ETA: 1034315.1s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.375s, learning 0.167s)
               Value function loss: 279.5168
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 8.54s
                        Total time: 30427.70s
                               ETA: 1034232.9s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.447s, learning 0.163s)
               Value function loss: 13.8279
                    Surrogate loss: -0.0033
             Mean action noise std: 0.70
                       Mean reward: 4.72
               Mean episode length: 41.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 8.61s
                        Total time: 30436.31s
                               ETA: 1034153.1s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.411s, learning 0.247s)
               Value function loss: 59.3776
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 1.83
               Mean episode length: 41.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 8.66s
                        Total time: 30444.96s
                               ETA: 1034074.9s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.524s, learning 0.194s)
               Value function loss: 17.3626
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 41.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 8.72s
                        Total time: 30453.68s
                               ETA: 1033998.8s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.225s, learning 0.220s)
               Value function loss: 46.8125
                    Surrogate loss: -0.0035
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 42.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 8.45s
                        Total time: 30462.13s
                               ETA: 1033913.5s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.319s, learning 0.161s)
               Value function loss: 31.4403
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 42.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 8.48s
                        Total time: 30470.61s
                               ETA: 1033829.5s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.133s, learning 0.194s)
               Value function loss: 17.4293
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 8.33s
                        Total time: 30478.93s
                               ETA: 1033740.3s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.176s)
               Value function loss: 29.8678
                    Surrogate loss: 0.0016
             Mean action noise std: 0.70
                       Mean reward: 12.22
               Mean episode length: 41.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 8.51s
                        Total time: 30487.44s
                               ETA: 1033657.3s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.233s, learning 0.174s)
               Value function loss: 291.9797
                    Surrogate loss: -0.0033
             Mean action noise std: 0.70
                       Mean reward: 7.42
               Mean episode length: 42.68
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 8.41s
                        Total time: 30495.85s
                               ETA: 1033571.0s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.394s, learning 0.180s)
               Value function loss: 3.1241
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 9.54
               Mean episode length: 41.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 8.57s
                        Total time: 30504.42s
                               ETA: 1033490.3s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.359s, learning 0.194s)
               Value function loss: 64.8310
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 41.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 8.55s
                        Total time: 30512.98s
                               ETA: 1033409.0s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.200s, learning 0.169s)
               Value function loss: 0.6433
                    Surrogate loss: -0.0281
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 8.37s
                        Total time: 30521.35s
                               ETA: 1033321.5s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.726s, learning 0.202s)
               Value function loss: 0.3740
                    Surrogate loss: -0.0193
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 8.93s
                        Total time: 30530.27s
                               ETA: 1033253.0s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.473s, learning 0.170s)
               Value function loss: 133.0859
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 8.64s
                        Total time: 30538.92s
                               ETA: 1033174.8s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.646s, learning 0.172s)
               Value function loss: 0.3309
                    Surrogate loss: -0.0230
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 41.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 8.82s
                        Total time: 30547.73s
                               ETA: 1033102.7s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.517s, learning 0.165s)
               Value function loss: 0.2498
                    Surrogate loss: -0.0336
             Mean action noise std: 0.70
                       Mean reward: 1.98
               Mean episode length: 40.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 8.68s
                        Total time: 30556.42s
                               ETA: 1033026.0s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.613s, learning 0.179s)
               Value function loss: 3.9701
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 1.76
               Mean episode length: 41.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 8.79s
                        Total time: 30565.21s
                               ETA: 1032953.0s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.470s, learning 0.192s)
               Value function loss: 17.3554
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 7.29
               Mean episode length: 41.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 8.66s
                        Total time: 30573.87s
                               ETA: 1032875.7s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.522s, learning 0.205s)
               Value function loss: 0.1908
                    Surrogate loss: -0.0351
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 8.73s
                        Total time: 30582.60s
                               ETA: 1032800.7s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.355s, learning 0.175s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0356
             Mean action noise std: 0.70
                       Mean reward: 1.85
               Mean episode length: 40.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 8.53s
                        Total time: 30591.13s
                               ETA: 1032719.0s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.594s, learning 0.159s)
               Value function loss: 0.1377
                    Surrogate loss: -0.0359
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 41.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 8.75s
                        Total time: 30599.88s
                               ETA: 1032644.9s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.372s, learning 0.194s)
               Value function loss: 29.3642
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 8.57s
                        Total time: 30608.45s
                               ETA: 1032564.6s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.543s, learning 0.195s)
               Value function loss: 3.9824
                    Surrogate loss: -0.0065
             Mean action noise std: 0.70
                       Mean reward: 4.65
               Mean episode length: 41.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 8.74s
                        Total time: 30617.18s
                               ETA: 1032490.1s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.486s, learning 0.171s)
               Value function loss: 17.5760
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 40.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 8.66s
                        Total time: 30625.84s
                               ETA: 1032412.9s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.475s, learning 0.197s)
               Value function loss: 11.0831
                    Surrogate loss: -0.0053
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 41.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 8.67s
                        Total time: 30634.51s
                               ETA: 1032336.3s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.611s, learning 0.213s)
               Value function loss: 0.2008
                    Surrogate loss: -0.0329
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 8.82s
                        Total time: 30643.34s
                               ETA: 1032264.8s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.504s, learning 0.246s)
               Value function loss: 0.1844
                    Surrogate loss: -0.0321
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.75s
                        Total time: 30652.09s
                               ETA: 1032191.0s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.379s, learning 0.191s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0337
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.57s
                        Total time: 30660.66s
                               ETA: 1032111.1s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.198s, learning 0.232s)
               Value function loss: 0.1133
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.43s
                        Total time: 30669.09s
                               ETA: 1032026.5s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.414s, learning 0.249s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 8.66s
                        Total time: 30677.75s
                               ETA: 1031949.8s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.892s, learning 0.179s)
               Value function loss: 3.8282
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 4.75
               Mean episode length: 43.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 9.07s
                        Total time: 30686.82s
                               ETA: 1031886.9s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.476s, learning 0.263s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.74s
                        Total time: 30695.56s
                               ETA: 1031812.9s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1771 steps/s (collection: 8.967s, learning 0.280s)
               Value function loss: 42.4988
                    Surrogate loss: 0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 42.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 9.25s
                        Total time: 30704.81s
                               ETA: 1031755.9s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.484s, learning 0.218s)
               Value function loss: 89.3165
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 9.85
               Mean episode length: 43.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.70s
                        Total time: 30713.51s
                               ETA: 1031680.7s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.496s, learning 0.169s)
               Value function loss: 0.3919
                    Surrogate loss: -0.0338
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 43.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.67s
                        Total time: 30722.18s
                               ETA: 1031604.3s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.425s, learning 0.189s)
               Value function loss: 0.3355
                    Surrogate loss: -0.0222
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.61s
                        Total time: 30730.79s
                               ETA: 1031526.3s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.428s, learning 0.173s)
               Value function loss: 0.2627
                    Surrogate loss: -0.0264
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 41.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.60s
                        Total time: 30739.39s
                               ETA: 1031447.8s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.450s, learning 0.166s)
               Value function loss: 0.1788
                    Surrogate loss: -0.0293
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.62s
                        Total time: 30748.01s
                               ETA: 1031369.9s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.452s, learning 0.164s)
               Value function loss: 10.0106
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.62s
                        Total time: 30756.62s
                               ETA: 1031292.0s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.499s, learning 0.262s)
               Value function loss: 0.1383
                    Surrogate loss: -0.0356
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 40.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.76s
                        Total time: 30765.38s
                               ETA: 1031219.1s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.557s, learning 0.167s)
               Value function loss: 0.1211
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 43.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.72s
                        Total time: 30774.11s
                               ETA: 1031144.9s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.609s, learning 0.194s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 42.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.80s
                        Total time: 30782.91s
                               ETA: 1031073.5s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.374s, learning 0.187s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.56s
                        Total time: 30791.47s
                               ETA: 1030994.0s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.275s, learning 0.161s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.44s
                        Total time: 30799.91s
                               ETA: 1030910.3s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.163s, learning 0.158s)
               Value function loss: 82.8273
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 7.32
               Mean episode length: 41.84
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.32s
                        Total time: 30808.23s
                               ETA: 1030822.9s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.344s, learning 0.199s)
               Value function loss: 0.1250
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 44.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.54s
                        Total time: 30816.77s
                               ETA: 1030742.9s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.365s, learning 0.202s)
               Value function loss: 53.5345
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 43.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.57s
                        Total time: 30825.34s
                               ETA: 1030663.8s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.086s, learning 0.159s)
               Value function loss: 0.2636
                    Surrogate loss: -0.0338
             Mean action noise std: 0.70
                       Mean reward: 7.18
               Mean episode length: 42.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.25s
                        Total time: 30833.58s
                               ETA: 1030574.0s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.502s, learning 0.169s)
               Value function loss: 0.2058
                    Surrogate loss: -0.0305
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 42.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.67s
                        Total time: 30842.25s
                               ETA: 1030498.5s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.174s, learning 0.307s)
               Value function loss: 11.8510
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 42.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.48s
                        Total time: 30850.73s
                               ETA: 1030416.7s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1080 steps/s (collection: 14.940s, learning 0.217s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0363
             Mean action noise std: 0.70
                       Mean reward: 2.78
               Mean episode length: 44.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 15.16s
                        Total time: 30865.89s
                               ETA: 1030557.8s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.420s, learning 0.165s)
               Value function loss: 0.1093
                    Surrogate loss: -0.0354
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 42.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 16.58s
                        Total time: 30882.48s
                               ETA: 1030746.4s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.373s, learning 0.211s)
               Value function loss: 7.1246
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 16.58s
                        Total time: 30899.06s
                               ETA: 1030935.0s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.395s, learning 0.206s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 42.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 16.60s
                        Total time: 30915.66s
                               ETA: 1031123.9s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.551s, learning 0.215s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0346
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 43.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 16.77s
                        Total time: 30932.43s
                               ETA: 1031318.1s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.331s, learning 0.180s)
               Value function loss: 59.4457
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 41.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 16.51s
                        Total time: 30948.94s
                               ETA: 1031503.8s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.335s, learning 0.178s)
               Value function loss: 0.1428
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 43.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 16.51s
                        Total time: 30965.45s
                               ETA: 1031689.3s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.297s, learning 0.176s)
               Value function loss: 0.1521
                    Surrogate loss: -0.0338
             Mean action noise std: 0.70
                       Mean reward: 7.37
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 16.47s
                        Total time: 30981.92s
                               ETA: 1031873.4s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.740s, learning 0.162s)
               Value function loss: 0.1196
                    Surrogate loss: -0.0336
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 16.90s
                        Total time: 30998.83s
                               ETA: 1032071.7s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.481s, learning 0.185s)
               Value function loss: 0.1129
                    Surrogate loss: -0.0339
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 43.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 16.67s
                        Total time: 31015.49s
                               ETA: 1032261.9s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.304s, learning 0.172s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0366
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 42.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 16.48s
                        Total time: 31031.97s
                               ETA: 1032445.7s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.055s, learning 0.190s)
               Value function loss: 0.0985
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 43.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 16.25s
                        Total time: 31048.21s
                               ETA: 1032621.7s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.522s, learning 0.202s)
               Value function loss: 0.0964
                    Surrogate loss: -0.0349
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 16.72s
                        Total time: 31064.94s
                               ETA: 1032813.5s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.795s, learning 0.172s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 42.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 16.97s
                        Total time: 31081.91s
                               ETA: 1033013.1s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.985s, learning 0.200s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0331
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 42.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 16.19s
                        Total time: 31098.09s
                               ETA: 1033186.7s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.133s, learning 0.166s)
               Value function loss: 38.6986
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 7.49
               Mean episode length: 42.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 16.30s
                        Total time: 31114.39s
                               ETA: 1033364.0s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.245s, learning 0.164s)
               Value function loss: 29.2171
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 42.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 16.41s
                        Total time: 31130.80s
                               ETA: 1033544.7s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.154s, learning 0.160s)
               Value function loss: 54.1628
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 7.53
               Mean episode length: 42.53
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 16.31s
                        Total time: 31147.11s
                               ETA: 1033722.2s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.809s, learning 0.165s)
               Value function loss: 10.5494
                    Surrogate loss: -0.0104
             Mean action noise std: 0.70
                       Mean reward: 4.68
               Mean episode length: 42.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 15.97s
                        Total time: 31163.09s
                               ETA: 1033888.2s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.681s, learning 0.167s)
               Value function loss: 24.5181
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 4.91
               Mean episode length: 42.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 16.85s
                        Total time: 31179.94s
                               ETA: 1034083.1s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.192s, learning 0.193s)
               Value function loss: 17.9157
                    Surrogate loss: -0.0040
             Mean action noise std: 0.70
                       Mean reward: 4.84
               Mean episode length: 42.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 16.38s
                        Total time: 31196.32s
                               ETA: 1034262.5s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.433s, learning 0.168s)
               Value function loss: 0.2878
                    Surrogate loss: -0.0340
             Mean action noise std: 0.70
                       Mean reward: 4.79
               Mean episode length: 42.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 16.60s
                        Total time: 31212.92s
                               ETA: 1034448.9s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.280s, learning 0.180s)
               Value function loss: 0.2153
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 16.46s
                        Total time: 31229.38s
                               ETA: 1034630.5s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.532s, learning 0.222s)
               Value function loss: 7.0395
                    Surrogate loss: -0.0030
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 42.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 16.75s
                        Total time: 31246.14s
                               ETA: 1034821.7s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.638s, learning 0.182s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0359
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 16.82s
                        Total time: 31262.96s
                               ETA: 1035015.0s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.089s, learning 0.198s)
               Value function loss: 69.9746
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 43.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 16.29s
                        Total time: 31279.24s
                               ETA: 1035190.5s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.557s, learning 0.166s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 16.72s
                        Total time: 31295.97s
                               ETA: 1035380.2s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.515s, learning 0.287s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0326
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 41.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 16.80s
                        Total time: 31312.77s
                               ETA: 1035572.5s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.222s, learning 0.200s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0338
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 42.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 16.42s
                        Total time: 31329.19s
                               ETA: 1035752.0s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.582s, learning 0.162s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 42.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 16.74s
                        Total time: 31345.93s
                               ETA: 1035942.1s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.269s, learning 0.227s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0363
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 43.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 16.50s
                        Total time: 31362.43s
                               ETA: 1036123.7s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.189s, learning 0.179s)
               Value function loss: 11.8706
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 16.37s
                        Total time: 31378.80s
                               ETA: 1036301.1s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.746s, learning 0.185s)
               Value function loss: 18.2765
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 42.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 16.93s
                        Total time: 31395.73s
                               ETA: 1036496.9s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.510s, learning 0.181s)
               Value function loss: 16.3853
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 16.69s
                        Total time: 31412.42s
                               ETA: 1036684.6s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.719s, learning 0.203s)
               Value function loss: 0.4459
                    Surrogate loss: -0.0252
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 16.92s
                        Total time: 31429.34s
                               ETA: 1036879.9s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.193s, learning 0.228s)
               Value function loss: 0.2147
                    Surrogate loss: -0.0327
             Mean action noise std: 0.70
                       Mean reward: 1.95
               Mean episode length: 41.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 16.42s
                        Total time: 31445.76s
                               ETA: 1037058.4s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.430s, learning 0.170s)
               Value function loss: 7.1804
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 4.61
               Mean episode length: 42.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 16.60s
                        Total time: 31462.36s
                               ETA: 1037242.8s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.506s, learning 0.182s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 42.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 15.69s
                        Total time: 31478.05s
                               ETA: 1037396.9s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.169s)
               Value function loss: 0.0997
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 41.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.66s
                        Total time: 31486.72s
                               ETA: 1037319.5s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.271s, learning 0.205s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0393
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 43.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 8.48s
                        Total time: 31495.19s
                               ETA: 1037236.0s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.581s, learning 0.177s)
               Value function loss: 0.1085
                    Surrogate loss: -0.0351
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 42.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.76s
                        Total time: 31503.95s
                               ETA: 1037161.7s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.334s, learning 0.159s)
               Value function loss: 0.0929
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 41.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.49s
                        Total time: 31512.44s
                               ETA: 1037078.9s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.086s, learning 0.164s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 41.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 8.25s
                        Total time: 31520.69s
                               ETA: 1036988.1s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.226s, learning 0.208s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0362
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 41.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.43s
                        Total time: 31529.13s
                               ETA: 1036903.4s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.450s, learning 0.275s)
               Value function loss: 39.0750
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 43.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.72s
                        Total time: 31537.85s
                               ETA: 1036828.3s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.503s, learning 0.163s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0430
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.67s
                        Total time: 31546.52s
                               ETA: 1036751.3s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.311s, learning 0.201s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 8.51s
                        Total time: 31555.03s
                               ETA: 1036669.3s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.723s, learning 0.189s)
               Value function loss: 0.1180
                    Surrogate loss: -0.0311
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 42.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.91s
                        Total time: 31563.94s
                               ETA: 1036600.5s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.092s, learning 0.193s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 8.28s
                        Total time: 31572.23s
                               ETA: 1036511.1s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.293s, learning 0.165s)
               Value function loss: 0.0909
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 8.46s
                        Total time: 31580.69s
                               ETA: 1036427.5s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.901s, learning 0.192s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0383
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 41.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 9.09s
                        Total time: 31589.78s
                               ETA: 1036364.8s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.624s, learning 0.194s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.82s
                        Total time: 31598.60s
                               ETA: 1036293.0s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.455s, learning 0.178s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 1.82
               Mean episode length: 41.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.63s
                        Total time: 31607.23s
                               ETA: 1036215.3s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.641s, learning 0.175s)
               Value function loss: 16.7291
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.82s
                        Total time: 31616.05s
                               ETA: 1036143.6s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.491s, learning 0.182s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 41.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.67s
                        Total time: 31624.72s
                               ETA: 1036067.2s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.147s, learning 0.184s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 42.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.33s
                        Total time: 31633.05s
                               ETA: 1035979.7s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.375s, learning 0.329s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 42.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.70s
                        Total time: 31641.75s
                               ETA: 1035904.5s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.622s, learning 0.212s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 44.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.83s
                        Total time: 31650.59s
                               ETA: 1035833.6s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.635s, learning 0.174s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 41.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.81s
                        Total time: 31659.40s
                               ETA: 1035761.9s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.293s, learning 0.195s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.49s
                        Total time: 31667.89s
                               ETA: 1035679.7s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.593s, learning 0.198s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 42.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.79s
                        Total time: 31676.68s
                               ETA: 1035607.5s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.493s, learning 0.166s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 42.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.66s
                        Total time: 31685.34s
                               ETA: 1035531.0s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.640s, learning 0.164s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0409
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.80s
                        Total time: 31694.14s
                               ETA: 1035459.3s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.455s, learning 0.167s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 43.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 8.62s
                        Total time: 31702.76s
                               ETA: 1035381.7s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.429s, learning 0.170s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 42.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 8.60s
                        Total time: 31711.36s
                               ETA: 1035303.3s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.477s, learning 0.170s)
               Value function loss: 17.8021
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 1.96
               Mean episode length: 41.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.65s
                        Total time: 31720.01s
                               ETA: 1035226.7s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.388s, learning 0.167s)
               Value function loss: 17.5250
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 41.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.55s
                        Total time: 31728.56s
                               ETA: 1035147.0s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.169s, learning 0.191s)
               Value function loss: 0.1141
                    Surrogate loss: -0.0423
             Mean action noise std: 0.70
                       Mean reward: 1.81
               Mean episode length: 40.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.36s
                        Total time: 31736.92s
                               ETA: 1035061.0s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.167s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.41s
                        Total time: 31745.33s
                               ETA: 1034976.8s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.420s, learning 0.200s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 42.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.62s
                        Total time: 31753.95s
                               ETA: 1034899.4s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.674s, learning 0.188s)
               Value function loss: 0.0846
                    Surrogate loss: -0.0416
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.86s
                        Total time: 31762.82s
                               ETA: 1034830.0s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.209s, learning 0.175s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 1.79
               Mean episode length: 41.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.38s
                        Total time: 31771.20s
                               ETA: 1034745.0s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.169s)
               Value function loss: 0.0940
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 41.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.35s
                        Total time: 31779.55s
                               ETA: 1034658.8s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.654s, learning 0.210s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 43.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.86s
                        Total time: 31788.41s
                               ETA: 1034589.6s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.166s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0382
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.54s
                        Total time: 31796.95s
                               ETA: 1034509.9s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.919s, learning 0.192s)
               Value function loss: 46.6696
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 43.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 9.11s
                        Total time: 31806.06s
                               ETA: 1034448.8s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.351s, learning 0.184s)
               Value function loss: 11.9506
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 41.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.53s
                        Total time: 31814.60s
                               ETA: 1034368.9s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.026s, learning 0.290s)
               Value function loss: 0.1055
                    Surrogate loss: -0.0431
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 41.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 9.32s
                        Total time: 31823.91s
                               ETA: 1034314.5s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.514s, learning 0.167s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 41.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.68s
                        Total time: 31832.59s
                               ETA: 1034239.5s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.661s, learning 0.175s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0361
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 41.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.84s
                        Total time: 31841.43s
                               ETA: 1034169.6s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.248s, learning 0.170s)
               Value function loss: 59.4689
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.42s
                        Total time: 31849.85s
                               ETA: 1034086.1s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.469s, learning 0.159s)
               Value function loss: 0.1922
                    Surrogate loss: -0.0309
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 41.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.63s
                        Total time: 31858.48s
                               ETA: 1034009.5s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.457s, learning 0.158s)
               Value function loss: 0.2101
                    Surrogate loss: -0.0278
             Mean action noise std: 0.70
                       Mean reward: 1.74
               Mean episode length: 40.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.62s
                        Total time: 31867.09s
                               ETA: 1033932.6s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.793s, learning 0.191s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0316
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 41.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.98s
                        Total time: 31876.08s
                               ETA: 1033867.7s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.675s, learning 0.288s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.96s
                        Total time: 31885.04s
                               ETA: 1033802.1s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.491s, learning 0.165s)
               Value function loss: 0.0939
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 42.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.66s
                        Total time: 31893.70s
                               ETA: 1033726.6s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.604s, learning 0.206s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 41.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.81s
                        Total time: 31902.51s
                               ETA: 1033656.1s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.286s, learning 0.195s)
               Value function loss: 0.0980
                    Surrogate loss: -0.0341
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 41.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.48s
                        Total time: 31910.99s
                               ETA: 1033575.0s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.627s, learning 0.161s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 41.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.79s
                        Total time: 31919.78s
                               ETA: 1033503.9s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.167s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0359
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 40.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.66s
                        Total time: 31928.44s
                               ETA: 1033428.7s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.316s, learning 0.192s)
               Value function loss: 64.3762
                    Surrogate loss: -0.0000
             Mean action noise std: 0.70
                       Mean reward: 1.87
               Mean episode length: 40.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.51s
                        Total time: 31936.94s
                               ETA: 1033348.7s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.232s, learning 0.269s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 42.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.50s
                        Total time: 31945.44s
                               ETA: 1033268.4s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.259s, learning 0.172s)
               Value function loss: 17.2456
                    Surrogate loss: 0.0005
             Mean action noise std: 0.70
                       Mean reward: 7.01
               Mean episode length: 40.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.43s
                        Total time: 31953.87s
                               ETA: 1033185.9s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.547s, learning 0.161s)
               Value function loss: 17.1295
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 41.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.71s
                        Total time: 31962.58s
                               ETA: 1033112.5s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.768s, learning 0.198s)
               Value function loss: 0.2585
                    Surrogate loss: -0.0280
             Mean action noise std: 0.70
                       Mean reward: 1.70
               Mean episode length: 39.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.97s
                        Total time: 31971.55s
                               ETA: 1033047.4s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.523s, learning 0.264s)
               Value function loss: 0.1640
                    Surrogate loss: -0.0288
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.79s
                        Total time: 31980.34s
                               ETA: 1032976.6s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.555s, learning 0.168s)
               Value function loss: 0.1402
                    Surrogate loss: -0.0363
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 41.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.72s
                        Total time: 31989.06s
                               ETA: 1032903.7s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.299s, learning 0.176s)
               Value function loss: 0.1117
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 41.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.48s
                        Total time: 31997.53s
                               ETA: 1032822.9s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.011s, learning 0.166s)
               Value function loss: 0.0974
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 9.18s
                        Total time: 32006.71s
                               ETA: 1032764.8s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.963s, learning 0.187s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 42.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.15s
                        Total time: 32014.86s
                               ETA: 1032673.6s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.441s, learning 0.189s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 40.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.63s
                        Total time: 32023.49s
                               ETA: 1032597.9s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.404s, learning 0.171s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0383
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 41.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.57s
                        Total time: 32032.07s
                               ETA: 1032520.5s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.515s, learning 0.192s)
               Value function loss: 0.1029
                    Surrogate loss: -0.0362
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 40.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.71s
                        Total time: 32040.77s
                               ETA: 1032447.4s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.513s, learning 0.195s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 40.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.71s
                        Total time: 32049.48s
                               ETA: 1032374.4s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.349s, learning 0.188s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0385
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.54s
                        Total time: 32058.02s
                               ETA: 1032295.9s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.248s, learning 0.171s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0385
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.42s
                        Total time: 32066.44s
                               ETA: 1032213.6s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.525s, learning 0.167s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0391
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 41.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.69s
                        Total time: 32075.13s
                               ETA: 1032140.2s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.752s, learning 0.160s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0379
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.91s
                        Total time: 32084.04s
                               ETA: 1032073.9s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.456s, learning 0.183s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 41.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.64s
                        Total time: 32092.68s
                               ETA: 1031998.9s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.654s, learning 0.237s)
               Value function loss: 53.9663
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 43.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.89s
                        Total time: 32101.57s
                               ETA: 1031932.0s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.622s, learning 0.232s)
               Value function loss: 0.1122
                    Surrogate loss: -0.0436
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 40.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.85s
                        Total time: 32110.43s
                               ETA: 1031864.0s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.386s, learning 0.159s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 8.55s
                        Total time: 32118.97s
                               ETA: 1031786.0s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.168s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0346
             Mean action noise std: 0.70
                       Mean reward: 7.21
               Mean episode length: 41.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.63s
                        Total time: 32127.60s
                               ETA: 1031711.0s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.095s, learning 0.191s)
               Value function loss: 0.0959
                    Surrogate loss: -0.0391
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.29s
                        Total time: 32135.89s
                               ETA: 1031624.8s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.426s, learning 0.158s)
               Value function loss: 0.0962
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 41.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.58s
                        Total time: 32144.47s
                               ETA: 1031548.2s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.342s, learning 0.197s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0382
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.54s
                        Total time: 32153.01s
                               ETA: 1031470.3s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.107s, learning 0.180s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0393
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 42.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 8.29s
                        Total time: 32161.30s
                               ETA: 1031384.3s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.307s, learning 0.182s)
               Value function loss: 0.0909
                    Surrogate loss: -0.0377
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 42.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.49s
                        Total time: 32169.79s
                               ETA: 1031304.9s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.588s, learning 0.190s)
               Value function loss: 0.0996
                    Surrogate loss: -0.0364
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 41.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.78s
                        Total time: 32178.57s
                               ETA: 1031234.8s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.739s, learning 0.208s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 42.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.95s
                        Total time: 32187.51s
                               ETA: 1031170.1s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.635s, learning 0.193s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 42.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.83s
                        Total time: 32196.34s
                               ETA: 1031101.6s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.156s, learning 0.160s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0410
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 43.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.32s
                        Total time: 32204.66s
                               ETA: 1031016.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.567s, learning 0.311s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 8.88s
                        Total time: 32213.54s
                               ETA: 1030950.1s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.522s, learning 0.164s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 41.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 8.69s
                        Total time: 32222.22s
                               ETA: 1030877.2s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.455s, learning 0.163s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0378
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 41.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.62s
                        Total time: 32230.84s
                               ETA: 1030802.2s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.166s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.58s
                        Total time: 32239.42s
                               ETA: 1030725.9s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.877s, learning 0.162s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0410
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 9.04s
                        Total time: 32248.46s
                               ETA: 1030664.5s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.233s, learning 0.204s)
               Value function loss: 70.5668
                    Surrogate loss: -0.0003
             Mean action noise std: 0.70
                       Mean reward: 1.79
               Mean episode length: 40.95
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.44s
                        Total time: 32256.89s
                               ETA: 1030583.8s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.486s, learning 0.215s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0433
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 41.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 8.70s
                        Total time: 32265.59s
                               ETA: 1030511.6s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.162s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 42.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 8.57s
                        Total time: 32274.16s
                               ETA: 1030435.3s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.936s, learning 0.252s)
               Value function loss: 0.1009
                    Surrogate loss: -0.0385
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 41.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.19s
                        Total time: 32282.35s
                               ETA: 1030346.8s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.216s, learning 0.173s)
               Value function loss: 13.7940
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 43.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 8.39s
                        Total time: 32290.74s
                               ETA: 1030264.8s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.227s, learning 0.202s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0417
             Mean action noise std: 0.70
                       Mean reward: 4.27
               Mean episode length: 41.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.43s
                        Total time: 32299.17s
                               ETA: 1030184.1s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.444s, learning 0.178s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0385
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 41.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 8.62s
                        Total time: 32307.79s
                               ETA: 1030109.6s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.312s, learning 0.166s)
               Value function loss: 0.0940
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 40.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.48s
                        Total time: 32316.27s
                               ETA: 1030030.6s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.487s, learning 0.215s)
               Value function loss: 0.0962
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 42.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.70s
                        Total time: 32324.97s
                               ETA: 1029958.8s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.167s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0365
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 41.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 8.72s
                        Total time: 32333.69s
                               ETA: 1029887.5s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.763s, learning 0.201s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 42.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 8.96s
                        Total time: 32342.65s
                               ETA: 1029824.0s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.383s, learning 0.216s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 43.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 8.60s
                        Total time: 32351.25s
                               ETA: 1029749.0s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.510s, learning 0.264s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0366
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 8.77s
                        Total time: 32360.03s
                               ETA: 1029679.6s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.352s, learning 0.182s)
               Value function loss: 17.5990
                    Surrogate loss: 0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 8.53s
                        Total time: 32368.56s
                               ETA: 1029602.7s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.533s, learning 0.191s)
               Value function loss: 16.5292
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 41.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 8.72s
                        Total time: 32377.28s
                               ETA: 1029531.8s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.633s, learning 0.198s)
               Value function loss: 0.1945
                    Surrogate loss: -0.0436
             Mean action noise std: 0.70
                       Mean reward: 4.69
               Mean episode length: 43.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 8.83s
                        Total time: 32386.11s
                               ETA: 1029464.3s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.551s, learning 0.177s)
               Value function loss: 0.1381
                    Surrogate loss: -0.0397
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 8.73s
                        Total time: 32394.84s
                               ETA: 1029393.6s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.166s, learning 0.163s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0378
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 8.33s
                        Total time: 32403.17s
                               ETA: 1029310.3s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.474s, learning 0.301s)
               Value function loss: 3.8356
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 41.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 8.77s
                        Total time: 32411.95s
                               ETA: 1029241.2s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.179s, learning 0.218s)
               Value function loss: 0.1142
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 1.98
               Mean episode length: 41.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 8.40s
                        Total time: 32420.34s
                               ETA: 1029160.1s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.411s, learning 0.177s)
               Value function loss: 9.7912
                    Surrogate loss: 0.0030
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 42.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 8.59s
                        Total time: 32428.93s
                               ETA: 1029085.1s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.751s, learning 0.220s)
               Value function loss: 0.1327
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 43.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 8.97s
                        Total time: 32437.90s
                               ETA: 1029022.4s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.488s, learning 0.290s)
               Value function loss: 0.1133
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 4.62
               Mean episode length: 42.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 8.78s
                        Total time: 32446.68s
                               ETA: 1028953.5s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.080s, learning 0.164s)
               Value function loss: 0.1100
                    Surrogate loss: -0.0412
             Mean action noise std: 0.70
                       Mean reward: 2.69
               Mean episode length: 44.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 9.24s
                        Total time: 32455.92s
                               ETA: 1028899.5s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.406s, learning 0.169s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.66
               Mean episode length: 44.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 8.57s
                        Total time: 32464.50s
                               ETA: 1028824.3s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.165s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0378
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 42.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 8.66s
                        Total time: 32473.16s
                               ETA: 1028751.8s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.109s, learning 0.256s)
               Value function loss: 17.4624
                    Surrogate loss: 0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.54
               Mean episode length: 43.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 8.37s
                        Total time: 32481.52s
                               ETA: 1028670.0s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1070 steps/s (collection: 15.108s, learning 0.202s)
               Value function loss: 0.1446
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 42.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 15.31s
                        Total time: 32496.83s
                               ETA: 1028808.1s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.397s, learning 0.194s)
               Value function loss: 0.1144
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 4.72
               Mean episode length: 43.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 16.59s
                        Total time: 32513.42s
                               ETA: 1028986.7s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 950 steps/s (collection: 16.947s, learning 0.289s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 42.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 17.24s
                        Total time: 32530.66s
                               ETA: 1029185.6s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.581s, learning 0.261s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 16.84s
                        Total time: 32547.50s
                               ETA: 1029371.9s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.769s, learning 0.184s)
               Value function loss: 0.1067
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 42.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 16.95s
                        Total time: 32564.46s
                               ETA: 1029561.5s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.011s, learning 0.168s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 43.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 17.18s
                        Total time: 32581.63s
                               ETA: 1029758.1s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.610s, learning 0.170s)
               Value function loss: 0.0942
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 43.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 16.78s
                        Total time: 32598.41s
                               ETA: 1029942.0s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.646s, learning 0.162s)
               Value function loss: 261.0622
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 43.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 16.81s
                        Total time: 32615.22s
                               ETA: 1030126.7s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.056s, learning 0.177s)
               Value function loss: 0.2194
                    Surrogate loss: -0.0410
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 42.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 16.23s
                        Total time: 32631.45s
                               ETA: 1030293.0s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.790s, learning 0.174s)
               Value function loss: 0.1971
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 15.07
               Mean episode length: 44.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 16.96s
                        Total time: 32648.42s
                               ETA: 1030482.3s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.050s, learning 0.171s)
               Value function loss: 64.0153
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 43.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 16.22s
                        Total time: 32664.64s
                               ETA: 1030648.1s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.876s, learning 0.176s)
               Value function loss: 0.2237
                    Surrogate loss: -0.0420
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 17.05s
                        Total time: 32681.69s
                               ETA: 1030839.9s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.826s, learning 0.204s)
               Value function loss: 0.2029
                    Surrogate loss: -0.0301
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 44.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 17.03s
                        Total time: 32698.72s
                               ETA: 1031030.9s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.820s, learning 0.242s)
               Value function loss: 13.7858
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 43.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 17.06s
                        Total time: 32715.78s
                               ETA: 1031222.8s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.019s, learning 0.186s)
               Value function loss: 0.1870
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 2.70
               Mean episode length: 44.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 17.21s
                        Total time: 32732.99s
                               ETA: 1031419.0s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.453s, learning 0.185s)
               Value function loss: 7.1497
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 42.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 16.64s
                        Total time: 32749.63s
                               ETA: 1031597.3s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.403s, learning 0.185s)
               Value function loss: 0.1490
                    Surrogate loss: -0.0416
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 42.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 16.59s
                        Total time: 32766.21s
                               ETA: 1031773.8s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.294s, learning 0.179s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0390
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 44.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 16.47s
                        Total time: 32782.69s
                               ETA: 1031946.6s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.040s, learning 0.274s)
               Value function loss: 47.5042
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 44.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 16.31s
                        Total time: 32799.00s
                               ETA: 1032114.3s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.570s, learning 0.205s)
               Value function loss: 249.7004
                    Surrogate loss: -0.0035
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 41.64
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 16.78s
                        Total time: 32815.78s
                               ETA: 1032296.4s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.409s, learning 0.165s)
               Value function loss: 9.6803
                    Surrogate loss: -0.0166
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 43.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 16.57s
                        Total time: 32832.35s
                               ETA: 1032472.0s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.589s, learning 0.174s)
               Value function loss: 123.9194
                    Surrogate loss: 0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.59
               Mean episode length: 43.22
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 16.76s
                        Total time: 32849.11s
                               ETA: 1032653.4s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.058s, learning 0.162s)
               Value function loss: 18.6650
                    Surrogate loss: -0.0071
             Mean action noise std: 0.70
                       Mean reward: 2.59
               Mean episode length: 44.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 16.22s
                        Total time: 32865.33s
                               ETA: 1032817.6s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.205s, learning 0.250s)
               Value function loss: 0.6934
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 4.72
               Mean episode length: 42.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 16.46s
                        Total time: 32881.79s
                               ETA: 1032989.1s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.393s, learning 0.169s)
               Value function loss: 18.5490
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 5.18
               Mean episode length: 44.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 16.56s
                        Total time: 32898.35s
                               ETA: 1033163.9s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.471s, learning 0.160s)
               Value function loss: 0.4976
                    Surrogate loss: -0.0324
             Mean action noise std: 0.70
                       Mean reward: 2.68
               Mean episode length: 43.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 16.63s
                        Total time: 32914.98s
                               ETA: 1033340.6s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.935s, learning 0.163s)
               Value function loss: 0.3879
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.70
               Mean episode length: 44.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 17.10s
                        Total time: 32932.08s
                               ETA: 1033531.9s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.642s, learning 0.160s)
               Value function loss: 0.2694
                    Surrogate loss: -0.0349
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 43.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 16.80s
                        Total time: 32948.88s
                               ETA: 1033713.8s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.552s, learning 0.171s)
               Value function loss: 0.2426
                    Surrogate loss: -0.0339
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 44.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 16.72s
                        Total time: 32965.60s
                               ETA: 1033893.1s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.407s, learning 0.173s)
               Value function loss: 17.8293
                    Surrogate loss: -0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 45.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 16.58s
                        Total time: 32982.18s
                               ETA: 1034067.8s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.156s, learning 0.160s)
               Value function loss: 0.2285
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 43.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 16.32s
                        Total time: 32998.50s
                               ETA: 1034234.0s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.620s, learning 0.167s)
               Value function loss: 0.1898
                    Surrogate loss: -0.0369
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 44.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 16.79s
                        Total time: 33015.29s
                               ETA: 1034415.0s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.651s, learning 0.222s)
               Value function loss: 9.9744
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.68
               Mean episode length: 44.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 16.87s
                        Total time: 33032.16s
                               ETA: 1034598.5s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.550s, learning 0.194s)
               Value function loss: 0.1663
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 2.87
               Mean episode length: 45.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 16.74s
                        Total time: 33048.91s
                               ETA: 1034777.8s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.854s, learning 0.254s)
               Value function loss: 0.1734
                    Surrogate loss: -0.0408
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 17.11s
                        Total time: 33066.01s
                               ETA: 1034968.4s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.505s, learning 0.172s)
               Value function loss: 0.1650
                    Surrogate loss: -0.0391
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 44.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 16.68s
                        Total time: 33082.69s
                               ETA: 1035145.3s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.725s, learning 0.203s)
               Value function loss: 0.1486
                    Surrogate loss: -0.0395
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 43.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 16.93s
                        Total time: 33099.62s
                               ETA: 1035330.0s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1049 steps/s (collection: 15.422s, learning 0.183s)
               Value function loss: 0.1338
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 44.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 15.60s
                        Total time: 33115.22s
                               ETA: 1035473.2s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.429s, learning 0.231s)
               Value function loss: 0.1313
                    Surrogate loss: -0.0370
             Mean action noise std: 0.70
                       Mean reward: 3.01
               Mean episode length: 46.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 8.66s
                        Total time: 33123.88s
                               ETA: 1035399.2s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.657s, learning 0.242s)
               Value function loss: 0.1321
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 44.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 8.90s
                        Total time: 33132.78s
                               ETA: 1035332.7s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.367s, learning 0.183s)
               Value function loss: 19.1946
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 42.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.55s
                        Total time: 33141.33s
                               ETA: 1035255.3s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.585s, learning 0.164s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0449
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 43.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.75s
                        Total time: 33150.08s
                               ETA: 1035184.2s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.145s, learning 0.167s)
               Value function loss: 0.1388
                    Surrogate loss: -0.0391
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 43.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.31s
                        Total time: 33158.39s
                               ETA: 1035099.5s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.167s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0391
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 43.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 8.57s
                        Total time: 33166.96s
                               ETA: 1035022.7s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.050s, learning 0.165s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 2.97
               Mean episode length: 45.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.22s
                        Total time: 33175.17s
                               ETA: 1034935.1s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.456s, learning 0.167s)
               Value function loss: 7.1308
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 42.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.62s
                        Total time: 33183.80s
                               ETA: 1034860.2s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.638s, learning 0.172s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0410
             Mean action noise std: 0.70
                       Mean reward: 1.82
               Mean episode length: 41.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.81s
                        Total time: 33192.61s
                               ETA: 1034791.2s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.159s)
               Value function loss: 0.1115
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 43.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 8.27s
                        Total time: 33200.88s
                               ETA: 1034705.5s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.477s, learning 0.162s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0377
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 8.64s
                        Total time: 33209.52s
                               ETA: 1034631.3s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.511s, learning 0.172s)
               Value function loss: 0.0991
                    Surrogate loss: -0.0393
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 43.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 8.68s
                        Total time: 33218.20s
                               ETA: 1034558.5s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.629s, learning 0.165s)
               Value function loss: 0.1131
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 41.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.79s
                        Total time: 33226.99s
                               ETA: 1034489.1s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.406s, learning 0.168s)
               Value function loss: 0.1094
                    Surrogate loss: -0.0366
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 44.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.57s
                        Total time: 33235.57s
                               ETA: 1034413.0s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.079s, learning 0.167s)
               Value function loss: 11.8666
                    Surrogate loss: 0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 43.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 8.25s
                        Total time: 33243.81s
                               ETA: 1034326.7s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.359s, learning 0.257s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 43.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.62s
                        Total time: 33252.43s
                               ETA: 1034252.0s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.533s, learning 0.176s)
               Value function loss: 0.1067
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 4.67
               Mean episode length: 42.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 8.71s
                        Total time: 33261.14s
                               ETA: 1034180.2s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.755s, learning 0.166s)
               Value function loss: 0.1083
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 43.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 8.92s
                        Total time: 33270.06s
                               ETA: 1034115.0s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.374s, learning 0.187s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0390
             Mean action noise std: 0.70
                       Mean reward: 1.86
               Mean episode length: 42.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 8.56s
                        Total time: 33278.62s
                               ETA: 1034038.7s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.223s, learning 0.157s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0385
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 43.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 8.38s
                        Total time: 33287.00s
                               ETA: 1033956.8s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.655s, learning 0.166s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 44.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 8.82s
                        Total time: 33295.82s
                               ETA: 1033888.6s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.973s, learning 0.187s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 43.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 8.16s
                        Total time: 33303.98s
                               ETA: 1033800.0s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.597s, learning 0.162s)
               Value function loss: 29.6132
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 43.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 8.76s
                        Total time: 33312.74s
                               ETA: 1033730.0s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.538s, learning 0.265s)
               Value function loss: 7.0902
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 43.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 8.80s
                        Total time: 33321.54s
                               ETA: 1033661.4s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.372s, learning 0.190s)
               Value function loss: 0.1115
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 4.47
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 8.56s
                        Total time: 33330.11s
                               ETA: 1033585.4s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.708s, learning 0.168s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0388
             Mean action noise std: 0.70
                       Mean reward: 7.15
               Mean episode length: 43.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 8.88s
                        Total time: 33338.98s
                               ETA: 1033519.1s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.639s, learning 0.185s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 42.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 8.82s
                        Total time: 33347.81s
                               ETA: 1033451.3s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.592s, learning 0.210s)
               Value function loss: 0.0976
                    Surrogate loss: -0.0382
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 43.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 8.80s
                        Total time: 33356.61s
                               ETA: 1033382.8s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.268s, learning 0.167s)
               Value function loss: 12.1039
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 42.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 8.44s
                        Total time: 33365.04s
                               ETA: 1033303.0s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.793s, learning 0.170s)
               Value function loss: 0.1091
                    Surrogate loss: -0.0434
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 8.96s
                        Total time: 33374.01s
                               ETA: 1033239.6s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.927s, learning 0.201s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0383
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 43.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 8.13s
                        Total time: 33382.13s
                               ETA: 1033150.4s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.217s, learning 0.184s)
               Value function loss: 165.3784
                    Surrogate loss: -0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 44.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 8.40s
                        Total time: 33390.54s
                               ETA: 1033069.7s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.381s, learning 0.254s)
               Value function loss: 0.1417
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 43.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 8.64s
                        Total time: 33399.17s
                               ETA: 1032996.3s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.352s, learning 0.200s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.55s
                        Total time: 33407.72s
                               ETA: 1032920.3s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.585s, learning 0.171s)
               Value function loss: 9.7164
                    Surrogate loss: 0.0020
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 43.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.76s
                        Total time: 33416.48s
                               ETA: 1032850.7s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.178s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0408
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 42.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.30s
                        Total time: 33424.78s
                               ETA: 1032766.9s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.322s, learning 0.158s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0425
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 42.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 8.48s
                        Total time: 33433.26s
                               ETA: 1032688.9s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.573s, learning 0.177s)
               Value function loss: 0.1191
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 1.98
               Mean episode length: 42.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 8.75s
                        Total time: 33442.01s
                               ETA: 1032619.2s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.163s)
               Value function loss: 0.1164
                    Surrogate loss: -0.0369
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 42.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 8.55s
                        Total time: 33450.56s
                               ETA: 1032543.5s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.603s, learning 0.167s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 42.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 8.77s
                        Total time: 33459.33s
                               ETA: 1032474.6s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.612s, learning 0.252s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0413
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 42.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 8.86s
                        Total time: 33468.19s
                               ETA: 1032408.5s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.710s, learning 0.264s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 42.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.97s
                        Total time: 33477.17s
                               ETA: 1032345.9s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.304s, learning 0.169s)
               Value function loss: 0.0897
                    Surrogate loss: -0.0422
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 42.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.47s
                        Total time: 33485.64s
                               ETA: 1032267.9s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.238s, learning 0.163s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 8.40s
                        Total time: 33494.04s
                               ETA: 1032187.7s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.519s, learning 0.217s)
               Value function loss: 17.7953
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 40.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.74s
                        Total time: 33502.78s
                               ETA: 1032117.8s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.228s, learning 0.184s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0433
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 43.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 8.41s
                        Total time: 33511.19s
                               ETA: 1032038.1s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.326s, learning 0.180s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 43.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.51s
                        Total time: 33519.70s
                               ETA: 1031961.3s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.096s, learning 0.214s)
               Value function loss: 15.3174
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 43.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 8.31s
                        Total time: 33528.01s
                               ETA: 1031878.5s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.846s, learning 0.246s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0428
             Mean action noise std: 0.70
                       Mean reward: 1.81
               Mean episode length: 42.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 9.09s
                        Total time: 33537.10s
                               ETA: 1031819.7s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.260s, learning 0.207s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0413
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.47s
                        Total time: 33545.56s
                               ETA: 1031741.8s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.633s, learning 0.197s)
               Value function loss: 0.1014
                    Surrogate loss: -0.0405
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 42.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.83s
                        Total time: 33554.40s
                               ETA: 1031675.1s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.493s, learning 0.252s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 43.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 8.74s
                        Total time: 33563.14s
                               ETA: 1031605.9s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.183s, learning 0.179s)
               Value function loss: 9.7279
                    Surrogate loss: 0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 43.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.36s
                        Total time: 33571.50s
                               ETA: 1031524.9s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.211s, learning 0.335s)
               Value function loss: 0.1113
                    Surrogate loss: -0.0440
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 43.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.55s
                        Total time: 33580.05s
                               ETA: 1031449.5s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.241s, learning 0.238s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 43.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 8.48s
                        Total time: 33588.53s
                               ETA: 1031372.2s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.115s, learning 0.203s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0412
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 44.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.32s
                        Total time: 33596.84s
                               ETA: 1031290.0s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.461s, learning 0.185s)
               Value function loss: 0.1009
                    Surrogate loss: -0.0382
             Mean action noise std: 0.70
                       Mean reward: 1.84
               Mean episode length: 41.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.65s
                        Total time: 33605.49s
                               ETA: 1031217.9s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.288s, learning 0.171s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.46s
                        Total time: 33613.95s
                               ETA: 1031140.1s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.060s, learning 0.181s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0418
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 44.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.24s
                        Total time: 33622.19s
                               ETA: 1031055.7s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.174s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.81s
                        Total time: 33631.00s
                               ETA: 1030988.8s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.398s, learning 0.215s)
               Value function loss: 12.0110
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 44.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.61s
                        Total time: 33639.62s
                               ETA: 1030915.9s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.464s, learning 0.157s)
               Value function loss: 13.5892
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 44.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.62s
                        Total time: 33648.24s
                               ETA: 1030843.2s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.499s, learning 0.165s)
               Value function loss: 7.1972
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 45.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.66s
                        Total time: 33656.90s
                               ETA: 1030771.9s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.434s, learning 0.206s)
               Value function loss: 0.2136
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 7.42
               Mean episode length: 44.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.64s
                        Total time: 33665.54s
                               ETA: 1030699.9s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.244s, learning 0.163s)
               Value function loss: 0.1589
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 43.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.41s
                        Total time: 33673.95s
                               ETA: 1030620.8s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.529s, learning 0.191s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0366
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 44.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.72s
                        Total time: 33682.67s
                               ETA: 1030551.3s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.395s, learning 0.172s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0395
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 44.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.57s
                        Total time: 33691.24s
                               ETA: 1030477.2s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.294s, learning 0.173s)
               Value function loss: 0.1154
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 2.74
               Mean episode length: 45.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.47s
                        Total time: 33699.70s
                               ETA: 1030400.1s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.308s, learning 0.195s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 43.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.50s
                        Total time: 33708.21s
                               ETA: 1030324.1s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.236s, learning 0.186s)
               Value function loss: 0.0944
                    Surrogate loss: -0.0390
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 45.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.42s
                        Total time: 33716.63s
                               ETA: 1030245.7s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.404s, learning 0.195s)
               Value function loss: 160.9594
                    Surrogate loss: -0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 43.12
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.60s
                        Total time: 33725.23s
                               ETA: 1030172.7s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.484s, learning 0.171s)
               Value function loss: 59.3405
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 44.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.66s
                        Total time: 33733.88s
                               ETA: 1030101.5s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.549s, learning 0.184s)
               Value function loss: 0.3717
                    Surrogate loss: -0.0369
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 43.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.73s
                        Total time: 33742.62s
                               ETA: 1030032.7s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.523s, learning 0.202s)
               Value function loss: 46.3549
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 12.49
               Mean episode length: 44.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.73s
                        Total time: 33751.34s
                               ETA: 1029963.7s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.802s, learning 0.178s)
               Value function loss: 0.2352
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 43.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.98s
                        Total time: 33760.32s
                               ETA: 1029902.5s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.415s, learning 0.196s)
               Value function loss: 9.5932
                    Surrogate loss: 0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 44.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.61s
                        Total time: 33768.93s
                               ETA: 1029830.1s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.228s, learning 0.173s)
               Value function loss: 0.1740
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 44.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.40s
                        Total time: 33777.33s
                               ETA: 1029751.3s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.188s, learning 0.177s)
               Value function loss: 0.1414
                    Surrogate loss: -0.0375
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 45.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.36s
                        Total time: 33785.70s
                               ETA: 1029671.5s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.769s, learning 0.227s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0326
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 44.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 9.00s
                        Total time: 33794.69s
                               ETA: 1029611.0s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.741s, learning 0.169s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0361
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 43.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.91s
                        Total time: 33803.60s
                               ETA: 1029547.8s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.599s, learning 0.266s)
               Value function loss: 0.1052
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 42.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.86s
                        Total time: 33812.47s
                               ETA: 1029483.3s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.151s, learning 0.177s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0422
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 44.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.33s
                        Total time: 33820.80s
                               ETA: 1029402.5s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.304s, learning 0.174s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 1.83
               Mean episode length: 43.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.48s
                        Total time: 33829.27s
                               ETA: 1029326.4s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.982s, learning 0.192s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 44.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.17s
                        Total time: 33837.45s
                               ETA: 1029241.0s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.332s, learning 0.179s)
               Value function loss: 81.4938
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 42.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.51s
                        Total time: 33845.96s
                               ETA: 1029165.9s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.384s, learning 0.245s)
               Value function loss: 3.9955
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 43.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.63s
                        Total time: 33854.59s
                               ETA: 1029094.5s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.480s, learning 0.166s)
               Value function loss: 3.9472
                    Surrogate loss: -0.0018
             Mean action noise std: 0.70
                       Mean reward: 4.66
               Mean episode length: 44.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.65s
                        Total time: 33863.23s
                               ETA: 1029023.5s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.230s, learning 0.171s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0404
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 44.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.40s
                        Total time: 33871.64s
                               ETA: 1028945.2s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.800s, learning 0.174s)
               Value function loss: 0.1118
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 45.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.97s
                        Total time: 33880.61s
                               ETA: 1028884.4s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.690s, learning 0.163s)
               Value function loss: 0.1070
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 44.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.85s
                        Total time: 33889.46s
                               ETA: 1028819.9s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.555s, learning 0.265s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0400
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 44.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.82s
                        Total time: 33898.28s
                               ETA: 1028754.4s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.724s, learning 0.163s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.89s
                        Total time: 33907.17s
                               ETA: 1028691.0s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.438s, learning 0.170s)
               Value function loss: 0.1065
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 44.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 8.61s
                        Total time: 33915.78s
                               ETA: 1028619.2s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.764s, learning 0.166s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 44.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.93s
                        Total time: 33924.71s
                               ETA: 1028557.2s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.236s, learning 0.179s)
               Value function loss: 0.0932
                    Surrogate loss: -0.0395
             Mean action noise std: 0.70
                       Mean reward: 1.98
               Mean episode length: 43.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.41s
                        Total time: 33933.12s
                               ETA: 1028479.6s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.725s, learning 0.189s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0429
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 43.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.91s
                        Total time: 33942.04s
                               ETA: 1028417.2s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.681s, learning 0.193s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 43.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.87s
                        Total time: 33950.91s
                               ETA: 1028353.5s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.735s, learning 0.188s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0395
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 44.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.92s
                        Total time: 33959.83s
                               ETA: 1028291.4s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.247s, learning 0.196s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.39
               Mean episode length: 44.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.44s
                        Total time: 33968.28s
                               ETA: 1028214.9s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.586s, learning 0.186s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 43.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.77s
                        Total time: 33977.05s
                               ETA: 1028148.3s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.414s, learning 0.169s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0430
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 43.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.58s
                        Total time: 33985.63s
                               ETA: 1028076.0s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.546s, learning 0.188s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 43.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.73s
                        Total time: 33994.37s
                               ETA: 1028008.3s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.300s, learning 0.202s)
               Value function loss: 17.5958
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 43.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.50s
                        Total time: 34002.87s
                               ETA: 1027933.7s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.338s, learning 0.221s)
               Value function loss: 17.7960
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 42.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.56s
                        Total time: 34011.43s
                               ETA: 1027860.8s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.443s, learning 0.196s)
               Value function loss: 0.1639
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 44.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.64s
                        Total time: 34020.07s
                               ETA: 1027790.4s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.359s, learning 0.164s)
               Value function loss: 0.1222
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 9.88
               Mean episode length: 44.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.52s
                        Total time: 34028.59s
                               ETA: 1027716.5s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.609s, learning 0.178s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0401
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 45.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.79s
                        Total time: 34037.38s
                               ETA: 1027650.6s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.163s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0401
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 44.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.10s
                        Total time: 34045.48s
                               ETA: 1027564.2s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.310s, learning 0.192s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0366
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 43.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.50s
                        Total time: 34053.98s
                               ETA: 1027489.8s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.537s, learning 0.196s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0377
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 44.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.73s
                        Total time: 34062.72s
                               ETA: 1027422.4s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.549s, learning 0.184s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 44.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.73s
                        Total time: 34071.45s
                               ETA: 1027355.0s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.132s, learning 0.164s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0353
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 42.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.30s
                        Total time: 34079.75s
                               ETA: 1027274.6s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.557s, learning 0.228s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0378
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 44.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.78s
                        Total time: 34088.53s
                               ETA: 1027208.8s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.322s, learning 0.182s)
               Value function loss: 59.2214
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 43.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.50s
                        Total time: 34097.03s
                               ETA: 1027134.7s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.377s, learning 0.178s)
               Value function loss: 0.1403
                    Surrogate loss: -0.0401
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 44.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.56s
                        Total time: 34105.59s
                               ETA: 1027062.1s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.624s, learning 0.187s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 7.20
               Mean episode length: 43.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.81s
                        Total time: 34114.40s
                               ETA: 1026997.3s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.403s, learning 0.162s)
               Value function loss: 0.1016
                    Surrogate loss: -0.0365
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 42.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.57s
                        Total time: 34122.97s
                               ETA: 1026925.1s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.630s, learning 0.205s)
               Value function loss: 0.1174
                    Surrogate loss: -0.0360
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 43.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.83s
                        Total time: 34131.80s
                               ETA: 1026861.1s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.185s, learning 0.174s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0403
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.36s
                        Total time: 34140.16s
                               ETA: 1026782.8s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.611s, learning 0.184s)
               Value function loss: 0.0880
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 44.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.79s
                        Total time: 34148.95s
                               ETA: 1026717.7s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.305s, learning 0.205s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0401
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 43.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 8.51s
                        Total time: 34157.46s
                               ETA: 1026644.0s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.581s, learning 0.219s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0370
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.80s
                        Total time: 34166.26s
                               ETA: 1026579.0s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.319s, learning 0.195s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 43.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.51s
                        Total time: 34174.78s
                               ETA: 1026505.5s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.402s, learning 0.170s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 44.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.57s
                        Total time: 34183.35s
                               ETA: 1026433.8s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.376s, learning 0.202s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 8.58s
                        Total time: 34191.93s
                               ETA: 1026362.4s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.257s, learning 0.169s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 43.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 8.43s
                        Total time: 34200.35s
                               ETA: 1026286.3s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.937s, learning 0.164s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 42.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 16.10s
                        Total time: 34216.45s
                               ETA: 1026440.6s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.457s, learning 0.190s)
               Value function loss: 63.9991
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 43.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 16.65s
                        Total time: 34233.10s
                               ETA: 1026611.2s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.418s, learning 0.198s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0432
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 44.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 16.62s
                        Total time: 34249.72s
                               ETA: 1026780.7s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.390s, learning 0.170s)
               Value function loss: 0.1457
                    Surrogate loss: -0.0361
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 43.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 16.56s
                        Total time: 34266.28s
                               ETA: 1026948.3s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.520s, learning 0.161s)
               Value function loss: 0.1130
                    Surrogate loss: -0.0354
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 42.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 16.68s
                        Total time: 34282.96s
                               ETA: 1027119.6s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.597s, learning 0.168s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0388
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 44.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 16.77s
                        Total time: 34299.72s
                               ETA: 1027293.2s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.003s, learning 0.170s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 16.17s
                        Total time: 34315.90s
                               ETA: 1027448.9s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.672s, learning 0.199s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 16.87s
                        Total time: 34332.77s
                               ETA: 1027625.5s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.292s, learning 0.159s)
               Value function loss: 17.8425
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 43.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 16.45s
                        Total time: 34349.22s
                               ETA: 1027789.4s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.532s, learning 0.192s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 43.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 16.72s
                        Total time: 34365.94s
                               ETA: 1027961.3s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.597s, learning 0.200s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 43.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 16.80s
                        Total time: 34382.74s
                               ETA: 1028135.3s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.952s, learning 0.215s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0368
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 44.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 17.17s
                        Total time: 34399.91s
                               ETA: 1028320.2s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.455s, learning 0.195s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 43.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 16.65s
                        Total time: 34416.56s
                               ETA: 1028489.6s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.126s, learning 0.201s)
               Value function loss: 11.9923
                    Surrogate loss: 0.0008
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 44.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 17.33s
                        Total time: 34433.88s
                               ETA: 1028679.1s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.709s, learning 0.160s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 44.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 16.87s
                        Total time: 34450.75s
                               ETA: 1028854.7s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.077s, learning 0.167s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0409
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 43.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 16.24s
                        Total time: 34467.00s
                               ETA: 1029011.6s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.529s, learning 0.164s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0357
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 43.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 16.69s
                        Total time: 34483.69s
                               ETA: 1029181.8s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.575s, learning 0.168s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0397
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 44.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 16.74s
                        Total time: 34500.43s
                               ETA: 1029353.3s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.806s, learning 0.163s)
               Value function loss: 132.0435
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 43.83
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 16.97s
                        Total time: 34517.40s
                               ETA: 1029531.5s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.841s, learning 0.190s)
               Value function loss: 0.1427
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 44.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 17.03s
                        Total time: 34534.43s
                               ETA: 1029711.4s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.810s, learning 0.186s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0367
             Mean action noise std: 0.70
                       Mean reward: 12.47
               Mean episode length: 44.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 17.00s
                        Total time: 34551.43s
                               ETA: 1029890.1s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.324s, learning 0.215s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 43.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 16.54s
                        Total time: 34567.97s
                               ETA: 1030055.1s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.942s, learning 0.200s)
               Value function loss: 0.1014
                    Surrogate loss: -0.0346
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 43.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 17.14s
                        Total time: 34585.11s
                               ETA: 1030238.0s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.594s, learning 0.224s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 42.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 16.82s
                        Total time: 34601.93s
                               ETA: 1030411.1s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.900s, learning 0.173s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 43.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 17.07s
                        Total time: 34619.00s
                               ETA: 1030591.6s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.578s, learning 0.169s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 45.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 16.75s
                        Total time: 34635.75s
                               ETA: 1030762.4s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.194s, learning 0.188s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0357
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 42.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 16.38s
                        Total time: 34652.13s
                               ETA: 1030922.1s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.251s, learning 0.200s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0375
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 43.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 16.45s
                        Total time: 34668.58s
                               ETA: 1031083.8s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.101s, learning 0.171s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0371
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 42.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 16.27s
                        Total time: 34684.85s
                               ETA: 1031240.1s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.104s, learning 0.189s)
               Value function loss: 0.1007
                    Surrogate loss: -0.0356
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 42.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 16.29s
                        Total time: 34701.14s
                               ETA: 1031396.9s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.475s, learning 0.163s)
               Value function loss: 90.9773
                    Surrogate loss: 0.0000
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 42.83
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 16.64s
                        Total time: 34717.78s
                               ETA: 1031563.8s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.072s, learning 0.172s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0421
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 43.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 16.24s
                        Total time: 34734.02s
                               ETA: 1031718.9s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.196s, learning 0.173s)
               Value function loss: 64.0773
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 42.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 16.37s
                        Total time: 34750.39s
                               ETA: 1031877.6s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.419s, learning 0.204s)
               Value function loss: 0.2602
                    Surrogate loss: -0.0331
             Mean action noise std: 0.69
                       Mean reward: 2.10
               Mean episode length: 43.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 16.62s
                        Total time: 34767.02s
                               ETA: 1032043.8s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.413s, learning 0.177s)
               Value function loss: 0.1633
                    Surrogate loss: -0.0319
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 16.59s
                        Total time: 34783.61s
                               ETA: 1032208.9s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.456s, learning 0.299s)
               Value function loss: 16.5375
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 7.48
               Mean episode length: 43.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 16.76s
                        Total time: 34800.36s
                               ETA: 1032378.7s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.752s, learning 0.192s)
               Value function loss: 0.1668
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 44.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 16.94s
                        Total time: 34817.31s
                               ETA: 1032554.1s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1120 steps/s (collection: 14.400s, learning 0.227s)
               Value function loss: 0.1366
                    Surrogate loss: -0.0336
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 43.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 14.63s
                        Total time: 34831.93s
                               ETA: 1032660.6s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.403s, learning 0.174s)
               Value function loss: 0.1259
                    Surrogate loss: -0.0331
             Mean action noise std: 0.69
                       Mean reward: 2.18
               Mean episode length: 42.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 8.58s
                        Total time: 34840.51s
                               ETA: 1032587.7s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.078s, learning 0.270s)
               Value function loss: 0.1135
                    Surrogate loss: -0.0339
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 43.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.35s
                        Total time: 34848.86s
                               ETA: 1032508.1s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1784 steps/s (collection: 8.957s, learning 0.225s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0389
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 45.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 9.18s
                        Total time: 34858.04s
                               ETA: 1032453.3s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.012s, learning 0.211s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0361
             Mean action noise std: 0.69
                       Mean reward: 2.18
               Mean episode length: 42.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.22s
                        Total time: 34866.26s
                               ETA: 1032370.1s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.577s, learning 0.190s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0364
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 43.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 8.77s
                        Total time: 34875.03s
                               ETA: 1032303.1s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.171s)
               Value function loss: 0.0987
                    Surrogate loss: -0.0333
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 44.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.58s
                        Total time: 34883.61s
                               ETA: 1032230.6s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.435s, learning 0.204s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 43.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.64s
                        Total time: 34892.25s
                               ETA: 1032159.8s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.238s, learning 0.179s)
               Value function loss: 16.5842
                    Surrogate loss: 0.0015
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 43.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 8.42s
                        Total time: 34900.67s
                               ETA: 1032082.5s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.848s, learning 0.269s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0382
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 43.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 9.12s
                        Total time: 34909.79s
                               ETA: 1032025.9s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.482s, learning 0.208s)
               Value function loss: 15.2996
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.19
               Mean episode length: 43.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 8.69s
                        Total time: 34918.48s
                               ETA: 1031956.8s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.355s, learning 0.195s)
               Value function loss: 9.8984
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 5.15
               Mean episode length: 44.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 8.55s
                        Total time: 34927.03s
                               ETA: 1031883.5s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.573s, learning 0.206s)
               Value function loss: 0.1343
                    Surrogate loss: -0.0396
             Mean action noise std: 0.69
                       Mean reward: 4.72
               Mean episode length: 42.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 8.78s
                        Total time: 34935.81s
                               ETA: 1031817.0s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.286s, learning 0.165s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0402
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 45.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 8.45s
                        Total time: 34944.26s
                               ETA: 1031740.9s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.476s, learning 0.191s)
               Value function loss: 0.1027
                    Surrogate loss: -0.0343
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 44.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 8.67s
                        Total time: 34952.92s
                               ETA: 1031671.2s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.167s)
               Value function loss: 0.1065
                    Surrogate loss: -0.0351
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 43.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 8.33s
                        Total time: 34961.26s
                               ETA: 1031591.7s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.161s)
               Value function loss: 17.3892
                    Surrogate loss: 0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 43.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 8.70s
                        Total time: 34969.96s
                               ETA: 1031523.2s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.819s, learning 0.173s)
               Value function loss: 13.5668
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 43.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 8.99s
                        Total time: 34978.95s
                               ETA: 1031463.2s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.694s, learning 0.169s)
               Value function loss: 24.9237
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 7.69
               Mean episode length: 45.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 8.86s
                        Total time: 34987.82s
                               ETA: 1031399.5s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.414s, learning 0.203s)
               Value function loss: 11.9802
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 43.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 8.62s
                        Total time: 34996.43s
                               ETA: 1031328.5s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.522s, learning 0.183s)
               Value function loss: 0.1764
                    Surrogate loss: -0.0374
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 8.70s
                        Total time: 35005.14s
                               ETA: 1031260.1s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.347s, learning 0.170s)
               Value function loss: 0.1307
                    Surrogate loss: -0.0346
             Mean action noise std: 0.69
                       Mean reward: 5.07
               Mean episode length: 43.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 8.52s
                        Total time: 35013.65s
                               ETA: 1031186.2s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.165s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0346
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 44.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 8.76s
                        Total time: 35022.41s
                               ETA: 1031119.5s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.468s, learning 0.200s)
               Value function loss: 106.5371
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 45.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 8.67s
                        Total time: 35031.08s
                               ETA: 1031050.2s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.361s, learning 0.165s)
               Value function loss: 0.1701
                    Surrogate loss: -0.0397
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 43.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 8.53s
                        Total time: 35039.61s
                               ETA: 1030976.7s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.801s, learning 0.318s)
               Value function loss: 0.1498
                    Surrogate loss: -0.0361
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 9.12s
                        Total time: 35048.73s
                               ETA: 1030920.7s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.471s, learning 0.171s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0332
             Mean action noise std: 0.69
                       Mean reward: 2.35
               Mean episode length: 43.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 8.64s
                        Total time: 35057.37s
                               ETA: 1030850.8s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.358s, learning 0.185s)
               Value function loss: 32.9300
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 44.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 8.54s
                        Total time: 35065.91s
                               ETA: 1030777.9s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.541s, learning 0.161s)
               Value function loss: 0.2302
                    Surrogate loss: -0.0364
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 44.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 8.70s
                        Total time: 35074.61s
                               ETA: 1030709.8s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.460s, learning 0.188s)
               Value function loss: 42.8574
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.37
               Mean episode length: 43.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 8.65s
                        Total time: 35083.26s
                               ETA: 1030640.1s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.323s, learning 0.174s)
               Value function loss: 0.2530
                    Surrogate loss: -0.0337
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 45.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 8.50s
                        Total time: 35091.76s
                               ETA: 1030566.0s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.166s)
               Value function loss: 0.2135
                    Surrogate loss: -0.0282
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 43.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 8.51s
                        Total time: 35100.27s
                               ETA: 1030492.2s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.257s, learning 0.176s)
               Value function loss: 0.1613
                    Surrogate loss: -0.0332
             Mean action noise std: 0.69
                       Mean reward: 2.10
               Mean episode length: 42.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 8.43s
                        Total time: 35108.70s
                               ETA: 1030416.3s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.294s, learning 0.172s)
               Value function loss: 0.1283
                    Surrogate loss: -0.0354
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 45.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 8.47s
                        Total time: 35117.16s
                               ETA: 1030341.4s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.362s, learning 0.172s)
               Value function loss: 77.5798
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 43.80
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 8.53s
                        Total time: 35125.70s
                               ETA: 1030268.6s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.446s, learning 0.173s)
               Value function loss: 0.1373
                    Surrogate loss: -0.0414
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 43.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 8.62s
                        Total time: 35134.32s
                               ETA: 1030198.3s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.282s, learning 0.167s)
               Value function loss: 0.1398
                    Surrogate loss: -0.0352
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 8.45s
                        Total time: 35142.77s
                               ETA: 1030123.0s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.484s, learning 0.226s)
               Value function loss: 59.3874
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 43.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 8.71s
                        Total time: 35151.48s
                               ETA: 1030055.4s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.571s, learning 0.171s)
               Value function loss: 106.8754
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 43.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 8.74s
                        Total time: 35160.22s
                               ETA: 1029988.8s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.475s, learning 0.161s)
               Value function loss: 0.9050
                    Surrogate loss: -0.0386
             Mean action noise std: 0.69
                       Mean reward: 7.46
               Mean episode length: 44.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 8.64s
                        Total time: 35168.85s
                               ETA: 1029919.2s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.552s, learning 0.259s)
               Value function loss: 0.3775
                    Surrogate loss: -0.0304
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 43.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 8.81s
                        Total time: 35177.67s
                               ETA: 1029854.7s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.162s)
               Value function loss: 107.0157
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 8.55s
                        Total time: 35186.22s
                               ETA: 1029782.6s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.418s, learning 0.158s)
               Value function loss: 29.6809
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 45.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 8.58s
                        Total time: 35194.79s
                               ETA: 1029711.3s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.410s, learning 0.279s)
               Value function loss: 0.4043
                    Surrogate loss: -0.0323
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 43.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 8.69s
                        Total time: 35203.48s
                               ETA: 1029643.3s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.218s, learning 0.159s)
               Value function loss: 12.1487
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 7.31
               Mean episode length: 42.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 8.38s
                        Total time: 35211.86s
                               ETA: 1029566.3s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.027s, learning 0.169s)
               Value function loss: 53.0451
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.79
               Mean episode length: 45.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 8.20s
                        Total time: 35220.06s
                               ETA: 1029483.9s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.196s, learning 0.177s)
               Value function loss: 47.2249
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 44.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 8.37s
                        Total time: 35228.43s
                               ETA: 1029406.8s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.842s, learning 0.214s)
               Value function loss: 176.5101
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 45.26
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 9.06s
                        Total time: 35237.49s
                               ETA: 1029349.8s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.172s)
               Value function loss: 78.0180
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.71
               Mean episode length: 44.94
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.45s
                        Total time: 35245.94s
                               ETA: 1029275.1s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.671s, learning 0.161s)
               Value function loss: 163.5218
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 12.54
               Mean episode length: 44.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.83s
                        Total time: 35254.77s
                               ETA: 1029211.5s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.458s, learning 0.179s)
               Value function loss: 53.0725
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 15.15
               Mean episode length: 44.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.64s
                        Total time: 35263.41s
                               ETA: 1029142.3s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.196s)
               Value function loss: 18.8062
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 43.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.74s
                        Total time: 35272.15s
                               ETA: 1029076.0s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.598s, learning 0.174s)
               Value function loss: 17.9670
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 43.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.77s
                        Total time: 35280.92s
                               ETA: 1029010.8s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.313s, learning 0.177s)
               Value function loss: 64.9064
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 44.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.49s
                        Total time: 35289.41s
                               ETA: 1028937.4s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.519s, learning 0.257s)
               Value function loss: 66.4810
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 44.18
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.78s
                        Total time: 35298.18s
                               ETA: 1028872.4s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.596s, learning 0.174s)
               Value function loss: 1.0475
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 7.76
               Mean episode length: 44.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 8.77s
                        Total time: 35306.95s
                               ETA: 1028807.2s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.581s, learning 0.233s)
               Value function loss: 16.7689
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 44.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.81s
                        Total time: 35315.77s
                               ETA: 1028743.3s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.556s, learning 0.175s)
               Value function loss: 31.2853
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 43.82
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.73s
                        Total time: 35324.50s
                               ETA: 1028677.1s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.085s, learning 0.308s)
               Value function loss: 60.9901
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 44.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.39s
                        Total time: 35332.89s
                               ETA: 1028601.1s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.377s, learning 0.173s)
               Value function loss: 18.1527
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 43.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.55s
                        Total time: 35341.44s
                               ETA: 1028529.6s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.178s, learning 0.168s)
               Value function loss: 0.3169
                    Surrogate loss: -0.0281
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 44.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.35s
                        Total time: 35349.79s
                               ETA: 1028452.3s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.753s, learning 0.288s)
               Value function loss: 12.0821
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 44.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 9.04s
                        Total time: 35358.83s
                               ETA: 1028395.2s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.727s, learning 0.175s)
               Value function loss: 54.4295
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 44.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.90s
                        Total time: 35367.73s
                               ETA: 1028334.1s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.270s, learning 0.206s)
               Value function loss: 59.3823
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 44.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.48s
                        Total time: 35376.21s
                               ETA: 1028260.7s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.346s, learning 0.192s)
               Value function loss: 15.6560
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 7.50
               Mean episode length: 44.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.54s
                        Total time: 35384.75s
                               ETA: 1028189.1s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.477s, learning 0.235s)
               Value function loss: 21.2976
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 7.73
               Mean episode length: 44.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.71s
                        Total time: 35393.46s
                               ETA: 1028122.5s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.651s, learning 0.169s)
               Value function loss: 0.4218
                    Surrogate loss: -0.0227
             Mean action noise std: 0.69
                       Mean reward: 5.22
               Mean episode length: 44.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.82s
                        Total time: 35402.28s
                               ETA: 1028059.2s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.380s, learning 0.163s)
               Value function loss: 33.1620
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 5.03
               Mean episode length: 44.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.54s
                        Total time: 35410.82s
                               ETA: 1027987.8s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.727s, learning 0.261s)
               Value function loss: 21.1697
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 45.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.99s
                        Total time: 35419.81s
                               ETA: 1027929.4s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.364s, learning 0.182s)
               Value function loss: 0.4645
                    Surrogate loss: -0.0314
             Mean action noise std: 0.69
                       Mean reward: 5.11
               Mean episode length: 45.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 8.55s
                        Total time: 35428.36s
                               ETA: 1027858.2s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.294s, learning 0.207s)
               Value function loss: 0.3098
                    Surrogate loss: -0.0321
             Mean action noise std: 0.69
                       Mean reward: 4.78
               Mean episode length: 43.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 8.50s
                        Total time: 35436.86s
                               ETA: 1027785.8s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.286s, learning 0.199s)
               Value function loss: 0.2442
                    Surrogate loss: -0.0307
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.49s
                        Total time: 35445.34s
                               ETA: 1027712.9s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.154s, learning 0.170s)
               Value function loss: 0.2231
                    Surrogate loss: -0.0333
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 44.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.32s
                        Total time: 35453.67s
                               ETA: 1027635.4s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.248s, learning 0.172s)
               Value function loss: 0.1999
                    Surrogate loss: -0.0348
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 46.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.42s
                        Total time: 35462.09s
                               ETA: 1027560.7s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.605s, learning 0.194s)
               Value function loss: 35.1636
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 45.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.80s
                        Total time: 35470.89s
                               ETA: 1027497.1s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.452s, learning 0.235s)
               Value function loss: 36.9728
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 43.23
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.69s
                        Total time: 35479.57s
                               ETA: 1027430.2s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.193s, learning 0.189s)
               Value function loss: 13.8724
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 7.45
               Mean episode length: 44.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.38s
                        Total time: 35487.96s
                               ETA: 1027354.5s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.327s, learning 0.198s)
               Value function loss: 160.4315
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 7.55
               Mean episode length: 44.27
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.53s
                        Total time: 35496.48s
                               ETA: 1027283.0s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.532s, learning 0.243s)
               Value function loss: 0.3758
                    Surrogate loss: -0.0281
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.77s
                        Total time: 35505.26s
                               ETA: 1027218.8s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.388s, learning 0.170s)
               Value function loss: 0.3311
                    Surrogate loss: -0.0312
             Mean action noise std: 0.69
                       Mean reward: 14.83
               Mean episode length: 44.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.56s
                        Total time: 35513.81s
                               ETA: 1027148.4s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.761s, learning 0.164s)
               Value function loss: 11.9102
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.74
               Mean episode length: 45.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.93s
                        Total time: 35522.74s
                               ETA: 1027088.5s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.993s, learning 0.195s)
               Value function loss: 24.9199
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 44.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.19s
                        Total time: 35530.93s
                               ETA: 1027007.4s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.438s, learning 0.195s)
               Value function loss: 18.8080
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 46.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.63s
                        Total time: 35539.56s
                               ETA: 1026939.3s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.344s, learning 0.162s)
               Value function loss: 0.4186
                    Surrogate loss: -0.0267
             Mean action noise std: 0.69
                       Mean reward: 5.17
               Mean episode length: 45.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 8.51s
                        Total time: 35548.07s
                               ETA: 1026867.4s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.167s)
               Value function loss: 0.3042
                    Surrogate loss: -0.0331
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 43.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.40s
                        Total time: 35556.47s
                               ETA: 1026792.6s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.462s, learning 0.160s)
               Value function loss: 0.2377
                    Surrogate loss: -0.0347
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 45.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.62s
                        Total time: 35565.09s
                               ETA: 1026724.2s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.473s, learning 0.180s)
               Value function loss: 3.9300
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 46.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.65s
                        Total time: 35573.74s
                               ETA: 1026656.8s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.047s, learning 0.201s)
               Value function loss: 7.8094
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 44.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.25s
                        Total time: 35581.99s
                               ETA: 1026577.6s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.065s, learning 0.213s)
               Value function loss: 0.1660
                    Surrogate loss: -0.0423
             Mean action noise std: 0.69
                       Mean reward: 4.78
               Mean episode length: 44.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.28s
                        Total time: 35590.27s
                               ETA: 1026499.4s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.404s, learning 0.196s)
               Value function loss: 32.9684
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 45.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.60s
                        Total time: 35598.87s
                               ETA: 1026430.5s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.623s, learning 0.159s)
               Value function loss: 9.8144
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 7.78
               Mean episode length: 45.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 8.78s
                        Total time: 35607.65s
                               ETA: 1026366.9s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.549s, learning 0.188s)
               Value function loss: 0.2296
                    Surrogate loss: -0.0345
             Mean action noise std: 0.69
                       Mean reward: 5.43
               Mean episode length: 45.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.74s
                        Total time: 35616.39s
                               ETA: 1026302.1s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.315s, learning 0.186s)
               Value function loss: 0.2198
                    Surrogate loss: -0.0358
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 45.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.50s
                        Total time: 35624.89s
                               ETA: 1026230.4s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.347s, learning 0.173s)
               Value function loss: 0.1726
                    Surrogate loss: -0.0388
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 44.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 8.52s
                        Total time: 35633.41s
                               ETA: 1026159.4s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.133s, learning 0.172s)
               Value function loss: 0.1692
                    Surrogate loss: -0.0397
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 44.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 8.31s
                        Total time: 35641.71s
                               ETA: 1026082.2s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.362s, learning 0.216s)
               Value function loss: 0.1498
                    Surrogate loss: -0.0381
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 45.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.58s
                        Total time: 35650.29s
                               ETA: 1026012.9s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.233s, learning 0.191s)
               Value function loss: 12.0540
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 44.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.42s
                        Total time: 35658.72s
                               ETA: 1025939.2s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.247s, learning 0.165s)
               Value function loss: 43.3939
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.69
               Mean episode length: 44.86
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 8.41s
                        Total time: 35667.13s
                               ETA: 1025865.2s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.422s, learning 0.218s)
               Value function loss: 0.1845
                    Surrogate loss: -0.0415
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 44.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.64s
                        Total time: 35675.77s
                               ETA: 1025797.8s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.712s, learning 0.165s)
               Value function loss: 0.1621
                    Surrogate loss: -0.0400
             Mean action noise std: 0.69
                       Mean reward: 4.88
               Mean episode length: 43.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.88s
                        Total time: 35684.65s
                               ETA: 1025737.2s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.065s, learning 0.191s)
               Value function loss: 0.1822
                    Surrogate loss: -0.0359
             Mean action noise std: 0.69
                       Mean reward: 2.39
               Mean episode length: 43.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.26s
                        Total time: 35692.90s
                               ETA: 1025658.8s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.375s, learning 0.208s)
               Value function loss: 0.1696
                    Surrogate loss: -0.0378
             Mean action noise std: 0.69
                       Mean reward: 2.16
               Mean episode length: 43.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 8.58s
                        Total time: 35701.48s
                               ETA: 1025589.9s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.412s, learning 0.197s)
               Value function loss: 0.1607
                    Surrogate loss: -0.0383
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 46.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.61s
                        Total time: 35710.09s
                               ETA: 1025521.7s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.352s, learning 0.164s)
               Value function loss: 59.6239
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 45.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.52s
                        Total time: 35718.61s
                               ETA: 1025450.9s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.484s, learning 0.166s)
               Value function loss: 56.3449
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.80
               Mean episode length: 44.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.65s
                        Total time: 35727.26s
                               ETA: 1025384.0s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.435s, learning 0.207s)
               Value function loss: 42.6462
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 7.45
               Mean episode length: 44.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.64s
                        Total time: 35735.90s
                               ETA: 1025316.9s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.535s, learning 0.171s)
               Value function loss: 0.5140
                    Surrogate loss: -0.0325
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 45.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.71s
                        Total time: 35744.61s
                               ETA: 1025251.7s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.178s, learning 0.171s)
               Value function loss: 12.1420
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 7.96
               Mean episode length: 45.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.35s
                        Total time: 35752.96s
                               ETA: 1025176.2s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.224s, learning 0.200s)
               Value function loss: 15.2603
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.52
               Mean episode length: 44.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.42s
                        Total time: 35761.38s
                               ETA: 1025103.0s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.162s, learning 0.214s)
               Value function loss: 109.2351
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 45.36
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.38s
                        Total time: 35769.76s
                               ETA: 1025028.4s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.405s, learning 0.161s)
               Value function loss: 1.3722
                    Surrogate loss: -0.0237
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 44.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.57s
                        Total time: 35778.32s
                               ETA: 1024959.3s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.165s)
               Value function loss: 0.4783
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 7.39
               Mean episode length: 43.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.39s
                        Total time: 35786.71s
                               ETA: 1024885.2s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.541s, learning 0.198s)
               Value function loss: 0.3066
                    Surrogate loss: -0.0299
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 44.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.74s
                        Total time: 35795.45s
                               ETA: 1024821.1s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.489s, learning 0.175s)
               Value function loss: 14.2656
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 45.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.66s
                        Total time: 35804.11s
                               ETA: 1024754.9s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.164s, learning 0.195s)
               Value function loss: 0.2343
                    Surrogate loss: -0.0363
             Mean action noise std: 0.69
                       Mean reward: 2.16
               Mean episode length: 44.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 8.36s
                        Total time: 35812.47s
                               ETA: 1024680.0s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.472s, learning 0.191s)
               Value function loss: 0.2001
                    Surrogate loss: -0.0258
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 43.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.66s
                        Total time: 35821.14s
                               ETA: 1024613.9s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.166s)
               Value function loss: 0.1663
                    Surrogate loss: -0.0361
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 44.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 8.51s
                        Total time: 35829.64s
                               ETA: 1024543.3s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.318s, learning 0.165s)
               Value function loss: 0.1654
                    Surrogate loss: -0.0378
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 8.48s
                        Total time: 35838.13s
                               ETA: 1024472.0s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.244s, learning 0.193s)
               Value function loss: 17.8192
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 45.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.44s
                        Total time: 35846.56s
                               ETA: 1024399.5s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.246s, learning 0.174s)
               Value function loss: 249.6403
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.39
               Mean episode length: 43.41
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.42s
                        Total time: 35854.98s
                               ETA: 1024326.6s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.309s, learning 0.173s)
               Value function loss: 0.3501
                    Surrogate loss: -0.0369
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 45.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 8.48s
                        Total time: 35863.46s
                               ETA: 1024255.5s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.737s, learning 0.160s)
               Value function loss: 64.0362
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 44.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.90s
                        Total time: 35872.36s
                               ETA: 1024196.2s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.436s, learning 0.166s)
               Value function loss: 0.2889
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 20.26
               Mean episode length: 44.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.60s
                        Total time: 35880.96s
                               ETA: 1024128.6s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.618s, learning 0.211s)
               Value function loss: 0.2998
                    Surrogate loss: -0.0329
             Mean action noise std: 0.69
                       Mean reward: 8.13
               Mean episode length: 46.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.83s
                        Total time: 35889.79s
                               ETA: 1024067.4s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.289s, learning 0.186s)
               Value function loss: 9.9593
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 44.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.47s
                        Total time: 35898.27s
                               ETA: 1023996.2s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.170s)
               Value function loss: 0.2686
                    Surrogate loss: -0.0309
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 44.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.66s
                        Total time: 35906.93s
                               ETA: 1023930.4s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.294s, learning 0.169s)
               Value function loss: 208.7127
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 44.70
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.46s
                        Total time: 35915.39s
                               ETA: 1023858.9s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.408s, learning 0.255s)
               Value function loss: 81.9482
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 42.36
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 8.66s
                        Total time: 35924.06s
                               ETA: 1023793.2s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.366s, learning 0.174s)
               Value function loss: 16.1296
                    Surrogate loss: 0.0055
             Mean action noise std: 0.69
                       Mean reward: 2.18
               Mean episode length: 42.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 8.54s
                        Total time: 35932.60s
                               ETA: 1023724.0s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.348s, learning 0.160s)
               Value function loss: 0.4482
                    Surrogate loss: -0.0308
             Mean action noise std: 0.69
                       Mean reward: 12.61
               Mean episode length: 43.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.51s
                        Total time: 35941.10s
                               ETA: 1023653.9s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.783s, learning 0.172s)
               Value function loss: 0.3601
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 44.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 9.95s
                        Total time: 35951.06s
                               ETA: 1023625.1s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.974s, learning 0.172s)
               Value function loss: 16.6148
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 43.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 17.15s
                        Total time: 35968.20s
                               ETA: 1023800.9s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.219s, learning 0.160s)
               Value function loss: 0.3018
                    Surrogate loss: -0.0333
             Mean action noise std: 0.69
                       Mean reward: 2.38
               Mean episode length: 44.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 16.38s
                        Total time: 35984.58s
                               ETA: 1023954.8s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.517s, learning 0.176s)
               Value function loss: 64.1336
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 44.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 16.69s
                        Total time: 36001.28s
                               ETA: 1024117.5s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.578s, learning 0.189s)
               Value function loss: 7.8965
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 43.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 16.77s
                        Total time: 36018.04s
                               ETA: 1024282.3s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.947s, learning 0.178s)
               Value function loss: 0.3664
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 4.85
               Mean episode length: 43.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 17.13s
                        Total time: 36035.17s
                               ETA: 1024457.1s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.550s, learning 0.164s)
               Value function loss: 0.2717
                    Surrogate loss: -0.0257
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 16.71s
                        Total time: 36051.88s
                               ETA: 1024620.1s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.618s, learning 0.233s)
               Value function loss: 0.2405
                    Surrogate loss: -0.0300
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 42.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 16.85s
                        Total time: 36068.73s
                               ETA: 1024786.9s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.619s, learning 0.178s)
               Value function loss: 0.2011
                    Surrogate loss: -0.0300
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 43.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 16.80s
                        Total time: 36085.53s
                               ETA: 1024952.1s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.962s, learning 0.193s)
               Value function loss: 0.1479
                    Surrogate loss: -0.0338
             Mean action noise std: 0.69
                       Mean reward: 2.14
               Mean episode length: 42.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 17.15s
                        Total time: 36102.68s
                               ETA: 1025127.3s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.569s, learning 0.163s)
               Value function loss: 0.1471
                    Surrogate loss: -0.0357
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 16.73s
                        Total time: 36119.42s
                               ETA: 1025290.4s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1009 steps/s (collection: 15.955s, learning 0.270s)
               Value function loss: 0.1245
                    Surrogate loss: -0.0350
             Mean action noise std: 0.69
                       Mean reward: 2.09
               Mean episode length: 42.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 16.22s
                        Total time: 36135.64s
                               ETA: 1025439.0s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.444s, learning 0.191s)
               Value function loss: 0.1163
                    Surrogate loss: -0.0337
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 43.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 16.64s
                        Total time: 36152.28s
                               ETA: 1025599.2s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.907s, learning 0.165s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0352
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 43.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 17.07s
                        Total time: 36169.35s
                               ETA: 1025771.7s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.739s, learning 0.281s)
               Value function loss: 0.1135
                    Surrogate loss: -0.0391
             Mean action noise std: 0.69
                       Mean reward: 2.05
               Mean episode length: 42.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 17.02s
                        Total time: 36186.37s
                               ETA: 1025942.5s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.524s, learning 0.166s)
               Value function loss: 7.0443
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 43.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 16.69s
                        Total time: 36203.06s
                               ETA: 1026103.9s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.676s, learning 0.282s)
               Value function loss: 36.1548
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 5.26
               Mean episode length: 43.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 16.96s
                        Total time: 36220.02s
                               ETA: 1026272.8s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.624s, learning 0.162s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0403
             Mean action noise std: 0.69
                       Mean reward: 2.19
               Mean episode length: 42.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 16.79s
                        Total time: 36236.80s
                               ETA: 1026436.7s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.649s, learning 0.196s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0373
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 42.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 16.84s
                        Total time: 36253.65s
                               ETA: 1026602.1s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.443s, learning 0.237s)
               Value function loss: 29.6277
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 16.68s
                        Total time: 36270.33s
                               ETA: 1026762.8s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.048s, learning 0.192s)
               Value function loss: 0.1402
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 43.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 17.24s
                        Total time: 36287.57s
                               ETA: 1026939.3s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.492s, learning 0.174s)
               Value function loss: 0.1097
                    Surrogate loss: -0.0325
             Mean action noise std: 0.69
                       Mean reward: 2.39
               Mean episode length: 42.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 16.67s
                        Total time: 36304.23s
                               ETA: 1027099.3s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.395s, learning 0.166s)
               Value function loss: 0.1003
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 43.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 16.56s
                        Total time: 36320.79s
                               ETA: 1027256.3s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.680s, learning 0.189s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 43.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 16.87s
                        Total time: 36337.66s
                               ETA: 1027421.9s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.605s, learning 0.164s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0358
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 43.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 16.77s
                        Total time: 36354.43s
                               ETA: 1027584.6s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.436s, learning 0.166s)
               Value function loss: 17.7486
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 42.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 16.60s
                        Total time: 36371.04s
                               ETA: 1027742.5s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.896s, learning 0.173s)
               Value function loss: 0.1699
                    Surrogate loss: -0.0394
             Mean action noise std: 0.69
                       Mean reward: 4.92
               Mean episode length: 43.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 17.07s
                        Total time: 36388.10s
                               ETA: 1027913.4s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.703s, learning 0.292s)
               Value function loss: 9.9443
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 17.00s
                        Total time: 36405.10s
                               ETA: 1028082.1s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.653s, learning 0.224s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0392
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 44.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 16.88s
                        Total time: 36421.98s
                               ETA: 1028247.4s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.580s, learning 0.319s)
               Value function loss: 15.1699
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.56
               Mean episode length: 42.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 16.90s
                        Total time: 36438.87s
                               ETA: 1028413.2s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.399s, learning 0.178s)
               Value function loss: 0.1592
                    Surrogate loss: -0.0367
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 16.58s
                        Total time: 36455.45s
                               ETA: 1028569.8s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.784s, learning 0.233s)
               Value function loss: 47.3950
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 4.90
               Mean episode length: 43.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 17.02s
                        Total time: 36472.47s
                               ETA: 1028738.8s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.938s, learning 0.156s)
               Value function loss: 7.0955
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 1.96
               Mean episode length: 41.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 17.09s
                        Total time: 36489.56s
                               ETA: 1028909.8s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.762s, learning 0.167s)
               Value function loss: 7.1465
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.20
               Mean episode length: 42.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 16.93s
                        Total time: 36506.49s
                               ETA: 1029076.0s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.009s, learning 0.201s)
               Value function loss: 15.2551
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 42.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 17.21s
                        Total time: 36523.70s
                               ETA: 1029250.0s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.878s, learning 0.184s)
               Value function loss: 9.6992
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 4.92
               Mean episode length: 43.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 17.06s
                        Total time: 36540.76s
                               ETA: 1029419.8s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.303s, learning 0.181s)
               Value function loss: 21.6996
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 4.98
               Mean episode length: 43.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 16.48s
                        Total time: 36557.25s
                               ETA: 1029573.2s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.555s, learning 0.174s)
               Value function loss: 117.6224
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.05
               Mean episode length: 41.27
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 16.73s
                        Total time: 36573.98s
                               ETA: 1029733.3s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.691s, learning 0.287s)
               Value function loss: 0.3824
                    Surrogate loss: -0.0326
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 42.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 12.98s
                        Total time: 36586.95s
                               ETA: 1029787.9s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.516s, learning 0.189s)
               Value function loss: 59.1811
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 44.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 8.70s
                        Total time: 36595.66s
                               ETA: 1029722.1s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.822s, learning 0.163s)
               Value function loss: 0.4026
                    Surrogate loss: -0.0346
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 44.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 8.98s
                        Total time: 36604.64s
                               ETA: 1029664.2s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.585s, learning 0.191s)
               Value function loss: 0.3631
                    Surrogate loss: -0.0293
             Mean action noise std: 0.69
                       Mean reward: 7.81
               Mean episode length: 44.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 8.78s
                        Total time: 36613.42s
                               ETA: 1029600.5s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.183s)
               Value function loss: 0.2713
                    Surrogate loss: -0.0292
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 8.58s
                        Total time: 36622.00s
                               ETA: 1029531.3s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.574s, learning 0.177s)
               Value function loss: 154.0492
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 44.88
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 8.75s
                        Total time: 36630.75s
                               ETA: 1029467.0s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.661s, learning 0.264s)
               Value function loss: 4.1017
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 43.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 8.93s
                        Total time: 36639.68s
                               ETA: 1029407.6s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.687s, learning 0.160s)
               Value function loss: 15.3070
                    Surrogate loss: 0.0020
             Mean action noise std: 0.69
                       Mean reward: 10.14
               Mean episode length: 43.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 8.85s
                        Total time: 36648.52s
                               ETA: 1029346.0s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.170s)
               Value function loss: 0.3776
                    Surrogate loss: -0.0307
             Mean action noise std: 0.69
                       Mean reward: 10.04
               Mean episode length: 43.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 8.59s
                        Total time: 36657.11s
                               ETA: 1029277.1s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.164s)
               Value function loss: 63.7455
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 43.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 8.47s
                        Total time: 36665.58s
                               ETA: 1029205.0s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.459s, learning 0.193s)
               Value function loss: 4.1114
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2.28
               Mean episode length: 42.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 8.65s
                        Total time: 36674.23s
                               ETA: 1029138.1s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.400s, learning 0.204s)
               Value function loss: 0.2872
                    Surrogate loss: -0.0268
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 43.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 8.60s
                        Total time: 36682.84s
                               ETA: 1029069.8s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.788s, learning 0.232s)
               Value function loss: 0.2168
                    Surrogate loss: -0.0306
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 44.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 9.02s
                        Total time: 36691.86s
                               ETA: 1029013.2s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.411s, learning 0.174s)
               Value function loss: 62.6413
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 43.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 8.58s
                        Total time: 36700.44s
                               ETA: 1028944.4s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.415s, learning 0.326s)
               Value function loss: 61.6285
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 43.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 8.74s
                        Total time: 36709.18s
                               ETA: 1028880.1s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.432s, learning 0.170s)
               Value function loss: 1.2011
                    Surrogate loss: -0.0314
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 43.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.60s
                        Total time: 36717.79s
                               ETA: 1028811.9s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.657s, learning 0.211s)
               Value function loss: 34.2169
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 43.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 8.87s
                        Total time: 36726.65s
                               ETA: 1028751.2s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.505s, learning 0.168s)
               Value function loss: 12.0651
                    Surrogate loss: -0.0070
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.67s
                        Total time: 36735.33s
                               ETA: 1028685.0s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.532s, learning 0.160s)
               Value function loss: 1.0581
                    Surrogate loss: -0.0157
             Mean action noise std: 0.69
                       Mean reward: 5.02
               Mean episode length: 44.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 8.69s
                        Total time: 36744.02s
                               ETA: 1028619.5s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.567s, learning 0.256s)
               Value function loss: 0.4663
                    Surrogate loss: -0.0240
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 43.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 8.82s
                        Total time: 36752.84s
                               ETA: 1028557.6s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.297s, learning 0.201s)
               Value function loss: 15.4078
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.56
               Mean episode length: 43.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.50s
                        Total time: 36761.34s
                               ETA: 1028486.7s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.165s)
               Value function loss: 0.2884
                    Surrogate loss: -0.0315
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 43.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.55s
                        Total time: 36769.89s
                               ETA: 1028417.2s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.168s)
               Value function loss: 0.2209
                    Surrogate loss: -0.0321
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 43.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 8.47s
                        Total time: 36778.36s
                               ETA: 1028345.6s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.602s, learning 0.169s)
               Value function loss: 16.6062
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 43.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.77s
                        Total time: 36787.13s
                               ETA: 1028282.4s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.458s, learning 0.164s)
               Value function loss: 0.2051
                    Surrogate loss: -0.0368
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 42.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 8.62s
                        Total time: 36795.75s
                               ETA: 1028215.0s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.325s, learning 0.162s)
               Value function loss: 17.2543
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 8.49s
                        Total time: 36804.24s
                               ETA: 1028144.0s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.538s, learning 0.170s)
               Value function loss: 52.1271
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 42.98
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 8.71s
                        Total time: 36812.95s
                               ETA: 1028079.1s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.760s, learning 0.177s)
               Value function loss: 0.3318
                    Surrogate loss: -0.0312
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 42.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 8.94s
                        Total time: 36821.89s
                               ETA: 1028020.7s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.685s, learning 0.276s)
               Value function loss: 29.4253
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 40.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 8.96s
                        Total time: 36830.85s
                               ETA: 1027962.9s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.170s)
               Value function loss: 0.1874
                    Surrogate loss: -0.0334
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 43.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 8.66s
                        Total time: 36839.51s
                               ETA: 1027896.9s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.509s, learning 0.182s)
               Value function loss: 17.2432
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 41.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 8.69s
                        Total time: 36848.20s
                               ETA: 1027831.7s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.675s, learning 0.164s)
               Value function loss: 0.1923
                    Surrogate loss: -0.0347
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 41.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 8.84s
                        Total time: 36857.04s
                               ETA: 1027770.6s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.165s)
               Value function loss: 13.6325
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 4.71
               Mean episode length: 42.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 8.33s
                        Total time: 36865.37s
                               ETA: 1027695.4s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.359s, learning 0.161s)
               Value function loss: 17.6869
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 43.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 8.52s
                        Total time: 36873.89s
                               ETA: 1027625.5s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.366s, learning 0.196s)
               Value function loss: 7.6819
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 43.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 8.56s
                        Total time: 36882.45s
                               ETA: 1027556.8s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.469s, learning 0.186s)
               Value function loss: 9.8923
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.39
               Mean episode length: 42.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 8.65s
                        Total time: 36891.11s
                               ETA: 1027490.8s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.157s, learning 0.160s)
               Value function loss: 131.7541
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 42.99
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.32s
                        Total time: 36899.42s
                               ETA: 1027415.3s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.680s, learning 0.161s)
               Value function loss: 0.5440
                    Surrogate loss: -0.0266
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 43.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.84s
                        Total time: 36908.26s
                               ETA: 1027354.5s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.566s, learning 0.171s)
               Value function loss: 0.3339
                    Surrogate loss: -0.0298
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.74s
                        Total time: 36917.00s
                               ETA: 1027290.8s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.164s)
               Value function loss: 0.2643
                    Surrogate loss: -0.0248
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 43.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.53s
                        Total time: 36925.53s
                               ETA: 1027221.5s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.160s)
               Value function loss: 17.9438
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 9.95
               Mean episode length: 42.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.49s
                        Total time: 36934.02s
                               ETA: 1027150.9s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.710s, learning 0.218s)
               Value function loss: 20.9434
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.01
               Mean episode length: 42.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.93s
                        Total time: 36942.95s
                               ETA: 1027092.6s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.661s, learning 0.159s)
               Value function loss: 0.4796
                    Surrogate loss: -0.0273
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 42.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.82s
                        Total time: 36951.77s
                               ETA: 1027031.4s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.521s, learning 0.168s)
               Value function loss: 12.1019
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 42.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.69s
                        Total time: 36960.45s
                               ETA: 1026966.5s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.318s, learning 0.163s)
               Value function loss: 54.2671
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 43.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 8.48s
                        Total time: 36968.94s
                               ETA: 1026896.0s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.557s, learning 0.171s)
               Value function loss: 18.8121
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 42.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 8.73s
                        Total time: 36977.66s
                               ETA: 1026832.3s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.228s, learning 0.173s)
               Value function loss: 0.3233
                    Surrogate loss: -0.0295
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 42.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.40s
                        Total time: 36986.07s
                               ETA: 1026759.5s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.386s, learning 0.199s)
               Value function loss: 0.2149
                    Surrogate loss: -0.0298
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 43.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 8.58s
                        Total time: 36994.65s
                               ETA: 1026691.9s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.162s)
               Value function loss: 0.1789
                    Surrogate loss: -0.0276
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 42.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.54s
                        Total time: 37003.19s
                               ETA: 1026623.2s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.305s, learning 0.217s)
               Value function loss: 0.1886
                    Surrogate loss: -0.0273
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 42.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.52s
                        Total time: 37011.71s
                               ETA: 1026553.9s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.508s, learning 0.253s)
               Value function loss: 9.8521
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.76s
                        Total time: 37020.48s
                               ETA: 1026491.3s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.158s)
               Value function loss: 64.1291
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 41.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.42s
                        Total time: 37028.90s
                               ETA: 1026419.3s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.164s)
               Value function loss: 25.2908
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.37
               Mean episode length: 42.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 8.38s
                        Total time: 37037.28s
                               ETA: 1026346.2s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.165s)
               Value function loss: 32.6877
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 42.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.48s
                        Total time: 37045.75s
                               ETA: 1026275.8s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.606s, learning 0.166s)
               Value function loss: 0.5472
                    Surrogate loss: -0.0279
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 42.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.77s
                        Total time: 37054.52s
                               ETA: 1026213.6s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.164s)
               Value function loss: 10.2360
                    Surrogate loss: 0.0029
             Mean action noise std: 0.69
                       Mean reward: 2.05
               Mean episode length: 41.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 8.50s
                        Total time: 37063.02s
                               ETA: 1026143.9s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.162s)
               Value function loss: 0.3862
                    Surrogate loss: -0.0248
             Mean action noise std: 0.69
                       Mean reward: 2.14
               Mean episode length: 41.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 8.53s
                        Total time: 37071.55s
                               ETA: 1026075.1s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.683s, learning 0.226s)
               Value function loss: 0.2717
                    Surrogate loss: -0.0271
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 42.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.91s
                        Total time: 37080.46s
                               ETA: 1026016.8s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.161s)
               Value function loss: 0.1677
                    Surrogate loss: -0.0354
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 42.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 8.64s
                        Total time: 37089.10s
                               ETA: 1025951.0s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.499s, learning 0.165s)
               Value function loss: 0.1640
                    Surrogate loss: -0.0382
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 42.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.66s
                        Total time: 37097.76s
                               ETA: 1025886.0s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.268s, learning 0.342s)
               Value function loss: 0.1563
                    Surrogate loss: -0.0303
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 41.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.61s
                        Total time: 37106.37s
                               ETA: 1025819.5s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.575s, learning 0.189s)
               Value function loss: 38.9156
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 42.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.76s
                        Total time: 37115.14s
                               ETA: 1025757.4s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.570s, learning 0.184s)
               Value function loss: 164.8925
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 43.37
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.75s
                        Total time: 37123.89s
                               ETA: 1025694.9s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.960s, learning 0.175s)
               Value function loss: 17.6809
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 42.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 9.14s
                        Total time: 37133.02s
                               ETA: 1025643.0s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.250s, learning 0.166s)
               Value function loss: 115.3987
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 42.10
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 8.42s
                        Total time: 37141.44s
                               ETA: 1025571.4s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.701s, learning 0.198s)
               Value function loss: 1.4124
                    Surrogate loss: -0.0269
             Mean action noise std: 0.69
                       Mean reward: 5.00
               Mean episode length: 41.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.90s
                        Total time: 37150.34s
                               ETA: 1025513.0s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.407s, learning 0.173s)
               Value function loss: 68.7530
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 42.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.58s
                        Total time: 37158.92s
                               ETA: 1025445.9s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.637s, learning 0.177s)
               Value function loss: 0.7300
                    Surrogate loss: -0.0305
             Mean action noise std: 0.69
                       Mean reward: 9.78
               Mean episode length: 41.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.81s
                        Total time: 37167.73s
                               ETA: 1025385.3s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.301s, learning 0.242s)
               Value function loss: 0.4284
                    Surrogate loss: -0.0257
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 42.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.54s
                        Total time: 37176.28s
                               ETA: 1025317.3s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.711s, learning 0.243s)
               Value function loss: 38.9420
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.11
               Mean episode length: 41.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.95s
                        Total time: 37185.23s
                               ETA: 1025260.6s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.056s, learning 0.209s)
               Value function loss: 16.0313
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 41.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.26s
                        Total time: 37193.50s
                               ETA: 1025184.9s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.415s, learning 0.169s)
               Value function loss: 62.1996
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 42.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.58s
                        Total time: 37202.08s
                               ETA: 1025118.1s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.371s, learning 0.161s)
               Value function loss: 18.0315
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 42.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.53s
                        Total time: 37210.61s
                               ETA: 1025049.9s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.462s, learning 0.167s)
               Value function loss: 100.2580
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 43.06
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.63s
                        Total time: 37219.24s
                               ETA: 1024984.4s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.485s, learning 0.188s)
               Value function loss: 1.0517
                    Surrogate loss: -0.0243
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 43.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.67s
                        Total time: 37227.92s
                               ETA: 1024920.1s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.536s, learning 0.169s)
               Value function loss: 0.4578
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.70s
                        Total time: 37236.62s
                               ETA: 1024856.7s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.383s, learning 0.173s)
               Value function loss: 9.9807
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 7.50
               Mean episode length: 42.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.56s
                        Total time: 37245.18s
                               ETA: 1024789.3s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.478s, learning 0.271s)
               Value function loss: 14.6764
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 4.70
               Mean episode length: 43.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.75s
                        Total time: 37253.93s
                               ETA: 1024727.2s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.508s, learning 0.177s)
               Value function loss: 0.3559
                    Surrogate loss: -0.0285
             Mean action noise std: 0.69
                       Mean reward: 4.66
               Mean episode length: 40.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.69s
                        Total time: 37262.61s
                               ETA: 1024663.4s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.176s)
               Value function loss: 0.2629
                    Surrogate loss: -0.0310
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 43.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.50s
                        Total time: 37271.12s
                               ETA: 1024594.7s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.513s, learning 0.210s)
               Value function loss: 0.1945
                    Surrogate loss: -0.0283
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 43.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.72s
                        Total time: 37279.84s
                               ETA: 1024532.0s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.194s, learning 0.258s)
               Value function loss: 22.2352
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 43.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.45s
                        Total time: 37288.29s
                               ETA: 1024461.8s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.737s, learning 0.169s)
               Value function loss: 11.8191
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 4.95
               Mean episode length: 43.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.91s
                        Total time: 37297.20s
                               ETA: 1024404.2s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.616s, learning 0.203s)
               Value function loss: 0.2573
                    Surrogate loss: -0.0236
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 42.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.82s
                        Total time: 37306.02s
                               ETA: 1024344.2s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.438s, learning 0.182s)
               Value function loss: 0.1825
                    Surrogate loss: -0.0353
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 44.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.62s
                        Total time: 37314.64s
                               ETA: 1024278.8s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.195s, learning 0.169s)
               Value function loss: 64.9036
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 42.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.36s
                        Total time: 37323.00s
                               ETA: 1024206.4s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.162s)
               Value function loss: 0.2903
                    Surrogate loss: -0.0304
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 43.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.57s
                        Total time: 37331.58s
                               ETA: 1024139.8s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.559s, learning 0.160s)
               Value function loss: 70.9767
                    Surrogate loss: 0.0005
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 43.31
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.72s
                        Total time: 37340.29s
                               ETA: 1024077.2s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.537s, learning 0.164s)
               Value function loss: 0.2122
                    Surrogate loss: -0.0286
             Mean action noise std: 0.69
                       Mean reward: 2.71
               Mean episode length: 42.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.70s
                        Total time: 37349.00s
                               ETA: 1024014.1s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.419s, learning 0.183s)
               Value function loss: 0.2002
                    Surrogate loss: -0.0269
             Mean action noise std: 0.69
                       Mean reward: 2.21
               Mean episode length: 42.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.60s
                        Total time: 37357.60s
                               ETA: 1023948.4s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.537s, learning 0.263s)
               Value function loss: 0.1591
                    Surrogate loss: -0.0279
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 42.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.80s
                        Total time: 37366.40s
                               ETA: 1023888.1s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.431s, learning 0.172s)
               Value function loss: 39.4214
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 2.52
               Mean episode length: 43.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.60s
                        Total time: 37375.00s
                               ETA: 1023822.4s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.522s, learning 0.199s)
               Value function loss: 39.2548
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 42.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.72s
                        Total time: 37383.72s
                               ETA: 1023760.1s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.559s, learning 0.186s)
               Value function loss: 0.2032
                    Surrogate loss: -0.0251
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 42.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.74s
                        Total time: 37392.47s
                               ETA: 1023698.3s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.699s, learning 0.197s)
               Value function loss: 3.8957
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.52
               Mean episode length: 42.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.90s
                        Total time: 37401.36s
                               ETA: 1023640.8s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.547s, learning 0.181s)
               Value function loss: 15.2371
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.07
               Mean episode length: 42.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.73s
                        Total time: 37410.09s
                               ETA: 1023578.7s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.618s, learning 0.170s)
               Value function loss: 9.7648
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 43.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.79s
                        Total time: 37418.88s
                               ETA: 1023518.2s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.804s, learning 0.182s)
               Value function loss: 7.0856
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 43.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.99s
                        Total time: 37427.86s
                               ETA: 1023463.3s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.507s, learning 0.172s)
               Value function loss: 64.0112
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 42.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.68s
                        Total time: 37436.54s
                               ETA: 1023399.9s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.726s, learning 0.190s)
               Value function loss: 0.2938
                    Surrogate loss: -0.0298
             Mean action noise std: 0.69
                       Mean reward: 5.08
               Mean episode length: 43.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.92s
                        Total time: 37445.46s
                               ETA: 1023343.0s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.547s, learning 0.177s)
               Value function loss: 309.4317
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 7.25
               Mean episode length: 42.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.72s
                        Total time: 37454.18s
                               ETA: 1023280.9s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.509s, learning 0.184s)
               Value function loss: 22.7873
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 42.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.69s
                        Total time: 37462.88s
                               ETA: 1023218.1s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.318s, learning 0.208s)
               Value function loss: 57.2938
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 42.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.53s
                        Total time: 37471.40s
                               ETA: 1023150.6s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.424s, learning 0.193s)
               Value function loss: 1.9343
                    Surrogate loss: -0.0228
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 43.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.62s
                        Total time: 37480.02s
                               ETA: 1023085.8s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.550s, learning 0.237s)
               Value function loss: 64.4231
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 42.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.79s
                        Total time: 37488.81s
                               ETA: 1023025.5s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.316s, learning 0.218s)
               Value function loss: 138.8004
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 42.46
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.53s
                        Total time: 37497.34s
                               ETA: 1022958.4s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.518s, learning 0.192s)
               Value function loss: 9.4890
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 42.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.71s
                        Total time: 37506.05s
                               ETA: 1022896.2s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.777s, learning 0.179s)
               Value function loss: 48.0518
                    Surrogate loss: 0.0054
             Mean action noise std: 0.69
                       Mean reward: 7.61
               Mean episode length: 43.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.96s
                        Total time: 37515.01s
                               ETA: 1022840.6s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.230s, learning 0.184s)
               Value function loss: 33.4193
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 41.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.41s
                        Total time: 37523.42s
                               ETA: 1022770.3s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.663s, learning 0.199s)
               Value function loss: 0.5115
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.79
               Mean episode length: 43.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.86s
                        Total time: 37532.28s
                               ETA: 1022712.3s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.611s, learning 0.184s)
               Value function loss: 131.7613
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.71
               Mean episode length: 42.63
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.79s
                        Total time: 37541.08s
                               ETA: 1022652.5s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.357s, learning 0.168s)
               Value function loss: 110.4437
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.86
               Mean episode length: 43.74
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.53s
                        Total time: 37549.60s
                               ETA: 1022585.3s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.557s, learning 0.181s)
               Value function loss: 99.6996
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 9.87
               Mean episode length: 42.47
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.74s
                        Total time: 37558.34s
                               ETA: 1022524.0s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.725s, learning 0.211s)
               Value function loss: 60.7904
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 12.53
               Mean episode length: 42.99
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.94s
                        Total time: 37567.28s
                               ETA: 1022468.1s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.275s, learning 0.218s)
               Value function loss: 15.2839
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 43.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.49s
                        Total time: 37575.77s
                               ETA: 1022400.2s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.571s, learning 0.166s)
               Value function loss: 10.2862
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 7.79
               Mean episode length: 43.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.74s
                        Total time: 37584.51s
                               ETA: 1022338.9s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.359s, learning 0.176s)
               Value function loss: 0.4748
                    Surrogate loss: -0.0343
             Mean action noise std: 0.69
                       Mean reward: 2.56
               Mean episode length: 43.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.54s
                        Total time: 37593.04s
                               ETA: 1022272.2s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.294s, learning 0.198s)
               Value function loss: 0.3593
                    Surrogate loss: -0.0291
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 43.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.49s
                        Total time: 37601.53s
                               ETA: 1022204.3s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.333s, learning 0.265s)
               Value function loss: 9.9736
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.60s
                        Total time: 37610.13s
                               ETA: 1022139.3s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.057s, learning 0.261s)
               Value function loss: 4.0573
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 4.90
               Mean episode length: 42.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 8.32s
                        Total time: 37618.45s
                               ETA: 1022066.8s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.627s, learning 0.177s)
               Value function loss: 56.4529
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 43.98
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.80s
                        Total time: 37627.26s
                               ETA: 1022007.5s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.597s, learning 0.166s)
               Value function loss: 0.2641
                    Surrogate loss: -0.0381
             Mean action noise std: 0.69
                       Mean reward: 2.71
               Mean episode length: 44.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.76s
                        Total time: 37636.02s
                               ETA: 1021947.2s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.572s, learning 0.165s)
               Value function loss: 80.9118
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 44.32
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.74s
                        Total time: 37644.76s
                               ETA: 1021886.1s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.357s, learning 0.195s)
               Value function loss: 4.1436
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 44.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.55s
                        Total time: 37653.31s
                               ETA: 1021820.1s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.416s, learning 0.180s)
               Value function loss: 54.4437
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 10.05
               Mean episode length: 44.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.60s
                        Total time: 37661.90s
                               ETA: 1021755.2s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.173s)
               Value function loss: 38.6019
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 44.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 8.59s
                        Total time: 37670.49s
                               ETA: 1021690.2s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.530s, learning 0.183s)
               Value function loss: 59.2494
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 45.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.71s
                        Total time: 37679.20s
                               ETA: 1021628.6s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.620s, learning 0.188s)
               Value function loss: 0.8448
                    Surrogate loss: -0.0302
             Mean action noise std: 0.69
                       Mean reward: 2.41
               Mean episode length: 44.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 16.81s
                        Total time: 37696.01s
                               ETA: 1021786.4s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.035s, learning 0.159s)
               Value function loss: 19.0986
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.40
               Mean episode length: 43.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 16.19s
                        Total time: 37712.20s
                               ETA: 1021927.6s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.035s, learning 0.185s)
               Value function loss: 30.9172
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 43.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 16.22s
                        Total time: 37728.42s
                               ETA: 1022069.3s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.340s, learning 0.189s)
               Value function loss: 0.7692
                    Surrogate loss: -0.0284
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 16.53s
                        Total time: 37744.95s
                               ETA: 1022219.3s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.732s, learning 0.168s)
               Value function loss: 17.0954
                    Surrogate loss: 0.0084
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 42.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 16.90s
                        Total time: 37761.85s
                               ETA: 1022379.3s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.984s, learning 0.169s)
               Value function loss: 7.2463
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 44.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 17.15s
                        Total time: 37779.00s
                               ETA: 1022546.1s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.979s, learning 0.170s)
               Value function loss: 15.5798
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 43.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 17.15s
                        Total time: 37796.15s
                               ETA: 1022712.6s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.871s, learning 0.165s)
               Value function loss: 63.4026
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 17.04s
                        Total time: 37813.19s
                               ETA: 1022875.9s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.337s, learning 0.227s)
               Value function loss: 47.3445
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 45.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 16.56s
                        Total time: 37829.75s
                               ETA: 1023026.4s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.590s, learning 0.184s)
               Value function loss: 0.4618
                    Surrogate loss: -0.0274
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 43.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 16.77s
                        Total time: 37846.53s
                               ETA: 1023182.5s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.373s, learning 0.166s)
               Value function loss: 15.4705
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 44.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 16.54s
                        Total time: 37863.06s
                               ETA: 1023332.1s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.347s, learning 0.181s)
               Value function loss: 15.0079
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 45.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 16.53s
                        Total time: 37879.59s
                               ETA: 1023481.3s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.494s, learning 0.215s)
               Value function loss: 0.3833
                    Surrogate loss: -0.0244
             Mean action noise std: 0.69
                       Mean reward: 15.22
               Mean episode length: 44.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 16.71s
                        Total time: 37896.30s
                               ETA: 1023635.4s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.828s, learning 0.163s)
               Value function loss: 0.2585
                    Surrogate loss: -0.0270
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 43.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 16.99s
                        Total time: 37913.29s
                               ETA: 1023796.9s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.244s, learning 0.188s)
               Value function loss: 31.7022
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 43.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 16.43s
                        Total time: 37929.73s
                               ETA: 1023943.3s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.285s, learning 0.185s)
               Value function loss: 241.4776
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 43.06
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 16.47s
                        Total time: 37946.19s
                               ETA: 1024090.6s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.350s, learning 0.174s)
               Value function loss: 16.7299
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 2.17
               Mean episode length: 42.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 16.52s
                        Total time: 37962.72s
                               ETA: 1024239.2s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.472s, learning 0.178s)
               Value function loss: 12.2935
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 16.65s
                        Total time: 37979.37s
                               ETA: 1024391.2s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.599s, learning 0.190s)
               Value function loss: 120.8817
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 43.42
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 16.79s
                        Total time: 37996.16s
                               ETA: 1024546.9s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.660s, learning 0.162s)
               Value function loss: 0.5725
                    Surrogate loss: -0.0276
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 44.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 16.82s
                        Total time: 38012.98s
                               ETA: 1024703.3s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.474s, learning 0.215s)
               Value function loss: 0.3622
                    Surrogate loss: -0.0256
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 43.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 16.69s
                        Total time: 38029.67s
                               ETA: 1024856.0s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.586s, learning 0.166s)
               Value function loss: 0.3345
                    Surrogate loss: -0.0196
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 43.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 16.75s
                        Total time: 38046.42s
                               ETA: 1025010.4s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.480s, learning 0.161s)
               Value function loss: 90.9904
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 44.20
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 16.64s
                        Total time: 38063.06s
                               ETA: 1025161.6s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.821s, learning 0.205s)
               Value function loss: 53.4452
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.38
               Mean episode length: 43.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 17.03s
                        Total time: 38080.09s
                               ETA: 1025323.1s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.127s, learning 0.204s)
               Value function loss: 9.9339
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 10.00
               Mean episode length: 43.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 16.33s
                        Total time: 38096.42s
                               ETA: 1025465.9s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.487s, learning 0.174s)
               Value function loss: 0.4051
                    Surrogate loss: -0.0171
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 43.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 16.66s
                        Total time: 38113.08s
                               ETA: 1025617.4s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.596s, learning 0.202s)
               Value function loss: 0.2881
                    Surrogate loss: -0.0310
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 44.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 16.80s
                        Total time: 38129.88s
                               ETA: 1025772.5s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.595s, learning 0.185s)
               Value function loss: 0.2139
                    Surrogate loss: -0.0293
             Mean action noise std: 0.69
                       Mean reward: 2.35
               Mean episode length: 43.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 16.78s
                        Total time: 38146.66s
                               ETA: 1025927.0s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.155s, learning 0.178s)
               Value function loss: 182.1496
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 44.25
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 17.33s
                        Total time: 38163.99s
                               ETA: 1026096.3s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.488s, learning 0.264s)
               Value function loss: 17.6162
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 43.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 16.75s
                        Total time: 38180.74s
                               ETA: 1026249.9s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.714s, learning 0.177s)
               Value function loss: 74.2942
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 9.67
               Mean episode length: 42.76
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 16.89s
                        Total time: 38197.64s
                               ETA: 1026407.1s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.279s, learning 0.174s)
               Value function loss: 62.9110
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 44.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 16.45s
                        Total time: 38214.09s
                               ETA: 1026552.4s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.913s, learning 0.188s)
               Value function loss: 18.3515
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 42.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 17.10s
                        Total time: 38231.19s
                               ETA: 1026715.1s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.550s, learning 0.192s)
               Value function loss: 0.5754
                    Surrogate loss: -0.0295
             Mean action noise std: 0.69
                       Mean reward: 9.76
               Mean episode length: 42.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 16.74s
                        Total time: 38247.93s
                               ETA: 1026868.0s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.268s, learning 0.202s)
               Value function loss: 7.4225
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.41
               Mean episode length: 43.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 16.47s
                        Total time: 38264.40s
                               ETA: 1027013.6s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.724s, learning 0.181s)
               Value function loss: 0.3443
                    Surrogate loss: -0.0281
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 43.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 16.90s
                        Total time: 38281.31s
                               ETA: 1027170.7s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.947s, learning 0.236s)
               Value function loss: 0.2531
                    Surrogate loss: -0.0315
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 43.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 17.18s
                        Total time: 38298.49s
                               ETA: 1027335.1s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1140 steps/s (collection: 14.188s, learning 0.174s)
               Value function loss: 0.1994
                    Surrogate loss: -0.0256
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 14.36s
                        Total time: 38312.85s
                               ETA: 1027423.9s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.661s, learning 0.171s)
               Value function loss: 53.8875
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.46
               Mean episode length: 42.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.83s
                        Total time: 38321.68s
                               ETA: 1027364.2s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.133s, learning 0.170s)
               Value function loss: 14.9589
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 2.69
               Mean episode length: 43.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.30s
                        Total time: 38329.99s
                               ETA: 1027290.5s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.357s, learning 0.170s)
               Value function loss: 0.5635
                    Surrogate loss: -0.0283
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 43.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.53s
                        Total time: 38338.51s
                               ETA: 1027222.8s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.163s, learning 0.168s)
               Value function loss: 0.3357
                    Surrogate loss: -0.0312
             Mean action noise std: 0.69
                       Mean reward: 2.20
               Mean episode length: 42.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.33s
                        Total time: 38346.84s
                               ETA: 1027149.9s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.221s, learning 0.172s)
               Value function loss: 0.2598
                    Surrogate loss: -0.0254
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 42.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.39s
                        Total time: 38355.24s
                               ETA: 1027078.6s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.498s, learning 0.267s)
               Value function loss: 0.2037
                    Surrogate loss: -0.0316
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 42.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.76s
                        Total time: 38364.00s
                               ETA: 1027017.4s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.169s)
               Value function loss: 7.2828
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 42.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.33s
                        Total time: 38372.33s
                               ETA: 1026944.5s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.407s, learning 0.183s)
               Value function loss: 0.1658
                    Surrogate loss: -0.0368
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 43.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.59s
                        Total time: 38380.92s
                               ETA: 1026878.6s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.293s, learning 0.159s)
               Value function loss: 17.7217
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 44.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.45s
                        Total time: 38389.37s
                               ETA: 1026809.1s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.550s, learning 0.239s)
               Value function loss: 67.3263
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 43.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.79s
                        Total time: 38398.16s
                               ETA: 1026748.7s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.161s)
               Value function loss: 18.4234
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 45.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.53s
                        Total time: 38406.69s
                               ETA: 1026681.3s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.587s, learning 0.168s)
               Value function loss: 0.3391
                    Surrogate loss: -0.0236
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.75s
                        Total time: 38415.44s
                               ETA: 1026620.0s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.743s, learning 0.159s)
               Value function loss: 17.6602
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 7.53
               Mean episode length: 43.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.90s
                        Total time: 38424.34s
                               ETA: 1026562.6s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.038s, learning 0.179s)
               Value function loss: 4.0819
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 42.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.22s
                        Total time: 38432.56s
                               ETA: 1026487.0s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.644s, learning 0.168s)
               Value function loss: 0.2697
                    Surrogate loss: -0.0334
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 43.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.81s
                        Total time: 38441.37s
                               ETA: 1026427.3s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.508s, learning 0.175s)
               Value function loss: 0.2155
                    Surrogate loss: -0.0318
             Mean action noise std: 0.69
                       Mean reward: 2.11
               Mean episode length: 42.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 8.68s
                        Total time: 38450.06s
                               ETA: 1026364.2s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.123s, learning 0.160s)
               Value function loss: 0.1745
                    Surrogate loss: -0.0323
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 43.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 8.28s
                        Total time: 38458.34s
                               ETA: 1026290.4s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.673s, learning 0.162s)
               Value function loss: 58.8859
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 41.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.84s
                        Total time: 38467.17s
                               ETA: 1026231.4s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.411s, learning 0.216s)
               Value function loss: 0.2295
                    Surrogate loss: -0.0345
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 43.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 8.63s
                        Total time: 38475.80s
                               ETA: 1026166.9s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.883s, learning 0.169s)
               Value function loss: 0.2121
                    Surrogate loss: -0.0295
             Mean action noise std: 0.69
                       Mean reward: 2.09
               Mean episode length: 42.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 9.05s
                        Total time: 38484.85s
                               ETA: 1026113.7s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.174s, learning 0.167s)
               Value function loss: 0.1473
                    Surrogate loss: -0.0298
             Mean action noise std: 0.69
                       Mean reward: 1.98
               Mean episode length: 41.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.34s
                        Total time: 38493.20s
                               ETA: 1026041.7s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.563s, learning 0.179s)
               Value function loss: 15.4231
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 45.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.74s
                        Total time: 38501.94s
                               ETA: 1025980.3s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.304s, learning 0.360s)
               Value function loss: 0.1622
                    Surrogate loss: -0.0366
             Mean action noise std: 0.69
                       Mean reward: 2.14
               Mean episode length: 41.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.66s
                        Total time: 38510.60s
                               ETA: 1025916.9s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.165s)
               Value function loss: 64.1781
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 43.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.60s
                        Total time: 38519.20s
                               ETA: 1025851.9s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.699s, learning 0.201s)
               Value function loss: 0.2461
                    Surrogate loss: -0.0323
             Mean action noise std: 0.69
                       Mean reward: 2.19
               Mean episode length: 42.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.90s
                        Total time: 38528.10s
                               ETA: 1025794.8s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.646s, learning 0.199s)
               Value function loss: 0.2127
                    Surrogate loss: -0.0236
             Mean action noise std: 0.69
                       Mean reward: 2.27
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.84s
                        Total time: 38536.95s
                               ETA: 1025736.3s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.268s, learning 0.181s)
               Value function loss: 0.1815
                    Surrogate loss: -0.0299
             Mean action noise std: 0.69
                       Mean reward: 2.21
               Mean episode length: 42.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.45s
                        Total time: 38545.40s
                               ETA: 1025667.3s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.510s, learning 0.204s)
               Value function loss: 38.8749
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.52
               Mean episode length: 43.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.71s
                        Total time: 38554.11s
                               ETA: 1025605.3s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.807s, learning 0.173s)
               Value function loss: 0.1558
                    Surrogate loss: -0.0352
             Mean action noise std: 0.69
                       Mean reward: 2.33
               Mean episode length: 42.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.98s
                        Total time: 38563.09s
                               ETA: 1025550.5s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.468s, learning 0.176s)
               Value function loss: 17.9254
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 7.23
               Mean episode length: 42.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.64s
                        Total time: 38571.73s
                               ETA: 1025486.8s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.895s, learning 0.264s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0344
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 43.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 9.16s
                        Total time: 38580.89s
                               ETA: 1025436.8s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.636s, learning 0.170s)
               Value function loss: 46.6545
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 2.08
               Mean episode length: 42.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.81s
                        Total time: 38589.70s
                               ETA: 1025377.4s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.427s, learning 0.171s)
               Value function loss: 31.9709
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 9.55
               Mean episode length: 41.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 8.60s
                        Total time: 38598.30s
                               ETA: 1025312.5s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.164s)
               Value function loss: 0.2021
                    Surrogate loss: -0.0305
             Mean action noise std: 0.69
                       Mean reward: 1.97
               Mean episode length: 41.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 8.64s
                        Total time: 38606.94s
                               ETA: 1025248.8s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.457s, learning 0.171s)
               Value function loss: 0.1495
                    Surrogate loss: -0.0279
             Mean action noise std: 0.69
                       Mean reward: 5.02
               Mean episode length: 43.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 8.63s
                        Total time: 38615.57s
                               ETA: 1025184.8s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.384s, learning 0.220s)
               Value function loss: 0.1400
                    Surrogate loss: -0.0304
             Mean action noise std: 0.69
                       Mean reward: 2.10
               Mean episode length: 41.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 8.60s
                        Total time: 38624.17s
                               ETA: 1025120.2s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.500s, learning 0.222s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0313
             Mean action noise std: 0.69
                       Mean reward: 1.92
               Mean episode length: 40.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 8.72s
                        Total time: 38632.89s
                               ETA: 1025058.7s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.171s, learning 0.167s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0345
             Mean action noise std: 0.69
                       Mean reward: 1.79
               Mean episode length: 41.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 8.34s
                        Total time: 38641.23s
                               ETA: 1024987.1s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.401s, learning 0.228s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0335
             Mean action noise std: 0.69
                       Mean reward: 2.02
               Mean episode length: 41.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 8.63s
                        Total time: 38649.86s
                               ETA: 1024923.2s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.513s, learning 0.197s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0354
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 42.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 8.71s
                        Total time: 38658.57s
                               ETA: 1024861.5s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.763s, learning 0.184s)
               Value function loss: 7.1305
                    Surrogate loss: 0.0005
             Mean action noise std: 0.69
                       Mean reward: 2.21
               Mean episode length: 42.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 8.95s
                        Total time: 38667.52s
                               ETA: 1024806.2s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.386s, learning 0.201s)
               Value function loss: 0.0972
                    Surrogate loss: -0.0375
             Mean action noise std: 0.69
                       Mean reward: 2.19
               Mean episode length: 41.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 8.59s
                        Total time: 38676.10s
                               ETA: 1024741.3s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.481s, learning 0.175s)
               Value function loss: 111.3521
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.12
               Mean episode length: 42.17
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 8.66s
                        Total time: 38684.76s
                               ETA: 1024678.2s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.727s, learning 0.162s)
               Value function loss: 0.1656
                    Surrogate loss: -0.0358
             Mean action noise std: 0.69
                       Mean reward: 2.17
               Mean episode length: 41.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 8.89s
                        Total time: 38693.65s
                               ETA: 1024621.4s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.404s, learning 0.177s)
               Value function loss: 7.1223
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.11
               Mean episode length: 42.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 8.58s
                        Total time: 38702.23s
                               ETA: 1024556.4s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.591s, learning 0.223s)
               Value function loss: 0.1946
                    Surrogate loss: -0.0326
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 41.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 8.81s
                        Total time: 38711.04s
                               ETA: 1024497.7s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.693s, learning 0.170s)
               Value function loss: 129.6560
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 42.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 8.86s
                        Total time: 38719.91s
                               ETA: 1024440.2s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.003s, learning 0.169s)
               Value function loss: 3.8339
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 1.94
               Mean episode length: 40.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 8.17s
                        Total time: 38728.08s
                               ETA: 1024364.6s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.428s, learning 0.166s)
               Value function loss: 0.3082
                    Surrogate loss: -0.0244
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 42.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 8.59s
                        Total time: 38736.67s
                               ETA: 1024300.1s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.479s, learning 0.186s)
               Value function loss: 7.0963
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 41.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 8.66s
                        Total time: 38745.34s
                               ETA: 1024237.5s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.465s, learning 0.171s)
               Value function loss: 65.2864
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.04
               Mean episode length: 41.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 8.64s
                        Total time: 38753.97s
                               ETA: 1024174.2s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.503s, learning 0.294s)
               Value function loss: 158.7641
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 1.90
               Mean episode length: 42.01
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 8.80s
                        Total time: 38762.77s
                               ETA: 1024115.1s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.527s, learning 0.290s)
               Value function loss: 7.3054
                    Surrogate loss: -0.0204
             Mean action noise std: 0.69
                       Mean reward: 1.93
               Mean episode length: 41.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 8.82s
                        Total time: 38771.59s
                               ETA: 1024056.7s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.652s, learning 0.246s)
               Value function loss: 0.9681
                    Surrogate loss: -0.0177
             Mean action noise std: 0.69
                       Mean reward: 2.18
               Mean episode length: 42.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 8.90s
                        Total time: 38780.48s
                               ETA: 1024000.3s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.890s, learning 0.195s)
               Value function loss: 0.5352
                    Surrogate loss: -0.0292
             Mean action noise std: 0.69
                       Mean reward: 2.03
               Mean episode length: 41.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 9.09s
                        Total time: 38789.57s
                               ETA: 1023949.0s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.353s, learning 0.164s)
               Value function loss: 0.2333
                    Surrogate loss: -0.0291
             Mean action noise std: 0.69
                       Mean reward: 2.09
               Mean episode length: 42.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 8.52s
                        Total time: 38798.09s
                               ETA: 1023882.7s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.498s, learning 0.215s)
               Value function loss: 0.1792
                    Surrogate loss: -0.0305
             Mean action noise std: 0.69
                       Mean reward: 2.28
               Mean episode length: 42.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 8.71s
                        Total time: 38806.80s
                               ETA: 1023821.6s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.418s, learning 0.162s)
               Value function loss: 0.1537
                    Surrogate loss: -0.0301
             Mean action noise std: 0.69
                       Mean reward: 1.92
               Mean episode length: 41.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 8.58s
                        Total time: 38815.38s
                               ETA: 1023757.0s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.446s, learning 0.163s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0340
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 42.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 8.61s
                        Total time: 38823.99s
                               ETA: 1023693.2s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.333s, learning 0.200s)
               Value function loss: 131.6158
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 42.14
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 8.53s
                        Total time: 38832.52s
                               ETA: 1023627.4s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.426s, learning 0.158s)
               Value function loss: 3.9802
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 1.99
               Mean episode length: 41.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 8.58s
                        Total time: 38841.11s
                               ETA: 1023563.0s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.160s)
               Value function loss: 0.2248
                    Surrogate loss: -0.0330
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 43.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 8.63s
                        Total time: 38849.74s
                               ETA: 1023500.0s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.329s, learning 0.190s)
               Value function loss: 0.1747
                    Surrogate loss: -0.0314
             Mean action noise std: 0.69
                       Mean reward: 2.05
               Mean episode length: 41.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 8.52s
                        Total time: 38858.26s
                               ETA: 1023433.9s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.385s, learning 0.161s)
               Value function loss: 16.6207
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 1.99
               Mean episode length: 41.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 8.55s
                        Total time: 38866.81s
                               ETA: 1023368.6s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.517s, learning 0.164s)
               Value function loss: 16.6811
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.13
               Mean episode length: 42.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 8.68s
                        Total time: 38875.49s
                               ETA: 1023306.9s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.653s, learning 0.161s)
               Value function loss: 13.7313
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 43.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 8.81s
                        Total time: 38884.30s
                               ETA: 1023248.7s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.565s, learning 0.403s)
               Value function loss: 17.5346
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 4.86
               Mean episode length: 42.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 8.97s
                        Total time: 38893.27s
                               ETA: 1023194.6s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.538s, learning 0.165s)
               Value function loss: 4.1126
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 2.22
               Mean episode length: 42.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 8.70s
                        Total time: 38901.97s
                               ETA: 1023133.6s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.643s, learning 0.182s)
               Value function loss: 0.2950
                    Surrogate loss: -0.0277
             Mean action noise std: 0.69
                       Mean reward: 2.37
               Mean episode length: 43.76
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 8.82s
                        Total time: 38910.80s
                               ETA: 1023075.7s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.196s, learning 0.175s)
               Value function loss: 0.1938
                    Surrogate loss: -0.0309
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 43.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 8.37s
                        Total time: 38919.17s
                               ETA: 1023006.0s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.479s, learning 0.161s)
               Value function loss: 0.1890
                    Surrogate loss: -0.0294
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 44.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 8.64s
                        Total time: 38927.81s
                               ETA: 1022943.4s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.099s, learning 0.166s)
               Value function loss: 0.1781
                    Surrogate loss: -0.0282
             Mean action noise std: 0.69
                       Mean reward: 2.16
               Mean episode length: 42.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 8.26s
                        Total time: 38936.07s
                               ETA: 1022870.9s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.591s, learning 0.215s)
               Value function loss: 0.1765
                    Surrogate loss: -0.0308
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 43.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 8.81s
                        Total time: 38944.88s
                               ETA: 1022812.7s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.436s, learning 0.198s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0320
             Mean action noise std: 0.69
                       Mean reward: 2.17
               Mean episode length: 41.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 8.63s
                        Total time: 38953.51s
                               ETA: 1022750.0s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.522s, learning 0.197s)
               Value function loss: 0.1104
                    Surrogate loss: -0.0350
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 8.72s
                        Total time: 38962.23s
                               ETA: 1022689.6s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.207s, learning 0.171s)
               Value function loss: 90.1434
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 43.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 8.38s
                        Total time: 38970.61s
                               ETA: 1022620.2s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.301s, learning 0.196s)
               Value function loss: 0.1803
                    Surrogate loss: -0.0347
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 8.50s
                        Total time: 38979.11s
                               ETA: 1022554.0s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.807s, learning 0.202s)
               Value function loss: 0.1452
                    Surrogate loss: -0.0342
             Mean action noise std: 0.69
                       Mean reward: 2.20
               Mean episode length: 43.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 9.01s
                        Total time: 38988.12s
                               ETA: 1022501.3s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.636s, learning 0.224s)
               Value function loss: 29.5938
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 43.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 8.86s
                        Total time: 38996.98s
                               ETA: 1022444.7s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.514s, learning 0.177s)
               Value function loss: 0.1440
                    Surrogate loss: -0.0368
             Mean action noise std: 0.69
                       Mean reward: 2.25
               Mean episode length: 42.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 8.69s
                        Total time: 39005.67s
                               ETA: 1022383.7s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.258s, learning 0.196s)
               Value function loss: 13.7091
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.51
               Mean episode length: 43.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 8.45s
                        Total time: 39014.12s
                               ETA: 1022316.5s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.727s, learning 0.170s)
               Value function loss: 424.6372
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.52
               Mean episode length: 43.34
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.90s
                        Total time: 39023.02s
                               ETA: 1022260.9s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.529s, learning 0.187s)
               Value function loss: 4.2768
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 5.32
               Mean episode length: 44.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 8.72s
                        Total time: 39031.74s
                               ETA: 1022200.6s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.968s, learning 0.179s)
               Value function loss: 0.5006
                    Surrogate loss: -0.0235
             Mean action noise std: 0.69
                       Mean reward: 2.07
               Mean episode length: 42.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 8.15s
                        Total time: 39039.88s
                               ETA: 1022125.5s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.586s, learning 0.265s)
               Value function loss: 0.3482
                    Surrogate loss: -0.0233
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 44.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.85s
                        Total time: 39048.73s
                               ETA: 1022068.8s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.644s, learning 0.165s)
               Value function loss: 0.2610
                    Surrogate loss: -0.0240
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 43.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.81s
                        Total time: 39057.54s
                               ETA: 1022011.0s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.261s, learning 0.229s)
               Value function loss: 0.2294
                    Surrogate loss: -0.0245
             Mean action noise std: 0.69
                       Mean reward: 2.32
               Mean episode length: 43.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.49s
                        Total time: 39066.03s
                               ETA: 1021944.9s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.561s, learning 0.187s)
               Value function loss: 0.1939
                    Surrogate loss: -0.0271
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.75s
                        Total time: 39074.78s
                               ETA: 1021885.6s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.162s)
               Value function loss: 9.6427
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 2.56
               Mean episode length: 43.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.63s
                        Total time: 39083.42s
                               ETA: 1021823.4s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.608s, learning 0.169s)
               Value function loss: 16.4615
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 43.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.78s
                        Total time: 39092.19s
                               ETA: 1021764.9s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.311s, learning 0.191s)
               Value function loss: 11.4931
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 2.36
               Mean episode length: 43.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.50s
                        Total time: 39100.70s
                               ETA: 1021699.3s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.256s, learning 0.204s)
               Value function loss: 0.3350
                    Surrogate loss: -0.0242
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 46.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.46s
                        Total time: 39109.16s
                               ETA: 1021632.6s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.604s, learning 0.199s)
               Value function loss: 191.5904
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 4.87
               Mean episode length: 43.47
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.80s
                        Total time: 39117.96s
                               ETA: 1021574.8s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.414s, learning 0.268s)
               Value function loss: 10.0398
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 44.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.68s
                        Total time: 39126.64s
                               ETA: 1021514.0s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.297s, learning 0.266s)
               Value function loss: 26.5108
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 43.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.56s
                        Total time: 39135.20s
                               ETA: 1021450.0s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.199s, learning 0.174s)
               Value function loss: 17.2218
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 12.51
               Mean episode length: 44.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.37s
                        Total time: 39143.58s
                               ETA: 1021381.2s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.165s)
               Value function loss: 4.4314
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 44.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.76s
                        Total time: 39152.34s
                               ETA: 1021322.4s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.622s, learning 0.185s)
               Value function loss: 0.6222
                    Surrogate loss: -0.0284
             Mean action noise std: 0.69
                       Mean reward: 2.06
               Mean episode length: 42.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.81s
                        Total time: 39161.14s
                               ETA: 1021264.9s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.278s, learning 0.157s)
               Value function loss: 0.3306
                    Surrogate loss: -0.0275
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 45.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.44s
                        Total time: 39169.58s
                               ETA: 1021197.8s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.286s, learning 0.198s)
               Value function loss: 59.5778
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 44.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 8.48s
                        Total time: 39178.06s
                               ETA: 1021131.9s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.449s, learning 0.168s)
               Value function loss: 14.9166
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 2.24
               Mean episode length: 43.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.62s
                        Total time: 39186.68s
                               ETA: 1021069.6s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.697s, learning 0.172s)
               Value function loss: 20.9749
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 7.94
               Mean episode length: 44.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.87s
                        Total time: 39195.55s
                               ETA: 1021013.8s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.372s, learning 0.214s)
               Value function loss: 26.6196
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 4.86
               Mean episode length: 43.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 8.59s
                        Total time: 39204.13s
                               ETA: 1020950.7s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.555s, learning 0.169s)
               Value function loss: 64.0343
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.72s
                        Total time: 39212.86s
                               ETA: 1020891.2s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.562s, learning 0.218s)
               Value function loss: 8.0307
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 43.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.78s
                        Total time: 39221.64s
                               ETA: 1020833.2s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.579s, learning 0.193s)
               Value function loss: 0.4117
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 7.28
               Mean episode length: 43.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.77s
                        Total time: 39230.41s
                               ETA: 1020775.0s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.668s, learning 0.166s)
               Value function loss: 0.3341
                    Surrogate loss: -0.0189
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 43.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.83s
                        Total time: 39239.24s
                               ETA: 1020718.5s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.312s, learning 0.265s)
               Value function loss: 0.2110
                    Surrogate loss: -0.0265
             Mean action noise std: 0.69
                       Mean reward: 2.29
               Mean episode length: 42.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.58s
                        Total time: 39247.82s
                               ETA: 1020655.3s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.501s, learning 0.174s)
               Value function loss: 0.2037
                    Surrogate loss: -0.0311
             Mean action noise std: 0.69
                       Mean reward: 2.50
               Mean episode length: 43.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 8.67s
                        Total time: 39256.50s
                               ETA: 1020594.7s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.399s, learning 0.190s)
               Value function loss: 0.2123
                    Surrogate loss: -0.0256
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 43.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.59s
                        Total time: 39265.09s
                               ETA: 1020531.9s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.169s)
               Value function loss: 0.1459
                    Surrogate loss: -0.0339
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 44.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.53s
                        Total time: 39273.61s
                               ETA: 1020467.6s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.243s, learning 0.166s)
               Value function loss: 11.7684
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 44.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.41s
                        Total time: 39282.02s
                               ETA: 1020400.1s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.255s, learning 0.181s)
               Value function loss: 47.3671
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 43.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.44s
                        Total time: 39290.46s
                               ETA: 1020333.4s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.700s, learning 0.167s)
               Value function loss: 0.1948
                    Surrogate loss: -0.0386
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 43.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.87s
                        Total time: 39299.33s
                               ETA: 1020277.9s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.981s, learning 0.171s)
               Value function loss: 13.9460
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 45.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 9.15s
                        Total time: 39308.48s
                               ETA: 1020229.9s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.500s, learning 0.163s)
               Value function loss: 76.5327
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 44.76
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.66s
                        Total time: 39317.14s
                               ETA: 1020169.1s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.587s, learning 0.177s)
               Value function loss: 93.5774
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 44.65
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.76s
                        Total time: 39325.91s
                               ETA: 1020111.0s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.915s, learning 0.187s)
               Value function loss: 0.5027
                    Surrogate loss: -0.0323
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 44.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 9.10s
                        Total time: 39335.01s
                               ETA: 1020061.8s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.663s, learning 0.171s)
               Value function loss: 24.2527
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 17.81
               Mean episode length: 44.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.83s
                        Total time: 39343.84s
                               ETA: 1020005.6s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.571s, learning 0.177s)
               Value function loss: 0.5715
                    Surrogate loss: -0.0308
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 44.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.75s
                        Total time: 39352.59s
                               ETA: 1019947.1s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.594s, learning 0.170s)
               Value function loss: 31.9739
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 4.79
               Mean episode length: 42.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.76s
                        Total time: 39361.35s
                               ETA: 1019889.1s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.594s, learning 0.267s)
               Value function loss: 32.4754
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 44.32
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.86s
                        Total time: 39370.22s
                               ETA: 1019833.7s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1242 steps/s (collection: 13.017s, learning 0.166s)
               Value function loss: 83.3575
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 43.66
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 13.18s
                        Total time: 39383.40s
                               ETA: 1019890.2s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.443s, learning 0.165s)
               Value function loss: 0.7729
                    Surrogate loss: -0.0225
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 43.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 16.61s
                        Total time: 39400.01s
                               ETA: 1020035.3s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.381s, learning 0.189s)
               Value function loss: 56.3889
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 44.11
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 16.57s
                        Total time: 39416.58s
                               ETA: 1020179.4s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.666s, learning 0.229s)
               Value function loss: 47.7088
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 44.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 16.90s
                        Total time: 39433.47s
                               ETA: 1020331.8s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.561s, learning 0.209s)
               Value function loss: 18.3311
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.43
               Mean episode length: 42.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 16.77s
                        Total time: 39450.24s
                               ETA: 1020480.9s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.935s, learning 0.181s)
               Value function loss: 374.8049
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.49
               Mean episode length: 43.21
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 17.12s
                        Total time: 39467.36s
                               ETA: 1020638.8s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.284s, learning 0.184s)
               Value function loss: 21.4221
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 43.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 16.47s
                        Total time: 39483.83s
                               ETA: 1020779.9s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.369s, learning 0.188s)
               Value function loss: 35.0398
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 22.68
               Mean episode length: 44.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 16.56s
                        Total time: 39500.38s
                               ETA: 1020923.2s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.502s, learning 0.261s)
               Value function loss: 5.1439
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 2.26
               Mean episode length: 42.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 16.76s
                        Total time: 39517.15s
                               ETA: 1021071.7s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.183s, learning 0.189s)
               Value function loss: 70.3879
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 5.05
               Mean episode length: 43.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 17.37s
                        Total time: 39534.52s
                               ETA: 1021235.9s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.628s, learning 0.296s)
               Value function loss: 22.3903
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 44.27
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 16.92s
                        Total time: 39551.44s
                               ETA: 1021388.4s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.661s, learning 0.191s)
               Value function loss: 12.3288
                    Surrogate loss: 0.0065
             Mean action noise std: 0.69
                       Mean reward: 10.12
               Mean episode length: 43.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 16.85s
                        Total time: 39568.29s
                               ETA: 1021539.0s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.660s, learning 0.184s)
               Value function loss: 29.5158
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.37
               Mean episode length: 43.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 16.84s
                        Total time: 39585.14s
                               ETA: 1021689.2s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.076s, learning 0.296s)
               Value function loss: 0.7174
                    Surrogate loss: -0.0282
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 43.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 16.37s
                        Total time: 39601.51s
                               ETA: 1021827.2s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.541s, learning 0.208s)
               Value function loss: 17.9821
                    Surrogate loss: 0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 44.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 16.75s
                        Total time: 39618.26s
                               ETA: 1021974.8s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.947s, learning 0.164s)
               Value function loss: 17.0156
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 44.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 17.11s
                        Total time: 39635.37s
                               ETA: 1022131.7s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.828s, learning 0.173s)
               Value function loss: 16.1071
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.21
               Mean episode length: 42.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 17.00s
                        Total time: 39652.37s
                               ETA: 1022285.7s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.169s, learning 0.202s)
               Value function loss: 18.3934
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 42.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 17.37s
                        Total time: 39669.74s
                               ETA: 1022449.1s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.322s, learning 0.168s)
               Value function loss: 71.1212
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 42.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 16.49s
                        Total time: 39686.23s
                               ETA: 1022589.7s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.566s, learning 0.178s)
               Value function loss: 21.5267
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 2.38
               Mean episode length: 42.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 16.74s
                        Total time: 39702.97s
                               ETA: 1022736.7s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.934s, learning 0.183s)
               Value function loss: 0.4656
                    Surrogate loss: -0.0254
             Mean action noise std: 0.69
                       Mean reward: 2.30
               Mean episode length: 42.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 17.12s
                        Total time: 39720.09s
                               ETA: 1022893.3s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.035s, learning 0.198s)
               Value function loss: 13.7805
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 43.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 17.23s
                        Total time: 39737.33s
                               ETA: 1023052.8s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.519s, learning 0.177s)
               Value function loss: 17.9527
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 44.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 16.70s
                        Total time: 39754.02s
                               ETA: 1023198.4s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.089s, learning 0.272s)
               Value function loss: 82.4232
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 17.36s
                        Total time: 39771.38s
                               ETA: 1023360.9s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.902s, learning 0.176s)
               Value function loss: 61.5907
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 43.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 17.08s
                        Total time: 39788.46s
                               ETA: 1023516.1s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.432s, learning 0.171s)
               Value function loss: 1.0986
                    Surrogate loss: -0.0274
             Mean action noise std: 0.69
                       Mean reward: 9.91
               Mean episode length: 42.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 16.60s
                        Total time: 39805.06s
                               ETA: 1023659.0s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.129s, learning 0.182s)
               Value function loss: 17.8840
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 42.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 16.31s
                        Total time: 39821.37s
                               ETA: 1023794.3s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.763s, learning 0.173s)
               Value function loss: 8.0151
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 5.48
               Mean episode length: 44.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 16.94s
                        Total time: 39838.31s
                               ETA: 1023945.6s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.574s, learning 0.206s)
               Value function loss: 69.6179
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 7.75
               Mean episode length: 44.06
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 16.78s
                        Total time: 39855.09s
                               ETA: 1024092.8s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.159s, learning 0.169s)
               Value function loss: 51.0641
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.23
               Mean episode length: 42.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 17.33s
                        Total time: 39872.42s
                               ETA: 1024254.0s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.337s, learning 0.161s)
               Value function loss: 15.5220
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 44.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 16.50s
                        Total time: 39888.92s
                               ETA: 1024393.8s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.408s, learning 0.231s)
               Value function loss: 0.5676
                    Surrogate loss: -0.0157
             Mean action noise std: 0.69
                       Mean reward: 7.63
               Mean episode length: 43.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 16.64s
                        Total time: 39905.56s
                               ETA: 1024537.1s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.223s, learning 0.166s)
               Value function loss: 30.4885
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 44.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 16.39s
                        Total time: 39921.94s
                               ETA: 1024673.9s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.385s, learning 0.190s)
               Value function loss: 0.4013
                    Surrogate loss: -0.0293
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 43.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 16.58s
                        Total time: 39938.52s
                               ETA: 1024815.4s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.419s, learning 0.173s)
               Value function loss: 4.1229
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 5.12
               Mean episode length: 43.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 16.59s
                        Total time: 39955.11s
                               ETA: 1024957.2s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.385s, learning 0.168s)
               Value function loss: 0.2903
                    Surrogate loss: -0.0329
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 43.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 16.55s
                        Total time: 39971.66s
                               ETA: 1025098.0s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.532s, learning 0.193s)
               Value function loss: 12.1418
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.88
               Mean episode length: 44.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 16.73s
                        Total time: 39988.39s
                               ETA: 1025243.1s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.616s, learning 0.160s)
               Value function loss: 7.2779
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.79
               Mean episode length: 45.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 16.78s
                        Total time: 40005.16s
                               ETA: 1025389.4s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.591s, learning 0.162s)
               Value function loss: 7.2201
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 5.18
               Mean episode length: 44.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 9.75s
                        Total time: 40014.92s
                               ETA: 1025355.6s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.977s, learning 0.213s)
               Value function loss: 188.3969
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.48
               Mean episode length: 44.48
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 9.19s
                        Total time: 40024.11s
                               ETA: 1025307.5s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.210s)
               Value function loss: 21.2980
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 43.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.55s
                        Total time: 40032.66s
                               ETA: 1025243.0s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.795s, learning 0.176s)
               Value function loss: 37.5436
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 7.64
               Mean episode length: 44.31
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.97s
                        Total time: 40041.63s
                               ETA: 1025189.3s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.695s, learning 0.163s)
               Value function loss: 4.6339
                    Surrogate loss: -0.0131
             Mean action noise std: 0.69
                       Mean reward: 7.71
               Mean episode length: 44.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 8.86s
                        Total time: 40050.49s
                               ETA: 1025132.7s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.786s, learning 0.168s)
               Value function loss: 144.6582
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 43.98
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 8.95s
                        Total time: 40059.44s
                               ETA: 1025078.6s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.579s, learning 0.278s)
               Value function loss: 27.6640
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 5.25
               Mean episode length: 44.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.86s
                        Total time: 40068.30s
                               ETA: 1025022.1s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.009s, learning 0.168s)
               Value function loss: 20.6930
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 7.20
               Mean episode length: 42.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 9.18s
                        Total time: 40077.48s
                               ETA: 1024973.7s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.616s, learning 0.191s)
               Value function loss: 46.4366
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 5.14
               Mean episode length: 44.62
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.81s
                        Total time: 40086.28s
                               ETA: 1024915.9s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.274s, learning 0.181s)
               Value function loss: 18.5467
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 5.15
               Mean episode length: 44.88
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.46s
                        Total time: 40094.74s
                               ETA: 1024849.2s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.757s, learning 0.250s)
               Value function loss: 0.6199
                    Surrogate loss: -0.0281
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 44.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 9.01s
                        Total time: 40103.74s
                               ETA: 1024796.6s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.167s)
               Value function loss: 10.0524
                    Surrogate loss: 0.0057
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 44.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.72s
                        Total time: 40112.46s
                               ETA: 1024736.6s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.384s, learning 0.166s)
               Value function loss: 23.6433
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 44.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.55s
                        Total time: 40121.01s
                               ETA: 1024672.3s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.614s, learning 0.329s)
               Value function loss: 17.9615
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 44.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.94s
                        Total time: 40129.96s
                               ETA: 1024618.2s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.162s)
               Value function loss: 14.9965
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 45.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.70s
                        Total time: 40138.65s
                               ETA: 1024557.8s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.166s)
               Value function loss: 168.4451
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 44.25
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 8.35s
                        Total time: 40147.00s
                               ETA: 1024488.4s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.876s, learning 0.177s)
               Value function loss: 4.6317
                    Surrogate loss: -0.0107
             Mean action noise std: 0.69
                       Mean reward: 4.84
               Mean episode length: 44.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 9.05s
                        Total time: 40156.05s
                               ETA: 1024437.2s

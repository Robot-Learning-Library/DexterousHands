/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-6uq4bhqe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandDoorCloseInward_ppo_20221020041912
wandb: ⭐️ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: 🚀 View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/6uq4bhqe
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:6
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Using VHACD cache directory '/data/zihan/.isaacgym/vhacd'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj': 4 hulls
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=6, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=6, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:6', seed=None, sim_device='cuda:6', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandDoorCloseInward', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_door_close_inward', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 250, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.5, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandDoorCloseInward', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandDoorCloseInward_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 7011
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:6
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 927 steps/s (collection: 13.411s, learning 4.247s)
               Value function loss: 15.1026
                    Surrogate loss: 0.0390
             Mean action noise std: 0.80
                  Mean reward/step: 0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 17.66s
                        Total time: 17.66s
                               ETA: 1765746.3s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1151 steps/s (collection: 13.660s, learning 0.562s)
               Value function loss: 4.9702
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                  Mean reward/step: 0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 14.22s
                        Total time: 31.88s
                               ETA: 1593982.2s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 963 steps/s (collection: 16.357s, learning 0.646s)
               Value function loss: 1.7581
                    Surrogate loss: -0.0144
             Mean action noise std: 0.80
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 17.00s
                        Total time: 48.88s
                               ETA: 1629409.8s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 835 steps/s (collection: 19.437s, learning 0.173s)
               Value function loss: 1.8801
                    Surrogate loss: -0.0159
             Mean action noise std: 0.80
                  Mean reward/step: 0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 19.61s
                        Total time: 68.49s
                               ETA: 1712292.9s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 820 steps/s (collection: 19.751s, learning 0.211s)
               Value function loss: 1.2411
                    Surrogate loss: -0.0237
             Mean action noise std: 0.80
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 19.96s
                        Total time: 88.46s
                               ETA: 1769041.8s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 788 steps/s (collection: 20.587s, learning 0.190s)
               Value function loss: 0.8866
                    Surrogate loss: -0.0251
             Mean action noise std: 0.80
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 20.78s
                        Total time: 109.23s
                               ETA: 1820466.9s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 769 steps/s (collection: 20.973s, learning 0.322s)
               Value function loss: 1.1314
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                  Mean reward/step: 0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 21.30s
                        Total time: 130.53s
                               ETA: 1864580.7s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 782 steps/s (collection: 20.737s, learning 0.194s)
               Value function loss: 0.9351
                    Surrogate loss: -0.0200
             Mean action noise std: 0.80
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 20.93s
                        Total time: 151.46s
                               ETA: 1893119.1s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 775 steps/s (collection: 20.948s, learning 0.167s)
               Value function loss: 1.1810
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                  Mean reward/step: 0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 21.12s
                        Total time: 172.58s
                               ETA: 1917349.7s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 752 steps/s (collection: 21.606s, learning 0.166s)
               Value function loss: 2.2262
                    Surrogate loss: 0.0013
             Mean action noise std: 0.80
                  Mean reward/step: 0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 21.77s
                        Total time: 194.35s
                               ETA: 1943300.4s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.297s, learning 0.239s)
               Value function loss: 1.4905
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                  Mean reward/step: 0.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 21.54s
                        Total time: 215.88s
                               ETA: 1962385.6s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 776 steps/s (collection: 20.926s, learning 0.165s)
               Value function loss: 1.1035
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                  Mean reward/step: 0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 21.09s
                        Total time: 236.97s
                               ETA: 1974573.4s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.044s, learning 0.165s)
               Value function loss: 0.9697
                    Surrogate loss: -0.0222
             Mean action noise std: 0.80
                  Mean reward/step: 0.66
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 21.21s
                        Total time: 258.18s
                               ETA: 1985787.1s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 770 steps/s (collection: 21.115s, learning 0.160s)
               Value function loss: 1.0862
                    Surrogate loss: -0.0090
             Mean action noise std: 0.80
                  Mean reward/step: 0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 21.27s
                        Total time: 279.46s
                               ETA: 1995868.7s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.186s, learning 0.165s)
               Value function loss: 1.2352
                    Surrogate loss: -0.0089
             Mean action noise std: 0.80
                  Mean reward/step: 0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 21.35s
                        Total time: 300.81s
                               ETA: 2005113.7s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 755 steps/s (collection: 21.513s, learning 0.173s)
               Value function loss: 1.6145
                    Surrogate loss: 0.0391
             Mean action noise std: 0.80
                  Mean reward/step: 0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 21.69s
                        Total time: 322.50s
                               ETA: 2015292.4s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 760 steps/s (collection: 21.333s, learning 0.197s)
               Value function loss: 1.5145
                    Surrogate loss: -0.0080
             Mean action noise std: 0.80
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 21.53s
                        Total time: 344.02s
                               ETA: 2023351.2s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 753 steps/s (collection: 21.567s, learning 0.170s)
               Value function loss: 1.1381
                    Surrogate loss: 0.0039
             Mean action noise std: 0.80
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 21.74s
                        Total time: 365.76s
                               ETA: 2031665.7s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.217s, learning 0.161s)
               Value function loss: 1.3562
                    Surrogate loss: -0.0121
             Mean action noise std: 0.80
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 21.38s
                        Total time: 387.14s
                               ETA: 2037212.5s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.248s, learning 0.169s)
               Value function loss: 1.3303
                    Surrogate loss: -0.0248
             Mean action noise std: 0.80
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 21.42s
                        Total time: 408.56s
                               ETA: 2042394.7s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.239s, learning 0.199s)
               Value function loss: 1.3725
                    Surrogate loss: 0.0123
             Mean action noise std: 0.80
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 21.44s
                        Total time: 429.99s
                               ETA: 2047182.8s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 786 steps/s (collection: 20.657s, learning 0.171s)
               Value function loss: 1.1710
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 20.83s
                        Total time: 450.82s
                               ETA: 2048765.3s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.233s, learning 0.187s)
               Value function loss: 1.0327
                    Surrogate loss: -0.0014
             Mean action noise std: 0.80
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 21.42s
                        Total time: 472.24s
                               ETA: 2052781.0s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.225s, learning 0.166s)
               Value function loss: 1.0499
                    Surrogate loss: 0.0290
             Mean action noise std: 0.80
                       Mean reward: 106.36
               Mean episode length: 189.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 21.39s
                        Total time: 493.63s
                               ETA: 2056338.8s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 757 steps/s (collection: 21.441s, learning 0.196s)
               Value function loss: 0.9076
                    Surrogate loss: -0.0117
             Mean action noise std: 0.80
                       Mean reward: 106.36
               Mean episode length: 189.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 21.64s
                        Total time: 515.27s
                               ETA: 2060593.3s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 765 steps/s (collection: 21.224s, learning 0.170s)
               Value function loss: 0.9741
                    Surrogate loss: -0.0142
             Mean action noise std: 0.80
                       Mean reward: 114.11
               Mean episode length: 192.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 21.39s
                        Total time: 536.67s
                               ETA: 2063586.1s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 767 steps/s (collection: 21.193s, learning 0.164s)
               Value function loss: 1.2462
                    Surrogate loss: 0.0001
             Mean action noise std: 0.80
                       Mean reward: 116.69
               Mean episode length: 193.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 21.36s
                        Total time: 558.02s
                               ETA: 2066214.8s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.019s, learning 0.190s)
               Value function loss: 1.0425
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 117.98
               Mean episode length: 193.50
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 21.21s
                        Total time: 579.23s
                               ETA: 2068127.9s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.121s, learning 0.189s)
               Value function loss: 0.9333
                    Surrogate loss: -0.0184
             Mean action noise std: 0.80
                       Mean reward: 118.76
               Mean episode length: 193.80
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 21.31s
                        Total time: 600.54s
                               ETA: 2070257.9s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.247s, learning 0.191s)
               Value function loss: 0.7739
                    Surrogate loss: -0.0154
             Mean action noise std: 0.80
                       Mean reward: 119.27
               Mean episode length: 194.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 21.44s
                        Total time: 621.98s
                               ETA: 2072669.2s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 785 steps/s (collection: 20.672s, learning 0.194s)
               Value function loss: 0.8176
                    Surrogate loss: -0.0082
             Mean action noise std: 0.80
                       Mean reward: 120.02
               Mean episode length: 197.27
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 20.87s
                        Total time: 642.85s
                               ETA: 2073076.8s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 780 steps/s (collection: 20.838s, learning 0.160s)
               Value function loss: 46.1939
                    Surrogate loss: 0.0389
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 21.00s
                        Total time: 663.84s
                               ETA: 2073871.5s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 772 steps/s (collection: 21.044s, learning 0.177s)
               Value function loss: 0.2850
                    Surrogate loss: -0.0205
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 21.22s
                        Total time: 685.07s
                               ETA: 2075291.0s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.868s, learning 0.172s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0416
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 21.04s
                        Total time: 706.11s
                               ETA: 2076096.1s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.980s, learning 0.160s)
               Value function loss: 0.1639
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 21.14s
                        Total time: 727.25s
                               ETA: 2077138.7s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 779 steps/s (collection: 20.844s, learning 0.163s)
               Value function loss: 0.6863
                    Surrogate loss: 0.0020
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 21.01s
                        Total time: 748.25s
                               ETA: 2077752.0s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 774 steps/s (collection: 20.964s, learning 0.182s)
               Value function loss: 0.9845
                    Surrogate loss: -0.0210
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 21.15s
                        Total time: 769.40s
                               ETA: 2078707.5s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.128s, learning 0.172s)
               Value function loss: 1.5993
                    Surrogate loss: -0.0225
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 16.30s
                        Total time: 785.70s
                               ETA: 2066861.7s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.634s, learning 0.164s)
               Value function loss: 1.7525
                    Surrogate loss: 0.0080
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 10.80s
                        Total time: 796.50s
                               ETA: 2041522.7s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.787s, learning 0.173s)
               Value function loss: 1.5452
                    Surrogate loss: -0.0212
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 10.96s
                        Total time: 807.46s
                               ETA: 2017853.6s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.095s, learning 0.188s)
               Value function loss: 1.3157
                    Surrogate loss: -0.0258
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 11.28s
                        Total time: 818.74s
                               ETA: 1996125.9s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.785s, learning 0.168s)
               Value function loss: 1.1380
                    Surrogate loss: -0.0357
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 10.95s
                        Total time: 829.69s
                               ETA: 1974647.6s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.793s, learning 0.160s)
               Value function loss: 0.9635
                    Surrogate loss: -0.0402
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 10.95s
                        Total time: 840.64s
                               ETA: 1954166.9s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.880s, learning 0.164s)
               Value function loss: 0.8788
                    Surrogate loss: -0.0260
             Mean action noise std: 0.80
                       Mean reward: 172.51
               Mean episode length: 249.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 11.04s
                        Total time: 851.69s
                               ETA: 1934825.6s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.367s, learning 0.168s)
               Value function loss: 0.8897
                    Surrogate loss: -0.0206
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 10.53s
                        Total time: 862.22s
                               ETA: 1915210.8s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1567 steps/s (collection: 10.289s, learning 0.163s)
               Value function loss: 0.7939
                    Surrogate loss: -0.0229
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 10.45s
                        Total time: 872.68s
                               ETA: 1896267.8s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.838s, learning 0.174s)
               Value function loss: 0.7402
                    Surrogate loss: -0.0168
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 11.01s
                        Total time: 883.69s
                               ETA: 1879321.9s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.548s, learning 0.161s)
               Value function loss: 0.6722
                    Surrogate loss: -0.0211
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 10.71s
                        Total time: 894.40s
                               ETA: 1862449.4s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.600s, learning 0.162s)
               Value function loss: 0.6612
                    Surrogate loss: -0.0251
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 10.76s
                        Total time: 905.16s
                               ETA: 1846373.7s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.831s, learning 0.167s)
               Value function loss: 0.6494
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 11.00s
                        Total time: 916.16s
                               ETA: 1831412.2s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.515s, learning 0.192s)
               Value function loss: 0.6256
                    Surrogate loss: -0.0322
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 10.71s
                        Total time: 926.86s
                               ETA: 1816467.6s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.485s, learning 0.191s)
               Value function loss: 0.7018
                    Surrogate loss: -0.0331
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 10.68s
                        Total time: 937.54s
                               ETA: 1802038.8s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.446s, learning 0.167s)
               Value function loss: 0.9352
                    Surrogate loss: -0.0237
             Mean action noise std: 0.80
                       Mean reward: 171.93
               Mean episode length: 247.57
                  Mean reward/step: 0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 10.61s
                        Total time: 948.15s
                               ETA: 1788035.0s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.506s, learning 0.159s)
               Value function loss: 1.5338
                    Surrogate loss: -0.0233
             Mean action noise std: 0.80
                       Mean reward: 171.52
               Mean episode length: 246.88
                  Mean reward/step: 0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 10.67s
                        Total time: 958.82s
                               ETA: 1774645.4s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.165s)
               Value function loss: 1.8703
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 171.89
               Mean episode length: 246.89
                  Mean reward/step: 0.63
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 10.75s
                        Total time: 969.57s
                               ETA: 1761901.8s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.512s, learning 0.166s)
               Value function loss: 1.8759
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: 171.40
               Mean episode length: 246.34
                  Mean reward/step: 0.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 10.68s
                        Total time: 980.25s
                               ETA: 1749478.8s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.160s)
               Value function loss: 3.2822
                    Surrogate loss: 0.0137
             Mean action noise std: 0.80
                       Mean reward: 170.04
               Mean episode length: 245.08
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 10.82s
                        Total time: 991.06s
                               ETA: 1737735.6s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.506s, learning 0.249s)
               Value function loss: 8.2076
                    Surrogate loss: -0.0090
             Mean action noise std: 0.80
                       Mean reward: 166.60
               Mean episode length: 242.16
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 10.76s
                        Total time: 1001.82s
                               ETA: 1726290.5s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.407s, learning 0.164s)
               Value function loss: 8.2651
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: 159.21
               Mean episode length: 236.23
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 10.57s
                        Total time: 1012.39s
                               ETA: 1714922.4s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.625s, learning 0.188s)
               Value function loss: 6.0502
                    Surrogate loss: -0.0167
             Mean action noise std: 0.80
                       Mean reward: 143.60
               Mean episode length: 228.60
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 10.81s
                        Total time: 1023.20s
                               ETA: 1704335.1s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.609s, learning 0.181s)
               Value function loss: 8.6577
                    Surrogate loss: -0.0141
             Mean action noise std: 0.80
                       Mean reward: 137.09
               Mean episode length: 234.15
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 10.79s
                        Total time: 1033.99s
                               ETA: 1694055.7s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.350s, learning 0.188s)
               Value function loss: 8.0032
                    Surrogate loss: -0.0183
             Mean action noise std: 0.80
                       Mean reward: 136.96
               Mean episode length: 243.79
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 10.54s
                        Total time: 1044.53s
                               ETA: 1683701.0s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.611s, learning 0.168s)
               Value function loss: 35.3705
                    Surrogate loss: 0.0025
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 4.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 10.78s
                        Total time: 1055.31s
                               ETA: 1674057.8s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.356s, learning 0.260s)
               Value function loss: 2.4010
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 10.62s
                        Total time: 1065.93s
                               ETA: 1664460.6s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.842s, learning 0.210s)
               Value function loss: 1.2048
                    Surrogate loss: -0.0244
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 11.05s
                        Total time: 1076.98s
                               ETA: 1655828.8s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.386s, learning 0.186s)
               Value function loss: 2.4207
                    Surrogate loss: 0.0245
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 10.57s
                        Total time: 1087.55s
                               ETA: 1646732.4s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.424s, learning 0.166s)
               Value function loss: 1.8458
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 10.59s
                        Total time: 1098.14s
                               ETA: 1637933.2s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1590 steps/s (collection: 10.115s, learning 0.188s)
               Value function loss: 1.8226
                    Surrogate loss: 0.0147
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 10.30s
                        Total time: 1108.44s
                               ETA: 1628971.0s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.166s)
               Value function loss: 2.0239
                    Surrogate loss: -0.0073
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 10.89s
                        Total time: 1119.33s
                               ETA: 1621120.0s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.420s, learning 0.163s)
               Value function loss: 1.8147
                    Surrogate loss: -0.0082
             Mean action noise std: 0.80
                       Mean reward: 167.48
               Mean episode length: 250.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 10.58s
                        Total time: 1129.92s
                               ETA: 1613053.4s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.514s, learning 0.193s)
               Value function loss: 1.6487
                    Surrogate loss: -0.0184
             Mean action noise std: 0.80
                       Mean reward: 165.68
               Mean episode length: 248.19
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 10.71s
                        Total time: 1140.62s
                               ETA: 1605387.2s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.432s, learning 0.163s)
               Value function loss: 1.8251
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 164.79
               Mean episode length: 246.41
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 10.59s
                        Total time: 1151.22s
                               ETA: 1597778.9s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.537s, learning 0.162s)
               Value function loss: 1.5692
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                       Mean reward: 164.79
               Mean episode length: 246.41
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 10.70s
                        Total time: 1161.92s
                               ETA: 1590521.5s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.405s, learning 0.160s)
               Value function loss: 1.4636
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 164.79
               Mean episode length: 246.41
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 10.57s
                        Total time: 1172.48s
                               ETA: 1583279.1s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1588 steps/s (collection: 10.139s, learning 0.173s)
               Value function loss: 1.4745
                    Surrogate loss: -0.0138
             Mean action noise std: 0.80
                       Mean reward: 164.79
               Mean episode length: 246.41
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 10.31s
                        Total time: 1182.79s
                               ETA: 1575891.4s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.393s, learning 0.207s)
               Value function loss: 1.3643
                    Surrogate loss: -0.0188
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 10.60s
                        Total time: 1193.39s
                               ETA: 1569077.5s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.342s, learning 0.184s)
               Value function loss: 0.9786
                    Surrogate loss: -0.0087
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 10.53s
                        Total time: 1203.92s
                               ETA: 1562344.0s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1558 steps/s (collection: 10.334s, learning 0.180s)
               Value function loss: 0.8111
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 10.51s
                        Total time: 1214.43s
                               ETA: 1555766.8s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.666s, learning 0.159s)
               Value function loss: 0.7233
                    Surrogate loss: -0.0115
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 10.83s
                        Total time: 1225.26s
                               ETA: 1549751.0s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.314s, learning 0.185s)
               Value function loss: 0.7330
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 10.50s
                        Total time: 1235.76s
                               ETA: 1543477.2s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1581 steps/s (collection: 10.192s, learning 0.167s)
               Value function loss: 0.6407
                    Surrogate loss: -0.0412
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 10.36s
                        Total time: 1246.12s
                               ETA: 1537185.5s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.331s, learning 0.168s)
               Value function loss: 0.7003
                    Surrogate loss: -0.0240
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 10.50s
                        Total time: 1256.62s
                               ETA: 1531216.8s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.380s, learning 0.184s)
               Value function loss: 0.8765
                    Surrogate loss: 0.0178
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 10.56s
                        Total time: 1267.18s
                               ETA: 1525470.1s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.450s, learning 0.185s)
               Value function loss: 1.1254
                    Surrogate loss: -0.0014
             Mean action noise std: 0.80
                       Mean reward: 164.54
               Mean episode length: 246.41
                  Mean reward/step: 0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 10.63s
                        Total time: 1277.81s
                               ETA: 1519944.8s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.491s, learning 0.166s)
               Value function loss: 1.5206
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 164.68
               Mean episode length: 246.41
                  Mean reward/step: 0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 10.66s
                        Total time: 1288.47s
                               ETA: 1514575.5s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.355s, learning 0.187s)
               Value function loss: 2.2493
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                       Mean reward: 164.40
               Mean episode length: 245.78
                  Mean reward/step: 0.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 10.54s
                        Total time: 1299.01s
                               ETA: 1509197.7s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.433s, learning 0.164s)
               Value function loss: 1.9196
                    Surrogate loss: -0.0070
             Mean action noise std: 0.80
                       Mean reward: 164.11
               Mean episode length: 244.57
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 10.60s
                        Total time: 1309.61s
                               ETA: 1504005.2s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.373s, learning 0.170s)
               Value function loss: 3.6636
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 161.49
               Mean episode length: 243.10
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 10.54s
                        Total time: 1320.15s
                               ETA: 1498869.3s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.694s, learning 0.167s)
               Value function loss: 5.5163
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                       Mean reward: 161.66
               Mean episode length: 239.15
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 10.86s
                        Total time: 1331.01s
                               ETA: 1494206.0s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.474s, learning 0.176s)
               Value function loss: 8.2620
                    Surrogate loss: -0.0275
             Mean action noise std: 0.80
                       Mean reward: 158.51
               Mean episode length: 236.08
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 10.65s
                        Total time: 1341.66s
                               ETA: 1489411.8s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.390s, learning 0.166s)
               Value function loss: 5.0741
                    Surrogate loss: -0.0234
             Mean action noise std: 0.80
                       Mean reward: 159.03
               Mean episode length: 236.64
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 10.56s
                        Total time: 1352.22s
                               ETA: 1484619.1s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.655s, learning 0.174s)
               Value function loss: 4.9295
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: 163.95
               Mean episode length: 244.97
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 10.83s
                        Total time: 1363.05s
                               ETA: 1480227.4s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.622s, learning 0.159s)
               Value function loss: 6.2401
                    Surrogate loss: -0.0137
             Mean action noise std: 0.80
                       Mean reward: 164.96
               Mean episode length: 248.52
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 10.78s
                        Total time: 1373.83s
                               ETA: 1475878.5s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.711s, learning 0.177s)
               Value function loss: 55.4061
                    Surrogate loss: 0.0260
             Mean action noise std: 0.80
                       Mean reward: 173.22
               Mean episode length: 250.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 4.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 10.89s
                        Total time: 1384.72s
                               ETA: 1471735.1s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1573 steps/s (collection: 10.236s, learning 0.179s)
               Value function loss: 1.1326
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                       Mean reward: 173.22
               Mean episode length: 250.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 10.41s
                        Total time: 1395.13s
                               ETA: 1467180.8s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.409s, learning 0.162s)
               Value function loss: 1.0553
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                       Mean reward: 173.22
               Mean episode length: 250.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 10.57s
                        Total time: 1405.70s
                               ETA: 1462884.0s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.426s, learning 0.170s)
               Value function loss: 0.9535
                    Surrogate loss: -0.0204
             Mean action noise std: 0.80
                       Mean reward: 173.22
               Mean episode length: 250.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 10.60s
                        Total time: 1416.30s
                               ETA: 1458701.3s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.337s, learning 0.234s)
               Value function loss: 0.9187
                    Surrogate loss: -0.0182
             Mean action noise std: 0.80
                       Mean reward: 171.92
               Mean episode length: 249.63
                  Mean reward/step: 0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 10.57s
                        Total time: 1426.87s
                               ETA: 1454578.7s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.576s, learning 0.167s)
               Value function loss: 0.7787
                    Surrogate loss: 0.0146
             Mean action noise std: 0.80
                       Mean reward: 171.92
               Mean episode length: 249.63
                  Mean reward/step: 0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 10.74s
                        Total time: 1437.61s
                               ETA: 1450712.3s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.594s, learning 0.168s)
               Value function loss: 0.7435
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: 171.92
               Mean episode length: 249.63
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 10.76s
                        Total time: 1448.38s
                               ETA: 1446941.3s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.375s, learning 0.172s)
               Value function loss: 0.6309
                    Surrogate loss: -0.0020
             Mean action noise std: 0.80
                       Mean reward: 171.92
               Mean episode length: 249.63
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 10.55s
                        Total time: 1458.92s
                               ETA: 1443033.0s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.677s, learning 0.179s)
               Value function loss: 0.8032
                    Surrogate loss: -0.0048
             Mean action noise std: 0.80
                       Mean reward: 171.92
               Mean episode length: 249.63
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 10.86s
                        Total time: 1469.78s
                               ETA: 1439504.4s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.365s, learning 0.172s)
               Value function loss: 0.8907
                    Surrogate loss: -0.0073
             Mean action noise std: 0.80
                       Mean reward: 171.40
               Mean episode length: 249.63
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 10.54s
                        Total time: 1480.32s
                               ETA: 1435734.1s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.670s, learning 0.159s)
               Value function loss: 0.7444
                    Surrogate loss: 0.0232
             Mean action noise std: 0.80
                       Mean reward: 171.40
               Mean episode length: 249.63
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 10.83s
                        Total time: 1491.15s
                               ETA: 1432316.7s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.502s, learning 0.162s)
               Value function loss: 0.9194
                    Surrogate loss: -0.0032
             Mean action noise std: 0.80
                       Mean reward: 171.40
               Mean episode length: 249.63
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 10.66s
                        Total time: 1501.81s
                               ETA: 1428807.3s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1562 steps/s (collection: 10.319s, learning 0.164s)
               Value function loss: 0.9176
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 171.40
               Mean episode length: 249.63
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 10.48s
                        Total time: 1512.29s
                               ETA: 1425193.0s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.644s, learning 0.205s)
               Value function loss: 0.9560
                    Surrogate loss: -0.0072
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 10.85s
                        Total time: 1523.14s
                               ETA: 1421987.4s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.167s)
               Value function loss: 0.9030
                    Surrogate loss: -0.0094
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 10.72s
                        Total time: 1533.87s
                               ETA: 1418725.8s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.164s)
               Value function loss: 1.4688
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 10.70s
                        Total time: 1544.56s
                               ETA: 1415499.2s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.640s, learning 0.161s)
               Value function loss: 0.8872
                    Surrogate loss: 0.0166
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 10.80s
                        Total time: 1555.36s
                               ETA: 1412425.7s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.451s, learning 0.159s)
               Value function loss: 1.5184
                    Surrogate loss: 0.0011
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 10.61s
                        Total time: 1565.97s
                               ETA: 1409235.0s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.890s, learning 0.184s)
               Value function loss: 0.8192
                    Surrogate loss: -0.0201
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 11.07s
                        Total time: 1577.05s
                               ETA: 1406515.3s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.646s, learning 0.219s)
               Value function loss: 0.7412
                    Surrogate loss: -0.0319
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 10.87s
                        Total time: 1587.91s
                               ETA: 1403658.9s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.616s, learning 0.159s)
               Value function loss: 0.6630
                    Surrogate loss: -0.0201
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 10.78s
                        Total time: 1598.69s
                               ETA: 1400773.7s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.534s, learning 0.172s)
               Value function loss: 0.6570
                    Surrogate loss: -0.0362
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 10.71s
                        Total time: 1609.39s
                               ETA: 1397878.0s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.406s, learning 0.169s)
               Value function loss: 0.6576
                    Surrogate loss: -0.0279
             Mean action noise std: 0.80
                       Mean reward: 171.07
               Mean episode length: 249.63
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 10.58s
                        Total time: 1619.97s
                               ETA: 1394919.7s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.162s)
               Value function loss: 0.8624
                    Surrogate loss: 0.0076
             Mean action noise std: 0.80
                       Mean reward: 171.81
               Mean episode length: 249.63
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 10.70s
                        Total time: 1630.67s
                               ETA: 1392115.2s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.372s, learning 0.165s)
               Value function loss: 1.0590
                    Surrogate loss: 0.0053
             Mean action noise std: 0.80
                       Mean reward: 173.56
               Mean episode length: 249.63
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 10.54s
                        Total time: 1641.20s
                               ETA: 1389222.3s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.765s, learning 0.161s)
               Value function loss: 1.3657
                    Surrogate loss: -0.0107
             Mean action noise std: 0.80
                       Mean reward: 174.43
               Mean episode length: 249.48
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 10.93s
                        Total time: 1652.13s
                               ETA: 1386705.3s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.426s, learning 0.179s)
               Value function loss: 2.3328
                    Surrogate loss: 0.0219
             Mean action noise std: 0.80
                       Mean reward: 175.68
               Mean episode length: 249.05
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 10.61s
                        Total time: 1662.73s
                               ETA: 1383962.7s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.496s, learning 0.161s)
               Value function loss: 2.4998
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                       Mean reward: 182.73
               Mean episode length: 249.05
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 10.66s
                        Total time: 1673.39s
                               ETA: 1381307.5s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.713s, learning 0.164s)
               Value function loss: 5.1657
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                       Mean reward: 199.12
               Mean episode length: 248.03
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 10.88s
                        Total time: 1684.27s
                               ETA: 1378876.5s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.995s, learning 0.166s)
               Value function loss: 6.1046
                    Surrogate loss: 0.0051
             Mean action noise std: 0.80
                       Mean reward: 208.31
               Mean episode length: 248.83
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 11.16s
                        Total time: 1695.43s
                               ETA: 1376716.0s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.597s, learning 0.174s)
               Value function loss: 9.9709
                    Surrogate loss: 0.0036
             Mean action noise std: 0.80
                       Mean reward: 209.87
               Mean episode length: 249.80
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 10.77s
                        Total time: 1706.20s
                               ETA: 1374275.1s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.718s, learning 0.171s)
               Value function loss: 110.9512
                    Surrogate loss: 0.0021
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 250.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 4.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 10.89s
                        Total time: 1717.09s
                               ETA: 1371967.7s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.486s, learning 0.170s)
               Value function loss: 15.5892
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 250.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 10.66s
                        Total time: 1727.75s
                               ETA: 1369512.4s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.562s, learning 0.166s)
               Value function loss: 2.9839
                    Surrogate loss: -0.0178
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 250.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 10.73s
                        Total time: 1738.47s
                               ETA: 1367151.4s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.801s, learning 0.237s)
               Value function loss: 1.9086
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 211.40
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 11.04s
                        Total time: 1749.51s
                               ETA: 1365069.6s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1631 steps/s (collection: 9.882s, learning 0.162s)
               Value function loss: 1.3228
                    Surrogate loss: 0.0072
             Mean action noise std: 0.80
                       Mean reward: 211.21
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 10.04s
                        Total time: 1759.56s
                               ETA: 1362250.5s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.450s, learning 0.161s)
               Value function loss: 1.8311
                    Surrogate loss: -0.0020
             Mean action noise std: 0.80
                       Mean reward: 211.21
               Mean episode length: 250.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 10.61s
                        Total time: 1770.17s
                               ETA: 1359910.2s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.596s, learning 0.246s)
               Value function loss: 1.5435
                    Surrogate loss: 0.0558
             Mean action noise std: 0.80
                       Mean reward: 211.21
               Mean episode length: 250.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 10.84s
                        Total time: 1781.01s
                               ETA: 1357781.4s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.610s, learning 0.174s)
               Value function loss: 1.4011
                    Surrogate loss: 0.0050
             Mean action noise std: 0.80
                       Mean reward: 211.21
               Mean episode length: 250.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 10.78s
                        Total time: 1791.79s
                               ETA: 1355640.9s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1611 steps/s (collection: 10.003s, learning 0.165s)
               Value function loss: 1.3445
                    Surrogate loss: 0.0052
             Mean action noise std: 0.80
                       Mean reward: 209.77
               Mean episode length: 248.25
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 10.17s
                        Total time: 1801.96s
                               ETA: 1353070.2s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.382s, learning 0.166s)
               Value function loss: 1.3020
                    Surrogate loss: 0.0069
             Mean action noise std: 0.80
                       Mean reward: 209.81
               Mean episode length: 248.25
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 10.55s
                        Total time: 1812.51s
                               ETA: 1350820.0s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.361s, learning 0.174s)
               Value function loss: 1.1909
                    Surrogate loss: 0.0031
             Mean action noise std: 0.80
                       Mean reward: 209.81
               Mean episode length: 248.25
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 10.53s
                        Total time: 1823.04s
                               ETA: 1348593.2s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.650s, learning 0.186s)
               Value function loss: 1.1847
                    Surrogate loss: -0.0124
             Mean action noise std: 0.80
                       Mean reward: 209.81
               Mean episode length: 248.25
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 10.84s
                        Total time: 1833.88s
                               ETA: 1346620.5s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.412s, learning 0.188s)
               Value function loss: 1.0035
                    Surrogate loss: 0.0210
             Mean action noise std: 0.80
                       Mean reward: 209.81
               Mean episode length: 248.25
                  Mean reward/step: 0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 10.60s
                        Total time: 1844.48s
                               ETA: 1344504.4s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.737s, learning 0.184s)
               Value function loss: 1.1194
                    Surrogate loss: 0.0040
             Mean action noise std: 0.80
                       Mean reward: 209.81
               Mean episode length: 248.25
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 10.92s
                        Total time: 1855.40s
                               ETA: 1342651.1s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.356s, learning 0.165s)
               Value function loss: 1.4947
                    Surrogate loss: 0.0038
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 10.52s
                        Total time: 1865.92s
                               ETA: 1340536.9s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.591s, learning 0.173s)
               Value function loss: 0.9079
                    Surrogate loss: 0.0202
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 10.76s
                        Total time: 1876.69s
                               ETA: 1338626.4s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.834s, learning 0.188s)
               Value function loss: 1.0963
                    Surrogate loss: -0.0045
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 11.02s
                        Total time: 1887.71s
                               ETA: 1336925.7s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.495s, learning 0.161s)
               Value function loss: 1.2513
                    Surrogate loss: -0.0230
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 10.66s
                        Total time: 1898.36s
                               ETA: 1334991.1s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.435s, learning 0.166s)
               Value function loss: 0.6375
                    Surrogate loss: -0.0346
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 10.60s
                        Total time: 1908.97s
                               ETA: 1333045.0s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.453s, learning 0.166s)
               Value function loss: 0.6021
                    Surrogate loss: -0.0343
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 10.62s
                        Total time: 1919.58s
                               ETA: 1331138.1s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.516s, learning 0.162s)
               Value function loss: 0.6411
                    Surrogate loss: -0.0243
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 10.68s
                        Total time: 1930.26s
                               ETA: 1329298.0s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.372s, learning 0.268s)
               Value function loss: 0.7713
                    Surrogate loss: 0.0057
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 10.64s
                        Total time: 1940.90s
                               ETA: 1327457.3s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.459s, learning 0.167s)
               Value function loss: 0.8355
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: 210.16
               Mean episode length: 248.25
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 10.63s
                        Total time: 1951.53s
                               ETA: 1325631.7s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.768s, learning 0.192s)
               Value function loss: 1.0127
                    Surrogate loss: 0.0030
             Mean action noise std: 0.80
                       Mean reward: 209.91
               Mean episode length: 248.25
                  Mean reward/step: 0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 10.96s
                        Total time: 1962.49s
                               ETA: 1324056.3s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.664s, learning 0.189s)
               Value function loss: 1.8294
                    Surrogate loss: 0.0041
             Mean action noise std: 0.80
                       Mean reward: 210.34
               Mean episode length: 248.25
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 10.85s
                        Total time: 1973.34s
                               ETA: 1322430.0s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1572 steps/s (collection: 10.229s, learning 0.192s)
               Value function loss: 2.5847
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 208.69
               Mean episode length: 247.76
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 10.42s
                        Total time: 1983.76s
                               ETA: 1320537.2s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1579 steps/s (collection: 10.212s, learning 0.159s)
               Value function loss: 2.9870
                    Surrogate loss: -0.0119
             Mean action noise std: 0.80
                       Mean reward: 206.88
               Mean episode length: 247.32
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 10.37s
                        Total time: 1994.13s
                               ETA: 1318637.0s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1568 steps/s (collection: 10.283s, learning 0.160s)
               Value function loss: 2.3627
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 205.66
               Mean episode length: 247.32
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 10.44s
                        Total time: 2004.58s
                               ETA: 1316808.7s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.543s, learning 0.269s)
               Value function loss: 4.8087
                    Surrogate loss: -0.0009
             Mean action noise std: 0.80
                       Mean reward: 200.99
               Mean episode length: 247.06
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 10.81s
                        Total time: 2015.39s
                               ETA: 1315244.9s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.577s, learning 0.170s)
               Value function loss: 5.0258
                    Surrogate loss: -0.0022
             Mean action noise std: 0.80
                       Mean reward: 195.61
               Mean episode length: 249.51
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 10.75s
                        Total time: 2026.14s
                               ETA: 1313659.2s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.440s, learning 0.168s)
               Value function loss: 10.0984
                    Surrogate loss: 0.0018
             Mean action noise std: 0.80
                       Mean reward: 198.51
               Mean episode length: 249.80
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 10.61s
                        Total time: 2036.74s
                               ETA: 1312004.3s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.687s, learning 0.172s)
               Value function loss: 8.2697
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 197.72
               Mean episode length: 249.96
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 10.86s
                        Total time: 2047.60s
                               ETA: 1310531.0s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.355s, learning 0.173s)
               Value function loss: 21.2918
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 194.67
               Mean episode length: 250.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 10.53s
                        Total time: 2058.13s
                               ETA: 1308865.7s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.528s, learning 0.194s)
               Value function loss: 0.2865
                    Surrogate loss: -0.0260
             Mean action noise std: 0.80
                       Mean reward: 194.67
               Mean episode length: 250.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 10.72s
                        Total time: 2068.85s
                               ETA: 1307344.6s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.341s, learning 0.157s)
               Value function loss: 0.2961
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 194.67
               Mean episode length: 250.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 10.50s
                        Total time: 2079.35s
                               ETA: 1305701.4s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1566 steps/s (collection: 10.298s, learning 0.163s)
               Value function loss: 0.5083
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 194.67
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 10.46s
                        Total time: 2089.81s
                               ETA: 1304055.8s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.342s, learning 0.157s)
               Value function loss: 0.9657
                    Surrogate loss: 0.0292
             Mean action noise std: 0.80
                       Mean reward: 194.40
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 10.50s
                        Total time: 2100.31s
                               ETA: 1302453.9s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1567 steps/s (collection: 10.287s, learning 0.164s)
               Value function loss: 0.6037
                    Surrogate loss: -0.0080
             Mean action noise std: 0.80
                       Mean reward: 194.40
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 10.45s
                        Total time: 2110.76s
                               ETA: 1300841.9s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.691s, learning 0.166s)
               Value function loss: 0.8833
                    Surrogate loss: 0.0363
             Mean action noise std: 0.80
                       Mean reward: 194.40
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 10.86s
                        Total time: 2121.62s
                               ETA: 1299498.0s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.450s, learning 0.163s)
               Value function loss: 0.9317
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 194.14
               Mean episode length: 250.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 10.61s
                        Total time: 2132.23s
                               ETA: 1298022.1s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.553s, learning 0.161s)
               Value function loss: 0.8652
                    Surrogate loss: -0.0281
             Mean action noise std: 0.80
                       Mean reward: 194.14
               Mean episode length: 250.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 10.71s
                        Total time: 2142.95s
                               ETA: 1296625.1s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.449s, learning 0.170s)
               Value function loss: 0.8260
                    Surrogate loss: -0.0092
             Mean action noise std: 0.80
                       Mean reward: 194.35
               Mean episode length: 250.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 10.62s
                        Total time: 2153.56s
                               ETA: 1295187.7s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.452s, learning 0.180s)
               Value function loss: 0.9200
                    Surrogate loss: -0.0111
             Mean action noise std: 0.80
                       Mean reward: 192.53
               Mean episode length: 246.94
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 10.63s
                        Total time: 2164.20s
                               ETA: 1293775.3s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.526s, learning 0.173s)
               Value function loss: 0.7220
                    Surrogate loss: -0.0124
             Mean action noise std: 0.80
                       Mean reward: 190.79
               Mean episode length: 245.33
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 10.70s
                        Total time: 2174.90s
                               ETA: 1292419.7s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.474s, learning 0.164s)
               Value function loss: 0.6459
                    Surrogate loss: -0.0089
             Mean action noise std: 0.80
                       Mean reward: 190.79
               Mean episode length: 245.33
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 10.64s
                        Total time: 2185.53s
                               ETA: 1291043.3s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.636s, learning 0.248s)
               Value function loss: 0.6395
                    Surrogate loss: -0.0072
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 10.88s
                        Total time: 2196.42s
                               ETA: 1289827.3s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.165s)
               Value function loss: 0.5420
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 10.82s
                        Total time: 2207.24s
                               ETA: 1288589.2s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.435s, learning 0.162s)
               Value function loss: 0.4922
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 10.60s
                        Total time: 2217.84s
                               ETA: 1287235.2s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.469s, learning 0.198s)
               Value function loss: 0.4551
                    Surrogate loss: -0.0150
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 10.67s
                        Total time: 2228.50s
                               ETA: 1285936.9s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.527s, learning 0.255s)
               Value function loss: 0.4281
                    Surrogate loss: -0.0317
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 10.78s
                        Total time: 2239.29s
                               ETA: 1284719.0s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.582s, learning 0.163s)
               Value function loss: 0.4436
                    Surrogate loss: -0.0319
             Mean action noise std: 0.80
                       Mean reward: 189.84
               Mean episode length: 245.33
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 10.74s
                        Total time: 2250.03s
                               ETA: 1283494.2s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.461s, learning 0.164s)
               Value function loss: 0.6100
                    Surrogate loss: -0.0251
             Mean action noise std: 0.80
                       Mean reward: 189.27
               Mean episode length: 244.39
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 10.63s
                        Total time: 2260.66s
                               ETA: 1282215.3s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.494s, learning 0.181s)
               Value function loss: 0.5806
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: 189.27
               Mean episode length: 244.39
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 10.68s
                        Total time: 2271.33s
                               ETA: 1280979.1s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.796s, learning 0.169s)
               Value function loss: 0.8329
                    Surrogate loss: -0.0121
             Mean action noise std: 0.80
                       Mean reward: 189.27
               Mean episode length: 244.39
                  Mean reward/step: 0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 10.97s
                        Total time: 2282.30s
                               ETA: 1279919.3s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.524s, learning 0.185s)
               Value function loss: 0.9649
                    Surrogate loss: 0.0282
             Mean action noise std: 0.80
                       Mean reward: 188.13
               Mean episode length: 244.22
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 10.71s
                        Total time: 2293.00s
                               ETA: 1278728.0s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.353s, learning 0.181s)
               Value function loss: 1.4308
                    Surrogate loss: 0.0195
             Mean action noise std: 0.80
                       Mean reward: 186.70
               Mean episode length: 243.26
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 10.53s
                        Total time: 2303.54s
                               ETA: 1277453.2s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.565s, learning 0.272s)
               Value function loss: 2.5757
                    Surrogate loss: 0.0266
             Mean action noise std: 0.80
                       Mean reward: 184.56
               Mean episode length: 240.36
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 10.84s
                        Total time: 2314.38s
                               ETA: 1276359.5s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.657s, learning 0.198s)
               Value function loss: 2.9081
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 178.48
               Mean episode length: 235.83
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 10.85s
                        Total time: 2325.23s
                               ETA: 1275287.2s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.430s, learning 0.171s)
               Value function loss: 3.4122
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 169.86
               Mean episode length: 229.16
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 10.60s
                        Total time: 2335.83s
                               ETA: 1274088.6s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1565 steps/s (collection: 10.288s, learning 0.178s)
               Value function loss: 5.8081
                    Surrogate loss: -0.0168
             Mean action noise std: 0.80
                       Mean reward: 170.21
               Mean episode length: 234.38
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 10.47s
                        Total time: 2346.30s
                               ETA: 1272829.1s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1569 steps/s (collection: 10.273s, learning 0.165s)
               Value function loss: 6.6835
                    Surrogate loss: -0.0241
             Mean action noise std: 0.80
                       Mean reward: 174.26
               Mean episode length: 241.49
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 10.44s
                        Total time: 2356.74s
                               ETA: 1271568.4s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.520s, learning 0.166s)
               Value function loss: 11.1672
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 176.22
               Mean episode length: 246.32
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 10.69s
                        Total time: 2367.42s
                               ETA: 1270453.8s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.627s, learning 0.196s)
               Value function loss: 13.0581
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                       Mean reward: 175.16
               Mean episode length: 248.65
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 10.82s
                        Total time: 2378.25s
                               ETA: 1269424.1s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.166s)
               Value function loss: 34.5300
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 184.44
               Mean episode length: 250.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 4.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 10.70s
                        Total time: 2388.95s
                               ETA: 1268339.5s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.583s, learning 0.226s)
               Value function loss: 1.3307
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 184.44
               Mean episode length: 250.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 10.81s
                        Total time: 2399.75s
                               ETA: 1267324.3s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.540s, learning 0.195s)
               Value function loss: 0.6539
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 184.44
               Mean episode length: 250.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 10.74s
                        Total time: 2410.49s
                               ETA: 1266281.0s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.629s, learning 0.164s)
               Value function loss: 0.7818
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: 184.44
               Mean episode length: 250.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 10.79s
                        Total time: 2421.28s
                               ETA: 1265279.0s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.363s, learning 0.192s)
               Value function loss: 0.9053
                    Surrogate loss: 0.0076
             Mean action noise std: 0.80
                       Mean reward: 184.76
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 10.55s
                        Total time: 2431.84s
                               ETA: 1264163.2s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.612s, learning 0.162s)
               Value function loss: 0.7800
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: 184.76
               Mean episode length: 250.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 10.77s
                        Total time: 2442.61s
                               ETA: 1263172.1s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.318s, learning 0.175s)
               Value function loss: 0.8026
                    Surrogate loss: 0.0021
             Mean action noise std: 0.80
                       Mean reward: 184.76
               Mean episode length: 250.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 10.49s
                        Total time: 2453.11s
                               ETA: 1262046.7s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.406s, learning 0.166s)
               Value function loss: 0.9376
                    Surrogate loss: 0.0295
             Mean action noise std: 0.80
                       Mean reward: 185.10
               Mean episode length: 250.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 10.57s
                        Total time: 2463.68s
                               ETA: 1260973.0s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.754s, learning 0.185s)
               Value function loss: 0.9293
                    Surrogate loss: 0.0133
             Mean action noise std: 0.80
                       Mean reward: 185.10
               Mean episode length: 250.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 10.94s
                        Total time: 2474.62s
                               ETA: 1260097.6s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.723s, learning 0.162s)
               Value function loss: 1.0638
                    Surrogate loss: 0.0614
             Mean action noise std: 0.80
                       Mean reward: 183.97
               Mean episode length: 248.40
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 10.88s
                        Total time: 2485.50s
                               ETA: 1259203.0s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.400s, learning 0.165s)
               Value function loss: 0.8411
                    Surrogate loss: 0.0017
             Mean action noise std: 0.80
                       Mean reward: 184.71
               Mean episode length: 248.40
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 10.57s
                        Total time: 2496.07s
                               ETA: 1258156.2s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.346s, learning 0.273s)
               Value function loss: 0.6457
                    Surrogate loss: -0.0027
             Mean action noise std: 0.80
                       Mean reward: 184.76
               Mean episode length: 248.40
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 10.62s
                        Total time: 2506.69s
                               ETA: 1257147.2s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.340s, learning 0.187s)
               Value function loss: 0.5623
                    Surrogate loss: 0.0047
             Mean action noise std: 0.80
                       Mean reward: 184.76
               Mean episode length: 248.40
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 10.53s
                        Total time: 2517.21s
                               ETA: 1256102.3s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.327s, learning 0.170s)
               Value function loss: 0.5506
                    Surrogate loss: 0.0163
             Mean action noise std: 0.80
                       Mean reward: 185.35
               Mean episode length: 248.40
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 10.50s
                        Total time: 2527.71s
                               ETA: 1255052.5s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.622s, learning 0.172s)
               Value function loss: 0.3843
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 185.35
               Mean episode length: 248.40
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 10.79s
                        Total time: 2538.50s
                               ETA: 1254159.6s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.634s, learning 0.165s)
               Value function loss: 0.3419
                    Surrogate loss: 0.0070
             Mean action noise std: 0.80
                       Mean reward: 185.35
               Mean episode length: 248.40
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 10.80s
                        Total time: 2549.30s
                               ETA: 1253278.0s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.639s, learning 0.184s)
               Value function loss: 0.3653
                    Surrogate loss: -0.0063
             Mean action noise std: 0.80
                       Mean reward: 185.35
               Mean episode length: 248.40
                  Mean reward/step: 0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 10.82s
                        Total time: 2560.13s
                               ETA: 1252416.5s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.590s, learning 0.178s)
               Value function loss: 0.3367
                    Surrogate loss: -0.0016
             Mean action noise std: 0.80
                       Mean reward: 185.35
               Mean episode length: 248.40
                  Mean reward/step: 0.92
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 10.77s
                        Total time: 2570.89s
                               ETA: 1251536.6s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.559s, learning 0.160s)
               Value function loss: 0.4817
                    Surrogate loss: -0.0195
             Mean action noise std: 0.80
                       Mean reward: 185.08
               Mean episode length: 247.38
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 10.72s
                        Total time: 2581.61s
                               ETA: 1250641.7s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.814s, learning 0.161s)
               Value function loss: 0.4711
                    Surrogate loss: -0.0320
             Mean action noise std: 0.80
                       Mean reward: 184.78
               Mean episode length: 247.38
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 10.98s
                        Total time: 2592.59s
                               ETA: 1249878.6s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.698s, learning 0.186s)
               Value function loss: 0.5212
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: 184.41
               Mean episode length: 246.61
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 10.88s
                        Total time: 2603.47s
                               ETA: 1249079.0s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.334s, learning 0.166s)
               Value function loss: 0.4886
                    Surrogate loss: -0.0096
             Mean action noise std: 0.80
                       Mean reward: 184.41
               Mean episode length: 246.61
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 10.50s
                        Total time: 2613.97s
                               ETA: 1248103.8s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.544s, learning 0.166s)
               Value function loss: 1.1007
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 184.43
               Mean episode length: 245.02
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 10.71s
                        Total time: 2624.68s
                               ETA: 1247237.1s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.168s)
               Value function loss: 1.5772
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                       Mean reward: 183.60
               Mean episode length: 243.87
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 11.01s
                        Total time: 2635.69s
                               ETA: 1246518.6s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.650s, learning 0.161s)
               Value function loss: 1.9040
                    Surrogate loss: -0.0034
             Mean action noise std: 0.80
                       Mean reward: 184.16
               Mean episode length: 241.67
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 10.81s
                        Total time: 2646.50s
                               ETA: 1245715.5s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.731s, learning 0.160s)
               Value function loss: 2.3094
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 186.53
               Mean episode length: 239.36
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 10.89s
                        Total time: 2657.39s
                               ETA: 1244956.7s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.664s, learning 0.188s)
               Value function loss: 3.3517
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 189.71
               Mean episode length: 236.99
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 10.85s
                        Total time: 2668.24s
                               ETA: 1244186.9s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.512s, learning 0.163s)
               Value function loss: 4.5902
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 195.90
               Mean episode length: 245.49
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 10.67s
                        Total time: 2678.92s
                               ETA: 1243342.0s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.703s, learning 0.176s)
               Value function loss: 5.6699
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 197.09
               Mean episode length: 247.95
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 10.88s
                        Total time: 2689.80s
                               ETA: 1242599.5s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.555s, learning 0.168s)
               Value function loss: 6.6265
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: 200.32
               Mean episode length: 248.93
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 10.72s
                        Total time: 2700.52s
                               ETA: 1241791.4s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.724s, learning 0.160s)
               Value function loss: 8.9719
                    Surrogate loss: -0.0139
             Mean action noise std: 0.80
                       Mean reward: 197.77
               Mean episode length: 249.37
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 10.88s
                        Total time: 2711.40s
                               ETA: 1241064.5s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.961s, learning 0.168s)
               Value function loss: 40.4905
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: 200.31
               Mean episode length: 250.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 4.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 11.13s
                        Total time: 2722.53s
                               ETA: 1240455.6s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1574 steps/s (collection: 10.248s, learning 0.160s)
               Value function loss: 1.3796
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 199.79
               Mean episode length: 249.47
                  Mean reward/step: 0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 10.41s
                        Total time: 2732.94s
                               ETA: 1239525.6s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.521s, learning 0.180s)
               Value function loss: 0.6182
                    Surrogate loss: -0.0294
             Mean action noise std: 0.80
                       Mean reward: 199.79
               Mean episode length: 249.47
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 10.70s
                        Total time: 2743.64s
                               ETA: 1238735.9s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.603s, learning 0.270s)
               Value function loss: 0.4915
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 199.79
               Mean episode length: 249.47
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 10.87s
                        Total time: 2754.52s
                               ETA: 1238030.5s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.365s, learning 0.171s)
               Value function loss: 0.8122
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 198.14
               Mean episode length: 247.58
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 10.54s
                        Total time: 2765.05s
                               ETA: 1237180.5s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.328s, learning 0.173s)
               Value function loss: 1.0364
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 198.14
               Mean episode length: 247.58
                  Mean reward/step: 0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 10.50s
                        Total time: 2775.55s
                               ETA: 1236322.4s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1572 steps/s (collection: 10.256s, learning 0.165s)
               Value function loss: 1.0408
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 198.14
               Mean episode length: 247.58
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 10.42s
                        Total time: 2785.97s
                               ETA: 1235436.8s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1573 steps/s (collection: 10.254s, learning 0.159s)
               Value function loss: 0.9096
                    Surrogate loss: -0.0232
             Mean action noise std: 0.79
                       Mean reward: 198.14
               Mean episode length: 247.58
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 10.41s
                        Total time: 2796.39s
                               ETA: 1234555.0s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.568s, learning 0.190s)
               Value function loss: 0.9430
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 196.73
               Mean episode length: 245.88
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 10.76s
                        Total time: 2807.14s
                               ETA: 1233832.4s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.656s, learning 0.162s)
               Value function loss: 0.7802
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 197.00
               Mean episode length: 245.88
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 10.82s
                        Total time: 2817.96s
                               ETA: 1233142.5s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.584s, learning 0.162s)
               Value function loss: 0.9589
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 197.41
               Mean episode length: 245.88
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 10.75s
                        Total time: 2828.71s
                               ETA: 1232427.0s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.446s, learning 0.159s)
               Value function loss: 0.7523
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 196.68
               Mean episode length: 244.24
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 10.60s
                        Total time: 2839.31s
                               ETA: 1231656.4s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.484s, learning 0.210s)
               Value function loss: 0.9268
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 196.68
               Mean episode length: 244.24
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 10.69s
                        Total time: 2850.01s
                               ETA: 1230931.0s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.536s, learning 0.159s)
               Value function loss: 0.7573
                    Surrogate loss: 0.0235
             Mean action noise std: 0.79
                       Mean reward: 196.65
               Mean episode length: 244.24
                  Mean reward/step: 1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 10.69s
                        Total time: 2860.70s
                               ETA: 1230211.9s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.542s, learning 0.252s)
               Value function loss: 0.6831
                    Surrogate loss: 0.0104
             Mean action noise std: 0.79
                       Mean reward: 195.85
               Mean episode length: 243.36
                  Mean reward/step: 1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 10.79s
                        Total time: 2871.49s
                               ETA: 1229541.9s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.604s, learning 0.195s)
               Value function loss: 0.6876
                    Surrogate loss: 0.0083
             Mean action noise std: 0.79
                       Mean reward: 195.54
               Mean episode length: 242.68
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 10.80s
                        Total time: 2882.29s
                               ETA: 1228879.2s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.684s, learning 0.184s)
               Value function loss: 0.6781
                    Surrogate loss: 0.0085
             Mean action noise std: 0.79
                       Mean reward: 194.91
               Mean episode length: 242.20
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 10.87s
                        Total time: 2893.16s
                               ETA: 1228251.8s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.388s, learning 0.194s)
               Value function loss: 0.6713
                    Surrogate loss: 0.0024
             Mean action noise std: 0.79
                       Mean reward: 194.03
               Mean episode length: 241.59
                  Mean reward/step: 1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 10.58s
                        Total time: 2903.74s
                               ETA: 1227508.1s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.623s, learning 0.162s)
               Value function loss: 1.4265
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 192.16
               Mean episode length: 238.07
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 10.79s
                        Total time: 2914.53s
                               ETA: 1226856.4s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.533s, learning 0.161s)
               Value function loss: 0.7805
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: 191.84
               Mean episode length: 237.64
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 10.69s
                        Total time: 2925.22s
                               ETA: 1226171.8s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1578 steps/s (collection: 10.207s, learning 0.173s)
               Value function loss: 1.3223
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 190.43
               Mean episode length: 234.93
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 10.38s
                        Total time: 2935.60s
                               ETA: 1225362.1s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.171s)
               Value function loss: 2.7218
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 186.69
               Mean episode length: 227.27
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 10.68s
                        Total time: 2946.28s
                               ETA: 1224683.5s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.785s, learning 0.170s)
               Value function loss: 2.6556
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 183.79
               Mean episode length: 221.77
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 10.96s
                        Total time: 2957.24s
                               ETA: 1224124.5s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.409s, learning 0.170s)
               Value function loss: 4.1730
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 177.22
               Mean episode length: 211.19
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 10.58s
                        Total time: 2967.82s
                               ETA: 1223415.0s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.165s)
               Value function loss: 4.3605
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 171.08
               Mean episode length: 203.57
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 10.93s
                        Total time: 2978.75s
                               ETA: 1222854.7s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.418s, learning 0.165s)
               Value function loss: 7.0881
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 176.30
               Mean episode length: 211.54
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 10.58s
                        Total time: 2989.33s
                               ETA: 1222157.9s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.421s, learning 0.167s)
               Value function loss: 8.6980
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 191.86
               Mean episode length: 225.58
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 10.59s
                        Total time: 2999.92s
                               ETA: 1221468.7s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1573 steps/s (collection: 10.246s, learning 0.163s)
               Value function loss: 9.8863
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 197.36
               Mean episode length: 233.14
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 10.41s
                        Total time: 3010.33s
                               ETA: 1220712.3s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.566s, learning 0.166s)
               Value function loss: 12.1942
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 199.32
               Mean episode length: 239.20
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 10.73s
                        Total time: 3021.06s
                               ETA: 1220092.1s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.323s, learning 0.166s)
               Value function loss: 10.6829
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 206.34
               Mean episode length: 244.40
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 10.49s
                        Total time: 3031.55s
                               ETA: 1219379.2s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.508s, learning 0.169s)
               Value function loss: 9.0853
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 210.76
               Mean episode length: 247.79
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 10.68s
                        Total time: 3042.23s
                               ETA: 1218747.4s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.594s, learning 0.161s)
               Value function loss: 64.6666
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 222.64
               Mean episode length: 250.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 4.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 10.75s
                        Total time: 3052.98s
                               ETA: 1218151.5s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.430s, learning 0.164s)
               Value function loss: 4.1556
                    Surrogate loss: 0.1734
             Mean action noise std: 0.79
                       Mean reward: 222.64
               Mean episode length: 250.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 10.59s
                        Total time: 3063.57s
                               ETA: 1217496.3s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1580 steps/s (collection: 10.200s, learning 0.165s)
               Value function loss: 1.4410
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 222.37
               Mean episode length: 249.51
                  Mean reward/step: 0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 10.36s
                        Total time: 3073.94s
                               ETA: 1216755.2s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.547s, learning 0.183s)
               Value function loss: 0.5042
                    Surrogate loss: -0.0201
             Mean action noise std: 0.79
                       Mean reward: 222.37
               Mean episode length: 249.51
                  Mean reward/step: 0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 10.73s
                        Total time: 3084.67s
                               ETA: 1216164.1s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.595s, learning 0.185s)
               Value function loss: 0.6240
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: 222.54
               Mean episode length: 249.51
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 10.78s
                        Total time: 3095.45s
                               ETA: 1215597.1s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.409s, learning 0.161s)
               Value function loss: 0.7738
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 222.54
               Mean episode length: 249.51
                  Mean reward/step: 0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 10.57s
                        Total time: 3106.02s
                               ETA: 1214952.3s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1604 steps/s (collection: 10.043s, learning 0.170s)
               Value function loss: 0.7403
                    Surrogate loss: 0.0180
             Mean action noise std: 0.79
                       Mean reward: 222.54
               Mean episode length: 249.51
                  Mean reward/step: 0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 10.21s
                        Total time: 3116.23s
                               ETA: 1214173.8s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.585s, learning 0.168s)
               Value function loss: 0.9012
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 220.74
               Mean episode length: 247.25
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 10.75s
                        Total time: 3126.98s
                               ETA: 1213610.4s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.490s, learning 0.166s)
               Value function loss: 1.0362
                    Surrogate loss: 0.0218
             Mean action noise std: 0.79
                       Mean reward: 216.88
               Mean episode length: 243.52
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 10.66s
                        Total time: 3137.64s
                               ETA: 1213014.1s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.544s, learning 0.167s)
               Value function loss: 0.7160
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 215.98
               Mean episode length: 243.10
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 11.71s
                        Total time: 3149.35s
                               ETA: 1212828.5s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.662s, learning 0.181s)
               Value function loss: 0.7875
                    Surrogate loss: 0.0492
             Mean action noise std: 0.79
                       Mean reward: 214.81
               Mean episode length: 241.13
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 10.84s
                        Total time: 3160.19s
                               ETA: 1212311.4s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.457s, learning 0.165s)
               Value function loss: 0.7735
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 213.94
               Mean episode length: 240.58
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 10.62s
                        Total time: 3170.82s
                               ETA: 1211713.6s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.328s, learning 0.167s)
               Value function loss: 0.4752
                    Surrogate loss: 0.0052
             Mean action noise std: 0.79
                       Mean reward: 213.94
               Mean episode length: 240.58
                  Mean reward/step: 1.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 10.50s
                        Total time: 3181.31s
                               ETA: 1211072.1s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1577 steps/s (collection: 10.226s, learning 0.161s)
               Value function loss: 0.6114
                    Surrogate loss: 0.0223
             Mean action noise std: 0.79
                       Mean reward: 213.15
               Mean episode length: 239.28
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 10.39s
                        Total time: 3191.70s
                               ETA: 1210394.5s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1593 steps/s (collection: 10.115s, learning 0.167s)
               Value function loss: 0.5413
                    Surrogate loss: 0.0075
             Mean action noise std: 0.79
                       Mean reward: 211.72
               Mean episode length: 238.30
                  Mean reward/step: 0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 10.28s
                        Total time: 3201.98s
                               ETA: 1209682.0s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.495s, learning 0.192s)
               Value function loss: 0.7366
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 209.58
               Mean episode length: 236.20
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 10.69s
                        Total time: 3212.67s
                               ETA: 1209127.3s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1569 steps/s (collection: 10.272s, learning 0.167s)
               Value function loss: 1.2589
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 208.13
               Mean episode length: 233.32
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 10.44s
                        Total time: 3223.11s
                               ETA: 1208483.8s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.686s, learning 0.159s)
               Value function loss: 1.3349
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 204.75
               Mean episode length: 231.06
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 10.84s
                        Total time: 3233.95s
                               ETA: 1207996.5s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.395s, learning 0.164s)
               Value function loss: 1.3832
                    Surrogate loss: 0.0083
             Mean action noise std: 0.79
                       Mean reward: 200.05
               Mean episode length: 227.64
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 10.56s
                        Total time: 3244.51s
                               ETA: 1207406.6s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1575 steps/s (collection: 10.238s, learning 0.162s)
               Value function loss: 2.3375
                    Surrogate loss: 0.0047
             Mean action noise std: 0.79
                       Mean reward: 193.94
               Mean episode length: 221.15
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 10.40s
                        Total time: 3254.91s
                               ETA: 1206761.9s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.645s, learning 0.227s)
               Value function loss: 2.5873
                    Surrogate loss: 0.0051
             Mean action noise std: 0.79
                       Mean reward: 184.88
               Mean episode length: 214.89
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 10.87s
                        Total time: 3265.78s
                               ETA: 1206296.2s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.605s, learning 0.182s)
               Value function loss: 4.6119
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 174.68
               Mean episode length: 207.54
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 10.79s
                        Total time: 3276.57s
                               ETA: 1205802.6s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.163s)
               Value function loss: 4.6174
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 173.54
               Mean episode length: 209.20
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 10.76s
                        Total time: 3287.34s
                               ETA: 1205304.2s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.600s, learning 0.229s)
               Value function loss: 7.1522
                    Surrogate loss: 0.0414
             Mean action noise std: 0.79
                       Mean reward: 175.50
               Mean episode length: 208.95
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 10.83s
                        Total time: 3298.16s
                               ETA: 1204832.9s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.469s, learning 0.166s)
               Value function loss: 7.8036
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 180.08
               Mean episode length: 214.71
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 10.64s
                        Total time: 3308.80s
                               ETA: 1204294.7s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.334s, learning 0.188s)
               Value function loss: 8.8741
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 181.95
               Mean episode length: 215.49
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 10.52s
                        Total time: 3319.32s
                               ETA: 1203719.0s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1587 steps/s (collection: 10.162s, learning 0.161s)
               Value function loss: 12.0289
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 182.44
               Mean episode length: 221.06
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 10.32s
                        Total time: 3329.65s
                               ETA: 1203075.7s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.430s, learning 0.172s)
               Value function loss: 8.1531
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 183.07
               Mean episode length: 225.82
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 10.60s
                        Total time: 3340.25s
                               ETA: 1202537.1s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.507s, learning 0.164s)
               Value function loss: 6.4614
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 194.33
               Mean episode length: 236.05
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 10.67s
                        Total time: 3350.92s
                               ETA: 1202027.3s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.720s, learning 0.170s)
               Value function loss: 5.3704
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 194.57
               Mean episode length: 238.72
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 10.89s
                        Total time: 3361.81s
                               ETA: 1201599.6s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1565 steps/s (collection: 10.272s, learning 0.195s)
               Value function loss: 4.1031
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 191.19
               Mean episode length: 243.92
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 10.47s
                        Total time: 3372.28s
                               ETA: 1201024.1s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1577 steps/s (collection: 10.216s, learning 0.171s)
               Value function loss: 5.4021
                    Surrogate loss: -0.0221
             Mean action noise std: 0.79
                       Mean reward: 195.12
               Mean episode length: 246.33
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 10.39s
                        Total time: 3382.66s
                               ETA: 1200424.2s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.732s, learning 0.254s)
               Value function loss: 7.2126
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 199.85
               Mean episode length: 249.14
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 10.99s
                        Total time: 3393.65s
                               ETA: 1200040.4s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.348s, learning 0.173s)
               Value function loss: 0.6688
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 199.85
               Mean episode length: 249.14
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 10.52s
                        Total time: 3404.17s
                               ETA: 1199495.2s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.361s, learning 0.175s)
               Value function loss: 0.6516
                    Surrogate loss: 0.0041
             Mean action noise std: 0.79
                       Mean reward: 199.50
               Mean episode length: 248.50
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 10.54s
                        Total time: 3414.71s
                               ETA: 1198959.1s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.466s, learning 0.160s)
               Value function loss: 0.8371
                    Surrogate loss: 0.0019
             Mean action noise std: 0.79
                       Mean reward: 199.58
               Mean episode length: 247.62
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 10.63s
                        Total time: 3425.33s
                               ETA: 1198458.0s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.495s, learning 0.159s)
               Value function loss: 0.8996
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 198.61
               Mean episode length: 246.54
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 10.65s
                        Total time: 3435.99s
                               ETA: 1197970.0s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.463s, learning 0.166s)
               Value function loss: 1.2797
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 196.34
               Mean episode length: 243.64
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 10.63s
                        Total time: 3446.62s
                               ETA: 1197476.7s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1580 steps/s (collection: 10.200s, learning 0.168s)
               Value function loss: 1.1166
                    Surrogate loss: 0.0122
             Mean action noise std: 0.79
                       Mean reward: 195.96
               Mean episode length: 242.56
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 10.37s
                        Total time: 3456.98s
                               ETA: 1196896.7s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.167s)
               Value function loss: 1.1506
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 193.53
               Mean episode length: 239.42
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 10.70s
                        Total time: 3467.68s
                               ETA: 1196433.5s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.405s, learning 0.164s)
               Value function loss: 1.0582
                    Surrogate loss: 0.0149
             Mean action noise std: 0.79
                       Mean reward: 190.50
               Mean episode length: 235.34
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 10.57s
                        Total time: 3478.25s
                               ETA: 1195929.8s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.617s, learning 0.162s)
               Value function loss: 1.9203
                    Surrogate loss: 0.0106
             Mean action noise std: 0.79
                       Mean reward: 184.44
               Mean episode length: 226.27
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 10.78s
                        Total time: 3489.03s
                               ETA: 1195501.6s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.603s, learning 0.159s)
               Value function loss: 2.3525
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 178.58
               Mean episode length: 215.32
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 10.76s
                        Total time: 3499.79s
                               ETA: 1195070.4s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1579 steps/s (collection: 10.197s, learning 0.173s)
               Value function loss: 1.9402
                    Surrogate loss: 0.0049
             Mean action noise std: 0.79
                       Mean reward: 171.52
               Mean episode length: 206.05
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 10.37s
                        Total time: 3510.16s
                               ETA: 1194508.9s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.162s)
               Value function loss: 2.7387
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 159.79
               Mean episode length: 191.44
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 10.99s
                        Total time: 3521.15s
                               ETA: 1194161.9s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.522s, learning 0.158s)
               Value function loss: 4.5906
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 148.13
               Mean episode length: 173.38
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 10.68s
                        Total time: 3531.83s
                               ETA: 1193711.5s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.521s, learning 0.267s)
               Value function loss: 4.5286
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 151.43
               Mean episode length: 175.40
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 10.79s
                        Total time: 3542.62s
                               ETA: 1193300.5s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.163s)
               Value function loss: 5.0346
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 157.35
               Mean episode length: 179.47
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 10.74s
                        Total time: 3553.36s
                               ETA: 1192875.0s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.393s, learning 0.161s)
               Value function loss: 5.5199
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 163.24
               Mean episode length: 183.48
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 10.55s
                        Total time: 3563.91s
                               ETA: 1192391.5s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.415s, learning 0.169s)
               Value function loss: 5.8483
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 170.67
               Mean episode length: 190.13
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 10.58s
                        Total time: 3574.50s
                               ETA: 1191921.2s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.476s, learning 0.159s)
               Value function loss: 5.9368
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 178.13
               Mean episode length: 195.74
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 10.63s
                        Total time: 3585.13s
                               ETA: 1191470.4s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.441s, learning 0.205s)
               Value function loss: 5.4744
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 183.17
               Mean episode length: 197.99
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 10.65s
                        Total time: 3595.78s
                               ETA: 1191026.3s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.623s, learning 0.165s)
               Value function loss: 6.8358
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 181.61
               Mean episode length: 198.87
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 10.79s
                        Total time: 3606.56s
                               ETA: 1190632.1s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.367s, learning 0.161s)
               Value function loss: 7.0014
                    Surrogate loss: 0.0024
             Mean action noise std: 0.79
                       Mean reward: 191.87
               Mean episode length: 209.89
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 10.53s
                        Total time: 3617.09s
                               ETA: 1190154.7s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.459s, learning 0.158s)
               Value function loss: 6.5873
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 202.05
               Mean episode length: 218.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 10.62s
                        Total time: 3627.71s
                               ETA: 1189709.7s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.807s, learning 0.163s)
               Value function loss: 6.9299
                    Surrogate loss: 0.0040
             Mean action noise std: 0.79
                       Mean reward: 214.07
               Mean episode length: 230.95
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 10.97s
                        Total time: 3638.68s
                               ETA: 1189383.1s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.436s, learning 0.216s)
               Value function loss: 9.6222
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 218.84
               Mean episode length: 237.52
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 10.65s
                        Total time: 3649.33s
                               ETA: 1188954.7s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.452s, learning 0.207s)
               Value function loss: 15.1973
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 226.44
               Mean episode length: 240.82
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 10.66s
                        Total time: 3659.99s
                               ETA: 1188531.5s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.060s, learning 0.208s)
               Value function loss: 17.7107
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 229.73
               Mean episode length: 241.84
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 11.27s
                        Total time: 3671.26s
                               ETA: 1188308.0s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.597s, learning 0.161s)
               Value function loss: 11.9362
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 238.34
               Mean episode length: 246.38
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 10.76s
                        Total time: 3682.02s
                               ETA: 1187921.3s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.570s, learning 0.235s)
               Value function loss: 14.5430
                    Surrogate loss: -0.0191
             Mean action noise std: 0.79
                       Mean reward: 231.83
               Mean episode length: 245.69
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 10.81s
                        Total time: 3692.82s
                               ETA: 1187552.1s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.338s, learning 0.209s)
               Value function loss: 10.8603
                    Surrogate loss: 0.0104
             Mean action noise std: 0.79
                       Mean reward: 237.93
               Mean episode length: 247.41
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 10.55s
                        Total time: 3703.37s
                               ETA: 1187102.5s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.441s, learning 0.256s)
               Value function loss: 12.6128
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 237.30
               Mean episode length: 247.51
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 10.70s
                        Total time: 3714.07s
                               ETA: 1186703.7s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.413s, learning 0.160s)
               Value function loss: 21.4042
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 235.17
               Mean episode length: 247.96
                  Mean reward/step: 0.80
       Mean episode length/episode: 6.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 10.57s
                        Total time: 3724.64s
                               ETA: 1186267.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.374s, learning 0.173s)
               Value function loss: 1.3121
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 234.82
               Mean episode length: 247.56
                  Mean reward/step: 0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 10.55s
                        Total time: 3735.19s
                               ETA: 1185826.5s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.340s, learning 0.159s)
               Value function loss: 0.8666
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 234.36
               Mean episode length: 246.93
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 10.50s
                        Total time: 3745.69s
                               ETA: 1185372.7s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.666s, learning 0.200s)
               Value function loss: 0.9246
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: 230.75
               Mean episode length: 243.90
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 10.87s
                        Total time: 3756.55s
                               ETA: 1185037.7s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.391s, learning 0.193s)
               Value function loss: 0.9318
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 228.75
               Mean episode length: 242.39
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 10.58s
                        Total time: 3767.14s
                               ETA: 1184616.0s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.472s, learning 0.182s)
               Value function loss: 0.9210
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 226.66
               Mean episode length: 240.95
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 10.65s
                        Total time: 3777.79s
                               ETA: 1184218.6s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.552s, learning 0.183s)
               Value function loss: 0.6431
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 226.48
               Mean episode length: 240.92
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 10.74s
                        Total time: 3788.53s
                               ETA: 1183849.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.654s, learning 0.177s)
               Value function loss: 0.9411
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 223.53
               Mean episode length: 238.44
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 10.83s
                        Total time: 3799.36s
                               ETA: 1183511.5s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.682s, learning 0.186s)
               Value function loss: 0.5919
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 224.32
               Mean episode length: 238.44
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 10.87s
                        Total time: 3810.22s
                               ETA: 1183187.6s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.896s, learning 0.171s)
               Value function loss: 1.3739
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 222.10
               Mean episode length: 237.69
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 11.07s
                        Total time: 3821.29s
                               ETA: 1182927.0s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.428s, learning 0.193s)
               Value function loss: 1.5977
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 217.38
               Mean episode length: 235.34
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 10.62s
                        Total time: 3831.91s
                               ETA: 1182530.6s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.470s, learning 0.163s)
               Value function loss: 1.5802
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 213.31
               Mean episode length: 233.89
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 10.63s
                        Total time: 3842.55s
                               ETA: 1182140.1s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.479s, learning 0.168s)
               Value function loss: 2.1456
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 206.63
               Mean episode length: 232.12
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 10.65s
                        Total time: 3853.19s
                               ETA: 1181756.3s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.534s, learning 0.167s)
               Value function loss: 2.7695
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 211.36
               Mean episode length: 239.56
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 10.70s
                        Total time: 3863.89s
                               ETA: 1181391.3s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.807s, learning 0.196s)
               Value function loss: 3.3789
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 214.66
               Mean episode length: 243.94
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 11.00s
                        Total time: 3874.90s
                               ETA: 1181120.5s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.488s, learning 0.167s)
               Value function loss: 3.6899
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 216.22
               Mean episode length: 244.08
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 10.65s
                        Total time: 3885.55s
                               ETA: 1180745.4s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.533s, learning 0.174s)
               Value function loss: 4.1746
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 213.39
               Mean episode length: 242.39
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 10.71s
                        Total time: 3896.26s
                               ETA: 1180388.3s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.645s, learning 0.179s)
               Value function loss: 3.8928
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 211.88
               Mean episode length: 242.45
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 10.82s
                        Total time: 3907.08s
                               ETA: 1180068.8s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.162s)
               Value function loss: 4.1387
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 214.95
               Mean episode length: 244.34
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 10.85s
                        Total time: 3917.93s
                               ETA: 1179758.0s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.728s, learning 0.164s)
               Value function loss: 5.3514
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 211.17
               Mean episode length: 241.87
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 10.89s
                        Total time: 3928.82s
                               ETA: 1179462.6s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.776s, learning 0.160s)
               Value function loss: 4.6867
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 205.38
               Mean episode length: 238.91
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 10.94s
                        Total time: 3939.76s
                               ETA: 1179182.0s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.423s, learning 0.174s)
               Value function loss: 5.7023
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 208.18
               Mean episode length: 242.07
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 10.60s
                        Total time: 3950.35s
                               ETA: 1178801.8s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.496s, learning 0.169s)
               Value function loss: 5.2814
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 212.37
               Mean episode length: 244.55
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 10.66s
                        Total time: 3961.02s
                               ETA: 1178444.1s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.579s, learning 0.168s)
               Value function loss: 7.0706
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: 205.65
               Mean episode length: 240.38
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 10.75s
                        Total time: 3971.76s
                               ETA: 1178112.6s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.356s, learning 0.161s)
               Value function loss: 7.1607
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 210.25
               Mean episode length: 242.41
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 10.52s
                        Total time: 3982.28s
                               ETA: 1177715.1s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.584s, learning 0.167s)
               Value function loss: 7.6998
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 210.40
               Mean episode length: 245.24
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 10.75s
                        Total time: 3993.03s
                               ETA: 1177389.1s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.546s, learning 0.185s)
               Value function loss: 6.4631
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 212.32
               Mean episode length: 246.09
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 10.73s
                        Total time: 4003.76s
                               ETA: 1177058.9s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.642s, learning 0.178s)
               Value function loss: 9.4903
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 213.56
               Mean episode length: 246.43
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 10.82s
                        Total time: 4014.58s
                               ETA: 1176756.6s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.599s, learning 0.189s)
               Value function loss: 10.2367
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 210.11
               Mean episode length: 247.39
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 10.79s
                        Total time: 4025.37s
                               ETA: 1176446.9s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.448s, learning 0.171s)
               Value function loss: 7.9987
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 210.61
               Mean episode length: 247.98
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 10.62s
                        Total time: 4035.99s
                               ETA: 1176089.8s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.722s, learning 0.174s)
               Value function loss: 7.6386
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 212.13
               Mean episode length: 248.56
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 10.90s
                        Total time: 4046.89s
                               ETA: 1175814.9s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1611 steps/s (collection: 9.991s, learning 0.176s)
               Value function loss: 17.1796
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 208.21
               Mean episode length: 250.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 10.17s
                        Total time: 4057.05s
                               ETA: 1175330.7s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.418s, learning 0.277s)
               Value function loss: 1.3229
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 206.68
               Mean episode length: 248.66
                  Mean reward/step: 0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 10.69s
                        Total time: 4067.75s
                               ETA: 1175001.5s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.988s, learning 0.171s)
               Value function loss: 0.7794
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 205.20
               Mean episode length: 247.13
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 11.16s
                        Total time: 4078.91s
                               ETA: 1174807.7s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.594s, learning 0.166s)
               Value function loss: 0.7230
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 204.76
               Mean episode length: 247.13
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 10.76s
                        Total time: 4089.67s
                               ETA: 1174500.7s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.423s, learning 0.264s)
               Value function loss: 0.8687
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 203.36
               Mean episode length: 246.49
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 10.69s
                        Total time: 4100.35s
                               ETA: 1174174.3s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.472s, learning 0.164s)
               Value function loss: 0.7852
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 202.07
               Mean episode length: 245.65
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 10.64s
                        Total time: 4110.99s
                               ETA: 1173835.2s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.478s, learning 0.206s)
               Value function loss: 0.9245
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 199.90
               Mean episode length: 243.24
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 10.68s
                        Total time: 4121.68s
                               ETA: 1173511.6s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.571s, learning 0.164s)
               Value function loss: 1.0673
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 195.54
               Mean episode length: 239.83
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 10.74s
                        Total time: 4132.41s
                               ETA: 1173204.3s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.520s, learning 0.170s)
               Value function loss: 1.3025
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 192.98
               Mean episode length: 237.87
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 10.69s
                        Total time: 4143.10s
                               ETA: 1172885.8s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.491s, learning 0.159s)
               Value function loss: 1.6430
                    Surrogate loss: 0.0047
             Mean action noise std: 0.79
                       Mean reward: 186.22
               Mean episode length: 233.85
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 10.65s
                        Total time: 4153.75s
                               ETA: 1172557.6s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.393s, learning 0.177s)
               Value function loss: 1.9186
                    Surrogate loss: 0.0089
             Mean action noise std: 0.79
                       Mean reward: 179.06
               Mean episode length: 230.74
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 10.57s
                        Total time: 4164.32s
                               ETA: 1172209.0s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.497s, learning 0.164s)
               Value function loss: 2.3919
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 173.72
               Mean episode length: 228.12
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 10.66s
                        Total time: 4174.98s
                               ETA: 1171887.7s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.546s, learning 0.186s)
               Value function loss: 2.3620
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 171.54
               Mean episode length: 229.51
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 10.73s
                        Total time: 4185.71s
                               ETA: 1171588.0s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.372s, learning 0.159s)
               Value function loss: 2.9694
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 170.80
               Mean episode length: 232.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 10.53s
                        Total time: 4196.24s
                               ETA: 1171233.8s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.464s, learning 0.157s)
               Value function loss: 3.1128
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 167.28
               Mean episode length: 232.60
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 10.62s
                        Total time: 4206.86s
                               ETA: 1170906.5s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.505s, learning 0.161s)
               Value function loss: 3.9518
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 169.38
               Mean episode length: 236.41
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 10.67s
                        Total time: 4217.53s
                               ETA: 1170593.5s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.448s, learning 0.161s)
               Value function loss: 3.2000
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 170.26
               Mean episode length: 239.13
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 10.61s
                        Total time: 4228.14s
                               ETA: 1170266.5s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.797s, learning 0.163s)
               Value function loss: 3.9353
                    Surrogate loss: 0.0240
             Mean action noise std: 0.79
                       Mean reward: 159.26
               Mean episode length: 233.55
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 10.96s
                        Total time: 4239.10s
                               ETA: 1170038.2s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.553s, learning 0.164s)
               Value function loss: 3.7493
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 153.46
               Mean episode length: 231.24
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 10.72s
                        Total time: 4249.82s
                               ETA: 1169744.3s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.632s, learning 0.189s)
               Value function loss: 3.3731
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 157.45
               Mean episode length: 233.38
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 10.82s
                        Total time: 4260.64s
                               ETA: 1169480.3s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.166s)
               Value function loss: 3.4453
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 147.12
               Mean episode length: 227.11
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 10.75s
                        Total time: 4271.39s
                               ETA: 1169199.3s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.686s, learning 0.175s)
               Value function loss: 3.7392
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 141.72
               Mean episode length: 225.55
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 10.86s
                        Total time: 4282.25s
                               ETA: 1168949.2s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.788s, learning 0.190s)
               Value function loss: 3.7625
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 141.21
               Mean episode length: 226.05
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 10.98s
                        Total time: 4293.23s
                               ETA: 1168731.9s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.740s, learning 0.208s)
               Value function loss: 4.5236
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 142.30
               Mean episode length: 227.07
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 10.95s
                        Total time: 4304.18s
                               ETA: 1168507.8s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.355s, learning 0.161s)
               Value function loss: 3.8847
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 137.93
               Mean episode length: 224.41
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 10.52s
                        Total time: 4314.69s
                               ETA: 1168168.0s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.340s, learning 0.157s)
               Value function loss: 3.8254
                    Surrogate loss: -0.0225
             Mean action noise std: 0.79
                       Mean reward: 138.37
               Mean episode length: 225.78
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 10.50s
                        Total time: 4325.19s
                               ETA: 1167824.8s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.546s, learning 0.164s)
               Value function loss: 4.0135
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 135.08
               Mean episode length: 227.70
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 10.71s
                        Total time: 4335.90s
                               ETA: 1167540.6s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.704s, learning 0.173s)
               Value function loss: 3.5619
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 135.39
               Mean episode length: 230.44
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 10.88s
                        Total time: 4346.78s
                               ETA: 1167302.8s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.634s, learning 0.175s)
               Value function loss: 3.8209
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 137.30
               Mean episode length: 232.63
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 10.81s
                        Total time: 4357.59s
                               ETA: 1167048.3s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.609s, learning 0.159s)
               Value function loss: 3.0370
                    Surrogate loss: -0.0212
             Mean action noise std: 0.79
                       Mean reward: 149.18
               Mean episode length: 238.06
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 10.77s
                        Total time: 4368.35s
                               ETA: 1166783.9s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.466s, learning 0.173s)
               Value function loss: 2.4753
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 146.71
               Mean episode length: 242.05
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 10.64s
                        Total time: 4378.99s
                               ETA: 1166486.5s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.441s, learning 0.192s)
               Value function loss: 3.2741
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 155.73
               Mean episode length: 248.35
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 10.63s
                        Total time: 4389.63s
                               ETA: 1166189.0s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.712s, learning 0.175s)
               Value function loss: 0.8782
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 153.19
               Mean episode length: 245.69
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 10.89s
                        Total time: 4400.51s
                               ETA: 1165960.4s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.605s, learning 0.195s)
               Value function loss: 0.8880
                    Surrogate loss: -0.0172
             Mean action noise std: 0.79
                       Mean reward: 152.60
               Mean episode length: 245.11
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 10.80s
                        Total time: 4411.31s
                               ETA: 1165709.9s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.740s, learning 0.159s)
               Value function loss: 0.8631
                    Surrogate loss: -0.0304
             Mean action noise std: 0.79
                       Mean reward: 150.66
               Mean episode length: 243.19
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 10.90s
                        Total time: 4422.21s
                               ETA: 1165486.7s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.475s, learning 0.168s)
               Value function loss: 1.2351
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 149.82
               Mean episode length: 240.69
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 10.64s
                        Total time: 4432.85s
                               ETA: 1165197.4s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.736s, learning 0.159s)
               Value function loss: 1.0877
                    Surrogate loss: -0.0288
             Mean action noise std: 0.79
                       Mean reward: 147.40
               Mean episode length: 238.82
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 10.90s
                        Total time: 4443.75s
                               ETA: 1164975.7s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.397s, learning 0.184s)
               Value function loss: 1.1849
                    Surrogate loss: -0.0292
             Mean action noise std: 0.79
                       Mean reward: 144.80
               Mean episode length: 236.44
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 10.58s
                        Total time: 4454.33s
                               ETA: 1164673.2s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.620s, learning 0.169s)
               Value function loss: 1.1707
                    Surrogate loss: -0.0244
             Mean action noise std: 0.79
                       Mean reward: 144.01
               Mean episode length: 236.09
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 10.79s
                        Total time: 4465.12s
                               ETA: 1164426.2s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.457s, learning 0.162s)
               Value function loss: 1.2665
                    Surrogate loss: -0.0222
             Mean action noise std: 0.79
                       Mean reward: 141.22
               Mean episode length: 234.91
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 10.62s
                        Total time: 4475.74s
                               ETA: 1164136.1s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.601s, learning 0.184s)
               Value function loss: 1.3834
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 138.61
               Mean episode length: 233.25
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 10.79s
                        Total time: 4486.52s
                               ETA: 1163890.7s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1567 steps/s (collection: 10.285s, learning 0.165s)
               Value function loss: 1.5108
                    Surrogate loss: -0.0250
             Mean action noise std: 0.79
                       Mean reward: 133.80
               Mean episode length: 231.83
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 10.45s
                        Total time: 4496.97s
                               ETA: 1163559.6s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.345s, learning 0.178s)
               Value function loss: 1.7888
                    Surrogate loss: -0.0228
             Mean action noise std: 0.79
                       Mean reward: 138.78
               Mean episode length: 235.98
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 10.52s
                        Total time: 4507.50s
                               ETA: 1163249.4s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.470s, learning 0.201s)
               Value function loss: 1.7112
                    Surrogate loss: -0.0321
             Mean action noise std: 0.79
                       Mean reward: 142.59
               Mean episode length: 241.49
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 10.67s
                        Total time: 4518.17s
                               ETA: 1162978.5s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1595 steps/s (collection: 10.097s, learning 0.170s)
               Value function loss: 1.9417
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 145.46
               Mean episode length: 245.73
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 10.27s
                        Total time: 4528.43s
                               ETA: 1162605.3s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.719s, learning 0.167s)
               Value function loss: 1.8637
                    Surrogate loss: -0.0295
             Mean action noise std: 0.79
                       Mean reward: 144.51
               Mean episode length: 247.13
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 10.89s
                        Total time: 4539.32s
                               ETA: 1162392.6s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.431s, learning 0.291s)
               Value function loss: 1.8835
                    Surrogate loss: -0.0316
             Mean action noise std: 0.79
                       Mean reward: 138.68
               Mean episode length: 245.29
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 10.72s
                        Total time: 4550.04s
                               ETA: 1162139.1s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.596s, learning 0.172s)
               Value function loss: 2.2967
                    Surrogate loss: -0.0301
             Mean action noise std: 0.79
                       Mean reward: 135.74
               Mean episode length: 244.98
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 10.77s
                        Total time: 4560.81s
                               ETA: 1161898.4s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.655s, learning 0.165s)
               Value function loss: 2.5507
                    Surrogate loss: -0.0251
             Mean action noise std: 0.79
                       Mean reward: 144.45
               Mean episode length: 248.10
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 10.82s
                        Total time: 4571.63s
                               ETA: 1161672.0s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.786s, learning 0.172s)
               Value function loss: 2.5110
                    Surrogate loss: -0.0318
             Mean action noise std: 0.79
                       Mean reward: 143.31
               Mean episode length: 249.47
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 10.96s
                        Total time: 4582.59s
                               ETA: 1161481.8s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.534s, learning 0.159s)
               Value function loss: 2.8151
                    Surrogate loss: -0.0252
             Mean action noise std: 0.79
                       Mean reward: 141.37
               Mean episode length: 247.77
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 10.69s
                        Total time: 4593.28s
                               ETA: 1161225.5s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.551s, learning 0.164s)
               Value function loss: 2.8175
                    Surrogate loss: -0.0282
             Mean action noise std: 0.79
                       Mean reward: 149.20
               Mean episode length: 248.40
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 10.71s
                        Total time: 4603.99s
                               ETA: 1160976.0s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.579s, learning 0.206s)
               Value function loss: 2.9920
                    Surrogate loss: -0.0308
             Mean action noise std: 0.79
                       Mean reward: 156.58
               Mean episode length: 248.46
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 10.78s
                        Total time: 4614.78s
                               ETA: 1160745.3s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.467s, learning 0.161s)
               Value function loss: 3.3446
                    Surrogate loss: -0.0237
             Mean action noise std: 0.79
                       Mean reward: 165.27
               Mean episode length: 249.13
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 10.63s
                        Total time: 4625.41s
                               ETA: 1160476.2s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.447s, learning 0.166s)
               Value function loss: 4.1836
                    Surrogate loss: -0.0215
             Mean action noise std: 0.79
                       Mean reward: 166.18
               Mean episode length: 248.99
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 10.61s
                        Total time: 4636.02s
                               ETA: 1160204.8s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.562s, learning 0.173s)
               Value function loss: 4.7008
                    Surrogate loss: -0.0263
             Mean action noise std: 0.79
                       Mean reward: 161.18
               Mean episode length: 245.19
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 10.73s
                        Total time: 4646.76s
                               ETA: 1159965.2s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1564 steps/s (collection: 10.306s, learning 0.169s)
               Value function loss: 4.3901
                    Surrogate loss: -0.0274
             Mean action noise std: 0.79
                       Mean reward: 168.06
               Mean episode length: 247.71
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 10.47s
                        Total time: 4657.23s
                               ETA: 1159661.9s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.391s, learning 0.186s)
               Value function loss: 5.0264
                    Surrogate loss: -0.0186
             Mean action noise std: 0.79
                       Mean reward: 170.03
               Mean episode length: 246.69
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 10.58s
                        Total time: 4667.81s
                               ETA: 1159385.5s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.531s, learning 0.170s)
               Value function loss: 4.2836
                    Surrogate loss: -0.0252
             Mean action noise std: 0.79
                       Mean reward: 172.98
               Mean episode length: 247.73
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 10.70s
                        Total time: 4678.51s
                               ETA: 1159141.0s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.606s, learning 0.219s)
               Value function loss: 4.8729
                    Surrogate loss: -0.0268
             Mean action noise std: 0.79
                       Mean reward: 174.54
               Mean episode length: 243.37
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 10.83s
                        Total time: 4689.33s
                               ETA: 1158928.5s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.384s, learning 0.161s)
               Value function loss: 3.6547
                    Surrogate loss: -0.0208
             Mean action noise std: 0.79
                       Mean reward: 188.60
               Mean episode length: 249.77
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 10.54s
                        Total time: 4699.88s
                               ETA: 1158647.8s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.379s, learning 0.176s)
               Value function loss: 2.8110
                    Surrogate loss: -0.0197
             Mean action noise std: 0.79
                       Mean reward: 190.09
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 10.55s
                        Total time: 4710.43s
                               ETA: 1158370.8s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.653s, learning 0.188s)
               Value function loss: 2.3512
                    Surrogate loss: -0.0205
             Mean action noise std: 0.79
                       Mean reward: 188.28
               Mean episode length: 250.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 10.84s
                        Total time: 4721.27s
                               ETA: 1158165.5s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.554s, learning 0.168s)
               Value function loss: 1.6177
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 191.97
               Mean episode length: 250.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 10.72s
                        Total time: 4732.00s
                               ETA: 1157932.1s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.593s, learning 0.188s)
               Value function loss: 0.8337
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 188.48
               Mean episode length: 247.80
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 10.78s
                        Total time: 4742.78s
                               ETA: 1157714.1s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.505s, learning 0.163s)
               Value function loss: 0.5892
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 188.15
               Mean episode length: 247.80
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 10.67s
                        Total time: 4753.44s
                               ETA: 1157469.5s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.529s, learning 0.160s)
               Value function loss: 0.6793
                    Surrogate loss: -0.0190
             Mean action noise std: 0.79
                       Mean reward: 187.11
               Mean episode length: 247.80
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 10.69s
                        Total time: 4764.13s
                               ETA: 1157231.1s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.474s, learning 0.172s)
               Value function loss: 0.9022
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 189.19
               Mean episode length: 247.80
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 10.65s
                        Total time: 4774.78s
                               ETA: 1156983.4s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.448s, learning 0.180s)
               Value function loss: 1.0412
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 190.46
               Mean episode length: 247.80
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 10.63s
                        Total time: 4785.41s
                               ETA: 1156732.5s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.552s, learning 0.192s)
               Value function loss: 0.9711
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 190.06
               Mean episode length: 247.80
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 10.74s
                        Total time: 4796.15s
                               ETA: 1156510.8s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.390s, learning 0.171s)
               Value function loss: 1.1905
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 192.80
               Mean episode length: 247.80
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 10.56s
                        Total time: 4806.71s
                               ETA: 1156245.9s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.419s, learning 0.171s)
               Value function loss: 1.3027
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 193.04
               Mean episode length: 247.80
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 10.59s
                        Total time: 4817.30s
                               ETA: 1155989.4s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.780s, learning 0.163s)
               Value function loss: 1.5711
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 193.20
               Mean episode length: 247.80
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 10.94s
                        Total time: 4828.24s
                               ETA: 1155818.6s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.729s, learning 0.173s)
               Value function loss: 2.0006
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 201.67
               Mean episode length: 247.80
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 10.90s
                        Total time: 4839.14s
                               ETA: 1155638.8s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.005s, learning 0.172s)
               Value function loss: 2.0484
                    Surrogate loss: -0.0178
             Mean action noise std: 0.79
                       Mean reward: 213.27
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 11.18s
                        Total time: 4850.32s
                               ETA: 1155525.1s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.519s, learning 0.164s)
               Value function loss: 2.1034
                    Surrogate loss: -0.0239
             Mean action noise std: 0.79
                       Mean reward: 215.75
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 10.68s
                        Total time: 4861.00s
                               ETA: 1155294.7s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.595s, learning 0.263s)
               Value function loss: 2.3894
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 218.49
               Mean episode length: 250.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 10.86s
                        Total time: 4871.86s
                               ETA: 1155106.9s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.477s, learning 0.169s)
               Value function loss: 2.1551
                    Surrogate loss: -0.0214
             Mean action noise std: 0.79
                       Mean reward: 212.77
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 10.65s
                        Total time: 4882.51s
                               ETA: 1154869.9s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.545s, learning 0.167s)
               Value function loss: 2.9942
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 206.61
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 10.71s
                        Total time: 4893.22s
                               ETA: 1154649.3s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.603s, learning 0.175s)
               Value function loss: 3.4444
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 211.26
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 10.78s
                        Total time: 4904.00s
                               ETA: 1154445.1s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.604s, learning 0.167s)
               Value function loss: 3.6659
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 208.71
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 10.77s
                        Total time: 4914.77s
                               ETA: 1154240.3s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.631s, learning 0.169s)
               Value function loss: 4.4717
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 213.10
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 10.80s
                        Total time: 4925.57s
                               ETA: 1154043.4s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.761s, learning 0.189s)
               Value function loss: 4.3873
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 210.60
               Mean episode length: 250.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 10.95s
                        Total time: 4936.52s
                               ETA: 1153882.4s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.659s, learning 0.205s)
               Value function loss: 4.7951
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 217.54
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 10.86s
                        Total time: 4947.38s
                               ETA: 1153702.0s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.616s, learning 0.190s)
               Value function loss: 5.8893
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 212.75
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 10.81s
                        Total time: 4958.19s
                               ETA: 1153508.8s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.553s, learning 0.164s)
               Value function loss: 7.9361
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 205.28
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 10.72s
                        Total time: 4968.91s
                               ETA: 1153295.9s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.339s, learning 0.160s)
               Value function loss: 6.8457
                    Surrogate loss: -0.0199
             Mean action noise std: 0.79
                       Mean reward: 212.40
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 10.50s
                        Total time: 4979.41s
                               ETA: 1153033.6s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.772s, learning 0.160s)
               Value function loss: 7.4698
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 205.02
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 10.93s
                        Total time: 4990.34s
                               ETA: 1152872.3s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.820s, learning 0.273s)
               Value function loss: 7.8731
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 208.14
               Mean episode length: 247.74
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 11.09s
                        Total time: 5001.43s
                               ETA: 1152748.8s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.553s, learning 0.168s)
               Value function loss: 7.4434
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 208.21
               Mean episode length: 250.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 10.72s
                        Total time: 5012.15s
                               ETA: 1152540.2s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.684s, learning 0.163s)
               Value function loss: 8.4683
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 209.83
               Mean episode length: 250.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 10.85s
                        Total time: 5023.00s
                               ETA: 1152361.4s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.625s, learning 0.160s)
               Value function loss: 5.1213
                    Surrogate loss: -0.0212
             Mean action noise std: 0.79
                       Mean reward: 217.56
               Mean episode length: 250.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 10.79s
                        Total time: 5033.78s
                               ETA: 1152169.3s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.890s, learning 0.170s)
               Value function loss: 4.2779
                    Surrogate loss: -0.0219
             Mean action noise std: 0.79
                       Mean reward: 217.98
               Mean episode length: 250.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 11.06s
                        Total time: 5044.84s
                               ETA: 1152040.7s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1573 steps/s (collection: 10.192s, learning 0.223s)
               Value function loss: 2.9560
                    Surrogate loss: -0.0267
             Mean action noise std: 0.79
                       Mean reward: 214.49
               Mean episode length: 250.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 10.41s
                        Total time: 5055.26s
                               ETA: 1151765.8s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.705s, learning 0.169s)
               Value function loss: 2.5517
                    Surrogate loss: -0.0223
             Mean action noise std: 0.79
                       Mean reward: 220.59
               Mean episode length: 250.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 10.87s
                        Total time: 5066.13s
                               ETA: 1151596.5s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.539s, learning 0.191s)
               Value function loss: 0.5941
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 221.88
               Mean episode length: 250.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 10.73s
                        Total time: 5076.86s
                               ETA: 1151395.3s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.499s, learning 0.172s)
               Value function loss: 0.5262
                    Surrogate loss: -0.0189
             Mean action noise std: 0.79
                       Mean reward: 222.22
               Mean episode length: 250.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 10.67s
                        Total time: 5087.53s
                               ETA: 1151181.6s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.549s, learning 0.168s)
               Value function loss: 0.7200
                    Surrogate loss: -0.0203
             Mean action noise std: 0.79
                       Mean reward: 220.68
               Mean episode length: 247.77
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 10.72s
                        Total time: 5098.25s
                               ETA: 1150979.0s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.674s, learning 0.162s)
               Value function loss: 1.0126
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 220.16
               Mean episode length: 247.77
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 10.84s
                        Total time: 5109.09s
                               ETA: 1150804.2s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.667s, learning 0.180s)
               Value function loss: 0.7526
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: 221.28
               Mean episode length: 247.77
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 10.85s
                        Total time: 5119.93s
                               ETA: 1150632.6s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.359s, learning 0.169s)
               Value function loss: 1.0049
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 225.23
               Mean episode length: 247.77
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 10.53s
                        Total time: 5130.46s
                               ETA: 1150390.2s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.611s, learning 0.166s)
               Value function loss: 0.7206
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 225.21
               Mean episode length: 247.77
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 10.78s
                        Total time: 5141.24s
                               ETA: 1150204.6s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.430s, learning 0.163s)
               Value function loss: 1.1762
                    Surrogate loss: -0.0180
             Mean action noise std: 0.79
                       Mean reward: 228.40
               Mean episode length: 247.77
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 10.59s
                        Total time: 5151.83s
                               ETA: 1149978.8s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.407s, learning 0.193s)
               Value function loss: 1.7032
                    Surrogate loss: -0.0224
             Mean action noise std: 0.79
                       Mean reward: 232.79
               Mean episode length: 247.77
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 10.60s
                        Total time: 5162.43s
                               ETA: 1149755.2s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.467s, learning 0.165s)
               Value function loss: 2.1560
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 238.16
               Mean episode length: 247.77
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 10.63s
                        Total time: 5173.06s
                               ETA: 1149540.0s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.478s, learning 0.166s)
               Value function loss: 2.7125
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 247.09
               Mean episode length: 250.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 10.64s
                        Total time: 5183.71s
                               ETA: 1149328.2s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.582s, learning 0.178s)
               Value function loss: 3.1774
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 245.88
               Mean episode length: 250.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 10.76s
                        Total time: 5194.47s
                               ETA: 1149143.1s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.740s, learning 0.168s)
               Value function loss: 3.0322
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 245.99
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 10.91s
                        Total time: 5205.38s
                               ETA: 1148991.3s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.677s, learning 0.159s)
               Value function loss: 3.1325
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 245.81
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 10.84s
                        Total time: 5216.21s
                               ETA: 1148824.3s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.566s, learning 0.164s)
               Value function loss: 4.2937
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 249.20
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 10.73s
                        Total time: 5226.94s
                               ETA: 1148634.8s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.712s, learning 0.195s)
               Value function loss: 5.6265
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 248.30
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 10.91s
                        Total time: 5237.85s
                               ETA: 1148484.9s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.437s, learning 0.197s)
               Value function loss: 5.9761
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 246.09
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 10.63s
                        Total time: 5248.48s
                               ETA: 1148275.8s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.517s, learning 0.161s)
               Value function loss: 8.1588
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 246.09
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 10.68s
                        Total time: 5259.16s
                               ETA: 1148077.2s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.667s, learning 0.160s)
               Value function loss: 7.0701
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 252.37
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 10.83s
                        Total time: 5269.99s
                               ETA: 1147912.0s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.017s, learning 0.184s)
               Value function loss: 8.5684
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 241.65
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 11.20s
                        Total time: 5281.19s
                               ETA: 1147828.7s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.457s, learning 0.164s)
               Value function loss: 10.0728
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 246.97
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 10.62s
                        Total time: 5291.81s
                               ETA: 1147619.8s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.808s, learning 0.168s)
               Value function loss: 11.9669
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 250.20
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 10.98s
                        Total time: 5302.79s
                               ETA: 1147488.6s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.624s, learning 0.165s)
               Value function loss: 14.3038
                    Surrogate loss: 0.0159
             Mean action noise std: 0.79
                       Mean reward: 248.19
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 10.79s
                        Total time: 5313.58s
                               ETA: 1147317.5s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.622s, learning 0.173s)
               Value function loss: 13.6090
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 249.24
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 10.79s
                        Total time: 5324.37s
                               ETA: 1147148.4s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.676s, learning 0.160s)
               Value function loss: 14.5141
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 252.24
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 10.84s
                        Total time: 5335.21s
                               ETA: 1146988.8s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.720s, learning 0.164s)
               Value function loss: 14.8906
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 258.07
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 10.88s
                        Total time: 5346.09s
                               ETA: 1146840.3s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.696s, learning 0.168s)
               Value function loss: 12.6772
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 262.95
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 10.86s
                        Total time: 5356.96s
                               ETA: 1146688.0s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.778s, learning 0.183s)
               Value function loss: 14.6048
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 254.16
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 10.96s
                        Total time: 5367.92s
                               ETA: 1146557.0s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.171s)
               Value function loss: 14.2996
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 258.36
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 10.70s
                        Total time: 5378.62s
                               ETA: 1146370.7s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.502s, learning 0.262s)
               Value function loss: 8.9347
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 266.39
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 10.76s
                        Total time: 5389.38s
                               ETA: 1146199.0s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.931s, learning 0.186s)
               Value function loss: 9.1865
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 266.94
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 11.12s
                        Total time: 5400.50s
                               ETA: 1146102.8s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.915s, learning 0.161s)
               Value function loss: 1.9653
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 268.55
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 11.08s
                        Total time: 5411.57s
                               ETA: 1145998.5s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.448s, learning 0.164s)
               Value function loss: 1.3180
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 269.29
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 10.61s
                        Total time: 5422.19s
                               ETA: 1145796.4s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.640s, learning 0.191s)
               Value function loss: 1.2526
                    Surrogate loss: 0.0128
             Mean action noise std: 0.79
                       Mean reward: 271.04
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 10.83s
                        Total time: 5433.02s
                               ETA: 1145641.4s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1568 steps/s (collection: 10.274s, learning 0.175s)
               Value function loss: 1.8152
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 273.58
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 10.45s
                        Total time: 5443.47s
                               ETA: 1145406.5s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.496s, learning 0.171s)
               Value function loss: 1.6821
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 277.75
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 10.67s
                        Total time: 5454.13s
                               ETA: 1145218.3s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.419s, learning 0.185s)
               Value function loss: 1.8208
                    Surrogate loss: 0.0162
             Mean action noise std: 0.79
                       Mean reward: 278.99
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 10.60s
                        Total time: 5464.74s
                               ETA: 1145017.7s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.521s, learning 0.167s)
               Value function loss: 1.6491
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 279.44
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 10.69s
                        Total time: 5475.42s
                               ETA: 1144835.4s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.425s, learning 0.191s)
               Value function loss: 2.1408
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 279.96
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 10.62s
                        Total time: 5486.04s
                               ETA: 1144638.9s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.688s, learning 0.161s)
               Value function loss: 3.4805
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 281.33
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 10.85s
                        Total time: 5496.89s
                               ETA: 1144491.6s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.574s, learning 0.175s)
               Value function loss: 3.8816
                    Surrogate loss: 0.0139
             Mean action noise std: 0.79
                       Mean reward: 285.76
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 10.75s
                        Total time: 5507.64s
                               ETA: 1144323.9s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.694s, learning 0.185s)
               Value function loss: 4.2822
                    Surrogate loss: 0.0072
             Mean action noise std: 0.79
                       Mean reward: 285.49
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 10.88s
                        Total time: 5518.52s
                               ETA: 1144184.1s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.636s, learning 0.169s)
               Value function loss: 4.0699
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: 281.31
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 10.81s
                        Total time: 5529.32s
                               ETA: 1144029.5s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.568s, learning 0.161s)
               Value function loss: 4.8865
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: 277.32
               Mean episode length: 249.81
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 10.73s
                        Total time: 5540.05s
                               ETA: 1143859.8s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1558 steps/s (collection: 10.274s, learning 0.239s)
               Value function loss: 4.8347
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 275.97
               Mean episode length: 249.81
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 10.51s
                        Total time: 5550.56s
                               ETA: 1143646.1s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.580s, learning 0.170s)
               Value function loss: 5.0830
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 270.20
               Mean episode length: 249.51
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 10.75s
                        Total time: 5561.31s
                               ETA: 1143482.0s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.808s, learning 0.202s)
               Value function loss: 4.3638
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 264.53
               Mean episode length: 249.70
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 11.01s
                        Total time: 5572.32s
                               ETA: 1143372.1s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.163s)
               Value function loss: 5.4125
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 259.24
               Mean episode length: 249.63
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 10.83s
                        Total time: 5583.15s
                               ETA: 1143224.9s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.164s)
               Value function loss: 5.4079
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 258.81
               Mean episode length: 249.63
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 10.78s
                        Total time: 5593.93s
                               ETA: 1143068.4s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.434s, learning 0.164s)
               Value function loss: 6.2364
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 256.06
               Mean episode length: 249.93
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 10.60s
                        Total time: 5604.53s
                               ETA: 1142875.7s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.031s, learning 0.162s)
               Value function loss: 5.0863
                    Surrogate loss: -0.0241
             Mean action noise std: 0.79
                       Mean reward: 254.96
               Mean episode length: 249.93
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 11.19s
                        Total time: 5615.72s
                               ETA: 1142804.8s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.712s, learning 0.163s)
               Value function loss: 6.6221
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 252.84
               Mean episode length: 249.95
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 10.87s
                        Total time: 5626.59s
                               ETA: 1142669.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1574 steps/s (collection: 10.242s, learning 0.166s)
               Value function loss: 7.3997
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 253.65
               Mean episode length: 249.76
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 10.41s
                        Total time: 5637.00s
                               ETA: 1142440.3s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.617s, learning 0.182s)
               Value function loss: 10.0057
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 253.43
               Mean episode length: 250.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 10.80s
                        Total time: 5647.80s
                               ETA: 1142291.1s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.475s, learning 0.173s)
               Value function loss: 8.7853
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 257.16
               Mean episode length: 250.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 10.65s
                        Total time: 5658.45s
                               ETA: 1142111.9s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.411s, learning 0.161s)
               Value function loss: 8.3597
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 268.16
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 10.57s
                        Total time: 5669.02s
                               ETA: 1141918.0s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.396s, learning 0.161s)
               Value function loss: 9.2750
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 266.39
               Mean episode length: 250.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 10.56s
                        Total time: 5679.58s
                               ETA: 1141721.9s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.688s, learning 0.179s)
               Value function loss: 7.8194
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 271.20
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 10.87s
                        Total time: 5690.45s
                               ETA: 1141588.5s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.160s)
               Value function loss: 8.8007
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 271.74
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 10.77s
                        Total time: 5701.22s
                               ETA: 1141437.1s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.424s, learning 0.192s)
               Value function loss: 5.9796
                    Surrogate loss: -0.0217
             Mean action noise std: 0.79
                       Mean reward: 285.01
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 10.62s
                        Total time: 5711.84s
                               ETA: 1141254.9s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.653s, learning 0.191s)
               Value function loss: 3.9798
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 289.29
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 10.84s
                        Total time: 5722.68s
                               ETA: 1141118.8s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.669s, learning 0.188s)
               Value function loss: 5.8419
                    Surrogate loss: -0.0196
             Mean action noise std: 0.79
                       Mean reward: 299.00
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 10.86s
                        Total time: 5733.54s
                               ETA: 1140985.7s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.526s, learning 0.173s)
               Value function loss: 1.3509
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 296.39
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 10.70s
                        Total time: 5744.24s
                               ETA: 1140821.7s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.551s, learning 0.210s)
               Value function loss: 1.0616
                    Surrogate loss: 0.0042
             Mean action noise std: 0.79
                       Mean reward: 296.99
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 10.76s
                        Total time: 5755.00s
                               ETA: 1140670.5s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.574s, learning 0.172s)
               Value function loss: 1.2047
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 299.06
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 10.75s
                        Total time: 5765.74s
                               ETA: 1140517.0s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.858s, learning 0.164s)
               Value function loss: 1.6739
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 302.29
               Mean episode length: 249.99
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 11.02s
                        Total time: 5776.77s
                               ETA: 1140418.6s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.418s, learning 0.172s)
               Value function loss: 1.7905
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 304.52
               Mean episode length: 249.99
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 10.59s
                        Total time: 5787.36s
                               ETA: 1140235.3s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.687s, learning 0.164s)
               Value function loss: 1.9744
                    Surrogate loss: -0.0242
             Mean action noise std: 0.79
                       Mean reward: 304.73
               Mean episode length: 249.99
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 10.85s
                        Total time: 5798.21s
                               ETA: 1140103.9s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.481s, learning 0.167s)
               Value function loss: 2.0270
                    Surrogate loss: -0.0230
             Mean action noise std: 0.79
                       Mean reward: 306.27
               Mean episode length: 249.99
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 10.65s
                        Total time: 5808.86s
                               ETA: 1139933.4s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.529s, learning 0.272s)
               Value function loss: 2.3557
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 308.98
               Mean episode length: 249.99
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 10.80s
                        Total time: 5819.66s
                               ETA: 1139793.3s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.448s, learning 0.165s)
               Value function loss: 2.9433
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 313.27
               Mean episode length: 249.92
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 10.61s
                        Total time: 5830.27s
                               ETA: 1139617.1s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.536s, learning 0.160s)
               Value function loss: 3.3447
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 313.11
               Mean episode length: 249.60
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 10.70s
                        Total time: 5840.97s
                               ETA: 1139457.8s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.587s, learning 0.166s)
               Value function loss: 4.0979
                    Surrogate loss: 0.0048
             Mean action noise std: 0.79
                       Mean reward: 310.84
               Mean episode length: 249.60
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 10.75s
                        Total time: 5851.72s
                               ETA: 1139310.0s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.406s, learning 0.167s)
               Value function loss: 4.6751
                    Surrogate loss: 0.0018
             Mean action noise std: 0.79
                       Mean reward: 308.02
               Mean episode length: 249.60
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 10.57s
                        Total time: 5862.29s
                               ETA: 1139127.8s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.650s, learning 0.166s)
               Value function loss: 4.9499
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 302.24
               Mean episode length: 249.53
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 10.82s
                        Total time: 5873.11s
                               ETA: 1138993.4s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.733s, learning 0.161s)
               Value function loss: 5.0127
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 300.77
               Mean episode length: 249.85
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 10.89s
                        Total time: 5884.00s
                               ETA: 1138874.5s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.507s, learning 0.166s)
               Value function loss: 5.5324
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 299.47
               Mean episode length: 249.61
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 10.67s
                        Total time: 5894.67s
                               ETA: 1138713.5s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.371s, learning 0.174s)
               Value function loss: 6.6370
                    Surrogate loss: -0.0252
             Mean action noise std: 0.79
                       Mean reward: 299.88
               Mean episode length: 249.59
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 10.54s
                        Total time: 5905.22s
                               ETA: 1138528.4s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.584s, learning 0.168s)
               Value function loss: 8.9608
                    Surrogate loss: -0.0178
             Mean action noise std: 0.79
                       Mean reward: 301.41
               Mean episode length: 249.83
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 10.75s
                        Total time: 5915.97s
                               ETA: 1138383.6s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.781s, learning 0.163s)
               Value function loss: 8.9520
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 303.03
               Mean episode length: 249.82
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 10.94s
                        Total time: 5926.91s
                               ETA: 1138276.3s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.737s, learning 0.183s)
               Value function loss: 9.6327
                    Surrogate loss: -0.0228
             Mean action noise std: 0.79
                       Mean reward: 292.95
               Mean episode length: 249.64
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 10.92s
                        Total time: 5937.83s
                               ETA: 1138164.8s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.444s, learning 0.190s)
               Value function loss: 9.3347
                    Surrogate loss: -0.0256
             Mean action noise std: 0.79
                       Mean reward: 306.22
               Mean episode length: 249.29
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 10.63s
                        Total time: 5948.47s
                               ETA: 1137999.1s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.612s, learning 0.184s)
               Value function loss: 10.6591
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 302.75
               Mean episode length: 249.84
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 10.80s
                        Total time: 5959.26s
                               ETA: 1137864.9s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.162s)
               Value function loss: 12.8098
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 314.82
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 10.74s
                        Total time: 5970.00s
                               ETA: 1137719.5s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1582 steps/s (collection: 10.160s, learning 0.195s)
               Value function loss: 16.7488
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 317.90
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 10.36s
                        Total time: 5980.35s
                               ETA: 1137502.3s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.518s, learning 0.185s)
               Value function loss: 18.8293
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 311.32
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 10.70s
                        Total time: 5991.06s
                               ETA: 1137352.0s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.526s, learning 0.167s)
               Value function loss: 13.5580
                    Surrogate loss: -0.0254
             Mean action noise std: 0.79
                       Mean reward: 328.88
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 10.69s
                        Total time: 6001.75s
                               ETA: 1137200.2s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.013s, learning 0.167s)
               Value function loss: 16.0960
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 318.57
               Mean episode length: 249.81
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 11.18s
                        Total time: 6012.93s
                               ETA: 1137141.2s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.521s, learning 0.193s)
               Value function loss: 12.6869
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 328.20
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 10.71s
                        Total time: 6023.64s
                               ETA: 1136994.3s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.429s, learning 0.167s)
               Value function loss: 11.2815
                    Surrogate loss: -0.0262
             Mean action noise std: 0.79
                       Mean reward: 328.78
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 10.60s
                        Total time: 6034.24s
                               ETA: 1136825.7s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.630s, learning 0.176s)
               Value function loss: 9.9301
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 326.23
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 10.81s
                        Total time: 6045.05s
                               ETA: 1136697.3s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.988s, learning 0.164s)
               Value function loss: 7.4214
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 337.37
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 11.15s
                        Total time: 6056.20s
                               ETA: 1136634.2s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.603s, learning 0.171s)
               Value function loss: 5.3229
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 335.54
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 10.77s
                        Total time: 6066.97s
                               ETA: 1136500.5s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.167s)
               Value function loss: 2.7616
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 337.09
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 10.98s
                        Total time: 6077.95s
                               ETA: 1136406.1s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.434s, learning 0.164s)
               Value function loss: 1.4034
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 337.05
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 10.60s
                        Total time: 6088.55s
                               ETA: 1136240.4s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1563 steps/s (collection: 10.308s, learning 0.167s)
               Value function loss: 1.4544
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 337.63
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 10.48s
                        Total time: 6099.03s
                               ETA: 1136052.5s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.510s, learning 0.200s)
               Value function loss: 1.5256
                    Surrogate loss: -0.0244
             Mean action noise std: 0.78
                       Mean reward: 336.20
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 10.71s
                        Total time: 6109.74s
                               ETA: 1135908.8s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.516s, learning 0.187s)
               Value function loss: 2.2748
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 334.47
               Mean episode length: 249.64
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 10.70s
                        Total time: 6120.44s
                               ETA: 1135764.4s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.563s, learning 0.161s)
               Value function loss: 2.1193
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 340.68
               Mean episode length: 249.64
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 10.72s
                        Total time: 6131.17s
                               ETA: 1135624.2s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.483s, learning 0.163s)
               Value function loss: 2.0920
                    Surrogate loss: -0.0249
             Mean action noise std: 0.78
                       Mean reward: 340.39
               Mean episode length: 249.64
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 10.65s
                        Total time: 6141.81s
                               ETA: 1135470.1s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.744s, learning 0.210s)
               Value function loss: 2.5196
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 335.40
               Mean episode length: 249.28
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 10.95s
                        Total time: 6152.76s
                               ETA: 1135373.4s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.396s, learning 0.199s)
               Value function loss: 3.2140
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 334.74
               Mean episode length: 249.28
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 10.60s
                        Total time: 6163.36s
                               ETA: 1135211.0s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.566s, learning 0.162s)
               Value function loss: 3.6014
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 335.93
               Mean episode length: 249.28
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 10.73s
                        Total time: 6174.09s
                               ETA: 1135073.5s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.601s, learning 0.259s)
               Value function loss: 4.5537
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 329.33
               Mean episode length: 249.28
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 10.86s
                        Total time: 6184.95s
                               ETA: 1134960.7s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1568 steps/s (collection: 10.279s, learning 0.164s)
               Value function loss: 4.9598
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 333.83
               Mean episode length: 249.64
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 10.44s
                        Total time: 6195.39s
                               ETA: 1134771.9s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1559 steps/s (collection: 10.322s, learning 0.187s)
               Value function loss: 6.1918
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 332.43
               Mean episode length: 249.69
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 10.51s
                        Total time: 6205.90s
                               ETA: 1134595.8s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.567s, learning 0.167s)
               Value function loss: 6.8294
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 332.20
               Mean episode length: 249.30
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 10.73s
                        Total time: 6216.63s
                               ETA: 1134461.3s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1571 steps/s (collection: 10.257s, learning 0.170s)
               Value function loss: 5.6321
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 334.13
               Mean episode length: 248.99
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 10.43s
                        Total time: 6227.06s
                               ETA: 1134271.6s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.308s, learning 0.183s)
               Value function loss: 7.9552
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 329.54
               Mean episode length: 249.08
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 10.49s
                        Total time: 6237.55s
                               ETA: 1134094.1s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.566s, learning 0.160s)
               Value function loss: 9.5908
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 327.07
               Mean episode length: 248.92
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 10.73s
                        Total time: 6248.28s
                               ETA: 1133959.7s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.697s, learning 0.199s)
               Value function loss: 11.9408
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 326.92
               Mean episode length: 248.53
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 10.90s
                        Total time: 6259.17s
                               ETA: 1133856.6s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.621s, learning 0.161s)
               Value function loss: 12.4960
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 327.78
               Mean episode length: 248.46
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 10.78s
                        Total time: 6269.95s
                               ETA: 1133733.2s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.485s, learning 0.165s)
               Value function loss: 13.6933
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 325.50
               Mean episode length: 247.43
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 10.65s
                        Total time: 6280.61s
                               ETA: 1133586.6s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.907s, learning 0.163s)
               Value function loss: 14.1150
                    Surrogate loss: 0.0022
             Mean action noise std: 0.78
                       Mean reward: 324.07
               Mean episode length: 246.80
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 11.07s
                        Total time: 6291.68s
                               ETA: 1133515.9s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.845s, learning 0.168s)
               Value function loss: 17.2922
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 331.09
               Mean episode length: 247.05
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 11.01s
                        Total time: 6302.69s
                               ETA: 1133435.3s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.860s, learning 0.183s)
               Value function loss: 18.2880
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 331.59
               Mean episode length: 247.81
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 11.04s
                        Total time: 6313.73s
                               ETA: 1133360.2s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.585s, learning 0.159s)
               Value function loss: 17.8302
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 324.32
               Mean episode length: 246.20
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 10.74s
                        Total time: 6324.47s
                               ETA: 1133231.9s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.760s, learning 0.165s)
               Value function loss: 19.4196
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 338.16
               Mean episode length: 247.43
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 10.93s
                        Total time: 6335.40s
                               ETA: 1133136.5s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.706s, learning 0.166s)
               Value function loss: 20.7828
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 347.09
               Mean episode length: 249.40
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 10.87s
                        Total time: 6346.27s
                               ETA: 1133031.9s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.894s, learning 0.168s)
               Value function loss: 17.5506
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 349.29
               Mean episode length: 249.55
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 11.06s
                        Total time: 6357.34s
                               ETA: 1132961.4s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.086s, learning 0.163s)
               Value function loss: 18.6408
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 355.23
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 11.25s
                        Total time: 6368.58s
                               ETA: 1132924.4s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.665s, learning 0.187s)
               Value function loss: 10.5594
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 346.51
               Mean episode length: 249.55
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 10.85s
                        Total time: 6379.44s
                               ETA: 1132817.1s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.387s, learning 0.160s)
               Value function loss: 8.9261
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 347.78
               Mean episode length: 248.99
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 10.55s
                        Total time: 6389.98s
                               ETA: 1132656.0s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.566s, learning 0.189s)
               Value function loss: 6.2924
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 350.89
               Mean episode length: 248.96
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 10.76s
                        Total time: 6400.74s
                               ETA: 1132532.2s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.822s, learning 0.165s)
               Value function loss: 4.8463
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 353.41
               Mean episode length: 247.10
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 10.99s
                        Total time: 6411.73s
                               ETA: 1132449.7s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.672s, learning 0.201s)
               Value function loss: 1.8776
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 352.04
               Mean episode length: 246.55
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 10.87s
                        Total time: 6422.60s
                               ETA: 1132347.3s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.787s, learning 0.187s)
               Value function loss: 1.5677
                    Surrogate loss: -0.0237
             Mean action noise std: 0.78
                       Mean reward: 351.71
               Mean episode length: 246.55
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 10.97s
                        Total time: 6433.57s
                               ETA: 1132263.1s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.588s, learning 0.298s)
               Value function loss: 1.8492
                    Surrogate loss: 0.0023
             Mean action noise std: 0.78
                       Mean reward: 351.98
               Mean episode length: 246.59
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 10.89s
                        Total time: 6444.46s
                               ETA: 1132163.7s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.167s)
               Value function loss: 2.8845
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 346.85
               Mean episode length: 245.08
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 10.90s
                        Total time: 6455.36s
                               ETA: 1132066.9s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.647s, learning 0.193s)
               Value function loss: 2.5596
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 342.96
               Mean episode length: 243.60
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 10.84s
                        Total time: 6466.20s
                               ETA: 1131960.0s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.702s, learning 0.162s)
               Value function loss: 2.4778
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 342.26
               Mean episode length: 243.45
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 10.86s
                        Total time: 6477.06s
                               ETA: 1131857.7s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.708s, learning 0.166s)
               Value function loss: 3.0107
                    Surrogate loss: 0.0001
             Mean action noise std: 0.78
                       Mean reward: 335.39
               Mean episode length: 240.95
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 10.87s
                        Total time: 6487.93s
                               ETA: 1131757.4s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.576s, learning 0.162s)
               Value function loss: 3.9452
                    Surrogate loss: -0.0025
             Mean action noise std: 0.78
                       Mean reward: 327.40
               Mean episode length: 238.48
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 10.74s
                        Total time: 6498.67s
                               ETA: 1131633.9s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.536s, learning 0.186s)
               Value function loss: 3.8309
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 315.92
               Mean episode length: 235.39
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 10.72s
                        Total time: 6509.39s
                               ETA: 1131507.9s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.159s)
               Value function loss: 5.0163
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 313.61
               Mean episode length: 237.43
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 10.82s
                        Total time: 6520.22s
                               ETA: 1131399.7s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.539s, learning 0.168s)
               Value function loss: 5.2354
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 315.10
               Mean episode length: 238.56
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 10.71s
                        Total time: 6530.92s
                               ETA: 1131271.9s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.164s)
               Value function loss: 6.8803
                    Surrogate loss: 0.0070
             Mean action noise std: 0.78
                       Mean reward: 315.53
               Mean episode length: 239.27
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 10.70s
                        Total time: 6541.62s
                               ETA: 1131143.0s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.390s, learning 0.164s)
               Value function loss: 7.3196
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 310.32
               Mean episode length: 237.75
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 10.55s
                        Total time: 6552.17s
                               ETA: 1130989.5s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.088s, learning 0.163s)
               Value function loss: 6.9905
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 303.03
               Mean episode length: 236.91
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 11.25s
                        Total time: 6563.43s
                               ETA: 1130956.7s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.452s, learning 0.200s)
               Value function loss: 9.6182
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: 303.40
               Mean episode length: 237.21
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 10.65s
                        Total time: 6574.08s
                               ETA: 1130820.9s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1544 steps/s (collection: 10.446s, learning 0.163s)
               Value function loss: 9.9376
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 304.73
               Mean episode length: 239.72
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 10.61s
                        Total time: 6584.69s
                               ETA: 1130678.2s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.718s, learning 0.182s)
               Value function loss: 11.4445
                    Surrogate loss: 0.0111
             Mean action noise std: 0.78
                       Mean reward: 300.54
               Mean episode length: 238.73
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 10.90s
                        Total time: 6595.59s
                               ETA: 1130585.9s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.606s, learning 0.164s)
               Value function loss: 15.2466
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 324.90
               Mean episode length: 241.38
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 10.77s
                        Total time: 6606.36s
                               ETA: 1130471.5s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.514s, learning 0.166s)
               Value function loss: 14.6013
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 309.73
               Mean episode length: 238.88
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 10.68s
                        Total time: 6617.04s
                               ETA: 1130342.1s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.617s, learning 0.167s)
               Value function loss: 14.0964
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 319.82
               Mean episode length: 242.76
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 10.78s
                        Total time: 6627.82s
                               ETA: 1130230.8s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.808s, learning 0.167s)
               Value function loss: 16.5384
                    Surrogate loss: 0.0062
             Mean action noise std: 0.78
                       Mean reward: 313.64
               Mean episode length: 242.77
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 10.97s
                        Total time: 6638.79s
                               ETA: 1130152.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1564 steps/s (collection: 10.310s, learning 0.162s)
               Value function loss: 18.6602
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 333.16
               Mean episode length: 244.79
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 10.47s
                        Total time: 6649.27s
                               ETA: 1129988.7s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.696s, learning 0.161s)
               Value function loss: 20.2473
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 325.14
               Mean episode length: 244.48
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 10.86s
                        Total time: 6660.12s
                               ETA: 1129891.0s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.578s, learning 0.170s)
               Value function loss: 17.5797
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 334.83
               Mean episode length: 244.42
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 10.75s
                        Total time: 6670.87s
                               ETA: 1129775.0s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.953s, learning 0.160s)
               Value function loss: 16.4615
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 341.62
               Mean episode length: 245.82
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 11.11s
                        Total time: 6681.98s
                               ETA: 1129721.1s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.160s)
               Value function loss: 13.1912
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 345.84
               Mean episode length: 246.79
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 10.87s
                        Total time: 6692.85s
                               ETA: 1129625.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.717s, learning 0.167s)
               Value function loss: 12.4688
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 342.99
               Mean episode length: 245.83
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 10.88s
                        Total time: 6703.73s
                               ETA: 1129533.9s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.841s, learning 0.164s)
               Value function loss: 8.5696
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 348.63
               Mean episode length: 247.46
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 11.00s
                        Total time: 6714.74s
                               ETA: 1129462.2s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.656s, learning 0.170s)
               Value function loss: 6.5385
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 346.66
               Mean episode length: 246.69
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 10.83s
                        Total time: 6725.56s
                               ETA: 1129360.9s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.752s, learning 0.160s)
               Value function loss: 5.1317
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 351.01
               Mean episode length: 247.75
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 10.91s
                        Total time: 6736.48s
                               ETA: 1129274.3s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.579s, learning 0.180s)
               Value function loss: 5.5189
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 349.36
               Mean episode length: 248.14
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 10.76s
                        Total time: 6747.24s
                               ETA: 1129162.4s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.662s, learning 0.161s)
               Value function loss: 2.4561
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 341.56
               Mean episode length: 245.98
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 10.82s
                        Total time: 6758.06s
                               ETA: 1129061.6s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.165s)
               Value function loss: 2.3718
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 339.12
               Mean episode length: 244.97
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 10.87s
                        Total time: 6768.93s
                               ETA: 1128969.1s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.006s, learning 0.179s)
               Value function loss: 2.6370
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 335.89
               Mean episode length: 244.22
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 11.19s
                        Total time: 6780.12s
                               ETA: 1128929.1s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.684s, learning 0.199s)
               Value function loss: 2.8510
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 330.15
               Mean episode length: 242.14
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 10.88s
                        Total time: 6791.00s
                               ETA: 1128838.9s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.405s, learning 0.158s)
               Value function loss: 3.1160
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 326.61
               Mean episode length: 241.19
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 10.56s
                        Total time: 6801.56s
                               ETA: 1128696.0s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.654s, learning 0.190s)
               Value function loss: 3.8115
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 322.38
               Mean episode length: 240.23
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 10.84s
                        Total time: 6812.41s
                               ETA: 1128600.0s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.775s, learning 0.174s)
               Value function loss: 4.1504
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 316.43
               Mean episode length: 237.53
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 10.95s
                        Total time: 6823.35s
                               ETA: 1128521.6s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.574s, learning 0.157s)
               Value function loss: 4.8452
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 301.50
               Mean episode length: 233.92
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 10.73s
                        Total time: 6834.09s
                               ETA: 1128407.5s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1485 steps/s (collection: 10.870s, learning 0.157s)
               Value function loss: 5.1185
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 305.19
               Mean episode length: 234.20
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.03s
                        Total time: 6845.11s
                               ETA: 1128342.4s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.647s, learning 0.171s)
               Value function loss: 5.8778
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 301.49
               Mean episode length: 234.38
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 10.82s
                        Total time: 6855.93s
                               ETA: 1128243.3s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.159s)
               Value function loss: 5.3718
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 303.84
               Mean episode length: 236.01
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 10.85s
                        Total time: 6866.78s
                               ETA: 1128149.8s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.790s, learning 0.205s)
               Value function loss: 5.5109
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 312.31
               Mean episode length: 240.59
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 10.99s
                        Total time: 6877.78s
                               ETA: 1128080.1s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.527s, learning 0.169s)
               Value function loss: 8.2965
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 319.43
               Mean episode length: 244.55
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 10.70s
                        Total time: 6888.47s
                               ETA: 1127961.7s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.561s, learning 0.179s)
               Value function loss: 7.2175
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 313.85
               Mean episode length: 241.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 10.74s
                        Total time: 6899.21s
                               ETA: 1127851.1s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.715s, learning 0.162s)
               Value function loss: 7.6846
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 314.29
               Mean episode length: 241.10
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 10.88s
                        Total time: 6910.09s
                               ETA: 1127763.0s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.539s, learning 0.171s)
               Value function loss: 8.7525
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 331.33
               Mean episode length: 244.46
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 10.71s
                        Total time: 6920.80s
                               ETA: 1127648.0s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.665s, learning 0.166s)
               Value function loss: 10.3831
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 327.92
               Mean episode length: 243.89
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 10.83s
                        Total time: 6931.63s
                               ETA: 1127552.8s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.712s, learning 0.182s)
               Value function loss: 12.7981
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 322.06
               Mean episode length: 244.30
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 10.89s
                        Total time: 6942.52s
                               ETA: 1127468.3s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.777s, learning 0.190s)
               Value function loss: 13.1404
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 336.34
               Mean episode length: 244.74
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 10.97s
                        Total time: 6953.49s
                               ETA: 1127395.8s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.599s, learning 0.200s)
               Value function loss: 13.8294
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 334.87
               Mean episode length: 245.14
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 10.80s
                        Total time: 6964.29s
                               ETA: 1127296.2s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.824s, learning 0.162s)
               Value function loss: 15.2063
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 338.00
               Mean episode length: 247.45
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 10.99s
                        Total time: 6975.28s
                               ETA: 1127227.2s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.784s, learning 0.192s)
               Value function loss: 14.8341
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 349.14
               Mean episode length: 247.63
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 10.98s
                        Total time: 6986.25s
                               ETA: 1127156.9s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.421s, learning 0.211s)
               Value function loss: 20.3470
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 345.19
               Mean episode length: 248.79
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 10.63s
                        Total time: 6996.88s
                               ETA: 1127031.3s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.166s)
               Value function loss: 19.8264
                    Surrogate loss: -0.0002
             Mean action noise std: 0.78
                       Mean reward: 354.68
               Mean episode length: 248.70
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 10.75s
                        Total time: 7007.64s
                               ETA: 1126925.7s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.695s, learning 0.203s)
               Value function loss: 16.9392
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 353.03
               Mean episode length: 248.38
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 10.90s
                        Total time: 7018.54s
                               ETA: 1126843.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.965s, learning 0.172s)
               Value function loss: 16.3696
                    Surrogate loss: 0.0034
             Mean action noise std: 0.78
                       Mean reward: 364.30
               Mean episode length: 249.67
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 11.14s
                        Total time: 7029.67s
                               ETA: 1126799.8s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.531s, learning 0.163s)
               Value function loss: 14.5102
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 351.03
               Mean episode length: 249.32
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 10.69s
                        Total time: 7040.37s
                               ETA: 1126685.3s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.571s, learning 0.171s)
               Value function loss: 12.4991
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 365.62
               Mean episode length: 249.76
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 10.74s
                        Total time: 7051.11s
                               ETA: 1126578.8s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.996s, learning 0.171s)
               Value function loss: 10.4471
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 363.52
               Mean episode length: 249.07
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 11.17s
                        Total time: 7062.28s
                               ETA: 1126540.6s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1573 steps/s (collection: 10.251s, learning 0.158s)
               Value function loss: 7.6028
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 357.86
               Mean episode length: 246.23
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 10.41s
                        Total time: 7072.68s
                               ETA: 1126381.6s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.583s, learning 0.179s)
               Value function loss: 9.8170
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 362.16
               Mean episode length: 247.37
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 10.76s
                        Total time: 7083.45s
                               ETA: 1126279.3s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.638s, learning 0.177s)
               Value function loss: 3.2437
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 363.87
               Mean episode length: 246.82
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 10.81s
                        Total time: 7094.26s
                               ETA: 1126185.6s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.593s, learning 0.173s)
               Value function loss: 3.4694
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 366.04
               Mean episode length: 247.10
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 10.77s
                        Total time: 7105.03s
                               ETA: 1126084.4s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.607s, learning 0.200s)
               Value function loss: 4.1805
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 356.10
               Mean episode length: 244.57
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 10.81s
                        Total time: 7115.83s
                               ETA: 1125990.1s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.743s, learning 0.195s)
               Value function loss: 4.4212
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 353.26
               Mean episode length: 242.28
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 10.94s
                        Total time: 7126.77s
                               ETA: 1125916.8s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1493 steps/s (collection: 10.785s, learning 0.187s)
               Value function loss: 3.8972
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 349.50
               Mean episode length: 241.75
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 10.97s
                        Total time: 7137.75s
                               ETA: 1125849.0s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.162s)
               Value function loss: 4.7402
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 338.48
               Mean episode length: 238.81
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 10.87s
                        Total time: 7148.61s
                               ETA: 1125764.7s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.415s, learning 0.205s)
               Value function loss: 4.9578
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 326.27
               Mean episode length: 236.36
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 10.62s
                        Total time: 7159.23s
                               ETA: 1125641.8s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.840s, learning 0.164s)
               Value function loss: 6.3649
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 323.08
               Mean episode length: 235.11
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 11.00s
                        Total time: 7170.23s
                               ETA: 1125579.6s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.717s, learning 0.176s)
               Value function loss: 5.5081
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 320.14
               Mean episode length: 235.92
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 10.89s
                        Total time: 7181.13s
                               ETA: 1125500.2s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.734s, learning 0.160s)
               Value function loss: 7.0572
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 313.04
               Mean episode length: 237.26
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 10.89s
                        Total time: 7192.02s
                               ETA: 1125421.1s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.631s, learning 0.159s)
               Value function loss: 6.6650
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 313.64
               Mean episode length: 238.78
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 10.79s
                        Total time: 7202.81s
                               ETA: 1125326.0s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.570s, learning 0.167s)
               Value function loss: 5.7351
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 299.80
               Mean episode length: 238.30
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 10.74s
                        Total time: 7213.55s
                               ETA: 1125222.9s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.592s, learning 0.242s)
               Value function loss: 9.3722
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 308.16
               Mean episode length: 242.24
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 10.83s
                        Total time: 7224.38s
                               ETA: 1125135.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.630s, learning 0.160s)
               Value function loss: 8.2104
                    Surrogate loss: 0.0003
             Mean action noise std: 0.78
                       Mean reward: 306.76
               Mean episode length: 245.17
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 10.79s
                        Total time: 7235.17s
                               ETA: 1125040.9s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.440s, learning 0.173s)
               Value function loss: 9.5773
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 320.08
               Mean episode length: 246.34
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 10.61s
                        Total time: 7245.78s
                               ETA: 1124919.4s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.866s, learning 0.233s)
               Value function loss: 9.9928
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 324.92
               Mean episode length: 245.72
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 11.10s
                        Total time: 7256.88s
                               ETA: 1124873.6s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1589 steps/s (collection: 10.142s, learning 0.167s)
               Value function loss: 11.8443
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 318.22
               Mean episode length: 246.44
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 10.31s
                        Total time: 7267.19s
                               ETA: 1124705.6s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.635s, learning 0.208s)
               Value function loss: 14.5364
                    Surrogate loss: 0.0031
             Mean action noise std: 0.78
                       Mean reward: 323.09
               Mean episode length: 248.91
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 10.84s
                        Total time: 7278.04s
                               ETA: 1124620.7s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.164s)
               Value function loss: 13.2197
                    Surrogate loss: -0.0197
             Mean action noise std: 0.78
                       Mean reward: 347.52
               Mean episode length: 248.57
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 10.87s
                        Total time: 7288.90s
                               ETA: 1124539.9s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1560 steps/s (collection: 10.338s, learning 0.163s)
               Value function loss: 16.8339
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 329.40
               Mean episode length: 249.08
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 10.50s
                        Total time: 7299.41s
                               ETA: 1124402.7s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.745s, learning 0.162s)
               Value function loss: 17.1499
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 336.82
               Mean episode length: 247.93
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 10.91s
                        Total time: 7310.31s
                               ETA: 1124328.5s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1579 steps/s (collection: 10.211s, learning 0.160s)
               Value function loss: 18.5388
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 346.24
               Mean episode length: 249.19
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 10.37s
                        Total time: 7320.68s
                               ETA: 1124172.0s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.546s, learning 0.161s)
               Value function loss: 24.5425
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 338.75
               Mean episode length: 249.45
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 10.71s
                        Total time: 7331.39s
                               ETA: 1124067.6s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.623s, learning 0.210s)
               Value function loss: 23.8644
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 349.08
               Mean episode length: 248.97
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 10.83s
                        Total time: 7342.22s
                               ETA: 1123982.6s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.530s, learning 0.163s)
               Value function loss: 20.3393
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 354.62
               Mean episode length: 245.81
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 10.69s
                        Total time: 7352.92s
                               ETA: 1123876.6s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.614s, learning 0.171s)
               Value function loss: 19.8298
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 366.02
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 10.79s
                        Total time: 7363.70s
                               ETA: 1123784.8s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.508s, learning 0.178s)
               Value function loss: 18.8916
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: 373.14
               Mean episode length: 249.83
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 10.69s
                        Total time: 7374.39s
                               ETA: 1123678.1s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.565s, learning 0.174s)
               Value function loss: 20.0246
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 372.28
               Mean episode length: 249.44
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 10.74s
                        Total time: 7385.13s
                               ETA: 1123579.8s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.456s, learning 0.175s)
               Value function loss: 13.5051
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 360.67
               Mean episode length: 247.39
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 10.63s
                        Total time: 7395.76s
                               ETA: 1123465.4s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.437s, learning 0.185s)
               Value function loss: 10.9506
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 361.00
               Mean episode length: 247.45
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 10.62s
                        Total time: 7406.38s
                               ETA: 1123349.9s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.590s, learning 0.168s)
               Value function loss: 7.0094
                    Surrogate loss: 0.0059
             Mean action noise std: 0.78
                       Mean reward: 378.53
               Mean episode length: 248.57
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 10.76s
                        Total time: 7417.14s
                               ETA: 1123255.4s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1553 steps/s (collection: 10.362s, learning 0.183s)
               Value function loss: 4.4734
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 382.40
               Mean episode length: 248.57
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 10.55s
                        Total time: 7427.68s
                               ETA: 1123129.0s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.466s, learning 0.168s)
               Value function loss: 3.2036
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 381.19
               Mean episode length: 248.57
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 10.63s
                        Total time: 7438.32s
                               ETA: 1123016.3s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.628s, learning 0.164s)
               Value function loss: 3.8753
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: 379.85
               Mean episode length: 246.41
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 10.79s
                        Total time: 7449.11s
                               ETA: 1122927.8s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.624s, learning 0.156s)
               Value function loss: 4.5322
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 384.72
               Mean episode length: 246.42
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 10.78s
                        Total time: 7459.89s
                               ETA: 1122837.7s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.712s, learning 0.197s)
               Value function loss: 4.5438
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 379.40
               Mean episode length: 244.48
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 10.91s
                        Total time: 7470.80s
                               ETA: 1122767.2s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.508s, learning 0.167s)
               Value function loss: 4.9576
                    Surrogate loss: 0.0040
             Mean action noise std: 0.78
                       Mean reward: 376.65
               Mean episode length: 243.49
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 10.68s
                        Total time: 7481.47s
                               ETA: 1122661.8s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.654s, learning 0.163s)
               Value function loss: 4.3612
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 376.09
               Mean episode length: 243.29
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 10.82s
                        Total time: 7492.29s
                               ETA: 1122578.0s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.646s, learning 0.262s)
               Value function loss: 6.7681
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 364.84
               Mean episode length: 240.70
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 10.91s
                        Total time: 7503.20s
                               ETA: 1122508.0s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.370s, learning 0.163s)
               Value function loss: 7.3209
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 358.08
               Mean episode length: 239.69
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 10.53s
                        Total time: 7513.73s
                               ETA: 1122382.1s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.519s, learning 0.164s)
               Value function loss: 8.6276
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 348.94
               Mean episode length: 241.03
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 10.68s
                        Total time: 7524.42s
                               ETA: 1122279.0s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.630s, learning 0.209s)
               Value function loss: 7.7994
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 340.39
               Mean episode length: 238.97
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 10.84s
                        Total time: 7535.26s
                               ETA: 1122199.5s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.478s, learning 0.159s)
               Value function loss: 6.9862
                    Surrogate loss: -0.0254
             Mean action noise std: 0.78
                       Mean reward: 345.84
               Mean episode length: 243.88
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 10.64s
                        Total time: 7545.89s
                               ETA: 1122090.0s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.632s, learning 0.180s)
               Value function loss: 9.6655
                    Surrogate loss: -0.0248
             Mean action noise std: 0.78
                       Mean reward: 352.25
               Mean episode length: 243.13
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 10.81s
                        Total time: 7556.70s
                               ETA: 1122006.8s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1546 steps/s (collection: 10.430s, learning 0.166s)
               Value function loss: 10.7702
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 368.40
               Mean episode length: 243.88
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 10.60s
                        Total time: 7567.30s
                               ETA: 1121891.8s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.860s, learning 0.257s)
               Value function loss: 12.8880
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 354.08
               Mean episode length: 240.64
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 11.12s
                        Total time: 7578.42s
                               ETA: 1121854.3s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.345s, learning 0.176s)
               Value function loss: 13.3373
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 342.16
               Mean episode length: 243.21
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 10.52s
                        Total time: 7588.94s
                               ETA: 1121728.7s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.772s, learning 0.188s)
               Value function loss: 15.2284
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 355.87
               Mean episode length: 246.37
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 10.96s
                        Total time: 7599.90s
                               ETA: 1121668.2s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.610s, learning 0.166s)
               Value function loss: 23.9815
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 380.03
               Mean episode length: 247.66
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 10.78s
                        Total time: 7610.67s
                               ETA: 1121580.8s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.633s, learning 0.186s)
               Value function loss: 26.1970
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 366.36
               Mean episode length: 247.40
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 10.82s
                        Total time: 7621.49s
                               ETA: 1121499.9s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.578s, learning 0.162s)
               Value function loss: 23.0632
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 383.53
               Mean episode length: 248.49
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 10.74s
                        Total time: 7632.23s
                               ETA: 1121407.6s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.902s, learning 0.169s)
               Value function loss: 17.9293
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 375.58
               Mean episode length: 248.99
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 11.07s
                        Total time: 7643.30s
                               ETA: 1121364.2s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.160s)
               Value function loss: 20.5754
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 374.96
               Mean episode length: 247.94
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 11.04s
                        Total time: 7654.35s
                               ETA: 1121316.8s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.591s, learning 0.171s)
               Value function loss: 20.5863
                    Surrogate loss: -0.0221
             Mean action noise std: 0.78
                       Mean reward: 388.16
               Mean episode length: 249.03
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 10.76s
                        Total time: 7665.11s
                               ETA: 1121228.4s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.698s, learning 0.163s)
               Value function loss: 22.2202
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 376.84
               Mean episode length: 248.72
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 10.86s
                        Total time: 7675.97s
                               ETA: 1121154.6s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1555 steps/s (collection: 10.369s, learning 0.168s)
               Value function loss: 14.8588
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 385.48
               Mean episode length: 249.82
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 10.54s
                        Total time: 7686.51s
                               ETA: 1121033.7s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.530s, learning 0.202s)
               Value function loss: 13.8962
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 362.33
               Mean episode length: 249.72
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 10.73s
                        Total time: 7697.24s
                               ETA: 1120941.5s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.406s, learning 0.162s)
               Value function loss: 10.2370
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 373.78
               Mean episode length: 249.47
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 10.57s
                        Total time: 7707.81s
                               ETA: 1120825.8s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.698s, learning 0.193s)
               Value function loss: 13.0693
                    Surrogate loss: 0.0041
             Mean action noise std: 0.78
                       Mean reward: 366.95
               Mean episode length: 249.47
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 10.89s
                        Total time: 7718.70s
                               ETA: 1120757.3s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1575 steps/s (collection: 10.237s, learning 0.163s)
               Value function loss: 7.9205
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 375.19
               Mean episode length: 248.41
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 10.40s
                        Total time: 7729.10s
                               ETA: 1120617.7s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.165s)
               Value function loss: 6.5012
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 355.53
               Mean episode length: 247.59
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 10.70s
                        Total time: 7739.80s
                               ETA: 1120521.9s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.571s, learning 0.166s)
               Value function loss: 4.6577
                    Surrogate loss: -0.0221
             Mean action noise std: 0.78
                       Mean reward: 362.94
               Mean episode length: 248.54
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 10.74s
                        Total time: 7750.53s
                               ETA: 1120431.7s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.446s, learning 0.257s)
               Value function loss: 4.1397
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 370.14
               Mean episode length: 249.98
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 10.70s
                        Total time: 7761.24s
                               ETA: 1120336.8s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.557s, learning 0.178s)
               Value function loss: 2.5936
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 370.14
               Mean episode length: 249.58
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 10.73s
                        Total time: 7771.97s
                               ETA: 1120246.8s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.446s, learning 0.171s)
               Value function loss: 2.9950
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 365.81
               Mean episode length: 249.41
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 10.62s
                        Total time: 7782.59s
                               ETA: 1120140.0s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.567s, learning 0.191s)
               Value function loss: 2.8994
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 360.88
               Mean episode length: 248.42
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 10.76s
                        Total time: 7793.35s
                               ETA: 1120053.8s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1559 steps/s (collection: 10.325s, learning 0.178s)
               Value function loss: 4.2535
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 355.67
               Mean episode length: 247.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 10.50s
                        Total time: 7803.85s
                               ETA: 1119931.3s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.679s, learning 0.213s)
               Value function loss: 3.3204
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 354.96
               Mean episode length: 247.32
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 10.89s
                        Total time: 7814.74s
                               ETA: 1119864.6s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.626s, learning 0.167s)
               Value function loss: 4.2635
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 350.77
               Mean episode length: 247.32
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 10.79s
                        Total time: 7825.53s
                               ETA: 1119784.2s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1571 steps/s (collection: 10.252s, learning 0.172s)
               Value function loss: 5.3611
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 338.43
               Mean episode length: 246.08
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 10.42s
                        Total time: 7835.96s
                               ETA: 1119651.1s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.605s, learning 0.200s)
               Value function loss: 6.2348
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 340.34
               Mean episode length: 246.09
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 10.81s
                        Total time: 7846.76s
                               ETA: 1119572.8s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.759s, learning 0.184s)
               Value function loss: 6.8387
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 340.06
               Mean episode length: 246.12
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 10.94s
                        Total time: 7857.70s
                               ETA: 1119514.4s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1558 steps/s (collection: 10.353s, learning 0.161s)
               Value function loss: 7.5932
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 346.17
               Mean episode length: 244.23
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 10.51s
                        Total time: 7868.22s
                               ETA: 1119395.0s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.663s, learning 0.173s)
               Value function loss: 7.7043
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 349.55
               Mean episode length: 244.70
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 10.84s
                        Total time: 7879.05s
                               ETA: 1119321.6s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.399s, learning 0.167s)
               Value function loss: 9.3464
                    Surrogate loss: -0.0043
             Mean action noise std: 0.78
                       Mean reward: 343.04
               Mean episode length: 244.09
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 10.57s
                        Total time: 7889.62s
                               ETA: 1119210.2s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.618s, learning 0.178s)
               Value function loss: 10.6227
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 348.80
               Mean episode length: 244.95
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 10.80s
                        Total time: 7900.42s
                               ETA: 1119131.6s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.169s)
               Value function loss: 11.7453
                    Surrogate loss: 0.0058
             Mean action noise std: 0.78
                       Mean reward: 344.57
               Mean episode length: 244.66
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 10.76s
                        Total time: 7911.18s
                               ETA: 1119048.8s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.569s, learning 0.189s)
               Value function loss: 14.4463
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 341.41
               Mean episode length: 240.93
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 10.76s
                        Total time: 7921.94s
                               ETA: 1118965.3s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1570 steps/s (collection: 10.267s, learning 0.163s)
               Value function loss: 13.9783
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 361.39
               Mean episode length: 245.46
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 10.43s
                        Total time: 7932.37s
                               ETA: 1118835.7s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.742s, learning 0.161s)
               Value function loss: 14.5753
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 371.02
               Mean episode length: 247.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 10.90s
                        Total time: 7943.27s
                               ETA: 1118773.2s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1539 steps/s (collection: 10.485s, learning 0.160s)
               Value function loss: 17.8751
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 374.02
               Mean episode length: 248.15
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 10.65s
                        Total time: 7953.92s
                               ETA: 1118674.5s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1526 steps/s (collection: 10.533s, learning 0.202s)
               Value function loss: 20.3246
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 387.46
               Mean episode length: 249.32
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 10.73s
                        Total time: 7964.65s
                               ETA: 1118588.6s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.830s, learning 0.286s)
               Value function loss: 21.3929
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 367.95
               Mean episode length: 246.63
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 11.12s
                        Total time: 7975.77s
                               ETA: 1118556.4s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.715s, learning 0.171s)
               Value function loss: 23.3063
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 382.77
               Mean episode length: 248.96
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 10.89s
                        Total time: 7986.65s
                               ETA: 1118492.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.658s, learning 0.192s)
               Value function loss: 15.6124
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 386.19
               Mean episode length: 249.40
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 10.85s
                        Total time: 7997.50s
                               ETA: 1118422.8s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.813s, learning 0.193s)
               Value function loss: 20.2191
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 372.79
               Mean episode length: 249.40
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 11.01s
                        Total time: 8008.51s
                               ETA: 1118375.4s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.798s, learning 0.187s)
               Value function loss: 17.9543
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 383.83
               Mean episode length: 249.63
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 10.99s
                        Total time: 8019.49s
                               ETA: 1118325.3s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.563s, learning 0.165s)
               Value function loss: 17.4607
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 376.67
               Mean episode length: 249.64
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 10.73s
                        Total time: 8030.22s
                               ETA: 1118239.5s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.166s)
               Value function loss: 13.8284
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 373.82
               Mean episode length: 249.13
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 10.78s
                        Total time: 8041.00s
                               ETA: 1118161.4s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.718s, learning 0.165s)
               Value function loss: 13.3701
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 368.08
               Mean episode length: 248.11
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 10.88s
                        Total time: 8051.89s
                               ETA: 1118097.6s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.540s, learning 0.209s)
               Value function loss: 10.2757
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 364.11
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 10.75s
                        Total time: 8062.64s
                               ETA: 1118015.2s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.766s, learning 0.192s)
               Value function loss: 9.6952
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 360.67
               Mean episode length: 246.92
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 10.96s
                        Total time: 8073.59s
                               ETA: 1117962.0s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.751s, learning 0.166s)
               Value function loss: 7.0865
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 362.62
               Mean episode length: 244.27
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 10.92s
                        Total time: 8084.51s
                               ETA: 1117903.3s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.717s, learning 0.187s)
               Value function loss: 7.5341
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 371.12
               Mean episode length: 245.32
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 10.90s
                        Total time: 8095.42s
                               ETA: 1117843.0s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.498s, learning 0.258s)
               Value function loss: 2.7424
                    Surrogate loss: -0.0239
             Mean action noise std: 0.78
                       Mean reward: 368.52
               Mean episode length: 245.32
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 10.76s
                        Total time: 8106.17s
                               ETA: 1117762.3s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1556 steps/s (collection: 10.359s, learning 0.164s)
               Value function loss: 2.6334
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 369.35
               Mean episode length: 245.27
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 10.52s
                        Total time: 8116.70s
                               ETA: 1117649.8s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.910s, learning 0.168s)
               Value function loss: 3.6826
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 362.23
               Mean episode length: 244.04
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 11.08s
                        Total time: 8127.77s
                               ETA: 1117614.0s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.794s, learning 0.168s)
               Value function loss: 4.6290
                    Surrogate loss: -0.0219
             Mean action noise std: 0.77
                       Mean reward: 354.99
               Mean episode length: 241.15
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 10.96s
                        Total time: 8138.74s
                               ETA: 1117562.1s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.647s, learning 0.161s)
               Value function loss: 4.7474
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 345.12
               Mean episode length: 237.89
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 10.81s
                        Total time: 8149.54s
                               ETA: 1117489.3s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.883s, learning 0.168s)
               Value function loss: 5.8258
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 329.11
               Mean episode length: 234.14
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 11.05s
                        Total time: 8160.59s
                               ETA: 1117449.9s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.713s, learning 0.177s)
               Value function loss: 6.2199
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 313.59
               Mean episode length: 229.81
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 10.89s
                        Total time: 8171.48s
                               ETA: 1117388.6s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.623s, learning 0.179s)
               Value function loss: 7.9532
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 313.51
               Mean episode length: 227.83
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 10.80s
                        Total time: 8182.29s
                               ETA: 1117315.5s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.707s, learning 0.169s)
               Value function loss: 7.6541
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 315.67
               Mean episode length: 228.60
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 10.88s
                        Total time: 8193.16s
                               ETA: 1117252.5s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.551s, learning 0.162s)
               Value function loss: 8.4616
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 332.20
               Mean episode length: 232.18
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 10.71s
                        Total time: 8203.88s
                               ETA: 1117167.5s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.570s, learning 0.173s)
               Value function loss: 9.1717
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 349.31
               Mean episode length: 233.87
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 10.74s
                        Total time: 8214.62s
                               ETA: 1117086.8s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.549s, learning 0.161s)
               Value function loss: 8.1199
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 339.55
               Mean episode length: 231.49
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 10.71s
                        Total time: 8225.33s
                               ETA: 1117001.8s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.876s, learning 0.172s)
               Value function loss: 9.2684
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 327.54
               Mean episode length: 237.41
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 11.05s
                        Total time: 8236.38s
                               ETA: 1116962.8s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.680s, learning 0.167s)
               Value function loss: 9.2584
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 317.98
               Mean episode length: 235.73
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 10.85s
                        Total time: 8247.22s
                               ETA: 1116896.7s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.393s, learning 0.170s)
               Value function loss: 12.5189
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 329.29
               Mean episode length: 237.19
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 10.56s
                        Total time: 8257.78s
                               ETA: 1116792.3s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.550s, learning 0.173s)
               Value function loss: 10.0790
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 325.69
               Mean episode length: 236.11
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 10.72s
                        Total time: 8268.51s
                               ETA: 1116709.7s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.768s, learning 0.165s)
               Value function loss: 12.8010
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 335.97
               Mean episode length: 242.71
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 10.93s
                        Total time: 8279.44s
                               ETA: 1116655.8s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.661s, learning 0.166s)
               Value function loss: 13.9194
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 343.70
               Mean episode length: 244.52
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 10.83s
                        Total time: 8290.27s
                               ETA: 1116587.7s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1578 steps/s (collection: 10.214s, learning 0.166s)
               Value function loss: 13.9792
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 348.18
               Mean episode length: 246.74
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 10.38s
                        Total time: 8300.65s
                               ETA: 1116459.7s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.626s, learning 0.182s)
               Value function loss: 13.4714
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 351.44
               Mean episode length: 246.97
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 10.81s
                        Total time: 8311.46s
                               ETA: 1116389.4s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.770s, learning 0.160s)
               Value function loss: 15.1221
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 348.32
               Mean episode length: 248.89
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 10.93s
                        Total time: 8322.39s
                               ETA: 1116335.7s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.611s, learning 0.177s)
               Value function loss: 13.3090
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 341.21
               Mean episode length: 243.24
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 10.79s
                        Total time: 8333.18s
                               ETA: 1116263.1s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.522s, learning 0.168s)
               Value function loss: 16.3185
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 351.81
               Mean episode length: 249.59
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 10.69s
                        Total time: 8343.87s
                               ETA: 1116177.6s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1520 steps/s (collection: 10.595s, learning 0.184s)
               Value function loss: 14.2349
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 363.65
               Mean episode length: 249.28
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 10.78s
                        Total time: 8354.64s
                               ETA: 1116104.1s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.569s, learning 0.159s)
               Value function loss: 13.3580
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 366.12
               Mean episode length: 247.87
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 10.73s
                        Total time: 8365.37s
                               ETA: 1116023.9s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.708s, learning 0.169s)
               Value function loss: 11.8630
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 363.09
               Mean episode length: 249.06
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 10.88s
                        Total time: 8376.25s
                               ETA: 1115963.7s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.654s, learning 0.176s)
               Value function loss: 10.7040
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 364.58
               Mean episode length: 248.14
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 10.83s
                        Total time: 8387.08s
                               ETA: 1115897.6s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.494s, learning 0.164s)
               Value function loss: 10.2154
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 369.04
               Mean episode length: 248.69
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 10.66s
                        Total time: 8397.74s
                               ETA: 1115808.7s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1487 steps/s (collection: 10.846s, learning 0.170s)
               Value function loss: 7.4690
                    Surrogate loss: -0.0235
             Mean action noise std: 0.77
                       Mean reward: 356.95
               Mean episode length: 247.01
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 11.02s
                        Total time: 8408.75s
                               ETA: 1115767.5s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.505s, learning 0.167s)
               Value function loss: 5.1954
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 375.27
               Mean episode length: 248.49
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 10.67s
                        Total time: 8419.43s
                               ETA: 1115680.9s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.600s, learning 0.200s)
               Value function loss: 5.4786
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 372.69
               Mean episode length: 247.67
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 10.80s
                        Total time: 8430.23s
                               ETA: 1115611.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.586s, learning 0.185s)
               Value function loss: 2.3352
                    Surrogate loss: -0.0230
             Mean action noise std: 0.77
                       Mean reward: 367.15
               Mean episode length: 245.65
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 10.77s
                        Total time: 8441.00s
                               ETA: 1115538.1s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.536s, learning 0.173s)
               Value function loss: 2.7474
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 361.90
               Mean episode length: 243.15
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 10.71s
                        Total time: 8451.71s
                               ETA: 1115456.7s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1567 steps/s (collection: 10.284s, learning 0.166s)
               Value function loss: 3.0258
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 353.37
               Mean episode length: 240.58
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 10.45s
                        Total time: 8462.16s
                               ETA: 1115341.6s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.591s, learning 0.166s)
               Value function loss: 2.9269
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 346.56
               Mean episode length: 237.59
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 10.76s
                        Total time: 8472.91s
                               ETA: 1115267.0s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.368s, learning 0.173s)
               Value function loss: 2.9504
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 348.80
               Mean episode length: 236.52
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 10.54s
                        Total time: 8483.46s
                               ETA: 1115164.2s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.572s, learning 0.181s)
               Value function loss: 3.3273
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 340.41
               Mean episode length: 233.68
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 10.75s
                        Total time: 8494.21s
                               ETA: 1115089.5s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.679s, learning 0.160s)
               Value function loss: 3.3343
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 340.35
               Mean episode length: 233.20
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 10.84s
                        Total time: 8505.05s
                               ETA: 1115026.2s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.025s, learning 0.174s)
               Value function loss: 6.1530
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 351.05
               Mean episode length: 236.35
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 11.20s
                        Total time: 8516.25s
                               ETA: 1115010.2s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.159s)
               Value function loss: 3.9322
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 351.46
               Mean episode length: 236.63
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 11.11s
                        Total time: 8527.35s
                               ETA: 1114982.3s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.471s, learning 0.165s)
               Value function loss: 6.6631
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 364.17
               Mean episode length: 240.40
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 10.64s
                        Total time: 8537.99s
                               ETA: 1114892.8s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1486 steps/s (collection: 10.855s, learning 0.166s)
               Value function loss: 5.8615
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 371.15
               Mean episode length: 241.93
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 11.02s
                        Total time: 8549.01s
                               ETA: 1114853.8s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1559 steps/s (collection: 10.343s, learning 0.161s)
               Value function loss: 6.8514
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 368.49
               Mean episode length: 238.48
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 10.50s
                        Total time: 8559.51s
                               ETA: 1114747.4s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.163s)
               Value function loss: 6.9003
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 369.88
               Mean episode length: 239.92
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 10.89s
                        Total time: 8570.40s
                               ETA: 1114691.4s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.637s, learning 0.163s)
               Value function loss: 7.7813
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 382.51
               Mean episode length: 244.37
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 10.80s
                        Total time: 8581.20s
                               ETA: 1114623.9s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1566 steps/s (collection: 10.295s, learning 0.162s)
               Value function loss: 6.7262
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 377.00
               Mean episode length: 245.51
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 10.46s
                        Total time: 8591.66s
                               ETA: 1114512.1s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.404s, learning 0.185s)
               Value function loss: 8.1808
                    Surrogate loss: -0.0224
             Mean action noise std: 0.77
                       Mean reward: 373.00
               Mean episode length: 244.78
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 10.59s
                        Total time: 8602.25s
                               ETA: 1114417.6s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.696s, learning 0.181s)
               Value function loss: 8.9570
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 381.80
               Mean episode length: 247.26
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 10.88s
                        Total time: 8613.12s
                               ETA: 1114360.7s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.803s, learning 0.161s)
               Value function loss: 10.5668
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 377.33
               Mean episode length: 246.86
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 10.96s
                        Total time: 8624.09s
                               ETA: 1114315.2s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.493s, learning 0.177s)
               Value function loss: 9.5758
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 376.59
               Mean episode length: 246.11
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 10.67s
                        Total time: 8634.76s
                               ETA: 1114231.8s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.645s, learning 0.195s)
               Value function loss: 12.3687
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 377.96
               Mean episode length: 247.27
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 10.84s
                        Total time: 8645.60s
                               ETA: 1114170.5s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1484 steps/s (collection: 10.872s, learning 0.162s)
               Value function loss: 12.4327
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 373.89
               Mean episode length: 247.25
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 11.03s
                        Total time: 8656.63s
                               ETA: 1114134.4s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.514s, learning 0.162s)
               Value function loss: 11.7064
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 383.77
               Mean episode length: 247.95
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 10.68s
                        Total time: 8667.31s
                               ETA: 1114052.2s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.546s, learning 0.173s)
               Value function loss: 15.3025
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 379.09
               Mean episode length: 247.85
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 10.72s
                        Total time: 8678.03s
                               ETA: 1113975.8s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.585s, learning 0.231s)
               Value function loss: 16.7210
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 379.22
               Mean episode length: 247.58
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 10.82s
                        Total time: 8688.84s
                               ETA: 1113911.9s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.919s, learning 0.168s)
               Value function loss: 13.6906
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 377.02
               Mean episode length: 247.94
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 11.09s
                        Total time: 8699.93s
                               ETA: 1113882.8s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.724s, learning 0.171s)
               Value function loss: 15.6351
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 380.82
               Mean episode length: 248.73
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 10.90s
                        Total time: 8710.82s
                               ETA: 1113829.3s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.623s, learning 0.167s)
               Value function loss: 12.7176
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 386.34
               Mean episode length: 249.87
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 10.79s
                        Total time: 8721.61s
                               ETA: 1113762.5s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.664s, learning 0.197s)
               Value function loss: 12.0991
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 373.41
               Mean episode length: 247.45
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 10.86s
                        Total time: 8732.48s
                               ETA: 1113704.9s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.652s, learning 0.188s)
               Value function loss: 10.5930
                    Surrogate loss: 0.0151
             Mean action noise std: 0.77
                       Mean reward: 385.34
               Mean episode length: 248.86
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 10.84s
                        Total time: 8743.31s
                               ETA: 1113644.6s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.539s, learning 0.176s)
               Value function loss: 6.7602
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 381.00
               Mean episode length: 248.18
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 10.72s
                        Total time: 8754.03s
                               ETA: 1113568.8s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.554s, learning 0.164s)
               Value function loss: 5.2937
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 377.42
               Mean episode length: 245.54
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 10.72s
                        Total time: 8764.75s
                               ETA: 1113493.3s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.590s, learning 0.167s)
               Value function loss: 3.5805
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 373.81
               Mean episode length: 243.68
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 10.76s
                        Total time: 8775.51s
                               ETA: 1113423.1s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.773s, learning 0.191s)
               Value function loss: 3.0527
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 372.24
               Mean episode length: 242.96
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 10.96s
                        Total time: 8786.47s
                               ETA: 1113379.2s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.754s, learning 0.165s)
               Value function loss: 4.9527
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 367.14
               Mean episode length: 241.52
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 10.92s
                        Total time: 8797.39s
                               ETA: 1113329.6s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.660s, learning 0.161s)
               Value function loss: 4.5194
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 350.96
               Mean episode length: 236.23
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 10.82s
                        Total time: 8808.21s
                               ETA: 1113267.9s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1574 steps/s (collection: 10.239s, learning 0.166s)
               Value function loss: 5.5436
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 339.02
               Mean episode length: 231.03
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 10.41s
                        Total time: 8818.61s
                               ETA: 1113153.8s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.558s, learning 0.179s)
               Value function loss: 4.3068
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 341.93
               Mean episode length: 231.68
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 10.74s
                        Total time: 8829.35s
                               ETA: 1113081.7s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.612s, learning 0.190s)
               Value function loss: 5.8489
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 340.25
               Mean episode length: 230.21
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 10.80s
                        Total time: 8840.15s
                               ETA: 1113018.0s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.448s, learning 0.167s)
               Value function loss: 10.0627
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: 340.60
               Mean episode length: 232.40
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 10.62s
                        Total time: 8850.77s
                               ETA: 1112931.0s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.760s, learning 0.160s)
               Value function loss: 6.0364
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 326.15
               Mean episode length: 227.19
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 10.92s
                        Total time: 8861.69s
                               ETA: 1112882.4s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1558 steps/s (collection: 10.317s, learning 0.197s)
               Value function loss: 9.0832
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 335.74
               Mean episode length: 233.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 10.51s
                        Total time: 8872.20s
                               ETA: 1112782.9s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.447s, learning 0.187s)
               Value function loss: 9.8883
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 347.95
               Mean episode length: 236.25
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 10.63s
                        Total time: 8882.84s
                               ETA: 1112698.8s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.716s, learning 0.185s)
               Value function loss: 9.3108
                    Surrogate loss: 0.0057
             Mean action noise std: 0.77
                       Mean reward: 353.55
               Mean episode length: 237.76
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 10.90s
                        Total time: 8893.74s
                               ETA: 1112648.3s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.576s, learning 0.168s)
               Value function loss: 9.9964
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 357.72
               Mean episode length: 240.02
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 10.74s
                        Total time: 8904.48s
                               ETA: 1112578.2s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.797s, learning 0.155s)
               Value function loss: 11.4421
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 359.48
               Mean episode length: 239.75
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 10.95s
                        Total time: 8915.44s
                               ETA: 1112534.2s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.543s, learning 0.167s)
               Value function loss: 12.7786
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 361.56
               Mean episode length: 242.93
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 10.71s
                        Total time: 8926.15s
                               ETA: 1112460.2s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.623s, learning 0.167s)
               Value function loss: 12.8551
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 364.69
               Mean episode length: 243.21
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 10.79s
                        Total time: 8936.94s
                               ETA: 1112396.3s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.785s, learning 0.176s)
               Value function loss: 14.0240
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 357.66
               Mean episode length: 243.74
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 10.96s
                        Total time: 8947.90s
                               ETA: 1112353.8s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1509 steps/s (collection: 10.685s, learning 0.172s)
               Value function loss: 15.5690
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 358.87
               Mean episode length: 245.74
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 10.86s
                        Total time: 8958.76s
                               ETA: 1112298.4s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.695s, learning 0.262s)
               Value function loss: 14.9103
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 357.90
               Mean episode length: 242.70
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 10.96s
                        Total time: 8969.71s
                               ETA: 1112255.6s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.662s, learning 0.216s)
               Value function loss: 22.5326
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 370.39
               Mean episode length: 247.09
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 10.88s
                        Total time: 8980.59s
                               ETA: 1112203.0s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1559 steps/s (collection: 10.342s, learning 0.167s)
               Value function loss: 22.1841
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 358.21
               Mean episode length: 246.84
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 10.51s
                        Total time: 8991.10s
                               ETA: 1112104.9s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.550s, learning 0.168s)
               Value function loss: 24.0855
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 368.66
               Mean episode length: 248.38
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 10.72s
                        Total time: 9001.82s
                               ETA: 1112032.7s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.516s, learning 0.156s)
               Value function loss: 19.7084
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 366.78
               Mean episode length: 247.47
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 10.67s
                        Total time: 9012.49s
                               ETA: 1111955.2s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.164s)
               Value function loss: 19.5600
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 367.10
               Mean episode length: 249.06
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 10.76s
                        Total time: 9023.25s
                               ETA: 1111889.1s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.351s, learning 0.171s)
               Value function loss: 20.0623
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 366.85
               Mean episode length: 249.88
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 10.52s
                        Total time: 9033.78s
                               ETA: 1111793.2s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.653s, learning 0.170s)
               Value function loss: 16.1951
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 362.22
               Mean episode length: 248.94
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 10.82s
                        Total time: 9044.60s
                               ETA: 1111734.6s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.467s, learning 0.164s)
               Value function loss: 15.0375
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 358.10
               Mean episode length: 247.95
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 10.63s
                        Total time: 9055.23s
                               ETA: 1111652.7s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.547s, learning 0.169s)
               Value function loss: 16.1181
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 358.23
               Mean episode length: 249.49
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 10.72s
                        Total time: 9065.94s
                               ETA: 1111581.2s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1547 steps/s (collection: 10.408s, learning 0.176s)
               Value function loss: 9.7158
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 355.05
               Mean episode length: 248.49
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 10.58s
                        Total time: 9076.53s
                               ETA: 1111493.8s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.654s, learning 0.160s)
               Value function loss: 8.2675
                    Surrogate loss: -0.0219
             Mean action noise std: 0.77
                       Mean reward: 360.76
               Mean episode length: 247.85
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 10.81s
                        Total time: 9087.34s
                               ETA: 1111434.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.655s, learning 0.188s)
               Value function loss: 6.2791
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 357.13
               Mean episode length: 248.24
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 10.84s
                        Total time: 9098.19s
                               ETA: 1111379.2s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1519 steps/s (collection: 10.594s, learning 0.186s)
               Value function loss: 5.6120
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 356.67
               Mean episode length: 247.32
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 10.78s
                        Total time: 9108.97s
                               ETA: 1111316.3s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1541 steps/s (collection: 10.461s, learning 0.167s)
               Value function loss: 3.3510
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 354.68
               Mean episode length: 245.24
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 10.63s
                        Total time: 9119.59s
                               ETA: 1111234.8s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.448s, learning 0.173s)
               Value function loss: 5.5159
                    Surrogate loss: -0.0200
             Mean action noise std: 0.77
                       Mean reward: 352.14
               Mean episode length: 243.39
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 10.62s
                        Total time: 9130.22s
                               ETA: 1111152.8s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1538 steps/s (collection: 10.451s, learning 0.197s)
               Value function loss: 4.7244
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 353.85
               Mean episode length: 242.49
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 10.65s
                        Total time: 9140.86s
                               ETA: 1111074.1s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.514s, learning 0.172s)
               Value function loss: 5.2990
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 351.71
               Mean episode length: 240.65
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 10.69s
                        Total time: 9151.55s
                               ETA: 1111000.3s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.572s, learning 0.177s)
               Value function loss: 5.1396
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 361.05
               Mean episode length: 243.08
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 10.75s
                        Total time: 9162.30s
                               ETA: 1110934.2s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.568s, learning 0.191s)
               Value function loss: 6.6635
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 367.08
               Mean episode length: 246.01
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 10.76s
                        Total time: 9173.06s
                               ETA: 1110869.5s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.549s, learning 0.172s)
               Value function loss: 7.9177
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 361.36
               Mean episode length: 246.49
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 10.72s
                        Total time: 9183.78s
                               ETA: 1110800.4s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.457s, learning 0.176s)
               Value function loss: 8.0780
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 354.46
               Mean episode length: 247.84
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 10.63s
                        Total time: 9194.41s
                               ETA: 1110720.7s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1510 steps/s (collection: 10.677s, learning 0.169s)
               Value function loss: 10.2110
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 347.94
               Mean episode length: 246.53
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 10.85s
                        Total time: 9205.26s
                               ETA: 1110666.9s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.510s, learning 0.165s)
               Value function loss: 11.0921
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 355.19
               Mean episode length: 246.61
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 10.68s
                        Total time: 9215.93s
                               ETA: 1110592.6s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1580 steps/s (collection: 10.178s, learning 0.188s)
               Value function loss: 8.7430
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 354.06
               Mean episode length: 246.95
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 10.37s
                        Total time: 9226.30s
                               ETA: 1110481.3s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.672s, learning 0.195s)
               Value function loss: 9.9805
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 355.13
               Mean episode length: 247.66
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 10.87s
                        Total time: 9237.17s
                               ETA: 1110430.5s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.815s, learning 0.193s)
               Value function loss: 14.6924
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 359.73
               Mean episode length: 248.61
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 11.01s
                        Total time: 9248.17s
                               ETA: 1110396.6s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.493s, learning 0.197s)
               Value function loss: 12.6061
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 364.60
               Mean episode length: 247.59
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 10.69s
                        Total time: 9258.86s
                               ETA: 1110324.8s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.778s, learning 0.166s)
               Value function loss: 16.5398
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 347.09
               Mean episode length: 246.90
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 10.94s
                        Total time: 9269.81s
                               ETA: 1110283.4s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.650s, learning 0.171s)
               Value function loss: 12.5994
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 347.86
               Mean episode length: 247.29
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 10.82s
                        Total time: 9280.63s
                               ETA: 1110227.3s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1545 steps/s (collection: 10.399s, learning 0.205s)
               Value function loss: 12.6786
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 353.66
               Mean episode length: 248.82
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 10.60s
                        Total time: 9291.23s
                               ETA: 1110145.5s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.507s, learning 0.176s)
               Value function loss: 15.6663
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 361.42
               Mean episode length: 249.51
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 10.68s
                        Total time: 9301.91s
                               ETA: 1110073.2s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.615s, learning 0.181s)
               Value function loss: 19.7477
                    Surrogate loss: 0.0001
             Mean action noise std: 0.77
                       Mean reward: 357.03
               Mean episode length: 247.12
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 10.80s
                        Total time: 9312.71s
                               ETA: 1110014.6s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.685s, learning 0.196s)
               Value function loss: 17.8577
                    Surrogate loss: 0.0049
             Mean action noise std: 0.77
                       Mean reward: 358.85
               Mean episode length: 248.14
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 10.88s
                        Total time: 9323.59s
                               ETA: 1109966.3s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.742s, learning 0.196s)
               Value function loss: 16.9337
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 351.33
               Mean episode length: 249.34
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 10.94s
                        Total time: 9334.53s
                               ETA: 1109924.8s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.401s, learning 0.170s)
               Value function loss: 17.3644
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 337.85
               Mean episode length: 249.12
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 10.57s
                        Total time: 9345.10s
                               ETA: 1109839.8s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.821s, learning 0.172s)
               Value function loss: 18.8550
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 346.86
               Mean episode length: 249.91
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 10.99s
                        Total time: 9356.09s
                               ETA: 1109805.0s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.708s, learning 0.178s)
               Value function loss: 17.7291
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 345.64
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 10.89s
                        Total time: 9366.98s
                               ETA: 1109757.7s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.397s, learning 0.175s)
               Value function loss: 18.5754
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 347.07
               Mean episode length: 249.51
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 10.57s
                        Total time: 9377.55s
                               ETA: 1109673.3s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.734s, learning 0.180s)
               Value function loss: 20.2999
                    Surrogate loss: 0.0004
             Mean action noise std: 0.77
                       Mean reward: 340.12
               Mean episode length: 249.02
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 10.91s
                        Total time: 9388.47s
                               ETA: 1109629.5s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.620s, learning 0.168s)
               Value function loss: 18.4625
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 352.07
               Mean episode length: 248.99
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 10.79s
                        Total time: 9399.26s
                               ETA: 1109570.9s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1515 steps/s (collection: 10.640s, learning 0.171s)
               Value function loss: 10.8514
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 355.62
               Mean episode length: 249.95
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 10.81s
                        Total time: 9410.07s
                               ETA: 1109515.0s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.480s, learning 0.205s)
               Value function loss: 8.5628
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 357.01
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 10.68s
                        Total time: 9420.75s
                               ETA: 1109444.4s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.678s, learning 0.195s)
               Value function loss: 6.7203
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 342.68
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 10.87s
                        Total time: 9431.62s
                               ETA: 1109396.1s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.496s, learning 0.161s)
               Value function loss: 6.2963
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 340.08
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 10.66s
                        Total time: 9442.28s
                               ETA: 1109322.5s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1579 steps/s (collection: 10.208s, learning 0.167s)
               Value function loss: 3.7769
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 339.49
               Mean episode length: 249.18
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 10.38s
                        Total time: 9452.66s
                               ETA: 1109216.0s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.457s, learning 0.163s)
               Value function loss: 4.9566
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 339.31
               Mean episode length: 246.98
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 10.62s
                        Total time: 9463.28s
                               ETA: 1109138.4s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.636s, learning 0.166s)
               Value function loss: 5.6368
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 343.80
               Mean episode length: 245.71
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 10.80s
                        Total time: 9474.08s
                               ETA: 1109082.3s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.529s, learning 0.176s)
               Value function loss: 4.9185
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 349.06
               Mean episode length: 243.59
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 10.70s
                        Total time: 9484.78s
                               ETA: 1109015.0s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.731s, learning 0.191s)
               Value function loss: 5.6123
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 349.34
               Mean episode length: 243.55
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 10.92s
                        Total time: 9495.70s
                               ETA: 1108973.0s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.764s, learning 0.193s)
               Value function loss: 6.3509
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 349.01
               Mean episode length: 243.64
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 10.96s
                        Total time: 9506.66s
                               ETA: 1108935.3s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.698s, learning 0.160s)
               Value function loss: 6.4744
                    Surrogate loss: -0.0011
             Mean action noise std: 0.77
                       Mean reward: 337.86
               Mean episode length: 242.33
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 10.86s
                        Total time: 9517.52s
                               ETA: 1108886.1s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.718s, learning 0.168s)
               Value function loss: 9.0303
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 335.89
               Mean episode length: 243.01
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 10.89s
                        Total time: 9528.41s
                               ETA: 1108840.2s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.417s, learning 0.194s)
               Value function loss: 7.5971
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 337.33
               Mean episode length: 243.11
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 10.61s
                        Total time: 9539.02s
                               ETA: 1108762.5s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.744s, learning 0.166s)
               Value function loss: 12.5158
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 343.02
               Mean episode length: 245.33
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 10.91s
                        Total time: 9549.93s
                               ETA: 1108719.6s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.754s, learning 0.187s)
               Value function loss: 8.6714
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 341.51
               Mean episode length: 245.52
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 10.94s
                        Total time: 9560.87s
                               ETA: 1108680.5s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.972s, learning 0.162s)
               Value function loss: 12.0271
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 342.80
               Mean episode length: 242.78
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 11.13s
                        Total time: 9572.00s
                               ETA: 1108663.7s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.621s, learning 0.166s)
               Value function loss: 13.9390
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 358.40
               Mean episode length: 242.68
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 10.79s
                        Total time: 9582.79s
                               ETA: 1108606.7s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.679s, learning 0.163s)
               Value function loss: 14.0631
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 358.75
               Mean episode length: 241.54
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 10.84s
                        Total time: 9593.63s
                               ETA: 1108556.3s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.873s, learning 0.231s)
               Value function loss: 13.9475
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 362.40
               Mean episode length: 243.83
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 11.10s
                        Total time: 9604.74s
                               ETA: 1108536.3s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.563s, learning 0.186s)
               Value function loss: 13.7247
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 365.62
               Mean episode length: 246.23
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 10.75s
                        Total time: 9615.48s
                               ETA: 1108475.2s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.671s, learning 0.169s)
               Value function loss: 13.8040
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 357.30
               Mean episode length: 246.32
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 10.84s
                        Total time: 9626.32s
                               ETA: 1108424.8s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1514 steps/s (collection: 10.635s, learning 0.186s)
               Value function loss: 19.1605
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 376.89
               Mean episode length: 246.91
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 10.82s
                        Total time: 9637.14s
                               ETA: 1108372.2s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1532 steps/s (collection: 10.527s, learning 0.168s)
               Value function loss: 17.6639
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 364.26
               Mean episode length: 245.15
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 10.69s
                        Total time: 9647.84s
                               ETA: 1108305.3s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1540 steps/s (collection: 10.467s, learning 0.172s)
               Value function loss: 19.1228
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 382.90
               Mean episode length: 246.92
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 10.64s
                        Total time: 9658.48s
                               ETA: 1108232.0s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.840s, learning 0.257s)
               Value function loss: 18.4716
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 374.82
               Mean episode length: 248.58
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 11.10s
                        Total time: 9669.57s
                               ETA: 1108211.4s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1533 steps/s (collection: 10.519s, learning 0.162s)
               Value function loss: 18.7200
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 379.68
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 10.68s
                        Total time: 9680.25s
                               ETA: 1108143.2s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.788s, learning 0.165s)
               Value function loss: 22.6694
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 370.08
               Mean episode length: 245.86
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 10.95s
                        Total time: 9691.21s
                               ETA: 1108106.3s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.514s, learning 0.280s)
               Value function loss: 22.6373
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 393.71
               Mean episode length: 249.26
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 10.79s
                        Total time: 9702.00s
                               ETA: 1108051.3s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.786s, learning 0.166s)
               Value function loss: 21.0117
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 386.61
               Mean episode length: 248.51
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 10.95s
                        Total time: 9712.95s
                               ETA: 1108014.4s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1529 steps/s (collection: 10.540s, learning 0.175s)
               Value function loss: 17.5132
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 386.73
               Mean episode length: 249.19
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 10.71s
                        Total time: 9723.67s
                               ETA: 1107950.5s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1503 steps/s (collection: 10.681s, learning 0.219s)
               Value function loss: 16.7569
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 380.90
               Mean episode length: 247.20
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 10.90s
                        Total time: 9734.57s
                               ETA: 1107907.9s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.755s, learning 0.191s)
               Value function loss: 16.3207
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 372.45
               Mean episode length: 245.02
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 10.95s
                        Total time: 9745.51s
                               ETA: 1107870.5s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1481 steps/s (collection: 10.885s, learning 0.176s)
               Value function loss: 13.6956
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 384.42
               Mean episode length: 248.17
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 11.06s
                        Total time: 9756.58s
                               ETA: 1107846.3s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1518 steps/s (collection: 10.616s, learning 0.174s)
               Value function loss: 10.5029
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 390.40
               Mean episode length: 247.56
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 10.79s
                        Total time: 9767.36s
                               ETA: 1107791.3s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.599s, learning 0.169s)
               Value function loss: 9.3308
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 384.41
               Mean episode length: 245.44
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 10.77s
                        Total time: 9778.13s
                               ETA: 1107733.8s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.798s, learning 0.177s)
               Value function loss: 5.0321
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 369.16
               Mean episode length: 240.47
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 10.98s
                        Total time: 9789.11s
                               ETA: 1107700.0s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.410s, learning 0.166s)
               Value function loss: 6.2551
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 365.06
               Mean episode length: 239.71
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 10.58s
                        Total time: 9799.68s
                               ETA: 1107621.2s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.832s, learning 0.176s)
               Value function loss: 7.5768
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 364.06
               Mean episode length: 239.85
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 11.01s
                        Total time: 9810.69s
                               ETA: 1107591.2s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.391s, learning 0.160s)
               Value function loss: 7.9743
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 352.46
               Mean episode length: 235.51
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 10.55s
                        Total time: 9821.24s
                               ETA: 1107509.8s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.572s, learning 0.184s)
               Value function loss: 6.5123
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 357.18
               Mean episode length: 236.90
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 10.76s
                        Total time: 9832.00s
                               ETA: 1107451.7s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.547s, learning 0.177s)
               Value function loss: 6.2156
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 345.15
               Mean episode length: 235.23
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 10.72s
                        Total time: 9842.72s
                               ETA: 1107390.0s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.551s, learning 0.170s)
               Value function loss: 8.8533
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 343.47
               Mean episode length: 234.42
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 10.72s
                        Total time: 9853.44s
                               ETA: 1107328.2s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1552 steps/s (collection: 10.390s, learning 0.163s)
               Value function loss: 10.6528
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 354.70
               Mean episode length: 238.27
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 10.55s
                        Total time: 9864.00s
                               ETA: 1107247.6s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.752s, learning 0.159s)
               Value function loss: 7.9356
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 354.26
               Mean episode length: 237.84
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 10.91s
                        Total time: 9874.91s
                               ETA: 1107207.2s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.790s, learning 0.165s)
               Value function loss: 12.3063
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 361.73
               Mean episode length: 241.78
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 10.95s
                        Total time: 9885.86s
                               ETA: 1107171.9s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.828s, learning 0.174s)
               Value function loss: 12.3293
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 355.10
               Mean episode length: 239.88
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 11.00s
                        Total time: 9896.86s
                               ETA: 1107141.9s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1566 steps/s (collection: 10.258s, learning 0.203s)
               Value function loss: 14.3382
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 361.82
               Mean episode length: 238.44
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 10.46s
                        Total time: 9907.33s
                               ETA: 1107051.5s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.656s, learning 0.176s)
               Value function loss: 11.3579
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 361.02
               Mean episode length: 239.27
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 10.83s
                        Total time: 9918.16s
                               ETA: 1107002.6s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.591s, learning 0.160s)
               Value function loss: 13.9485
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 364.41
               Mean episode length: 244.45
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 10.75s
                        Total time: 9928.91s
                               ETA: 1106944.8s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.689s, learning 0.194s)
               Value function loss: 13.4772
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 348.40
               Mean episode length: 243.64
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 10.88s
                        Total time: 9939.79s
                               ETA: 1106901.9s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.572s, learning 0.191s)
               Value function loss: 13.8478
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 330.54
               Mean episode length: 239.76
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 10.76s
                        Total time: 9950.55s
                               ETA: 1106845.7s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1530 steps/s (collection: 10.550s, learning 0.158s)
               Value function loss: 15.7894
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 354.61
               Mean episode length: 241.54
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 10.71s
                        Total time: 9961.26s
                               ETA: 1106783.5s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.638s, learning 0.165s)
               Value function loss: 16.0212
                    Surrogate loss: -0.0198
             Mean action noise std: 0.77
                       Mean reward: 362.25
               Mean episode length: 244.83
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 10.80s
                        Total time: 9972.07s
                               ETA: 1106731.8s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.170s)
               Value function loss: 15.4507
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 343.97
               Mean episode length: 243.72
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 10.93s
                        Total time: 9982.99s
                               ETA: 1106694.2s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1525 steps/s (collection: 10.556s, learning 0.186s)
               Value function loss: 20.6317
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 382.79
               Mean episode length: 248.46
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 10.74s
                        Total time: 9993.74s
                               ETA: 1106635.9s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1480 steps/s (collection: 10.907s, learning 0.161s)
               Value function loss: 17.0684
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 362.89
               Mean episode length: 247.51
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 11.07s
                        Total time: 10004.80s
                               ETA: 1106613.9s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.663s, learning 0.163s)
               Value function loss: 18.9330
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 350.29
               Mean episode length: 244.58
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 10.83s
                        Total time: 10015.63s
                               ETA: 1106565.1s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1502 steps/s (collection: 10.713s, learning 0.192s)
               Value function loss: 21.5741
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 367.79
               Mean episode length: 247.33
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 10.91s
                        Total time: 10026.53s
                               ETA: 1106525.2s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1577 steps/s (collection: 10.217s, learning 0.171s)
               Value function loss: 22.0919
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 246.72
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 10.39s
                        Total time: 10036.92s
                               ETA: 1106428.4s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1504 steps/s (collection: 10.723s, learning 0.166s)
               Value function loss: 18.8808
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 366.60
               Mean episode length: 247.04
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 10.89s
                        Total time: 10047.81s
                               ETA: 1106386.9s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.451s, learning 0.171s)
               Value function loss: 18.7737
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 363.64
               Mean episode length: 246.70
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 10.62s
                        Total time: 10058.43s
                               ETA: 1106316.1s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.605s, learning 0.167s)
               Value function loss: 15.7799
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 371.82
               Mean episode length: 248.75
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 10.77s
                        Total time: 10069.21s
                               ETA: 1106261.9s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.699s, learning 0.164s)
               Value function loss: 15.8442
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 379.07
               Mean episode length: 247.78
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 10.86s
                        Total time: 10080.07s
                               ETA: 1106217.9s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1549 steps/s (collection: 10.393s, learning 0.179s)
               Value function loss: 12.6581
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 365.29
               Mean episode length: 247.77
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 10.57s
                        Total time: 10090.64s
                               ETA: 1106141.9s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1505 steps/s (collection: 10.706s, learning 0.178s)
               Value function loss: 8.6996
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 362.62
               Mean episode length: 248.04
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 10.88s
                        Total time: 10101.53s
                               ETA: 1106100.3s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.395s, learning 0.262s)
               Value function loss: 7.7865
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 365.33
               Mean episode length: 247.48
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 10.66s
                        Total time: 10112.18s
                               ETA: 1106033.9s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1535 steps/s (collection: 10.505s, learning 0.167s)
               Value function loss: 6.2411
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 361.65
               Mean episode length: 247.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 10.67s
                        Total time: 10122.85s
                               ETA: 1105969.2s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.763s, learning 0.213s)
               Value function loss: 5.2962
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 352.29
               Mean episode length: 242.26
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 10.98s
                        Total time: 10133.83s
                               ETA: 1105937.9s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.828s, learning 0.164s)
               Value function loss: 8.8928
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 349.04
               Mean episode length: 240.01
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 10.99s
                        Total time: 10144.82s
                               ETA: 1105908.3s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.694s, learning 0.165s)
               Value function loss: 6.7964
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 334.42
               Mean episode length: 234.04
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 10.86s
                        Total time: 10155.68s
                               ETA: 1105864.4s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.679s, learning 0.179s)
               Value function loss: 7.9678
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 338.66
               Mean episode length: 236.37
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 10.86s
                        Total time: 10166.54s
                               ETA: 1105820.3s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.488s, learning 0.168s)
               Value function loss: 6.4431
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 327.51
               Mean episode length: 233.76
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 10.66s
                        Total time: 10177.19s
                               ETA: 1105754.3s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.450s, learning 0.173s)
               Value function loss: 8.0493
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 324.96
               Mean episode length: 236.07
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 10.62s
                        Total time: 10187.82s
                               ETA: 1105685.0s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.551s, learning 0.172s)
               Value function loss: 12.2312
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 324.96
               Mean episode length: 237.36
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 10.72s
                        Total time: 10198.54s
                               ETA: 1105626.7s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.597s, learning 0.206s)
               Value function loss: 10.6335
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 337.31
               Mean episode length: 240.02
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 10.80s
                        Total time: 10209.34s
                               ETA: 1105577.1s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1537 steps/s (collection: 10.490s, learning 0.165s)
               Value function loss: 13.6969
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 338.42
               Mean episode length: 241.12
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 10.66s
                        Total time: 10220.00s
                               ETA: 1105511.6s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.670s, learning 0.169s)
               Value function loss: 12.8052
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 339.11
               Mean episode length: 244.12
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 10.84s
                        Total time: 10230.84s
                               ETA: 1105466.0s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.661s, learning 0.172s)
               Value function loss: 13.3663
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 330.96
               Mean episode length: 244.47
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 10.83s
                        Total time: 10241.67s
                               ETA: 1105419.9s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1499 steps/s (collection: 10.703s, learning 0.226s)
               Value function loss: 12.0954
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 322.03
               Mean episode length: 239.11
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 10.93s
                        Total time: 10252.60s
                               ETA: 1105384.2s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1513 steps/s (collection: 10.645s, learning 0.180s)
               Value function loss: 15.0158
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 329.90
               Mean episode length: 239.16
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 10.83s
                        Total time: 10263.42s
                               ETA: 1105337.4s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.584s, learning 0.163s)
               Value function loss: 14.4926
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 339.98
               Mean episode length: 243.90
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 10.75s
                        Total time: 10274.17s
                               ETA: 1105282.2s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1528 steps/s (collection: 10.555s, learning 0.167s)
               Value function loss: 20.1769
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 348.15
               Mean episode length: 244.05
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 10.72s
                        Total time: 10284.89s
                               ETA: 1105224.5s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1524 steps/s (collection: 10.581s, learning 0.168s)
               Value function loss: 17.2751
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 324.44
               Mean episode length: 238.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 10.75s
                        Total time: 10295.64s
                               ETA: 1105169.8s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1548 steps/s (collection: 10.405s, learning 0.173s)
               Value function loss: 18.3254
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 335.51
               Mean episode length: 240.73
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 10.58s
                        Total time: 10306.22s
                               ETA: 1105096.8s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1506 steps/s (collection: 10.716s, learning 0.160s)
               Value function loss: 16.5962
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 346.68
               Mean episode length: 244.13
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 10.88s
                        Total time: 10317.10s
                               ETA: 1105055.8s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.088s, learning 0.194s)
               Value function loss: 19.4162
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 332.48
               Mean episode length: 244.52
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 16.28s
                        Total time: 10333.38s
                               ETA: 1105593.4s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 779 steps/s (collection: 20.824s, learning 0.185s)
               Value function loss: 19.6579
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 353.79
               Mean episode length: 246.51
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 21.01s
                        Total time: 10354.39s
                               ETA: 1106635.0s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 791 steps/s (collection: 20.519s, learning 0.171s)
               Value function loss: 20.1631
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 352.84
               Mean episode length: 246.77
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 20.69s
                        Total time: 10375.08s
                               ETA: 1107640.2s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.834s, learning 0.299s)
               Value function loss: 16.7941
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 336.01
               Mean episode length: 244.01
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 21.13s
                        Total time: 10396.21s
                               ETA: 1108690.5s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 785 steps/s (collection: 20.666s, learning 0.189s)
               Value function loss: 20.4514
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 352.31
               Mean episode length: 246.85
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 20.85s
                        Total time: 10417.07s
                               ETA: 1109708.8s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 781 steps/s (collection: 20.816s, learning 0.157s)
               Value function loss: 15.9150
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 332.62
               Mean episode length: 245.79
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 20.97s
                        Total time: 10438.04s
                               ETA: 1110737.4s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.941s, learning 0.176s)
               Value function loss: 16.5645
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 347.05
               Mean episode length: 247.70
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 21.12s
                        Total time: 10459.16s
                               ETA: 1111779.2s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 763 steps/s (collection: 21.282s, learning 0.183s)
               Value function loss: 13.1348
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 319.69
               Mean episode length: 244.64
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 21.46s
                        Total time: 10480.62s
                               ETA: 1112855.5s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 784 steps/s (collection: 20.709s, learning 0.176s)
               Value function loss: 14.3669
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 329.26
               Mean episode length: 244.86
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 20.89s
                        Total time: 10501.51s
                               ETA: 1113868.0s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 790 steps/s (collection: 20.569s, learning 0.171s)
               Value function loss: 11.2901
                    Surrogate loss: -0.0224
             Mean action noise std: 0.77
                       Mean reward: 321.52
               Mean episode length: 244.30
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 20.74s
                        Total time: 10522.25s
                               ETA: 1114862.8s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.965s, learning 0.162s)
               Value function loss: 7.4971
                    Surrogate loss: -0.0229
             Mean action noise std: 0.77
                       Mean reward: 333.66
               Mean episode length: 245.82
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 21.13s
                        Total time: 10543.37s
                               ETA: 1115896.6s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 791 steps/s (collection: 20.515s, learning 0.183s)
               Value function loss: 5.6771
                    Surrogate loss: -0.0249
             Mean action noise std: 0.77
                       Mean reward: 334.46
               Mean episode length: 248.16
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 20.70s
                        Total time: 10564.07s
                               ETA: 1116882.7s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 787 steps/s (collection: 20.641s, learning 0.171s)
               Value function loss: 6.5787
                    Surrogate loss: -0.0245
             Mean action noise std: 0.77
                       Mean reward: 318.82
               Mean episode length: 247.54
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 20.81s
                        Total time: 10584.88s
                               ETA: 1117878.7s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.966s, learning 0.159s)
               Value function loss: 4.8492
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 306.42
               Mean episode length: 242.06
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 21.13s
                        Total time: 10606.01s
                               ETA: 1118905.6s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 766 steps/s (collection: 21.075s, learning 0.298s)
               Value function loss: 7.0373
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 304.08
               Mean episode length: 241.51
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 21.37s
                        Total time: 10627.38s
                               ETA: 1119956.3s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 764 steps/s (collection: 21.248s, learning 0.176s)
               Value function loss: 5.6984
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 301.68
               Mean episode length: 240.90
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 21.42s
                        Total time: 10648.81s
                               ETA: 1121010.2s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.962s, learning 0.173s)
               Value function loss: 7.0465
                    Surrogate loss: -0.0227
             Mean action noise std: 0.77
                       Mean reward: 293.91
               Mean episode length: 243.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 21.14s
                        Total time: 10669.94s
                               ETA: 1122031.4s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.874s, learning 0.210s)
               Value function loss: 5.4941
                    Surrogate loss: -0.0265
             Mean action noise std: 0.77
                       Mean reward: 296.72
               Mean episode length: 243.04
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 21.08s
                        Total time: 10691.02s
                               ETA: 1123045.0s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 779 steps/s (collection: 20.845s, learning 0.169s)
               Value function loss: 8.2532
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 298.96
               Mean episode length: 243.87
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 21.01s
                        Total time: 10712.04s
                               ETA: 1124049.1s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 786 steps/s (collection: 20.666s, learning 0.172s)
               Value function loss: 9.0518
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 311.03
               Mean episode length: 243.52
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 20.84s
                        Total time: 10732.88s
                               ETA: 1125032.6s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 780 steps/s (collection: 20.824s, learning 0.171s)
               Value function loss: 10.5405
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 317.30
               Mean episode length: 246.33
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 21.00s
                        Total time: 10753.87s
                               ETA: 1126030.4s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 809 steps/s (collection: 20.065s, learning 0.167s)
               Value function loss: 9.8584
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 311.27
               Mean episode length: 248.11
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 20.23s
                        Total time: 10774.10s
                               ETA: 1126946.3s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.854s, learning 0.180s)
               Value function loss: 11.2024
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 313.51
               Mean episode length: 248.80
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 21.03s
                        Total time: 10795.14s
                               ETA: 1127943.9s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 794 steps/s (collection: 20.375s, learning 0.242s)
               Value function loss: 9.1922
                    Surrogate loss: -0.0236
             Mean action noise std: 0.76
                       Mean reward: 313.52
               Mean episode length: 248.80
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 20.62s
                        Total time: 10815.76s
                               ETA: 1128895.9s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 787 steps/s (collection: 20.608s, learning 0.194s)
               Value function loss: 12.3701
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 304.86
               Mean episode length: 248.55
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 20.80s
                        Total time: 10836.56s
                               ETA: 1129865.1s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 783 steps/s (collection: 20.756s, learning 0.167s)
               Value function loss: 11.7794
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 309.51
               Mean episode length: 248.85
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 20.92s
                        Total time: 10857.48s
                               ETA: 1130844.8s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 786 steps/s (collection: 20.632s, learning 0.212s)
               Value function loss: 11.7190
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 305.81
               Mean episode length: 247.30
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 20.84s
                        Total time: 10878.32s
                               ETA: 1131814.2s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 787 steps/s (collection: 20.625s, learning 0.192s)
               Value function loss: 13.9987
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 313.79
               Mean episode length: 247.33
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 20.82s
                        Total time: 10899.14s
                               ETA: 1132778.7s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 780 steps/s (collection: 20.702s, learning 0.282s)
               Value function loss: 13.5871
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 320.64
               Mean episode length: 249.02
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 20.98s
                        Total time: 10920.13s
                               ETA: 1133758.6s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 768 steps/s (collection: 21.158s, learning 0.170s)
               Value function loss: 12.1470
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 327.42
               Mean episode length: 247.84
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 21.33s
                        Total time: 10941.45s
                               ETA: 1134771.9s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 782 steps/s (collection: 20.729s, learning 0.197s)
               Value function loss: 12.0116
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 312.05
               Mean episode length: 249.05
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 20.93s
                        Total time: 10962.38s
                               ETA: 1135741.5s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 788 steps/s (collection: 20.597s, learning 0.179s)
               Value function loss: 12.6887
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 303.67
               Mean episode length: 247.19
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 20.78s
                        Total time: 10983.15s
                               ETA: 1136693.4s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 778 steps/s (collection: 20.877s, learning 0.163s)
               Value function loss: 12.7566
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 312.90
               Mean episode length: 249.18
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 21.04s
                        Total time: 11004.19s
                               ETA: 1137670.6s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 775 steps/s (collection: 20.964s, learning 0.171s)
               Value function loss: 13.2124
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 297.71
               Mean episode length: 248.58
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 21.13s
                        Total time: 11025.33s
                               ETA: 1138655.5s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 769 steps/s (collection: 21.065s, learning 0.217s)
               Value function loss: 11.1506
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 303.26
               Mean episode length: 248.57
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 21.28s
                        Total time: 11046.61s
                               ETA: 1139653.5s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 777 steps/s (collection: 20.900s, learning 0.160s)
               Value function loss: 15.6604
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 322.76
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 21.06s
                        Total time: 11067.67s
                               ETA: 1140626.5s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 783 steps/s (collection: 20.740s, learning 0.179s)
               Value function loss: 12.7401
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 321.49
               Mean episode length: 249.47
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 20.92s
                        Total time: 11088.59s
                               ETA: 1141583.0s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 788 steps/s (collection: 20.602s, learning 0.174s)
               Value function loss: 13.3570
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 322.64
               Mean episode length: 249.20
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 20.78s
                        Total time: 11109.37s
                               ETA: 1142522.8s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.729s, learning 0.175s)
               Value function loss: 11.4680
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 308.00
               Mean episode length: 249.77
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 11.90s
                        Total time: 11121.27s
                               ETA: 1142549.0s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1517 steps/s (collection: 10.632s, learning 0.164s)
               Value function loss: 12.0441
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 321.50
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 10.80s
                        Total time: 11132.07s
                               ETA: 1142461.4s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1507 steps/s (collection: 10.682s, learning 0.183s)
               Value function loss: 9.7280
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 314.30
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 10.87s
                        Total time: 11142.93s
                               ETA: 1142381.1s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1512 steps/s (collection: 10.655s, learning 0.180s)
               Value function loss: 7.3209
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 328.40
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 10.84s
                        Total time: 11153.77s
                               ETA: 1142297.9s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.687s, learning 0.171s)
               Value function loss: 6.0075
                    Surrogate loss: -0.0254
             Mean action noise std: 0.76
                       Mean reward: 324.42
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 10.86s
                        Total time: 11164.62s
                               ETA: 1142217.2s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1511 steps/s (collection: 10.680s, learning 0.160s)
               Value function loss: 7.2064
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 312.59
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 10.84s
                        Total time: 11175.46s
                               ETA: 1142134.7s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.712s, learning 0.229s)
               Value function loss: 6.1369
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 308.97
               Mean episode length: 249.98
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 10.94s
                        Total time: 11186.40s
                               ETA: 1142062.7s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1516 steps/s (collection: 10.642s, learning 0.165s)
               Value function loss: 5.9873
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 309.51
               Mean episode length: 249.98
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 10.81s
                        Total time: 11197.21s
                               ETA: 1141977.2s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.695s, learning 0.165s)
               Value function loss: 7.1445
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 306.47
               Mean episode length: 249.98
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 10.86s
                        Total time: 11208.07s
                               ETA: 1141897.3s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1521 steps/s (collection: 10.581s, learning 0.185s)
               Value function loss: 8.4043
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 310.50
               Mean episode length: 249.98
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 10.77s
                        Total time: 11218.84s
                               ETA: 1141807.9s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.739s, learning 0.179s)
               Value function loss: 8.4222
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 307.45
               Mean episode length: 247.81
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 10.92s
                        Total time: 11229.76s
                               ETA: 1141734.1s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.508s, learning 0.169s)
               Value function loss: 9.6989
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 295.87
               Mean episode length: 247.74
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 10.68s
                        Total time: 11240.43s
                               ETA: 1141636.0s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.840s, learning 0.270s)
               Value function loss: 10.2835
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 295.18
               Mean episode length: 246.91
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 11.11s
                        Total time: 11251.54s
                               ETA: 1141582.0s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1483 steps/s (collection: 10.858s, learning 0.188s)
               Value function loss: 16.2385
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 293.43
               Mean episode length: 249.17
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 11.05s
                        Total time: 11262.59s
                               ETA: 1141521.7s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.826s, learning 0.178s)
               Value function loss: 21.0558
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 299.05
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 11.00s
                        Total time: 11273.59s
                               ETA: 1141457.1s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.903s, learning 0.170s)
               Value function loss: 16.0357
                    Surrogate loss: 0.0264
             Mean action noise std: 0.76
                       Mean reward: 299.66
               Mean episode length: 249.49
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 11.07s
                        Total time: 11284.67s
                               ETA: 1141399.7s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1551 steps/s (collection: 10.398s, learning 0.165s)
               Value function loss: 12.0394
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 289.39
               Mean episode length: 248.45
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 10.56s
                        Total time: 11295.23s
                               ETA: 1141290.7s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.838s, learning 0.171s)
               Value function loss: 14.9125
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 284.72
               Mean episode length: 248.96
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 11.01s
                        Total time: 11306.24s
                               ETA: 1141227.1s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1495 steps/s (collection: 10.786s, learning 0.170s)
               Value function loss: 10.8617
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 292.14
               Mean episode length: 249.73
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 10.96s
                        Total time: 11317.19s
                               ETA: 1141158.1s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1542 steps/s (collection: 10.459s, learning 0.161s)
               Value function loss: 8.9749
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 291.91
               Mean episode length: 249.78
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 10.62s
                        Total time: 11327.81s
                               ETA: 1141055.5s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1482 steps/s (collection: 10.881s, learning 0.169s)
               Value function loss: 10.6720
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 280.66
               Mean episode length: 249.45
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 11.05s
                        Total time: 11338.86s
                               ETA: 1140996.2s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1561 steps/s (collection: 10.323s, learning 0.168s)
               Value function loss: 10.4343
                    Surrogate loss: 0.0024
             Mean action noise std: 0.76
                       Mean reward: 275.68
               Mean episode length: 248.31
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 10.49s
                        Total time: 11349.36s
                               ETA: 1140881.0s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1531 steps/s (collection: 10.507s, learning 0.188s)
               Value function loss: 9.6707
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 265.49
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 10.70s
                        Total time: 11360.05s
                               ETA: 1140786.4s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.741s, learning 0.171s)
               Value function loss: 10.3379
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 257.76
               Mean episode length: 249.12
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 10.91s
                        Total time: 11370.96s
                               ETA: 1140713.8s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1554 steps/s (collection: 10.381s, learning 0.161s)
               Value function loss: 9.2439
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 248.21
               Mean episode length: 249.25
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 10.54s
                        Total time: 11381.50s
                               ETA: 1140604.2s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.748s, learning 0.187s)
               Value function loss: 9.1408
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 238.83
               Mean episode length: 249.54
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 10.93s
                        Total time: 11392.44s
                               ETA: 1140534.1s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.746s, learning 0.169s)
               Value function loss: 9.1488
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 231.47
               Mean episode length: 249.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 10.91s
                        Total time: 11403.35s
                               ETA: 1140462.1s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1523 steps/s (collection: 10.560s, learning 0.195s)
               Value function loss: 9.0080
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 243.36
               Mean episode length: 249.81
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 10.76s
                        Total time: 11414.11s
                               ETA: 1140374.4s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1500 steps/s (collection: 10.760s, learning 0.160s)
               Value function loss: 11.3657
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 254.09
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 10.92s
                        Total time: 11425.03s
                               ETA: 1140303.2s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.099s, learning 0.183s)
               Value function loss: 10.8542
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 255.75
               Mean episode length: 249.74
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 11.28s
                        Total time: 11436.31s
                               ETA: 1140268.1s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.966s, learning 0.160s)
               Value function loss: 10.7879
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 266.92
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 11.13s
                        Total time: 11447.44s
                               ETA: 1140217.6s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.026s, learning 0.186s)
               Value function loss: 9.4696
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 255.83
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 11.21s
                        Total time: 11458.65s
                               ETA: 1140175.9s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.924s, learning 0.172s)
               Value function loss: 8.9427
                    Surrogate loss: -0.0240
             Mean action noise std: 0.76
                       Mean reward: 255.81
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 11.10s
                        Total time: 11469.74s
                               ETA: 1140122.6s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1496 steps/s (collection: 10.789s, learning 0.161s)
               Value function loss: 8.9105
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 253.29
               Mean episode length: 249.41
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 10.95s
                        Total time: 11480.69s
                               ETA: 1140054.8s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1494 steps/s (collection: 10.802s, learning 0.159s)
               Value function loss: 9.0048
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 257.04
               Mean episode length: 249.86
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 10.96s
                        Total time: 11491.66s
                               ETA: 1139988.4s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1527 steps/s (collection: 10.561s, learning 0.167s)
               Value function loss: 6.6514
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 256.07
               Mean episode length: 249.86
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 10.73s
                        Total time: 11502.38s
                               ETA: 1139898.9s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1536 steps/s (collection: 10.504s, learning 0.159s)
               Value function loss: 7.4920
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 263.79
               Mean episode length: 249.87
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 10.66s
                        Total time: 11513.05s
                               ETA: 1139803.2s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.864s, learning 0.210s)
               Value function loss: 6.0634
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 263.11
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 11.07s
                        Total time: 11524.12s
                               ETA: 1139748.3s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.679s, learning 0.168s)
               Value function loss: 6.3231
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 271.60
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 10.85s
                        Total time: 11534.97s
                               ETA: 1139671.0s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.493s, learning 0.173s)
               Value function loss: 7.2457
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 278.22
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 10.67s
                        Total time: 11545.64s
                               ETA: 1139576.1s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.702s, learning 0.169s)
               Value function loss: 9.3353
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 284.15
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 10.87s
                        Total time: 11556.51s
                               ETA: 1139501.4s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.565s, learning 0.171s)
               Value function loss: 7.5030
                    Surrogate loss: 0.0034
             Mean action noise std: 0.76
                       Mean reward: 292.66
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 10.74s
                        Total time: 11567.24s
                               ETA: 1139413.5s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.818s, learning 0.179s)
               Value function loss: 8.4347
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 300.59
               Mean episode length: 249.80
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 11.00s
                        Total time: 11578.24s
                               ETA: 1139351.5s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.874s, learning 0.171s)
               Value function loss: 8.2247
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 321.76
               Mean episode length: 249.80
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 11.05s
                        Total time: 11589.28s
                               ETA: 1139294.4s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.893s, learning 0.171s)
               Value function loss: 11.7547
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 329.54
               Mean episode length: 249.80
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.06s
                        Total time: 11600.35s
                               ETA: 1139239.2s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.724s, learning 0.169s)
               Value function loss: 11.5597
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 345.73
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 10.89s
                        Total time: 11611.24s
                               ETA: 1139167.3s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.720s, learning 0.244s)
               Value function loss: 12.4176
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 342.13
               Mean episode length: 249.90
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 10.96s
                        Total time: 11622.20s
                               ETA: 1139102.5s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.765s, learning 0.185s)
               Value function loss: 13.1956
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 345.43
               Mean episode length: 249.90
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 10.95s
                        Total time: 11633.15s
                               ETA: 1139036.4s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.615s, learning 0.174s)
               Value function loss: 15.5044
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 356.50
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 10.79s
                        Total time: 11643.94s
                               ETA: 1138954.7s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.711s, learning 0.202s)
               Value function loss: 12.4401
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 360.61
               Mean episode length: 249.42
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 10.91s
                        Total time: 11654.85s
                               ETA: 1138885.3s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.847s, learning 0.164s)
               Value function loss: 16.8992
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 359.25
               Mean episode length: 249.42
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 11.01s
                        Total time: 11665.87s
                               ETA: 1138825.5s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.731s, learning 0.170s)
               Value function loss: 15.9505
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 378.87
               Mean episode length: 249.42
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 10.90s
                        Total time: 11676.77s
                               ETA: 1138755.1s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.681s, learning 0.198s)
               Value function loss: 19.5268
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 373.33
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 10.88s
                        Total time: 11687.65s
                               ETA: 1138682.7s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.525s, learning 0.184s)
               Value function loss: 20.1630
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 371.00
               Mean episode length: 249.64
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 10.71s
                        Total time: 11698.35s
                               ETA: 1138593.8s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.825s, learning 0.160s)
               Value function loss: 21.4688
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 374.64
               Mean episode length: 249.76
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 10.99s
                        Total time: 11709.34s
                               ETA: 1138531.9s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.917s, learning 0.198s)
               Value function loss: 20.8112
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 366.14
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 11.12s
                        Total time: 11720.45s
                               ETA: 1138482.8s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.810s, learning 0.163s)
               Value function loss: 20.7741
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 369.84
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 10.97s
                        Total time: 11731.43s
                               ETA: 1138420.1s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.701s, learning 0.155s)
               Value function loss: 17.0674
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 369.52
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 10.86s
                        Total time: 11742.28s
                               ETA: 1138346.1s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.883s, learning 0.169s)
               Value function loss: 19.5981
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 365.02
               Mean episode length: 249.93
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 11.05s
                        Total time: 11753.34s
                               ETA: 1138291.1s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.787s, learning 0.165s)
               Value function loss: 19.4844
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 375.41
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 10.95s
                        Total time: 11764.29s
                               ETA: 1138226.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.829s, learning 0.158s)
               Value function loss: 20.6592
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 362.02
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 10.99s
                        Total time: 11775.28s
                               ETA: 1138165.5s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.971s, learning 0.187s)
               Value function loss: 17.8851
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 360.83
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 11.16s
                        Total time: 11786.43s
                               ETA: 1138121.1s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.934s, learning 0.164s)
               Value function loss: 16.2628
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 379.63
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 11.10s
                        Total time: 11797.53s
                               ETA: 1138070.9s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.790s, learning 0.173s)
               Value function loss: 14.5829
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 370.87
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 10.96s
                        Total time: 11808.50s
                               ETA: 1138007.8s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.946s, learning 0.203s)
               Value function loss: 13.0698
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 377.08
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 11.15s
                        Total time: 11819.64s
                               ETA: 1137962.7s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.068s, learning 0.174s)
               Value function loss: 12.3800
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 362.64
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 11.24s
                        Total time: 11830.89s
                               ETA: 1137926.6s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.971s, learning 0.168s)
               Value function loss: 6.5942
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 353.14
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 11.14s
                        Total time: 11842.02s
                               ETA: 1137880.5s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.814s, learning 0.172s)
               Value function loss: 6.0029
                    Surrogate loss: -0.0223
             Mean action noise std: 0.76
                       Mean reward: 350.14
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 10.99s
                        Total time: 11853.01s
                               ETA: 1137819.9s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.826s, learning 0.162s)
               Value function loss: 5.6637
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 352.99
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 10.99s
                        Total time: 11864.00s
                               ETA: 1137759.6s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.159s)
               Value function loss: 6.0287
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 348.74
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 11.09s
                        Total time: 11875.09s
                               ETA: 1137709.6s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.906s, learning 0.181s)
               Value function loss: 7.8863
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 355.13
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 11.09s
                        Total time: 11886.18s
                               ETA: 1137659.0s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.448s, learning 0.186s)
               Value function loss: 7.9498
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 354.60
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 10.63s
                        Total time: 11896.81s
                               ETA: 1137565.2s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.824s, learning 0.171s)
               Value function loss: 8.0705
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 355.48
               Mean episode length: 249.76
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 10.99s
                        Total time: 11907.81s
                               ETA: 1137505.9s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.762s, learning 0.194s)
               Value function loss: 7.7383
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 351.79
               Mean episode length: 249.76
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 10.96s
                        Total time: 11918.76s
                               ETA: 1137443.1s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.161s)
               Value function loss: 8.4951
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 346.12
               Mean episode length: 249.56
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 10.91s
                        Total time: 11929.67s
                               ETA: 1137376.0s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.880s, learning 0.185s)
               Value function loss: 10.2266
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 338.49
               Mean episode length: 249.80
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 11.07s
                        Total time: 11940.74s
                               ETA: 1137323.8s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.857s, learning 0.159s)
               Value function loss: 11.6410
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 330.52
               Mean episode length: 249.80
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 11.02s
                        Total time: 11951.75s
                               ETA: 1137266.9s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.644s, learning 0.185s)
               Value function loss: 12.6039
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 314.18
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 10.83s
                        Total time: 11962.58s
                               ETA: 1137192.4s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.921s, learning 0.185s)
               Value function loss: 12.7462
                    Surrogate loss: 0.0188
             Mean action noise std: 0.76
                       Mean reward: 304.02
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 11.11s
                        Total time: 11973.69s
                               ETA: 1137144.3s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.442s, learning 0.163s)
               Value function loss: 12.4985
                    Surrogate loss: 0.0020
             Mean action noise std: 0.76
                       Mean reward: 300.68
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 10.61s
                        Total time: 11984.29s
                               ETA: 1137048.8s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.419s, learning 0.167s)
               Value function loss: 11.5241
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 301.64
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 10.59s
                        Total time: 11994.88s
                               ETA: 1136951.5s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.848s, learning 0.168s)
               Value function loss: 13.4342
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 303.72
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 11.02s
                        Total time: 12005.90s
                               ETA: 1136895.2s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.653s, learning 0.164s)
               Value function loss: 15.1750
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 306.83
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 10.82s
                        Total time: 12016.71s
                               ETA: 1136820.2s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.163s)
               Value function loss: 13.5705
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 316.79
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 10.71s
                        Total time: 12027.42s
                               ETA: 1136735.2s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.167s)
               Value function loss: 10.9247
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 300.66
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 11.02s
                        Total time: 12038.44s
                               ETA: 1136679.2s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.715s, learning 0.164s)
               Value function loss: 11.7815
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 310.86
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 10.88s
                        Total time: 12049.32s
                               ETA: 1136610.4s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1548 steps/s (collection: 10.400s, learning 0.184s)
               Value function loss: 11.3209
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 305.85
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 10.58s
                        Total time: 12059.90s
                               ETA: 1136513.8s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.752s, learning 0.290s)
               Value function loss: 14.0891
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 309.80
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 11.04s
                        Total time: 12070.95s
                               ETA: 1136460.6s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.164s)
               Value function loss: 9.8821
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 313.87
               Mean episode length: 249.51
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 10.76s
                        Total time: 12081.71s
                               ETA: 1136381.1s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.511s, learning 0.166s)
               Value function loss: 12.3974
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 307.99
               Mean episode length: 249.48
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 10.68s
                        Total time: 12092.39s
                               ETA: 1136293.8s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.735s, learning 0.157s)
               Value function loss: 9.2812
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 315.25
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 10.89s
                        Total time: 12103.28s
                               ETA: 1136226.8s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.538s, learning 0.162s)
               Value function loss: 11.1949
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 307.38
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 10.70s
                        Total time: 12113.98s
                               ETA: 1136141.8s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.730s, learning 0.188s)
               Value function loss: 9.1749
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 310.63
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 10.92s
                        Total time: 12124.90s
                               ETA: 1136077.5s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.562s, learning 0.177s)
               Value function loss: 8.6717
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 307.83
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 10.74s
                        Total time: 12135.63s
                               ETA: 1135996.4s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.833s, learning 0.168s)
               Value function loss: 9.0863
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 322.50
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 11.00s
                        Total time: 12146.64s
                               ETA: 1135940.0s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.631s, learning 0.175s)
               Value function loss: 9.0882
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 307.46
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 10.81s
                        Total time: 12157.44s
                               ETA: 1135865.5s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.162s)
               Value function loss: 8.0591
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 301.81
               Mean episode length: 248.70
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 10.70s
                        Total time: 12168.14s
                               ETA: 1135780.9s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.867s, learning 0.169s)
               Value function loss: 5.8809
                    Surrogate loss: -0.0283
             Mean action noise std: 0.76
                       Mean reward: 304.06
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 11.04s
                        Total time: 12179.17s
                               ETA: 1135728.1s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.675s, learning 0.168s)
               Value function loss: 3.9988
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 297.88
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 10.84s
                        Total time: 12190.02s
                               ETA: 1135657.3s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.789s, learning 0.167s)
               Value function loss: 5.2209
                    Surrogate loss: -0.0247
             Mean action noise std: 0.76
                       Mean reward: 302.57
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 10.96s
                        Total time: 12200.97s
                               ETA: 1135597.2s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.014s, learning 0.271s)
               Value function loss: 4.6248
                    Surrogate loss: -0.0264
             Mean action noise std: 0.76
                       Mean reward: 309.76
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 11.28s
                        Total time: 12212.26s
                               ETA: 1135567.8s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.721s, learning 0.162s)
               Value function loss: 5.2230
                    Surrogate loss: -0.0250
             Mean action noise std: 0.76
                       Mean reward: 325.25
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 10.88s
                        Total time: 12223.14s
                               ETA: 1135501.1s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.602s, learning 0.172s)
               Value function loss: 5.2957
                    Surrogate loss: -0.0314
             Mean action noise std: 0.76
                       Mean reward: 330.75
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 10.77s
                        Total time: 12233.92s
                               ETA: 1135424.4s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.653s, learning 0.192s)
               Value function loss: 5.7878
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 329.66
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 10.85s
                        Total time: 12244.76s
                               ETA: 1135354.4s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.166s)
               Value function loss: 4.9716
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 324.90
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 10.90s
                        Total time: 12255.66s
                               ETA: 1135289.4s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.692s, learning 0.199s)
               Value function loss: 5.9355
                    Surrogate loss: -0.0259
             Mean action noise std: 0.76
                       Mean reward: 316.18
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 10.89s
                        Total time: 12266.55s
                               ETA: 1135223.9s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.664s, learning 0.166s)
               Value function loss: 6.2788
                    Surrogate loss: -0.0240
             Mean action noise std: 0.76
                       Mean reward: 319.19
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 10.83s
                        Total time: 12277.38s
                               ETA: 1135152.8s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.496s, learning 0.158s)
               Value function loss: 7.0784
                    Surrogate loss: -0.0285
             Mean action noise std: 0.76
                       Mean reward: 319.15
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 10.65s
                        Total time: 12288.03s
                               ETA: 1135065.5s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.631s, learning 0.174s)
               Value function loss: 7.2809
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 312.50
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 10.80s
                        Total time: 12298.84s
                               ETA: 1134992.3s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.712s, learning 0.187s)
               Value function loss: 7.8099
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 303.90
               Mean episode length: 249.87
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 10.90s
                        Total time: 12309.74s
                               ETA: 1134927.8s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.642s, learning 0.164s)
               Value function loss: 7.7282
                    Surrogate loss: -0.0247
             Mean action noise std: 0.76
                       Mean reward: 305.98
               Mean episode length: 249.87
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 10.81s
                        Total time: 12320.54s
                               ETA: 1134855.0s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.831s, learning 0.167s)
               Value function loss: 9.3471
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 318.70
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 11.00s
                        Total time: 12331.54s
                               ETA: 1134799.9s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.608s, learning 0.185s)
               Value function loss: 9.3253
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 310.21
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 10.79s
                        Total time: 12342.33s
                               ETA: 1134726.0s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.910s, learning 0.173s)
               Value function loss: 9.3588
                    Surrogate loss: -0.0218
             Mean action noise std: 0.76
                       Mean reward: 295.93
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 11.08s
                        Total time: 12353.41s
                               ETA: 1134678.9s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.826s, learning 0.186s)
               Value function loss: 10.7714
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 298.47
               Mean episode length: 249.20
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 11.01s
                        Total time: 12364.43s
                               ETA: 1134625.3s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.734s, learning 0.164s)
               Value function loss: 10.2612
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 287.79
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 10.90s
                        Total time: 12375.32s
                               ETA: 1134561.5s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.433s, learning 0.188s)
               Value function loss: 9.5444
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 287.17
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 10.62s
                        Total time: 12385.95s
                               ETA: 1134472.3s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.796s, learning 0.262s)
               Value function loss: 9.2104
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 283.71
               Mean episode length: 249.59
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 11.06s
                        Total time: 12397.00s
                               ETA: 1134423.2s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.750s, learning 0.181s)
               Value function loss: 10.2825
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 285.80
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 10.93s
                        Total time: 12407.93s
                               ETA: 1134362.7s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.239s, learning 0.160s)
               Value function loss: 10.8791
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 294.06
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 11.40s
                        Total time: 12419.33s
                               ETA: 1134345.0s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.605s, learning 0.174s)
               Value function loss: 10.1352
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 290.93
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 10.78s
                        Total time: 12430.11s
                               ETA: 1134270.7s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.477s, learning 0.155s)
               Value function loss: 9.4501
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 297.78
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 10.63s
                        Total time: 12440.74s
                               ETA: 1134183.2s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.827s, learning 0.199s)
               Value function loss: 11.1865
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 293.95
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 11.03s
                        Total time: 12451.77s
                               ETA: 1134131.6s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.608s, learning 0.160s)
               Value function loss: 10.1888
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 296.85
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 10.77s
                        Total time: 12462.54s
                               ETA: 1134056.7s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.166s)
               Value function loss: 9.0691
                    Surrogate loss: -0.0241
             Mean action noise std: 0.76
                       Mean reward: 291.43
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 11.27s
                        Total time: 12473.81s
                               ETA: 1134027.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.165s)
               Value function loss: 9.3201
                    Surrogate loss: -0.0248
             Mean action noise std: 0.76
                       Mean reward: 300.75
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 10.96s
                        Total time: 12484.77s
                               ETA: 1133970.5s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.761s, learning 0.167s)
               Value function loss: 8.4104
                    Surrogate loss: -0.0225
             Mean action noise std: 0.76
                       Mean reward: 304.70
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 10.93s
                        Total time: 12495.70s
                               ETA: 1133910.4s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.648s, learning 0.162s)
               Value function loss: 7.1206
                    Surrogate loss: -0.0242
             Mean action noise std: 0.76
                       Mean reward: 293.40
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 10.81s
                        Total time: 12506.51s
                               ETA: 1133839.7s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.644s, learning 0.167s)
               Value function loss: 5.8746
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 289.25
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 10.81s
                        Total time: 12517.32s
                               ETA: 1133769.2s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.387s, learning 0.167s)
               Value function loss: 4.5067
                    Surrogate loss: -0.0269
             Mean action noise std: 0.76
                       Mean reward: 289.35
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 10.55s
                        Total time: 12527.88s
                               ETA: 1133675.4s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.778s, learning 0.171s)
               Value function loss: 5.4170
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 279.50
               Mean episode length: 249.22
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 10.95s
                        Total time: 12538.83s
                               ETA: 1133617.6s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.816s, learning 0.159s)
               Value function loss: 3.9786
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 281.19
               Mean episode length: 249.22
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 10.97s
                        Total time: 12549.80s
                               ETA: 1133562.2s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.642s, learning 0.171s)
               Value function loss: 3.9701
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 285.71
               Mean episode length: 249.22
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 10.81s
                        Total time: 12560.61s
                               ETA: 1133492.3s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.717s, learning 0.161s)
               Value function loss: 4.9160
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 292.01
               Mean episode length: 249.75
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 10.88s
                        Total time: 12571.49s
                               ETA: 1133428.3s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.957s, learning 0.161s)
               Value function loss: 4.7854
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 281.45
               Mean episode length: 249.75
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 11.12s
                        Total time: 12582.61s
                               ETA: 1133386.1s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.743s, learning 0.173s)
               Value function loss: 4.2153
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 274.54
               Mean episode length: 249.75
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 10.92s
                        Total time: 12593.53s
                               ETA: 1133325.7s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.652s, learning 0.177s)
               Value function loss: 5.2255
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 271.15
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 10.83s
                        Total time: 12604.36s
                               ETA: 1133257.6s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.739s, learning 0.164s)
               Value function loss: 4.9114
                    Surrogate loss: -0.0281
             Mean action noise std: 0.76
                       Mean reward: 274.98
               Mean episode length: 249.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 10.90s
                        Total time: 12615.26s
                               ETA: 1133196.3s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.616s, learning 0.196s)
               Value function loss: 6.0344
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 275.77
               Mean episode length: 249.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 10.81s
                        Total time: 12626.07s
                               ETA: 1133126.8s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.608s, learning 0.165s)
               Value function loss: 6.3204
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 281.32
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 10.77s
                        Total time: 12636.84s
                               ETA: 1133054.0s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.034s, learning 0.169s)
               Value function loss: 7.6301
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 277.42
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 11.20s
                        Total time: 12648.05s
                               ETA: 1133019.8s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.374s, learning 0.167s)
               Value function loss: 6.5286
                    Surrogate loss: -0.0251
             Mean action noise std: 0.76
                       Mean reward: 270.41
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 10.54s
                        Total time: 12658.59s
                               ETA: 1132926.4s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.943s, learning 0.177s)
               Value function loss: 8.3642
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 268.19
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 11.12s
                        Total time: 12669.71s
                               ETA: 1132885.0s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.864s, learning 0.176s)
               Value function loss: 7.3941
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 272.85
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 11.04s
                        Total time: 12680.75s
                               ETA: 1132836.4s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.161s)
               Value function loss: 7.0449
                    Surrogate loss: -0.0254
             Mean action noise std: 0.76
                       Mean reward: 273.27
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 10.76s
                        Total time: 12691.51s
                               ETA: 1132763.1s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.762s, learning 0.177s)
               Value function loss: 8.6553
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 278.82
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 10.94s
                        Total time: 12702.45s
                               ETA: 1132705.7s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.167s)
               Value function loss: 7.7800
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 278.36
               Mean episode length: 249.37
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 10.96s
                        Total time: 12713.41s
                               ETA: 1132650.6s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.476s, learning 0.199s)
               Value function loss: 8.7442
                    Surrogate loss: -0.0218
             Mean action noise std: 0.76
                       Mean reward: 286.78
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 10.67s
                        Total time: 12724.09s
                               ETA: 1132569.8s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.812s, learning 0.174s)
               Value function loss: 9.4106
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 274.96
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 10.99s
                        Total time: 12735.07s
                               ETA: 1132516.9s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.885s, learning 0.216s)
               Value function loss: 8.7481
                    Surrogate loss: -0.0247
             Mean action noise std: 0.76
                       Mean reward: 277.49
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 11.10s
                        Total time: 12746.17s
                               ETA: 1132474.2s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.515s, learning 0.170s)
               Value function loss: 8.6807
                    Surrogate loss: -0.0226
             Mean action noise std: 0.76
                       Mean reward: 283.12
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 10.68s
                        Total time: 12756.86s
                               ETA: 1132394.6s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.958s, learning 0.180s)
               Value function loss: 8.1109
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 280.78
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 11.14s
                        Total time: 12768.00s
                               ETA: 1132355.3s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.504s, learning 0.168s)
               Value function loss: 8.7397
                    Surrogate loss: -0.0224
             Mean action noise std: 0.76
                       Mean reward: 280.23
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 10.67s
                        Total time: 12778.67s
                               ETA: 1132274.8s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1548 steps/s (collection: 10.418s, learning 0.160s)
               Value function loss: 9.6128
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 296.92
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 10.58s
                        Total time: 12789.25s
                               ETA: 1132186.1s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.693s, learning 0.165s)
               Value function loss: 9.0439
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 284.68
               Mean episode length: 249.93
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 10.86s
                        Total time: 12800.11s
                               ETA: 1132122.4s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.781s, learning 0.173s)
               Value function loss: 7.8819
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 274.52
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 10.95s
                        Total time: 12811.06s
                               ETA: 1132067.2s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.510s, learning 0.192s)
               Value function loss: 7.2854
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 273.46
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 10.70s
                        Total time: 12821.76s
                               ETA: 1131989.8s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.649s, learning 0.166s)
               Value function loss: 7.0636
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 286.32
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 10.82s
                        Total time: 12832.58s
                               ETA: 1131922.6s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.741s, learning 0.160s)
               Value function loss: 6.8677
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 292.90
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 10.90s
                        Total time: 12843.48s
                               ETA: 1131863.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1563 steps/s (collection: 10.312s, learning 0.164s)
               Value function loss: 5.8808
                    Surrogate loss: -0.0239
             Mean action noise std: 0.76
                       Mean reward: 284.43
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 10.48s
                        Total time: 12853.95s
                               ETA: 1131766.1s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.169s)
               Value function loss: 4.6978
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 281.52
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 10.95s
                        Total time: 12864.91s
                               ETA: 1131711.1s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.396s, learning 0.164s)
               Value function loss: 5.0632
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 287.35
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 10.56s
                        Total time: 12875.47s
                               ETA: 1131621.8s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.972s, learning 0.171s)
               Value function loss: 3.7758
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 292.36
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 11.14s
                        Total time: 12886.61s
                               ETA: 1131583.9s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.625s, learning 0.162s)
               Value function loss: 3.6600
                    Surrogate loss: -0.0268
             Mean action noise std: 0.76
                       Mean reward: 291.10
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 10.79s
                        Total time: 12897.40s
                               ETA: 1131514.7s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.600s, learning 0.163s)
               Value function loss: 4.8026
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 293.15
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 10.76s
                        Total time: 12908.16s
                               ETA: 1131443.6s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.420s, learning 0.191s)
               Value function loss: 5.1789
                    Surrogate loss: -0.0225
             Mean action noise std: 0.76
                       Mean reward: 298.51
               Mean episode length: 249.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 10.61s
                        Total time: 12918.77s
                               ETA: 1131359.3s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.950s, learning 0.167s)
               Value function loss: 4.4601
                    Surrogate loss: -0.0287
             Mean action noise std: 0.76
                       Mean reward: 300.82
               Mean episode length: 249.95
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 11.12s
                        Total time: 12929.89s
                               ETA: 1131319.3s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.167s)
               Value function loss: 4.4860
                    Surrogate loss: -0.0293
             Mean action noise std: 0.76
                       Mean reward: 301.49
               Mean episode length: 249.95
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 11.12s
                        Total time: 12941.01s
                               ETA: 1131279.7s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.748s, learning 0.171s)
               Value function loss: 4.6109
                    Surrogate loss: -0.0310
             Mean action noise std: 0.76
                       Mean reward: 290.20
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 10.92s
                        Total time: 12951.93s
                               ETA: 1131222.6s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.753s, learning 0.188s)
               Value function loss: 6.8995
                    Surrogate loss: -0.0257
             Mean action noise std: 0.76
                       Mean reward: 276.37
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 10.94s
                        Total time: 12962.87s
                               ETA: 1131167.4s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.768s, learning 0.164s)
               Value function loss: 6.1745
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 273.59
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 10.93s
                        Total time: 12973.80s
                               ETA: 1131111.6s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.810s, learning 0.163s)
               Value function loss: 7.3533
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 283.07
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 10.97s
                        Total time: 12984.77s
                               ETA: 1131059.4s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.548s, learning 0.224s)
               Value function loss: 6.8862
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 283.79
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 10.77s
                        Total time: 12995.54s
                               ETA: 1130989.7s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.517s, learning 0.162s)
               Value function loss: 8.1247
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 282.90
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 10.68s
                        Total time: 13006.22s
                               ETA: 1130912.2s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.768s, learning 0.187s)
               Value function loss: 6.3561
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 266.44
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 10.95s
                        Total time: 13017.18s
                               ETA: 1130858.6s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.920s, learning 0.248s)
               Value function loss: 7.7801
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 272.28
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 11.17s
                        Total time: 13028.34s
                               ETA: 1130823.7s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.165s)
               Value function loss: 8.0147
                    Surrogate loss: -0.0226
             Mean action noise std: 0.76
                       Mean reward: 276.99
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 10.92s
                        Total time: 13039.27s
                               ETA: 1130767.7s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.790s, learning 0.166s)
               Value function loss: 9.3950
                    Surrogate loss: -0.0226
             Mean action noise std: 0.76
                       Mean reward: 282.34
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 10.96s
                        Total time: 13050.22s
                               ETA: 1130714.5s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.803s, learning 0.165s)
               Value function loss: 8.9252
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 290.47
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 10.97s
                        Total time: 13061.19s
                               ETA: 1130662.4s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.658s, learning 0.186s)
               Value function loss: 9.6459
                    Surrogate loss: -0.0242
             Mean action noise std: 0.76
                       Mean reward: 296.05
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 10.84s
                        Total time: 13072.04s
                               ETA: 1130599.7s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.673s, learning 0.162s)
               Value function loss: 9.0097
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 296.00
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 10.84s
                        Total time: 13082.87s
                               ETA: 1130536.3s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.160s)
               Value function loss: 8.6840
                    Surrogate loss: -0.0248
             Mean action noise std: 0.76
                       Mean reward: 275.24
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 11.12s
                        Total time: 13093.99s
                               ETA: 1130497.2s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.612s, learning 0.161s)
               Value function loss: 8.1585
                    Surrogate loss: -0.0237
             Mean action noise std: 0.76
                       Mean reward: 278.52
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 10.77s
                        Total time: 13104.76s
                               ETA: 1130428.5s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.927s, learning 0.165s)
               Value function loss: 9.5207
                    Surrogate loss: -0.0239
             Mean action noise std: 0.76
                       Mean reward: 275.99
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 11.09s
                        Total time: 13115.85s
                               ETA: 1130387.5s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.620s, learning 0.170s)
               Value function loss: 10.0507
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 290.17
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 10.79s
                        Total time: 13126.64s
                               ETA: 1130320.5s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.486s, learning 0.298s)
               Value function loss: 11.3106
                    Surrogate loss: -0.0240
             Mean action noise std: 0.76
                       Mean reward: 301.52
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 10.78s
                        Total time: 13137.43s
                               ETA: 1130253.1s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.939s, learning 0.215s)
               Value function loss: 9.9734
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 305.44
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 11.15s
                        Total time: 13148.58s
                               ETA: 1130217.6s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.788s, learning 0.189s)
               Value function loss: 8.7767
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 311.07
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 10.98s
                        Total time: 13159.56s
                               ETA: 1130167.0s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.163s)
               Value function loss: 7.9038
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 313.64
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 10.80s
                        Total time: 13170.35s
                               ETA: 1130100.8s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.605s, learning 0.165s)
               Value function loss: 8.9430
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 308.74
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 10.77s
                        Total time: 13181.12s
                               ETA: 1130032.6s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.600s, learning 0.167s)
               Value function loss: 7.2491
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 309.00
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 10.77s
                        Total time: 13191.89s
                               ETA: 1129964.2s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.695s, learning 0.162s)
               Value function loss: 5.3507
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 314.67
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 10.86s
                        Total time: 13202.75s
                               ETA: 1129903.6s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.689s, learning 0.162s)
               Value function loss: 4.5520
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 316.51
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 10.85s
                        Total time: 13213.60s
                               ETA: 1129842.6s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.590s, learning 0.169s)
               Value function loss: 4.8714
                    Surrogate loss: -0.0237
             Mean action noise std: 0.76
                       Mean reward: 319.83
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 10.76s
                        Total time: 13224.36s
                               ETA: 1129773.8s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.632s, learning 0.186s)
               Value function loss: 4.3792
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 317.32
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 10.82s
                        Total time: 13235.17s
                               ETA: 1129710.1s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.576s, learning 0.172s)
               Value function loss: 5.6627
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 322.37
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 10.75s
                        Total time: 13245.92s
                               ETA: 1129640.5s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.725s, learning 0.184s)
               Value function loss: 6.2593
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 314.61
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 10.91s
                        Total time: 13256.83s
                               ETA: 1129584.8s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.629s, learning 0.161s)
               Value function loss: 5.8673
                    Surrogate loss: -0.0245
             Mean action noise std: 0.76
                       Mean reward: 317.78
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 10.79s
                        Total time: 13267.62s
                               ETA: 1129519.1s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.994s, learning 0.197s)
               Value function loss: 5.8683
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 318.74
               Mean episode length: 249.75
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 11.19s
                        Total time: 13278.81s
                               ETA: 1129487.5s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.981s, learning 0.168s)
               Value function loss: 6.4299
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 321.40
               Mean episode length: 249.75
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 11.15s
                        Total time: 13289.96s
                               ETA: 1129452.4s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.689s, learning 0.165s)
               Value function loss: 8.3381
                    Surrogate loss: -0.0231
             Mean action noise std: 0.76
                       Mean reward: 317.32
               Mean episode length: 249.75
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 10.85s
                        Total time: 13300.82s
                               ETA: 1129392.4s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.612s, learning 0.158s)
               Value function loss: 8.6085
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 314.81
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 10.77s
                        Total time: 13311.59s
                               ETA: 1129325.2s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.894s, learning 0.160s)
               Value function loss: 9.6276
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 314.20
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 11.05s
                        Total time: 13322.64s
                               ETA: 1129282.2s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.902s, learning 0.174s)
               Value function loss: 10.1716
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 313.35
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 11.08s
                        Total time: 13333.72s
                               ETA: 1129241.2s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.596s, learning 0.192s)
               Value function loss: 11.0516
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 328.70
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 10.79s
                        Total time: 13344.50s
                               ETA: 1129175.8s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.813s, learning 0.176s)
               Value function loss: 10.1195
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 318.36
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 10.99s
                        Total time: 13355.49s
                               ETA: 1129127.5s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.456s, learning 0.186s)
               Value function loss: 12.0279
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 319.06
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 10.64s
                        Total time: 13366.14s
                               ETA: 1129050.0s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.882s, learning 0.183s)
               Value function loss: 13.0268
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 316.57
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 11.06s
                        Total time: 13377.20s
                               ETA: 1129008.3s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.156s, learning 0.164s)
               Value function loss: 15.0178
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 326.70
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 10.32s
                        Total time: 13387.52s
                               ETA: 1128903.8s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.432s, learning 0.163s)
               Value function loss: 14.0844
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 332.18
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 10.60s
                        Total time: 13398.12s
                               ETA: 1128822.6s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.742s, learning 0.163s)
               Value function loss: 16.0359
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 336.92
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 10.91s
                        Total time: 13409.02s
                               ETA: 1128767.7s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.561s, learning 0.197s)
               Value function loss: 14.2135
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 337.69
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 10.76s
                        Total time: 13419.78s
                               ETA: 1128700.5s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.604s, learning 0.203s)
               Value function loss: 17.4456
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 334.29
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 10.81s
                        Total time: 13430.59s
                               ETA: 1128637.5s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.587s, learning 0.185s)
               Value function loss: 15.0381
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 349.93
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 10.77s
                        Total time: 13441.36s
                               ETA: 1128571.6s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.492s, learning 0.164s)
               Value function loss: 17.7873
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 352.98
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 10.66s
                        Total time: 13452.01s
                               ETA: 1128496.0s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.817s, learning 0.171s)
               Value function loss: 15.9653
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 348.36
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 10.99s
                        Total time: 13463.00s
                               ETA: 1128448.4s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.799s, learning 0.157s)
               Value function loss: 15.8041
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 346.72
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 10.96s
                        Total time: 13473.96s
                               ETA: 1128398.2s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.688s, learning 0.170s)
               Value function loss: 14.3117
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 346.92
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 10.86s
                        Total time: 13484.81s
                               ETA: 1128339.9s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.690s, learning 0.168s)
               Value function loss: 13.7128
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 360.09
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 10.86s
                        Total time: 13495.67s
                               ETA: 1128281.6s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.681s, learning 0.158s)
               Value function loss: 12.1710
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 349.91
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 10.84s
                        Total time: 13506.51s
                               ETA: 1128221.9s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.780s, learning 0.160s)
               Value function loss: 14.2064
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 352.35
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 10.94s
                        Total time: 13517.45s
                               ETA: 1128170.6s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.761s, learning 0.162s)
               Value function loss: 8.8466
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 350.30
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 10.92s
                        Total time: 13528.37s
                               ETA: 1128118.0s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.800s, learning 0.253s)
               Value function loss: 7.6513
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 356.48
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 11.05s
                        Total time: 13539.43s
                               ETA: 1128076.3s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.648s, learning 0.174s)
               Value function loss: 6.3512
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 357.40
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 10.82s
                        Total time: 13550.25s
                               ETA: 1128015.4s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.902s, learning 0.166s)
               Value function loss: 7.1949
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 353.18
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 11.07s
                        Total time: 13561.32s
                               ETA: 1127975.1s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1567 steps/s (collection: 10.292s, learning 0.160s)
               Value function loss: 5.8987
                    Surrogate loss: -0.0231
             Mean action noise std: 0.76
                       Mean reward: 355.84
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 10.45s
                        Total time: 13571.77s
                               ETA: 1127883.6s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.692s, learning 0.191s)
               Value function loss: 6.5032
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 348.28
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 10.88s
                        Total time: 13582.65s
                               ETA: 1127828.0s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.660s, learning 0.160s)
               Value function loss: 7.7205
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 353.90
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 10.82s
                        Total time: 13593.47s
                               ETA: 1127767.4s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.566s, learning 0.164s)
               Value function loss: 9.4559
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 348.12
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 10.73s
                        Total time: 13604.20s
                               ETA: 1127699.3s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.646s, learning 0.158s)
               Value function loss: 6.3623
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 346.20
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 10.80s
                        Total time: 13615.01s
                               ETA: 1127637.4s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1606 steps/s (collection: 10.033s, learning 0.164s)
               Value function loss: 8.6078
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 350.96
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 10.20s
                        Total time: 13625.20s
                               ETA: 1127525.4s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.165s)
               Value function loss: 10.2634
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 362.10
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 10.76s
                        Total time: 13635.97s
                               ETA: 1127460.4s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.835s, learning 0.173s)
               Value function loss: 13.3935
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 366.40
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 11.01s
                        Total time: 13646.97s
                               ETA: 1127415.7s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.677s, learning 0.175s)
               Value function loss: 12.9143
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 365.55
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 10.85s
                        Total time: 13657.83s
                               ETA: 1127358.2s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.169s)
               Value function loss: 14.5381
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 361.70
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 10.76s
                        Total time: 13668.59s
                               ETA: 1127293.3s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.802s, learning 0.184s)
               Value function loss: 12.4450
                    Surrogate loss: -0.0225
             Mean action noise std: 0.76
                       Mean reward: 362.23
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 10.99s
                        Total time: 13679.57s
                               ETA: 1127247.1s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.463s, learning 0.199s)
               Value function loss: 15.5758
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 355.17
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 10.66s
                        Total time: 13690.23s
                               ETA: 1127174.1s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.645s, learning 0.161s)
               Value function loss: 16.2525
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 357.54
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 10.81s
                        Total time: 13701.04s
                               ETA: 1127113.1s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.852s, learning 0.191s)
               Value function loss: 18.7882
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 358.32
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 11.04s
                        Total time: 13712.08s
                               ETA: 1127071.6s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.524s, learning 0.166s)
               Value function loss: 21.4067
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 358.85
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 10.69s
                        Total time: 13722.77s
                               ETA: 1127001.3s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.769s, learning 0.167s)
               Value function loss: 19.5565
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 362.14
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 10.94s
                        Total time: 13733.71s
                               ETA: 1126951.2s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.595s, learning 0.172s)
               Value function loss: 18.8174
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 355.77
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 10.77s
                        Total time: 13744.48s
                               ETA: 1126887.3s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.539s, learning 0.162s)
               Value function loss: 20.2971
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 355.53
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 10.70s
                        Total time: 13755.18s
                               ETA: 1126818.1s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.483s, learning 0.227s)
               Value function loss: 21.1743
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 363.51
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 10.71s
                        Total time: 13765.89s
                               ETA: 1126749.8s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.619s, learning 0.160s)
               Value function loss: 19.8155
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 366.04
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 10.78s
                        Total time: 13776.67s
                               ETA: 1126687.2s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.766s, learning 0.232s)
               Value function loss: 24.0706
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 369.48
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 11.00s
                        Total time: 13787.66s
                               ETA: 1126642.6s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.556s, learning 0.164s)
               Value function loss: 18.9046
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 364.33
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 10.72s
                        Total time: 13798.38s
                               ETA: 1126575.4s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.526s, learning 0.169s)
               Value function loss: 25.8413
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 377.53
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 10.70s
                        Total time: 13809.08s
                               ETA: 1126506.1s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.806s, learning 0.168s)
               Value function loss: 21.7654
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 381.62
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 10.97s
                        Total time: 13820.05s
                               ETA: 1126459.7s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.552s, learning 0.167s)
               Value function loss: 20.1876
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 371.71
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 10.72s
                        Total time: 13830.77s
                               ETA: 1126392.7s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.032s, learning 0.204s)
               Value function loss: 21.4597
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 375.56
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 11.24s
                        Total time: 13842.01s
                               ETA: 1126367.8s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.848s, learning 0.273s)
               Value function loss: 19.7071
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 382.73
               Mean episode length: 249.41
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 11.12s
                        Total time: 13853.13s
                               ETA: 1126333.5s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.573s, learning 0.197s)
               Value function loss: 14.4752
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 384.10
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 10.77s
                        Total time: 13863.90s
                               ETA: 1126270.8s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.446s, learning 0.203s)
               Value function loss: 12.0561
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 380.24
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 10.65s
                        Total time: 13874.55s
                               ETA: 1126198.3s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1458 steps/s (collection: 10.955s, learning 0.276s)
               Value function loss: 7.9285
                    Surrogate loss: -0.0223
             Mean action noise std: 0.76
                       Mean reward: 381.48
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 11.23s
                        Total time: 13885.78s
                               ETA: 1126173.1s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.819s, learning 0.167s)
               Value function loss: 9.7050
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 389.13
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 10.99s
                        Total time: 13896.76s
                               ETA: 1126128.1s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.774s, learning 0.172s)
               Value function loss: 6.1396
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 384.16
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 10.95s
                        Total time: 13907.71s
                               ETA: 1126080.0s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.678s, learning 0.164s)
               Value function loss: 6.6765
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 390.85
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 10.84s
                        Total time: 13918.55s
                               ETA: 1126023.4s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.886s, learning 0.163s)
               Value function loss: 8.4844
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 388.01
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 11.05s
                        Total time: 13929.60s
                               ETA: 1125983.6s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.164s)
               Value function loss: 7.7161
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 381.18
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 10.90s
                        Total time: 13940.50s
                               ETA: 1125931.6s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.370s, learning 0.161s)
               Value function loss: 7.6723
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 385.79
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 10.53s
                        Total time: 13951.03s
                               ETA: 1125850.2s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.807s, learning 0.163s)
               Value function loss: 10.1132
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 390.44
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 10.97s
                        Total time: 13962.00s
                               ETA: 1125804.3s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.891s, learning 0.167s)
               Value function loss: 7.8309
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 392.94
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 11.06s
                        Total time: 13973.06s
                               ETA: 1125765.6s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.521s, learning 0.161s)
               Value function loss: 11.4520
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 391.70
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 10.68s
                        Total time: 13983.74s
                               ETA: 1125696.7s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.474s, learning 0.173s)
               Value function loss: 12.3060
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 394.73
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 10.65s
                        Total time: 13994.39s
                               ETA: 1125625.0s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.805s, learning 0.163s)
               Value function loss: 16.2087
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 390.41
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 10.97s
                        Total time: 14005.35s
                               ETA: 1125579.1s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.332s, learning 0.172s)
               Value function loss: 12.4125
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 389.35
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 10.50s
                        Total time: 14015.86s
                               ETA: 1125496.1s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.892s, learning 0.164s)
               Value function loss: 16.5585
                    Surrogate loss: -0.0182
             Mean action noise std: 0.76
                       Mean reward: 387.86
               Mean episode length: 250.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 11.06s
                        Total time: 14026.91s
                               ETA: 1125457.5s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.812s, learning 0.220s)
               Value function loss: 17.4994
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 391.34
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 11.03s
                        Total time: 14037.95s
                               ETA: 1125417.1s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.922s, learning 0.186s)
               Value function loss: 15.5165
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 391.47
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 11.11s
                        Total time: 14049.05s
                               ETA: 1125382.7s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.012s, learning 0.249s)
               Value function loss: 21.8654
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 391.55
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 11.26s
                        Total time: 14060.31s
                               ETA: 1125360.7s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.799s, learning 0.175s)
               Value function loss: 19.5654
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 393.91
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 10.97s
                        Total time: 14071.29s
                               ETA: 1125315.7s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.525s, learning 0.193s)
               Value function loss: 21.2114
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 382.35
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 10.72s
                        Total time: 14082.01s
                               ETA: 1125250.3s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.636s, learning 0.221s)
               Value function loss: 21.4194
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 394.83
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 10.86s
                        Total time: 14092.86s
                               ETA: 1125196.0s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.570s, learning 0.165s)
               Value function loss: 20.3715
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 390.31
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 10.74s
                        Total time: 14103.60s
                               ETA: 1125132.2s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.874s, learning 0.164s)
               Value function loss: 19.3283
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 391.72
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 11.04s
                        Total time: 14114.64s
                               ETA: 1125092.5s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.555s, learning 0.194s)
               Value function loss: 19.1797
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 385.58
               Mean episode length: 247.83
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 10.75s
                        Total time: 14125.38s
                               ETA: 1125029.9s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.729s, learning 0.172s)
               Value function loss: 17.8834
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 384.36
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 10.90s
                        Total time: 14136.28s
                               ETA: 1124979.4s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.699s, learning 0.164s)
               Value function loss: 22.0899
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 393.12
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 10.86s
                        Total time: 14147.15s
                               ETA: 1124926.0s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.809s, learning 0.218s)
               Value function loss: 21.1792
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 382.51
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 11.03s
                        Total time: 14158.17s
                               ETA: 1124885.7s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.591s, learning 0.171s)
               Value function loss: 16.5767
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 382.65
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 10.76s
                        Total time: 14168.94s
                               ETA: 1124824.5s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.539s, learning 0.164s)
               Value function loss: 15.8106
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 377.94
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 10.70s
                        Total time: 14179.64s
                               ETA: 1124758.5s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.850s, learning 0.229s)
               Value function loss: 15.6018
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 393.13
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 11.08s
                        Total time: 14190.72s
                               ETA: 1124722.6s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.470s, learning 0.184s)
               Value function loss: 15.7951
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 399.92
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 10.65s
                        Total time: 14201.37s
                               ETA: 1124653.0s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.639s, learning 0.161s)
               Value function loss: 13.0335
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 393.68
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 10.80s
                        Total time: 14212.17s
                               ETA: 1124595.0s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.799s, learning 0.190s)
               Value function loss: 8.5112
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 397.71
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 10.99s
                        Total time: 14223.16s
                               ETA: 1124552.1s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.860s, learning 0.211s)
               Value function loss: 8.2867
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 398.96
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 11.07s
                        Total time: 14234.23s
                               ETA: 1124515.6s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.774s, learning 0.185s)
               Value function loss: 6.6628
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 398.52
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 10.96s
                        Total time: 14245.19s
                               ETA: 1124470.5s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.683s, learning 0.168s)
               Value function loss: 8.1116
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 393.46
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 10.85s
                        Total time: 14256.04s
                               ETA: 1124416.8s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.492s, learning 0.158s)
               Value function loss: 7.6641
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 392.09
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 10.65s
                        Total time: 14266.69s
                               ETA: 1124347.4s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.569s, learning 0.168s)
               Value function loss: 8.9646
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 404.42
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 10.74s
                        Total time: 14277.43s
                               ETA: 1124284.9s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1566 steps/s (collection: 10.300s, learning 0.160s)
               Value function loss: 8.5895
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 403.00
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 10.46s
                        Total time: 14287.89s
                               ETA: 1124200.6s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.715s, learning 0.166s)
               Value function loss: 9.5639
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 402.87
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 10.88s
                        Total time: 14298.77s
                               ETA: 1124149.6s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.734s, learning 0.175s)
               Value function loss: 8.1577
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 396.46
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 10.91s
                        Total time: 14309.68s
                               ETA: 1124100.9s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.634s, learning 0.185s)
               Value function loss: 11.6471
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 391.31
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 10.82s
                        Total time: 14320.50s
                               ETA: 1124045.2s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.166s)
               Value function loss: 11.4764
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 386.74
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 11.11s
                        Total time: 14331.61s
                               ETA: 1124012.7s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.572s, learning 0.167s)
               Value function loss: 17.1487
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 396.00
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 10.74s
                        Total time: 14342.35s
                               ETA: 1123950.8s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.694s, learning 0.189s)
               Value function loss: 15.5368
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 396.22
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 10.88s
                        Total time: 14353.23s
                               ETA: 1123900.3s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.747s, learning 0.189s)
               Value function loss: 16.5979
                    Surrogate loss: -0.0110
             Mean action noise std: 0.76
                       Mean reward: 386.59
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 10.94s
                        Total time: 14364.17s
                               ETA: 1123854.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.622s, learning 0.168s)
               Value function loss: 13.6181
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 391.00
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 10.79s
                        Total time: 14374.96s
                               ETA: 1123796.3s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.499s, learning 0.164s)
               Value function loss: 16.6432
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 386.79
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 10.66s
                        Total time: 14385.62s
                               ETA: 1123728.8s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1599 steps/s (collection: 10.064s, learning 0.178s)
               Value function loss: 17.6458
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 393.20
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 10.24s
                        Total time: 14395.86s
                               ETA: 1123628.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.867s, learning 0.164s)
               Value function loss: 23.5494
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 397.14
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 11.03s
                        Total time: 14406.89s
                               ETA: 1123589.8s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.619s, learning 0.177s)
               Value function loss: 19.9842
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 399.55
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 10.80s
                        Total time: 14417.69s
                               ETA: 1123532.9s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.580s, learning 0.169s)
               Value function loss: 21.3495
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 399.16
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 10.75s
                        Total time: 14428.44s
                               ETA: 1123472.5s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.421s, learning 0.223s)
               Value function loss: 23.1366
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 393.38
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 10.64s
                        Total time: 14439.08s
                               ETA: 1123403.9s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.673s, learning 0.172s)
               Value function loss: 25.6485
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 400.58
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 10.85s
                        Total time: 14449.93s
                               ETA: 1123351.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.671s, learning 0.164s)
               Value function loss: 20.9857
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 400.46
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 10.84s
                        Total time: 14460.76s
                               ETA: 1123297.5s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.679s, learning 0.175s)
               Value function loss: 28.3504
                    Surrogate loss: -0.0207
             Mean action noise std: 0.76
                       Mean reward: 405.05
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 10.85s
                        Total time: 14471.62s
                               ETA: 1123245.6s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.735s, learning 0.171s)
               Value function loss: 27.3673
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 391.65
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 10.91s
                        Total time: 14482.52s
                               ETA: 1123197.7s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.160s)
               Value function loss: 29.9632
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 392.86
               Mean episode length: 248.62
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 10.76s
                        Total time: 14493.29s
                               ETA: 1123138.6s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.695s, learning 0.159s)
               Value function loss: 23.7734
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 391.57
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 10.85s
                        Total time: 14504.14s
                               ETA: 1123086.7s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.750s, learning 0.188s)
               Value function loss: 26.4462
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 397.18
               Mean episode length: 247.81
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 10.94s
                        Total time: 14515.08s
                               ETA: 1123041.5s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.688s, learning 0.186s)
               Value function loss: 24.3820
                    Surrogate loss: -0.0204
             Mean action noise std: 0.76
                       Mean reward: 393.66
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 10.87s
                        Total time: 14525.95s
                               ETA: 1122991.3s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.458s, learning 0.171s)
               Value function loss: 23.7394
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 389.27
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 10.63s
                        Total time: 14536.58s
                               ETA: 1122922.3s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.968s, learning 0.165s)
               Value function loss: 21.8299
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 388.76
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 11.13s
                        Total time: 14547.71s
                               ETA: 1122892.2s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.651s, learning 0.187s)
               Value function loss: 12.6183
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 395.26
               Mean episode length: 247.76
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 10.84s
                        Total time: 14558.55s
                               ETA: 1122839.4s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.722s, learning 0.185s)
               Value function loss: 9.6692
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 400.90
               Mean episode length: 247.76
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 10.91s
                        Total time: 14569.46s
                               ETA: 1122792.0s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.617s, learning 0.194s)
               Value function loss: 9.0723
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 391.13
               Mean episode length: 247.76
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 10.81s
                        Total time: 14580.27s
                               ETA: 1122737.4s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.658s, learning 0.174s)
               Value function loss: 10.7389
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 395.56
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 10.83s
                        Total time: 14591.10s
                               ETA: 1122684.3s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.677s, learning 0.158s)
               Value function loss: 11.0389
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 392.66
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 10.83s
                        Total time: 14601.93s
                               ETA: 1122631.6s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.698s, learning 0.194s)
               Value function loss: 11.4054
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 389.40
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 10.89s
                        Total time: 14612.82s
                               ETA: 1122583.3s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.937s, learning 0.161s)
               Value function loss: 10.9085
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 381.67
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 11.10s
                        Total time: 14623.92s
                               ETA: 1122550.9s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.672s, learning 0.159s)
               Value function loss: 11.2580
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 378.39
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 10.83s
                        Total time: 14634.75s
                               ETA: 1122498.1s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1560 steps/s (collection: 10.334s, learning 0.165s)
               Value function loss: 13.3875
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 381.91
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 10.50s
                        Total time: 14645.25s
                               ETA: 1122419.9s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.715s, learning 0.171s)
               Value function loss: 12.6941
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 391.08
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 10.89s
                        Total time: 14656.14s
                               ETA: 1122371.4s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.729s, learning 0.173s)
               Value function loss: 15.1885
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 390.21
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 10.90s
                        Total time: 14667.04s
                               ETA: 1122324.1s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.734s, learning 0.186s)
               Value function loss: 18.9632
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 372.10
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 10.92s
                        Total time: 14677.96s
                               ETA: 1122278.4s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.661s, learning 0.207s)
               Value function loss: 19.6293
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 377.80
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 10.87s
                        Total time: 14688.83s
                               ETA: 1122228.7s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.165s)
               Value function loss: 19.4400
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 393.45
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 11.15s
                        Total time: 14699.98s
                               ETA: 1122200.7s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.919s, learning 0.163s)
               Value function loss: 18.4392
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: 391.56
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 11.08s
                        Total time: 14711.06s
                               ETA: 1122167.5s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.164s)
               Value function loss: 19.4843
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 370.91
               Mean episode length: 247.78
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 10.93s
                        Total time: 14721.99s
                               ETA: 1122122.5s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.594s, learning 0.161s)
               Value function loss: 23.1400
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 373.90
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 10.76s
                        Total time: 14732.74s
                               ETA: 1122064.4s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.634s, learning 0.187s)
               Value function loss: 28.6928
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 381.28
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 10.82s
                        Total time: 14743.57s
                               ETA: 1122011.5s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.702s, learning 0.186s)
               Value function loss: 26.4876
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 399.88
               Mean episode length: 249.28
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 10.89s
                        Total time: 14754.45s
                               ETA: 1121963.6s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.747s, learning 0.162s)
               Value function loss: 28.7652
                    Surrogate loss: 0.0011
             Mean action noise std: 0.76
                       Mean reward: 396.40
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 10.91s
                        Total time: 14765.36s
                               ETA: 1121917.4s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.614s, learning 0.168s)
               Value function loss: 26.3348
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 394.07
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 10.78s
                        Total time: 14776.14s
                               ETA: 1121861.6s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.680s, learning 0.262s)
               Value function loss: 30.8356
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 399.26
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 10.94s
                        Total time: 14787.09s
                               ETA: 1121818.1s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1560 steps/s (collection: 10.328s, learning 0.171s)
               Value function loss: 26.3019
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 404.62
               Mean episode length: 249.31
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 10.50s
                        Total time: 14797.58s
                               ETA: 1121741.0s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.556s, learning 0.175s)
               Value function loss: 29.1352
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 392.68
               Mean episode length: 248.43
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 10.73s
                        Total time: 14808.32s
                               ETA: 1121681.6s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.676s, learning 0.182s)
               Value function loss: 24.6782
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 393.37
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 10.86s
                        Total time: 14819.17s
                               ETA: 1121631.9s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.666s, learning 0.165s)
               Value function loss: 28.7364
                    Surrogate loss: 0.0040
             Mean action noise std: 0.76
                       Mean reward: 390.42
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 10.83s
                        Total time: 14830.00s
                               ETA: 1121580.2s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.144s, learning 0.169s)
               Value function loss: 22.9135
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 390.41
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 11.31s
                        Total time: 14841.32s
                               ETA: 1121564.9s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.879s, learning 0.184s)
               Value function loss: 21.6370
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 387.99
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.06s
                        Total time: 14852.38s
                               ETA: 1121530.8s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.748s, learning 0.164s)
               Value function loss: 19.9843
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 380.36
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 10.91s
                        Total time: 14863.29s
                               ETA: 1121485.3s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.874s, learning 0.164s)
               Value function loss: 19.8372
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 394.88
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 11.04s
                        Total time: 14874.33s
                               ETA: 1121449.4s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.567s, learning 0.160s)
               Value function loss: 14.2802
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 391.11
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 10.73s
                        Total time: 14885.06s
                               ETA: 1121390.1s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.520s, learning 0.172s)
               Value function loss: 10.7471
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 390.65
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 10.69s
                        Total time: 14895.75s
                               ETA: 1121328.2s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.609s, learning 0.164s)
               Value function loss: 7.9911
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 386.73
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 10.77s
                        Total time: 14906.52s
                               ETA: 1121272.5s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.757s, learning 0.185s)
               Value function loss: 8.7615
                    Surrogate loss: -0.0223
             Mean action noise std: 0.76
                       Mean reward: 388.03
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 10.94s
                        Total time: 14917.46s
                               ETA: 1121229.7s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.833s, learning 0.290s)
               Value function loss: 7.0302
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 389.54
               Mean episode length: 249.45
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.12s
                        Total time: 14928.59s
                               ETA: 1121200.5s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.781s, learning 0.174s)
               Value function loss: 8.6840
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 388.92
               Mean episode length: 249.09
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 10.95s
                        Total time: 14939.54s
                               ETA: 1121158.6s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.646s, learning 0.182s)
               Value function loss: 8.4758
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 394.68
               Mean episode length: 249.09
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 10.83s
                        Total time: 14950.37s
                               ETA: 1121107.3s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.535s, learning 0.159s)
               Value function loss: 9.2462
                    Surrogate loss: -0.0239
             Mean action noise std: 0.76
                       Mean reward: 386.29
               Mean episode length: 249.64
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 10.69s
                        Total time: 14961.06s
                               ETA: 1121046.0s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.823s, learning 0.213s)
               Value function loss: 8.6330
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 389.84
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.04s
                        Total time: 14972.10s
                               ETA: 1121010.3s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.982s, learning 0.197s)
               Value function loss: 10.5679
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 380.85
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 11.18s
                        Total time: 14983.28s
                               ETA: 1120985.4s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.691s, learning 0.171s)
               Value function loss: 11.2686
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 379.58
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 10.86s
                        Total time: 14994.14s
                               ETA: 1120936.9s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.794s, learning 0.164s)
               Value function loss: 13.9081
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 374.54
               Mean episode length: 249.93
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 10.96s
                        Total time: 15005.10s
                               ETA: 1120895.5s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.625s, learning 0.173s)
               Value function loss: 12.8910
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 365.65
               Mean episode length: 249.93
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 10.80s
                        Total time: 15015.90s
                               ETA: 1120842.3s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.856s, learning 0.172s)
               Value function loss: 16.7797
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 356.21
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.03s
                        Total time: 15026.92s
                               ETA: 1120806.3s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.392s, learning 0.241s)
               Value function loss: 15.4765
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 357.49
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 10.63s
                        Total time: 15037.56s
                               ETA: 1120740.9s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.673s, learning 0.162s)
               Value function loss: 18.0481
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 372.88
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 10.84s
                        Total time: 15048.39s
                               ETA: 1120690.6s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.714s, learning 0.162s)
               Value function loss: 19.2557
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 358.93
               Mean episode length: 249.94
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 10.88s
                        Total time: 15059.27s
                               ETA: 1120643.4s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.579s, learning 0.203s)
               Value function loss: 17.7468
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 355.32
               Mean episode length: 249.94
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 10.78s
                        Total time: 15070.05s
                               ETA: 1120589.3s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.593s, learning 0.165s)
               Value function loss: 21.3445
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 352.78
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 10.76s
                        Total time: 15080.81s
                               ETA: 1120533.4s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.437s, learning 0.159s)
               Value function loss: 19.4587
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 345.56
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 10.60s
                        Total time: 15091.40s
                               ETA: 1120465.6s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.698s, learning 0.286s)
               Value function loss: 17.6551
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 328.92
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 10.98s
                        Total time: 15102.39s
                               ETA: 1120426.8s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.357s, learning 0.216s)
               Value function loss: 21.5194
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 329.36
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 10.57s
                        Total time: 15112.96s
                               ETA: 1120357.4s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1565 steps/s (collection: 10.303s, learning 0.160s)
               Value function loss: 25.1428
                    Surrogate loss: -0.0175
             Mean action noise std: 0.76
                       Mean reward: 320.65
               Mean episode length: 249.82
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 10.46s
                        Total time: 15123.42s
                               ETA: 1120280.0s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.641s, learning 0.252s)
               Value function loss: 20.5885
                    Surrogate loss: -0.0224
             Mean action noise std: 0.76
                       Mean reward: 345.64
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 10.89s
                        Total time: 15134.32s
                               ETA: 1120234.6s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.497s, learning 0.161s)
               Value function loss: 25.5667
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 322.56
               Mean episode length: 247.78
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 10.66s
                        Total time: 15144.97s
                               ETA: 1120171.8s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.433s, learning 0.169s)
               Value function loss: 19.4784
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 316.86
               Mean episode length: 249.88
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 10.60s
                        Total time: 15155.58s
                               ETA: 1120104.9s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.911s, learning 0.166s)
               Value function loss: 24.4902
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 345.46
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 11.08s
                        Total time: 15166.65s
                               ETA: 1120073.2s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.928s, learning 0.166s)
               Value function loss: 20.5399
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 326.24
               Mean episode length: 249.68
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 11.09s
                        Total time: 15177.75s
                               ETA: 1120042.8s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.669s, learning 0.180s)
               Value function loss: 17.2494
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 327.56
               Mean episode length: 249.49
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 10.85s
                        Total time: 15188.60s
                               ETA: 1119994.4s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.582s, learning 0.188s)
               Value function loss: 16.9063
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 350.89
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 10.77s
                        Total time: 15199.37s
                               ETA: 1119940.2s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1573 steps/s (collection: 10.239s, learning 0.173s)
               Value function loss: 17.1159
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 334.09
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 10.41s
                        Total time: 15209.78s
                               ETA: 1119859.6s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.773s, learning 0.262s)
               Value function loss: 14.4835
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 339.55
               Mean episode length: 249.75
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 11.04s
                        Total time: 15220.81s
                               ETA: 1119825.0s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.374s, learning 0.163s)
               Value function loss: 10.1395
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 331.53
               Mean episode length: 249.75
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 10.54s
                        Total time: 15231.35s
                               ETA: 1119753.9s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.453s, learning 0.173s)
               Value function loss: 7.5929
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 327.55
               Mean episode length: 249.63
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 10.63s
                        Total time: 15241.98s
                               ETA: 1119689.3s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.467s, learning 0.193s)
               Value function loss: 9.1007
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 329.43
               Mean episode length: 249.63
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 10.66s
                        Total time: 15252.64s
                               ETA: 1119627.4s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.633s, learning 0.193s)
               Value function loss: 6.4392
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 336.41
               Mean episode length: 249.22
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 10.83s
                        Total time: 15263.46s
                               ETA: 1119577.7s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.713s, learning 0.161s)
               Value function loss: 7.0560
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 336.44
               Mean episode length: 249.59
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 10.87s
                        Total time: 15274.34s
                               ETA: 1119531.6s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1579 steps/s (collection: 10.209s, learning 0.164s)
               Value function loss: 8.2548
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 339.83
               Mean episode length: 249.17
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 10.37s
                        Total time: 15284.71s
                               ETA: 1119448.9s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.479s, learning 0.157s)
               Value function loss: 8.0013
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 340.00
               Mean episode length: 249.31
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 10.64s
                        Total time: 15295.34s
                               ETA: 1119385.5s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.871s, learning 0.165s)
               Value function loss: 7.5466
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 336.22
               Mean episode length: 248.53
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 11.04s
                        Total time: 15306.38s
                               ETA: 1119351.4s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.514s, learning 0.164s)
               Value function loss: 8.8539
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 333.75
               Mean episode length: 248.33
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 10.68s
                        Total time: 15317.06s
                               ETA: 1119291.2s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.817s, learning 0.165s)
               Value function loss: 7.8333
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 325.14
               Mean episode length: 248.24
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 10.98s
                        Total time: 15328.04s
                               ETA: 1119253.3s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.547s, learning 0.159s)
               Value function loss: 10.5619
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 324.33
               Mean episode length: 248.33
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 10.71s
                        Total time: 15338.75s
                               ETA: 1119195.3s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.475s, learning 0.164s)
               Value function loss: 11.6963
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 326.55
               Mean episode length: 247.78
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 10.64s
                        Total time: 15349.38s
                               ETA: 1119132.4s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.655s, learning 0.162s)
               Value function loss: 15.1838
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 331.22
               Mean episode length: 248.43
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 10.82s
                        Total time: 15360.20s
                               ETA: 1119082.6s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.776s, learning 0.171s)
               Value function loss: 12.0324
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 327.13
               Mean episode length: 248.41
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 10.95s
                        Total time: 15371.15s
                               ETA: 1119042.3s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.451s, learning 0.200s)
               Value function loss: 14.8230
                    Surrogate loss: -0.0236
             Mean action noise std: 0.75
                       Mean reward: 328.65
               Mean episode length: 248.31
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 10.65s
                        Total time: 15381.80s
                               ETA: 1118980.5s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.164s)
               Value function loss: 16.0717
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 336.50
               Mean episode length: 249.24
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 10.78s
                        Total time: 15392.58s
                               ETA: 1118928.2s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.727s, learning 0.168s)
               Value function loss: 16.1363
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 323.24
               Mean episode length: 248.19
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 10.90s
                        Total time: 15403.47s
                               ETA: 1118884.3s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.581s, learning 0.226s)
               Value function loss: 20.0994
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 317.88
               Mean episode length: 247.17
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 10.81s
                        Total time: 15414.28s
                               ETA: 1118834.1s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.945s, learning 0.164s)
               Value function loss: 19.2169
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 323.27
               Mean episode length: 248.55
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 11.11s
                        Total time: 15425.39s
                               ETA: 1118805.8s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.707s, learning 0.160s)
               Value function loss: 19.2530
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 330.99
               Mean episode length: 248.58
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 10.87s
                        Total time: 15436.26s
                               ETA: 1118760.0s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.556s, learning 0.165s)
               Value function loss: 22.8273
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 334.20
               Mean episode length: 249.24
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 10.72s
                        Total time: 15446.98s
                               ETA: 1118703.7s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.553s, learning 0.162s)
               Value function loss: 21.7296
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 336.14
               Mean episode length: 246.60
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 10.71s
                        Total time: 15457.69s
                               ETA: 1118647.1s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.839s, learning 0.163s)
               Value function loss: 20.3071
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 332.52
               Mean episode length: 249.20
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 11.00s
                        Total time: 15468.69s
                               ETA: 1118611.2s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.709s, learning 0.170s)
               Value function loss: 19.9563
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 325.90
               Mean episode length: 248.94
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 10.88s
                        Total time: 15479.57s
                               ETA: 1118566.4s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.787s, learning 0.162s)
               Value function loss: 18.3774
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 333.56
               Mean episode length: 249.38
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 10.95s
                        Total time: 15490.52s
                               ETA: 1118526.9s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.538s, learning 0.165s)
               Value function loss: 22.1848
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 339.61
               Mean episode length: 249.35
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 10.70s
                        Total time: 15501.23s
                               ETA: 1118469.5s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.878s, learning 0.208s)
               Value function loss: 20.7574
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 341.50
               Mean episode length: 249.35
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 11.09s
                        Total time: 15512.31s
                               ETA: 1118440.0s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.467s, learning 0.165s)
               Value function loss: 18.9869
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 349.18
               Mean episode length: 249.56
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 10.63s
                        Total time: 15522.94s
                               ETA: 1118377.6s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.810s, learning 0.183s)
               Value function loss: 14.0508
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 337.23
               Mean episode length: 249.75
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 10.99s
                        Total time: 15533.94s
                               ETA: 1118341.4s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1572 steps/s (collection: 10.250s, learning 0.166s)
               Value function loss: 11.9962
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 327.90
               Mean episode length: 249.53
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 10.42s
                        Total time: 15544.35s
                               ETA: 1118263.7s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.454s, learning 0.185s)
               Value function loss: 11.7160
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 343.30
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 10.64s
                        Total time: 15554.99s
                               ETA: 1118202.1s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.442s, learning 0.161s)
               Value function loss: 9.9003
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 317.02
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 10.60s
                        Total time: 15565.59s
                               ETA: 1118138.0s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.780s, learning 0.162s)
               Value function loss: 7.2764
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 312.51
               Mean episode length: 249.65
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 10.94s
                        Total time: 15576.54s
                               ETA: 1118098.3s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.728s, learning 0.189s)
               Value function loss: 6.7846
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 321.88
               Mean episode length: 249.65
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 10.92s
                        Total time: 15587.45s
                               ETA: 1118056.9s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.529s, learning 0.173s)
               Value function loss: 6.0615
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 331.20
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 10.70s
                        Total time: 15598.16s
                               ETA: 1118000.0s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.445s, learning 0.164s)
               Value function loss: 6.4557
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 327.40
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 10.61s
                        Total time: 15608.76s
                               ETA: 1117936.6s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.869s, learning 0.172s)
               Value function loss: 7.3914
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 327.54
               Mean episode length: 249.32
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 11.04s
                        Total time: 15619.81s
                               ETA: 1117904.3s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.651s, learning 0.165s)
               Value function loss: 8.1648
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 313.93
               Mean episode length: 249.32
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 10.82s
                        Total time: 15630.62s
                               ETA: 1117855.8s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.815s, learning 0.167s)
               Value function loss: 6.1007
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 314.99
               Mean episode length: 249.32
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 10.98s
                        Total time: 15641.60s
                               ETA: 1117819.3s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.637s, learning 0.173s)
               Value function loss: 8.8824
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 323.40
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 10.81s
                        Total time: 15652.41s
                               ETA: 1117770.5s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.797s, learning 0.192s)
               Value function loss: 6.4470
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 323.84
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 10.99s
                        Total time: 15663.40s
                               ETA: 1117734.6s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.743s, learning 0.167s)
               Value function loss: 10.2157
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 309.47
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 10.91s
                        Total time: 15674.31s
                               ETA: 1117693.0s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.719s, learning 0.168s)
               Value function loss: 9.4529
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 313.45
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 10.89s
                        Total time: 15685.20s
                               ETA: 1117649.9s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.595s, learning 0.182s)
               Value function loss: 11.2107
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 314.84
               Mean episode length: 249.03
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 10.78s
                        Total time: 15695.98s
                               ETA: 1117598.9s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.843s, learning 0.199s)
               Value function loss: 11.2923
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 302.15
               Mean episode length: 248.13
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 11.04s
                        Total time: 15707.02s
                               ETA: 1117566.9s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.637s, learning 0.186s)
               Value function loss: 13.1560
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 304.11
               Mean episode length: 249.01
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 10.82s
                        Total time: 15717.84s
                               ETA: 1117519.3s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.685s, learning 0.207s)
               Value function loss: 10.9476
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 308.71
               Mean episode length: 249.75
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 10.89s
                        Total time: 15728.73s
                               ETA: 1117476.7s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.726s, learning 0.164s)
               Value function loss: 13.8749
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 318.58
               Mean episode length: 249.17
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 10.89s
                        Total time: 15739.62s
                               ETA: 1117433.9s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.767s, learning 0.169s)
               Value function loss: 13.9980
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 306.33
               Mean episode length: 249.57
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 10.94s
                        Total time: 15750.56s
                               ETA: 1117394.5s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.712s, learning 0.174s)
               Value function loss: 14.5090
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 310.03
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 10.89s
                        Total time: 15761.45s
                               ETA: 1117351.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.455s, learning 0.169s)
               Value function loss: 13.5326
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 308.23
               Mean episode length: 248.83
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 10.62s
                        Total time: 15772.07s
                               ETA: 1117290.2s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.163s)
               Value function loss: 15.4161
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 307.64
               Mean episode length: 248.10
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 10.74s
                        Total time: 15782.80s
                               ETA: 1117236.8s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.717s, learning 0.164s)
               Value function loss: 16.4357
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 299.64
               Mean episode length: 249.91
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 10.88s
                        Total time: 15793.69s
                               ETA: 1117193.6s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.697s, learning 0.165s)
               Value function loss: 15.5927
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 295.07
               Mean episode length: 248.75
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 10.86s
                        Total time: 15804.55s
                               ETA: 1117149.2s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.106s, learning 0.191s)
               Value function loss: 14.3892
                    Surrogate loss: -0.0223
             Mean action noise std: 0.75
                       Mean reward: 299.50
               Mean episode length: 249.08
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 11.30s
                        Total time: 15815.84s
                               ETA: 1117135.6s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.161s)
               Value function loss: 16.4865
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 294.26
               Mean episode length: 249.71
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 10.85s
                        Total time: 15826.70s
                               ETA: 1117090.6s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.606s, learning 0.179s)
               Value function loss: 14.9088
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 309.37
               Mean episode length: 249.70
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 10.79s
                        Total time: 15837.48s
                               ETA: 1117041.0s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.552s, learning 0.181s)
               Value function loss: 17.0523
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 302.45
               Mean episode length: 249.98
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 10.73s
                        Total time: 15848.22s
                               ETA: 1116987.6s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.169s)
               Value function loss: 13.5522
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 296.70
               Mean episode length: 249.16
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 10.76s
                        Total time: 15858.98s
                               ETA: 1116936.4s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.525s, learning 0.161s)
               Value function loss: 11.4081
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 304.77
               Mean episode length: 249.77
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 10.69s
                        Total time: 15869.66s
                               ETA: 1116879.9s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.619s, learning 0.165s)
               Value function loss: 10.8649
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 306.55
               Mean episode length: 249.56
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 10.78s
                        Total time: 15880.45s
                               ETA: 1116830.4s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.679s, learning 0.166s)
               Value function loss: 11.7597
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 305.27
               Mean episode length: 249.85
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 10.85s
                        Total time: 15891.29s
                               ETA: 1116785.2s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.767s, learning 0.173s)
               Value function loss: 9.9224
                    Surrogate loss: -0.0217
             Mean action noise std: 0.75
                       Mean reward: 300.91
               Mean episode length: 249.91
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 10.94s
                        Total time: 15902.23s
                               ETA: 1116746.7s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.434s, learning 0.192s)
               Value function loss: 6.5788
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 295.51
               Mean episode length: 249.89
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 10.63s
                        Total time: 15912.86s
                               ETA: 1116686.2s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.547s, learning 0.183s)
               Value function loss: 5.7828
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 296.40
               Mean episode length: 249.22
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 10.73s
                        Total time: 15923.59s
                               ETA: 1116633.1s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1565 steps/s (collection: 10.285s, learning 0.182s)
               Value function loss: 5.8581
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 283.77
               Mean episode length: 248.65
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 10.47s
                        Total time: 15934.06s
                               ETA: 1116561.7s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.477s, learning 0.166s)
               Value function loss: 6.0256
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 286.34
               Mean episode length: 248.08
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 10.64s
                        Total time: 15944.70s
                               ETA: 1116502.6s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.705s, learning 0.166s)
               Value function loss: 7.0809
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 281.09
               Mean episode length: 248.08
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 10.87s
                        Total time: 15955.57s
                               ETA: 1116459.6s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.765s, learning 0.171s)
               Value function loss: 7.4325
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 291.93
               Mean episode length: 248.59
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 10.94s
                        Total time: 15966.51s
                               ETA: 1116421.2s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.609s, learning 0.206s)
               Value function loss: 7.9397
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 296.74
               Mean episode length: 248.75
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 10.82s
                        Total time: 15977.32s
                               ETA: 1116374.3s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1569 steps/s (collection: 10.270s, learning 0.172s)
               Value function loss: 6.5818
                    Surrogate loss: -0.0241
             Mean action noise std: 0.75
                       Mean reward: 294.35
               Mean episode length: 248.75
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 10.44s
                        Total time: 15987.76s
                               ETA: 1116301.4s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.567s, learning 0.168s)
               Value function loss: 8.0509
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 298.25
               Mean episode length: 248.69
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 10.74s
                        Total time: 15998.50s
                               ETA: 1116249.1s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.729s, learning 0.160s)
               Value function loss: 9.7179
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 307.69
               Mean episode length: 249.51
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 10.89s
                        Total time: 16009.39s
                               ETA: 1116207.6s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.449s, learning 0.157s)
               Value function loss: 11.3935
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 317.83
               Mean episode length: 248.81
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 10.61s
                        Total time: 16019.99s
                               ETA: 1116146.4s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.654s, learning 0.166s)
               Value function loss: 12.2559
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 312.32
               Mean episode length: 248.05
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 10.82s
                        Total time: 16030.81s
                               ETA: 1116100.2s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.453s, learning 0.158s)
               Value function loss: 11.8836
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 305.88
               Mean episode length: 248.10
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 10.61s
                        Total time: 16041.43s
                               ETA: 1116039.4s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.519s, learning 0.169s)
               Value function loss: 12.8728
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 307.51
               Mean episode length: 248.59
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 10.69s
                        Total time: 16052.11s
                               ETA: 1115984.1s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.488s, learning 0.169s)
               Value function loss: 13.4152
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 309.90
               Mean episode length: 249.41
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 10.66s
                        Total time: 16062.77s
                               ETA: 1115926.7s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.465s, learning 0.159s)
               Value function loss: 12.1184
                    Surrogate loss: 0.0057
             Mean action noise std: 0.75
                       Mean reward: 301.26
               Mean episode length: 248.91
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 10.62s
                        Total time: 16073.39s
                               ETA: 1115867.1s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.744s, learning 0.157s)
               Value function loss: 13.4629
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 303.32
               Mean episode length: 248.87
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 10.90s
                        Total time: 16084.29s
                               ETA: 1115826.7s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.421s, learning 0.191s)
               Value function loss: 16.2216
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 307.40
               Mean episode length: 249.74
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 10.61s
                        Total time: 16094.91s
                               ETA: 1115766.3s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.458s, learning 0.159s)
               Value function loss: 14.7047
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 302.00
               Mean episode length: 249.35
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 10.62s
                        Total time: 16105.52s
                               ETA: 1115706.4s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.808s, learning 0.172s)
               Value function loss: 15.0839
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 287.56
               Mean episode length: 247.51
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 10.98s
                        Total time: 16116.50s
                               ETA: 1115671.7s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.362s, learning 0.211s)
               Value function loss: 14.4871
                    Surrogate loss: -0.0274
             Mean action noise std: 0.75
                       Mean reward: 288.90
               Mean episode length: 248.03
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 10.57s
                        Total time: 16127.08s
                               ETA: 1115608.8s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.817s, learning 0.165s)
               Value function loss: 16.8342
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 290.73
               Mean episode length: 249.52
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 10.98s
                        Total time: 16138.06s
                               ETA: 1115574.3s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.158s, learning 0.161s)
               Value function loss: 15.7165
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 312.38
               Mean episode length: 249.60
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 10.32s
                        Total time: 16148.38s
                               ETA: 1115494.1s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.581s, learning 0.167s)
               Value function loss: 17.4728
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 305.62
               Mean episode length: 249.98
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 10.75s
                        Total time: 16159.12s
                               ETA: 1115443.5s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.440s, learning 0.169s)
               Value function loss: 14.5438
                    Surrogate loss: -0.0223
             Mean action noise std: 0.75
                       Mean reward: 295.94
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 10.61s
                        Total time: 16169.73s
                               ETA: 1115383.4s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.607s, learning 0.159s)
               Value function loss: 14.6351
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 290.15
               Mean episode length: 249.93
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 10.77s
                        Total time: 16180.50s
                               ETA: 1115334.2s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.637s, learning 0.181s)
               Value function loss: 14.8873
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 293.54
               Mean episode length: 249.02
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 10.82s
                        Total time: 16191.32s
                               ETA: 1115288.7s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1568 steps/s (collection: 10.250s, learning 0.192s)
               Value function loss: 14.2038
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 297.34
               Mean episode length: 249.64
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 10.44s
                        Total time: 16201.76s
                               ETA: 1115217.3s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.496s, learning 0.159s)
               Value function loss: 12.3436
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 316.06
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 10.66s
                        Total time: 16212.41s
                               ETA: 1115160.7s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.446s, learning 0.190s)
               Value function loss: 12.5603
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 312.97
               Mean episode length: 249.26
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 10.64s
                        Total time: 16223.05s
                               ETA: 1115102.8s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.710s, learning 0.163s)
               Value function loss: 8.5109
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 301.11
               Mean episode length: 249.29
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 10.87s
                        Total time: 16233.92s
                               ETA: 1115061.2s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.500s, learning 0.172s)
               Value function loss: 6.2192
                    Surrogate loss: -0.0250
             Mean action noise std: 0.75
                       Mean reward: 305.51
               Mean episode length: 249.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 10.67s
                        Total time: 16244.60s
                               ETA: 1115006.0s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.754s, learning 0.161s)
               Value function loss: 5.3566
                    Surrogate loss: -0.0280
             Mean action noise std: 0.75
                       Mean reward: 304.44
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 10.92s
                        Total time: 16255.51s
                               ETA: 1114967.4s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.599s, learning 0.187s)
               Value function loss: 8.3751
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 298.61
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 10.79s
                        Total time: 16266.30s
                               ETA: 1114920.0s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.666s, learning 0.189s)
               Value function loss: 4.8855
                    Surrogate loss: -0.0261
             Mean action noise std: 0.75
                       Mean reward: 298.54
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 10.86s
                        Total time: 16277.15s
                               ETA: 1114877.5s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.727s, learning 0.189s)
               Value function loss: 6.3996
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 306.13
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 10.92s
                        Total time: 16288.07s
                               ETA: 1114839.1s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.570s, learning 0.166s)
               Value function loss: 7.2002
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 294.44
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 10.74s
                        Total time: 16298.80s
                               ETA: 1114788.4s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.609s, learning 0.157s)
               Value function loss: 7.7966
                    Surrogate loss: -0.0238
             Mean action noise std: 0.75
                       Mean reward: 299.69
               Mean episode length: 249.84
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 10.77s
                        Total time: 16309.57s
                               ETA: 1114739.8s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.161s)
               Value function loss: 5.8977
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 299.89
               Mean episode length: 249.84
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 11.01s
                        Total time: 16320.58s
                               ETA: 1114708.1s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.511s, learning 0.278s)
               Value function loss: 6.4335
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 302.76
               Mean episode length: 249.69
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 10.79s
                        Total time: 16331.37s
                               ETA: 1114661.2s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.608s, learning 0.165s)
               Value function loss: 9.3380
                    Surrogate loss: -0.0217
             Mean action noise std: 0.75
                       Mean reward: 299.95
               Mean episode length: 249.85
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 10.77s
                        Total time: 16342.14s
                               ETA: 1114613.3s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.630s, learning 0.172s)
               Value function loss: 10.3942
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 300.12
               Mean episode length: 249.85
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 10.80s
                        Total time: 16352.94s
                               ETA: 1114567.3s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.780s, learning 0.162s)
               Value function loss: 10.0727
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 304.02
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 10.94s
                        Total time: 16363.89s
                               ETA: 1114531.0s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.859s, learning 0.184s)
               Value function loss: 14.2895
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 304.91
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 11.04s
                        Total time: 16374.93s
                               ETA: 1114501.7s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1575 steps/s (collection: 10.234s, learning 0.165s)
               Value function loss: 11.1145
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 296.15
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 10.40s
                        Total time: 16385.33s
                               ETA: 1114428.5s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.316s, learning 0.188s)
               Value function loss: 14.1265
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 315.36
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 10.50s
                        Total time: 16395.83s
                               ETA: 1114362.6s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.396s, learning 0.164s)
               Value function loss: 14.7197
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 326.61
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 10.56s
                        Total time: 16406.39s
                               ETA: 1114300.5s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.679s, learning 0.161s)
               Value function loss: 16.9247
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 332.13
               Mean episode length: 249.42
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 10.84s
                        Total time: 16417.23s
                               ETA: 1114257.5s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.400s, learning 0.169s)
               Value function loss: 17.0692
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 322.94
               Mean episode length: 249.49
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 10.57s
                        Total time: 16427.80s
                               ETA: 1114196.1s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.867s, learning 0.286s)
               Value function loss: 17.2986
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 328.43
               Mean episode length: 249.72
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 11.15s
                        Total time: 16438.96s
                               ETA: 1114174.5s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.521s, learning 0.164s)
               Value function loss: 17.5115
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 333.60
               Mean episode length: 249.72
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 10.69s
                        Total time: 16449.64s
                               ETA: 1114121.2s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.700s, learning 0.162s)
               Value function loss: 17.7037
                    Surrogate loss: -0.0249
             Mean action noise std: 0.75
                       Mean reward: 331.51
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 10.86s
                        Total time: 16460.50s
                               ETA: 1114079.8s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.651s, learning 0.163s)
               Value function loss: 18.5241
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 337.63
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 10.81s
                        Total time: 16471.32s
                               ETA: 1114035.3s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.696s, learning 0.192s)
               Value function loss: 17.2590
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 338.75
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 10.89s
                        Total time: 16482.20s
                               ETA: 1113995.8s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.166s)
               Value function loss: 19.4384
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 345.34
               Mean episode length: 249.34
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 10.92s
                        Total time: 16493.12s
                               ETA: 1113958.2s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.640s, learning 0.161s)
               Value function loss: 15.9447
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 336.68
               Mean episode length: 249.48
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 10.80s
                        Total time: 16503.92s
                               ETA: 1113912.9s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.173s)
               Value function loss: 19.5435
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 348.03
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 10.78s
                        Total time: 16514.70s
                               ETA: 1113866.2s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.429s, learning 0.190s)
               Value function loss: 20.0644
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 341.31
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 10.62s
                        Total time: 16525.32s
                               ETA: 1113808.7s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.592s, learning 0.162s)
               Value function loss: 16.5608
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 343.10
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 10.75s
                        Total time: 16536.07s
                               ETA: 1113760.4s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.799s, learning 0.259s)
               Value function loss: 17.6156
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 355.58
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.06s
                        Total time: 16547.13s
                               ETA: 1113732.7s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.167s)
               Value function loss: 17.0534
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 351.73
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 10.93s
                        Total time: 16558.06s
                               ETA: 1113696.0s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.552s, learning 0.171s)
               Value function loss: 14.1806
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 350.94
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 10.72s
                        Total time: 16568.78s
                               ETA: 1113645.7s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.167s)
               Value function loss: 9.3811
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 357.75
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 10.75s
                        Total time: 16579.53s
                               ETA: 1113597.7s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.906s, learning 0.160s)
               Value function loss: 8.5346
                    Surrogate loss: -0.0230
             Mean action noise std: 0.75
                       Mean reward: 355.52
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 11.07s
                        Total time: 16590.60s
                               ETA: 1113570.6s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.352s, learning 0.159s)
               Value function loss: 8.5881
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 352.62
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 10.51s
                        Total time: 16601.11s
                               ETA: 1113506.2s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.784s, learning 0.187s)
               Value function loss: 7.6047
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 344.19
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 10.97s
                        Total time: 16612.08s
                               ETA: 1113472.8s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.500s, learning 0.162s)
               Value function loss: 7.0207
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 347.56
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 10.66s
                        Total time: 16622.74s
                               ETA: 1113418.8s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.555s, learning 0.182s)
               Value function loss: 8.1138
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 345.81
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 10.74s
                        Total time: 16633.48s
                               ETA: 1113369.7s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.378s, learning 0.159s)
               Value function loss: 8.9949
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 351.08
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 10.54s
                        Total time: 16644.02s
                               ETA: 1113307.4s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.684s, learning 0.162s)
               Value function loss: 8.3608
                    Surrogate loss: -0.0232
             Mean action noise std: 0.75
                       Mean reward: 344.50
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 10.85s
                        Total time: 16654.86s
                               ETA: 1113265.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.854s, learning 0.190s)
               Value function loss: 8.6731
                    Surrogate loss: -0.0291
             Mean action noise std: 0.75
                       Mean reward: 339.03
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 11.04s
                        Total time: 16665.91s
                               ETA: 1113237.5s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.568s, learning 0.189s)
               Value function loss: 7.6708
                    Surrogate loss: -0.0304
             Mean action noise std: 0.75
                       Mean reward: 331.09
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 10.76s
                        Total time: 16676.67s
                               ETA: 1113190.0s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.868s, learning 0.164s)
               Value function loss: 11.2721
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 323.80
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 11.03s
                        Total time: 16687.70s
                               ETA: 1113161.0s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.169s)
               Value function loss: 11.2324
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 319.13
               Mean episode length: 249.72
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 10.79s
                        Total time: 16698.48s
                               ETA: 1113115.5s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.700s, learning 0.208s)
               Value function loss: 14.0513
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 332.59
               Mean episode length: 249.72
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 10.91s
                        Total time: 16709.39s
                               ETA: 1113078.3s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.679s, learning 0.159s)
               Value function loss: 11.6944
                    Surrogate loss: -0.0257
             Mean action noise std: 0.75
                       Mean reward: 340.26
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 10.84s
                        Total time: 16720.23s
                               ETA: 1113036.4s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.164s)
               Value function loss: 14.7933
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 333.72
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 10.78s
                        Total time: 16731.01s
                               ETA: 1112990.7s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.822s, learning 0.160s)
               Value function loss: 15.0436
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 323.03
               Mean episode length: 249.92
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 10.98s
                        Total time: 16741.99s
                               ETA: 1112958.4s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.944s, learning 0.163s)
               Value function loss: 13.8208
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 331.12
               Mean episode length: 249.92
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 11.11s
                        Total time: 16753.10s
                               ETA: 1112934.5s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.503s, learning 0.172s)
               Value function loss: 14.6380
                    Surrogate loss: -0.0258
             Mean action noise std: 0.75
                       Mean reward: 331.68
               Mean episode length: 249.50
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 10.67s
                        Total time: 16763.78s
                               ETA: 1112882.0s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.478s, learning 0.164s)
               Value function loss: 13.0204
                    Surrogate loss: -0.0223
             Mean action noise std: 0.75
                       Mean reward: 319.46
               Mean episode length: 249.96
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 10.64s
                        Total time: 16774.42s
                               ETA: 1112827.3s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.966s, learning 0.161s)
               Value function loss: 15.2029
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 330.89
               Mean episode length: 249.99
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 11.13s
                        Total time: 16785.54s
                               ETA: 1112804.8s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.759s, learning 0.162s)
               Value function loss: 16.4250
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 337.40
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 10.92s
                        Total time: 16796.47s
                               ETA: 1112768.6s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.867s, learning 0.160s)
               Value function loss: 14.7277
                    Surrogate loss: -0.0267
             Mean action noise std: 0.75
                       Mean reward: 330.25
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 11.03s
                        Total time: 16807.49s
                               ETA: 1112739.6s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.736s, learning 0.281s)
               Value function loss: 13.6646
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 336.90
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 11.02s
                        Total time: 16818.51s
                               ETA: 1112709.8s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.810s, learning 0.162s)
               Value function loss: 12.7695
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 328.34
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 10.97s
                        Total time: 16829.48s
                               ETA: 1112677.2s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.547s, learning 0.159s)
               Value function loss: 12.3006
                    Surrogate loss: -0.0261
             Mean action noise std: 0.75
                       Mean reward: 336.87
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 10.71s
                        Total time: 16840.19s
                               ETA: 1112627.0s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.844s, learning 0.279s)
               Value function loss: 13.4513
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 331.57
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 11.12s
                        Total time: 16851.31s
                               ETA: 1112604.4s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.166s)
               Value function loss: 13.5971
                    Surrogate loss: -0.0250
             Mean action noise std: 0.75
                       Mean reward: 334.61
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 10.78s
                        Total time: 16862.09s
                               ETA: 1112559.2s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.163s)
               Value function loss: 12.1424
                    Surrogate loss: -0.0260
             Mean action noise std: 0.75
                       Mean reward: 344.67
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 10.90s
                        Total time: 16872.99s
                               ETA: 1112521.7s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1564 steps/s (collection: 10.305s, learning 0.169s)
               Value function loss: 11.3076
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 348.29
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 10.47s
                        Total time: 16883.46s
                               ETA: 1112456.3s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.164s)
               Value function loss: 10.2381
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 356.35
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 10.94s
                        Total time: 16894.40s
                               ETA: 1112421.8s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.529s, learning 0.169s)
               Value function loss: 10.6719
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 349.22
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 10.70s
                        Total time: 16905.10s
                               ETA: 1112371.3s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1566 steps/s (collection: 10.296s, learning 0.164s)
               Value function loss: 8.6140
                    Surrogate loss: -0.0262
             Mean action noise std: 0.75
                       Mean reward: 345.75
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 10.46s
                        Total time: 16915.56s
                               ETA: 1112305.3s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.673s, learning 0.167s)
               Value function loss: 7.4199
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 344.26
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 10.84s
                        Total time: 16926.40s
                               ETA: 1112264.3s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.810s, learning 0.164s)
               Value function loss: 7.3418
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 348.23
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 10.97s
                        Total time: 16937.37s
                               ETA: 1112232.2s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.651s, learning 0.163s)
               Value function loss: 6.0184
                    Surrogate loss: -0.0273
             Mean action noise std: 0.75
                       Mean reward: 358.93
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 10.81s
                        Total time: 16948.19s
                               ETA: 1112189.5s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1571 steps/s (collection: 10.259s, learning 0.165s)
               Value function loss: 5.5700
                    Surrogate loss: -0.0277
             Mean action noise std: 0.75
                       Mean reward: 360.51
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 10.42s
                        Total time: 16958.61s
                               ETA: 1112121.4s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.989s, learning 0.196s)
               Value function loss: 8.0408
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 355.60
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.18s
                        Total time: 16969.80s
                               ETA: 1112103.1s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.161s)
               Value function loss: 6.7684
                    Surrogate loss: -0.0340
             Mean action noise std: 0.75
                       Mean reward: 353.61
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 10.85s
                        Total time: 16980.65s
                               ETA: 1112063.2s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.490s, learning 0.184s)
               Value function loss: 6.2891
                    Surrogate loss: -0.0278
             Mean action noise std: 0.75
                       Mean reward: 348.91
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 10.67s
                        Total time: 16991.32s
                               ETA: 1112011.5s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.715s, learning 0.169s)
               Value function loss: 6.7101
                    Surrogate loss: -0.0251
             Mean action noise std: 0.75
                       Mean reward: 351.71
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 10.88s
                        Total time: 17002.21s
                               ETA: 1111973.7s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.157s, learning 0.165s)
               Value function loss: 6.7192
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 343.36
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 10.32s
                        Total time: 17012.53s
                               ETA: 1111899.1s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.809s, learning 0.161s)
               Value function loss: 10.7758
                    Surrogate loss: -0.0249
             Mean action noise std: 0.75
                       Mean reward: 342.89
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 10.97s
                        Total time: 17023.50s
                               ETA: 1111867.0s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.506s, learning 0.160s)
               Value function loss: 8.8347
                    Surrogate loss: -0.0288
             Mean action noise std: 0.75
                       Mean reward: 347.29
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 10.67s
                        Total time: 17034.16s
                               ETA: 1111815.1s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.450s, learning 0.205s)
               Value function loss: 10.9198
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 349.70
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 10.66s
                        Total time: 17044.82s
                               ETA: 1111762.5s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.681s, learning 0.161s)
               Value function loss: 9.8820
                    Surrogate loss: -0.0230
             Mean action noise std: 0.75
                       Mean reward: 340.94
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 10.84s
                        Total time: 17055.66s
                               ETA: 1111722.1s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.621s, learning 0.168s)
               Value function loss: 10.9875
                    Surrogate loss: -0.0212
             Mean action noise std: 0.75
                       Mean reward: 346.69
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 10.79s
                        Total time: 17066.45s
                               ETA: 1111678.4s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.163s)
               Value function loss: 9.2517
                    Surrogate loss: -0.0264
             Mean action noise std: 0.75
                       Mean reward: 350.83
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 10.71s
                        Total time: 17077.16s
                               ETA: 1111629.6s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.562s, learning 0.183s)
               Value function loss: 12.4324
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 356.57
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 10.74s
                        Total time: 17087.91s
                               ETA: 1111583.0s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.583s, learning 0.179s)
               Value function loss: 12.7677
                    Surrogate loss: -0.0263
             Mean action noise std: 0.75
                       Mean reward: 357.54
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 10.76s
                        Total time: 17098.67s
                               ETA: 1111537.7s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.770s, learning 0.164s)
               Value function loss: 13.6035
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 363.03
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 10.93s
                        Total time: 17109.60s
                               ETA: 1111503.5s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.820s, learning 0.169s)
               Value function loss: 13.2207
                    Surrogate loss: -0.0301
             Mean action noise std: 0.75
                       Mean reward: 362.60
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 10.99s
                        Total time: 17120.59s
                               ETA: 1111472.9s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.661s, learning 0.263s)
               Value function loss: 13.9381
                    Surrogate loss: -0.0273
             Mean action noise std: 0.75
                       Mean reward: 383.70
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 10.92s
                        Total time: 17131.52s
                               ETA: 1111438.1s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.753s, learning 0.162s)
               Value function loss: 13.8335
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 364.89
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 10.91s
                        Total time: 17142.43s
                               ETA: 1111402.8s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.688s, learning 0.159s)
               Value function loss: 13.4491
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 372.09
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 10.85s
                        Total time: 17153.28s
                               ETA: 1111363.1s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.535s, learning 0.190s)
               Value function loss: 12.5170
                    Surrogate loss: -0.0272
             Mean action noise std: 0.75
                       Mean reward: 358.12
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 10.73s
                        Total time: 17164.00s
                               ETA: 1111315.6s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.163s)
               Value function loss: 14.3500
                    Surrogate loss: -0.0232
             Mean action noise std: 0.75
                       Mean reward: 361.34
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 10.70s
                        Total time: 17174.70s
                               ETA: 1111266.2s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.432s, learning 0.160s)
               Value function loss: 13.5729
                    Surrogate loss: -0.0274
             Mean action noise std: 0.75
                       Mean reward: 359.52
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 10.59s
                        Total time: 17185.29s
                               ETA: 1111210.2s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.529s, learning 0.166s)
               Value function loss: 16.4329
                    Surrogate loss: -0.0257
             Mean action noise std: 0.75
                       Mean reward: 364.22
               Mean episode length: 249.88
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 10.69s
                        Total time: 17195.99s
                               ETA: 1111160.8s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.503s, learning 0.161s)
               Value function loss: 12.3490
                    Surrogate loss: -0.0257
             Mean action noise std: 0.75
                       Mean reward: 372.46
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 10.66s
                        Total time: 17206.65s
                               ETA: 1111109.5s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.643s, learning 0.163s)
               Value function loss: 11.1027
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 366.10
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 10.81s
                        Total time: 17217.45s
                               ETA: 1111067.4s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.918s, learning 0.156s)
               Value function loss: 10.5829
                    Surrogate loss: -0.0259
             Mean action noise std: 0.75
                       Mean reward: 357.77
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 11.07s
                        Total time: 17228.53s
                               ETA: 1111042.7s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.861s, learning 0.160s)
               Value function loss: 11.3302
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 361.25
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 11.02s
                        Total time: 17239.55s
                               ETA: 1111014.5s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.774s, learning 0.190s)
               Value function loss: 8.6962
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 347.78
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 10.96s
                        Total time: 17250.51s
                               ETA: 1110982.7s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.158s)
               Value function loss: 5.5704
                    Surrogate loss: -0.0276
             Mean action noise std: 0.75
                       Mean reward: 350.74
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 10.93s
                        Total time: 17261.44s
                               ETA: 1110948.8s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.718s, learning 0.171s)
               Value function loss: 4.9931
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 346.44
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 10.89s
                        Total time: 17272.33s
                               ETA: 1110912.2s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.668s, learning 0.163s)
               Value function loss: 5.5523
                    Surrogate loss: -0.0232
             Mean action noise std: 0.75
                       Mean reward: 353.82
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 10.83s
                        Total time: 17283.16s
                               ETA: 1110872.0s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.887s, learning 0.159s)
               Value function loss: 5.7454
                    Surrogate loss: -0.0265
             Mean action noise std: 0.75
                       Mean reward: 356.60
               Mean episode length: 249.69
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 11.05s
                        Total time: 17294.21s
                               ETA: 1110845.5s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.753s, learning 0.167s)
               Value function loss: 6.3360
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 358.82
               Mean episode length: 249.69
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 10.92s
                        Total time: 17305.13s
                               ETA: 1110811.1s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.429s, learning 0.208s)
               Value function loss: 6.5001
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 349.86
               Mean episode length: 249.69
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 10.64s
                        Total time: 17315.77s
                               ETA: 1110758.4s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.158s)
               Value function loss: 6.8293
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 354.77
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 10.81s
                        Total time: 17326.58s
                               ETA: 1110717.3s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.368s, learning 0.160s)
               Value function loss: 5.0754
                    Surrogate loss: -0.0281
             Mean action noise std: 0.75
                       Mean reward: 352.41
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 10.53s
                        Total time: 17337.11s
                               ETA: 1110657.8s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.514s, learning 0.158s)
               Value function loss: 7.8029
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 355.41
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 10.67s
                        Total time: 17347.78s
                               ETA: 1110607.6s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.976s, learning 0.158s)
               Value function loss: 8.7878
                    Surrogate loss: -0.0252
             Mean action noise std: 0.75
                       Mean reward: 344.45
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 11.13s
                        Total time: 17358.92s
                               ETA: 1110587.1s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.661s, learning 0.166s)
               Value function loss: 9.5769
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 338.75
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 10.83s
                        Total time: 17369.74s
                               ETA: 1110547.0s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.550s, learning 0.200s)
               Value function loss: 9.8480
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 344.71
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 10.75s
                        Total time: 17380.49s
                               ETA: 1110501.9s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.432s, learning 0.161s)
               Value function loss: 8.5042
                    Surrogate loss: -0.0300
             Mean action noise std: 0.75
                       Mean reward: 355.19
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 10.59s
                        Total time: 17391.09s
                               ETA: 1110446.8s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.687s, learning 0.167s)
               Value function loss: 9.8198
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 362.11
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 10.85s
                        Total time: 17401.94s
                               ETA: 1110408.5s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.515s, learning 0.162s)
               Value function loss: 9.6768
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 348.03
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 10.68s
                        Total time: 17412.62s
                               ETA: 1110358.9s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1571 steps/s (collection: 10.241s, learning 0.181s)
               Value function loss: 10.6704
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 355.23
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 10.42s
                        Total time: 17423.04s
                               ETA: 1110293.1s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.612s, learning 0.181s)
               Value function loss: 13.2099
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 355.05
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 10.79s
                        Total time: 17433.83s
                               ETA: 1110251.0s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.737s, learning 0.160s)
               Value function loss: 13.0116
                    Surrogate loss: -0.0249
             Mean action noise std: 0.75
                       Mean reward: 363.75
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 10.90s
                        Total time: 17444.73s
                               ETA: 1110215.5s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.630s, learning 0.162s)
               Value function loss: 12.6111
                    Surrogate loss: -0.0271
             Mean action noise std: 0.75
                       Mean reward: 357.65
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 10.79s
                        Total time: 17455.52s
                               ETA: 1110173.4s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.161s)
               Value function loss: 13.0657
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 363.45
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 10.93s
                        Total time: 17466.46s
                               ETA: 1110140.4s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.650s, learning 0.161s)
               Value function loss: 13.6048
                    Surrogate loss: -0.0275
             Mean action noise std: 0.75
                       Mean reward: 366.10
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 10.81s
                        Total time: 17477.27s
                               ETA: 1110099.6s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.804s, learning 0.252s)
               Value function loss: 15.7753
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 359.25
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 11.06s
                        Total time: 17488.32s
                               ETA: 1110074.3s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.453s, learning 0.163s)
               Value function loss: 16.9133
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 375.95
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 10.62s
                        Total time: 17498.94s
                               ETA: 1110021.2s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.524s, learning 0.159s)
               Value function loss: 16.0187
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 365.69
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 10.68s
                        Total time: 17509.62s
                               ETA: 1109972.4s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.931s, learning 0.212s)
               Value function loss: 14.2896
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 358.39
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 11.14s
                        Total time: 17520.76s
                               ETA: 1109952.7s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.160s)
               Value function loss: 15.2366
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 365.89
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 10.85s
                        Total time: 17531.62s
                               ETA: 1109914.7s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.801s, learning 0.161s)
               Value function loss: 16.3812
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 383.46
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 10.96s
                        Total time: 17542.58s
                               ETA: 1109883.7s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.548s, learning 0.157s)
               Value function loss: 13.5288
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 382.51
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 10.70s
                        Total time: 17553.28s
                               ETA: 1109836.4s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.632s, learning 0.161s)
               Value function loss: 14.4184
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 376.08
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 10.79s
                        Total time: 17564.08s
                               ETA: 1109794.8s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.739s, learning 0.159s)
               Value function loss: 15.0040
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 378.22
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 10.90s
                        Total time: 17574.97s
                               ETA: 1109759.8s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.647s, learning 0.159s)
               Value function loss: 9.9542
                    Surrogate loss: -0.0228
             Mean action noise std: 0.75
                       Mean reward: 375.93
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 10.81s
                        Total time: 17585.78s
                               ETA: 1109719.0s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.397s, learning 0.160s)
               Value function loss: 7.4679
                    Surrogate loss: -0.0217
             Mean action noise std: 0.75
                       Mean reward: 377.58
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 10.56s
                        Total time: 17596.34s
                               ETA: 1109662.6s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.763s, learning 0.159s)
               Value function loss: 7.2047
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 375.15
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 10.92s
                        Total time: 17607.26s
                               ETA: 1109629.2s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.618s, learning 0.168s)
               Value function loss: 8.1623
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 377.80
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 10.79s
                        Total time: 17618.04s
                               ETA: 1109587.4s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.821s, learning 0.160s)
               Value function loss: 5.8244
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 381.11
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 10.98s
                        Total time: 17629.03s
                               ETA: 1109557.8s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.572s, learning 0.166s)
               Value function loss: 6.6798
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 377.18
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 10.74s
                        Total time: 17639.76s
                               ETA: 1109512.9s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.699s, learning 0.216s)
               Value function loss: 6.7406
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 378.44
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 10.91s
                        Total time: 17650.68s
                               ETA: 1109479.2s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.674s, learning 0.162s)
               Value function loss: 7.5308
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 370.76
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 10.84s
                        Total time: 17661.51s
                               ETA: 1109440.6s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.507s, learning 0.159s)
               Value function loss: 5.9367
                    Surrogate loss: -0.0262
             Mean action noise std: 0.75
                       Mean reward: 362.61
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 10.67s
                        Total time: 17672.18s
                               ETA: 1109391.4s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.748s, learning 0.157s)
               Value function loss: 6.7464
                    Surrogate loss: -0.0268
             Mean action noise std: 0.75
                       Mean reward: 352.71
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 10.91s
                        Total time: 17683.08s
                               ETA: 1109357.2s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.659s, learning 0.160s)
               Value function loss: 6.9658
                    Surrogate loss: -0.0275
             Mean action noise std: 0.75
                       Mean reward: 348.99
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 10.82s
                        Total time: 17693.90s
                               ETA: 1109317.6s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.821s, learning 0.159s)
               Value function loss: 10.0411
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 347.45
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 10.98s
                        Total time: 17704.88s
                               ETA: 1109288.2s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1568 steps/s (collection: 10.286s, learning 0.157s)
               Value function loss: 10.0679
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 361.65
               Mean episode length: 249.91
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 10.44s
                        Total time: 17715.33s
                               ETA: 1109225.1s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.818s, learning 0.185s)
               Value function loss: 10.0845
                    Surrogate loss: -0.0290
             Mean action noise std: 0.75
                       Mean reward: 354.33
               Mean episode length: 249.51
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 11.00s
                        Total time: 17726.33s
                               ETA: 1109197.1s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.469s, learning 0.161s)
               Value function loss: 9.6712
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 342.51
               Mean episode length: 249.51
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 10.63s
                        Total time: 17736.96s
                               ETA: 1109145.8s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.376s, learning 0.158s)
               Value function loss: 10.8702
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 344.56
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 10.53s
                        Total time: 17747.49s
                               ETA: 1109088.7s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.684s, learning 0.159s)
               Value function loss: 12.2637
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 362.13
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 10.84s
                        Total time: 17758.34s
                               ETA: 1109050.9s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.873s, learning 0.212s)
               Value function loss: 12.9301
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 357.78
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 11.08s
                        Total time: 17769.42s
                               ETA: 1109028.2s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.562s, learning 0.158s)
               Value function loss: 12.2561
                    Surrogate loss: -0.0250
             Mean action noise std: 0.75
                       Mean reward: 358.72
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 10.72s
                        Total time: 17780.14s
                               ETA: 1108982.8s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1561 steps/s (collection: 10.306s, learning 0.187s)
               Value function loss: 12.8646
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 337.18
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 10.49s
                        Total time: 17790.63s
                               ETA: 1108923.3s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.686s, learning 0.162s)
               Value function loss: 13.4063
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 351.09
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 10.85s
                        Total time: 17801.48s
                               ETA: 1108885.9s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.482s, learning 0.159s)
               Value function loss: 15.1302
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 349.42
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 10.64s
                        Total time: 17812.12s
                               ETA: 1108835.7s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.654s, learning 0.160s)
               Value function loss: 15.3339
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 346.63
               Mean episode length: 249.69
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 10.81s
                        Total time: 17822.94s
                               ETA: 1108796.2s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.581s, learning 0.160s)
               Value function loss: 12.4166
                    Surrogate loss: -0.0238
             Mean action noise std: 0.75
                       Mean reward: 352.35
               Mean episode length: 249.66
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 10.74s
                        Total time: 17833.68s
                               ETA: 1108752.4s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.501s, learning 0.196s)
               Value function loss: 14.3981
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 353.24
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 10.70s
                        Total time: 17844.38s
                               ETA: 1108705.8s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1553 steps/s (collection: 10.387s, learning 0.160s)
               Value function loss: 12.5016
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 361.92
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 10.55s
                        Total time: 17854.92s
                               ETA: 1108649.9s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.727s, learning 0.182s)
               Value function loss: 16.2169
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 359.37
               Mean episode length: 249.75
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 10.91s
                        Total time: 17865.83s
                               ETA: 1108616.5s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.629s, learning 0.160s)
               Value function loss: 13.0799
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 357.16
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 10.79s
                        Total time: 17876.62s
                               ETA: 1108575.7s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1563 steps/s (collection: 10.324s, learning 0.158s)
               Value function loss: 11.6647
                    Surrogate loss: -0.0236
             Mean action noise std: 0.75
                       Mean reward: 366.06
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 10.48s
                        Total time: 17887.10s
                               ETA: 1108516.0s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.701s, learning 0.184s)
               Value function loss: 13.5424
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 372.07
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 10.89s
                        Total time: 17897.99s
                               ETA: 1108481.3s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.515s, learning 0.183s)
               Value function loss: 13.1800
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 371.76
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 10.70s
                        Total time: 17908.68s
                               ETA: 1108435.0s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.786s, learning 0.238s)
               Value function loss: 12.4225
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 360.90
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 11.02s
                        Total time: 17919.71s
                               ETA: 1108408.9s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1570 steps/s (collection: 10.227s, learning 0.204s)
               Value function loss: 8.5785
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 366.30
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 10.43s
                        Total time: 17930.14s
                               ETA: 1108346.2s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.880s, learning 0.160s)
               Value function loss: 5.9044
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 368.23
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 11.04s
                        Total time: 17941.18s
                               ETA: 1108321.2s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.418s, learning 0.210s)
               Value function loss: 7.9975
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 366.78
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 10.63s
                        Total time: 17951.81s
                               ETA: 1108270.8s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.668s, learning 0.183s)
               Value function loss: 5.5416
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 366.45
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 10.85s
                        Total time: 17962.66s
                               ETA: 1108234.2s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1575 steps/s (collection: 10.215s, learning 0.181s)
               Value function loss: 5.0598
                    Surrogate loss: -0.0287
             Mean action noise std: 0.75
                       Mean reward: 370.41
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 10.40s
                        Total time: 17973.06s
                               ETA: 1108169.6s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.897s, learning 0.198s)
               Value function loss: 6.9246
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 375.62
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 11.10s
                        Total time: 17984.15s
                               ETA: 1108148.1s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.440s, learning 0.158s)
               Value function loss: 6.8195
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 377.68
               Mean episode length: 249.57
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 10.60s
                        Total time: 17994.75s
                               ETA: 1108096.0s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.619s, learning 0.173s)
               Value function loss: 7.1416
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 376.25
               Mean episode length: 249.57
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 10.79s
                        Total time: 18005.54s
                               ETA: 1108055.9s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.680s, learning 0.207s)
               Value function loss: 7.2037
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 377.09
               Mean episode length: 249.57
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 10.89s
                        Total time: 18016.43s
                               ETA: 1108021.6s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1567 steps/s (collection: 10.295s, learning 0.158s)
               Value function loss: 6.7757
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 380.09
               Mean episode length: 249.82
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 10.45s
                        Total time: 18026.88s
                               ETA: 1107960.7s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.467s, learning 0.258s)
               Value function loss: 8.3214
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 383.13
               Mean episode length: 249.82
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 10.73s
                        Total time: 18037.61s
                               ETA: 1107916.6s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.515s, learning 0.171s)
               Value function loss: 9.4275
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 377.43
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 10.69s
                        Total time: 18048.29s
                               ETA: 1107870.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.577s, learning 0.222s)
               Value function loss: 10.5104
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 364.11
               Mean episode length: 249.98
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 10.80s
                        Total time: 18059.09s
                               ETA: 1107830.7s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.527s, learning 0.173s)
               Value function loss: 8.1513
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 366.48
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 10.70s
                        Total time: 18069.79s
                               ETA: 1107785.2s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.735s, learning 0.169s)
               Value function loss: 12.3949
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 367.97
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 10.90s
                        Total time: 18080.70s
                               ETA: 1107752.2s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.710s, learning 0.160s)
               Value function loss: 12.0786
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 370.60
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 10.87s
                        Total time: 18091.57s
                               ETA: 1107717.2s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.759s, learning 0.156s)
               Value function loss: 11.4024
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 362.20
               Mean episode length: 249.97
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 10.91s
                        Total time: 18102.48s
                               ETA: 1107685.0s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1564 steps/s (collection: 10.296s, learning 0.173s)
               Value function loss: 13.6282
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 371.02
               Mean episode length: 249.97
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 10.47s
                        Total time: 18112.95s
                               ETA: 1107625.5s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.455s, learning 0.176s)
               Value function loss: 11.6487
                    Surrogate loss: -0.0242
             Mean action noise std: 0.75
                       Mean reward: 374.53
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 10.63s
                        Total time: 18123.58s
                               ETA: 1107575.9s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.732s, learning 0.163s)
               Value function loss: 14.5324
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 373.77
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 10.90s
                        Total time: 18134.48s
                               ETA: 1107542.6s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.450s, learning 0.205s)
               Value function loss: 14.9281
                    Surrogate loss: -0.0236
             Mean action noise std: 0.75
                       Mean reward: 377.84
               Mean episode length: 249.89
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 10.65s
                        Total time: 18145.13s
                               ETA: 1107494.6s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.645s, learning 0.184s)
               Value function loss: 15.7546
                    Surrogate loss: -0.0253
             Mean action noise std: 0.75
                       Mean reward: 382.07
               Mean episode length: 249.94
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 10.83s
                        Total time: 18155.96s
                               ETA: 1107457.3s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.568s, learning 0.181s)
               Value function loss: 14.7733
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 383.35
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 10.75s
                        Total time: 18166.71s
                               ETA: 1107415.1s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.626s, learning 0.170s)
               Value function loss: 14.2730
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 378.65
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 10.80s
                        Total time: 18177.50s
                               ETA: 1107375.8s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.744s, learning 0.186s)
               Value function loss: 14.1016
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 383.61
               Mean episode length: 249.96
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 10.93s
                        Total time: 18188.43s
                               ETA: 1107344.7s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.542s, learning 0.204s)
               Value function loss: 16.4932
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 386.30
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 10.75s
                        Total time: 18199.18s
                               ETA: 1107302.5s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.411s, learning 0.158s)
               Value function loss: 16.7048
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 389.69
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 10.57s
                        Total time: 18209.75s
                               ETA: 1107249.5s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.760s, learning 0.163s)
               Value function loss: 13.2604
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 379.56
               Mean episode length: 249.94
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 10.92s
                        Total time: 18220.67s
                               ETA: 1107218.1s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.612s, learning 0.182s)
               Value function loss: 12.7087
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 380.58
               Mean episode length: 248.07
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 10.79s
                        Total time: 18231.47s
                               ETA: 1107178.9s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.660s, learning 0.165s)
               Value function loss: 12.3410
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 384.44
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 10.82s
                        Total time: 18242.29s
                               ETA: 1107141.6s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.745s, learning 0.277s)
               Value function loss: 11.5101
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 379.27
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 11.02s
                        Total time: 18253.31s
                               ETA: 1107116.3s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.813s, learning 0.187s)
               Value function loss: 8.9857
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 382.13
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 11.00s
                        Total time: 18264.31s
                               ETA: 1107089.7s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.697s, learning 0.177s)
               Value function loss: 7.7536
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 380.96
               Mean episode length: 249.61
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 10.87s
                        Total time: 18275.19s
                               ETA: 1107055.4s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.479s, learning 0.191s)
               Value function loss: 6.7884
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 374.93
               Mean episode length: 249.61
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 10.67s
                        Total time: 18285.86s
                               ETA: 1107008.8s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.666s, learning 0.181s)
               Value function loss: 5.1952
                    Surrogate loss: -0.0230
             Mean action noise std: 0.75
                       Mean reward: 374.93
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 10.85s
                        Total time: 18296.70s
                               ETA: 1106973.1s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.454s, learning 0.171s)
               Value function loss: 5.1686
                    Surrogate loss: -0.0284
             Mean action noise std: 0.75
                       Mean reward: 371.82
               Mean episode length: 249.16
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 10.62s
                        Total time: 18307.33s
                               ETA: 1106923.8s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.917s, learning 0.194s)
               Value function loss: 7.0616
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 369.65
               Mean episode length: 249.16
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 11.11s
                        Total time: 18318.44s
                               ETA: 1106904.1s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.161s)
               Value function loss: 6.5184
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 362.83
               Mean episode length: 249.16
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 11.31s
                        Total time: 18329.74s
                               ETA: 1106896.0s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.411s, learning 0.162s)
               Value function loss: 6.6515
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 365.53
               Mean episode length: 249.61
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 10.57s
                        Total time: 18340.32s
                               ETA: 1106843.8s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.160s)
               Value function loss: 6.0054
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 366.59
               Mean episode length: 249.61
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 10.82s
                        Total time: 18351.14s
                               ETA: 1106806.7s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.495s, learning 0.159s)
               Value function loss: 7.1434
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 372.60
               Mean episode length: 249.61
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 10.65s
                        Total time: 18361.80s
                               ETA: 1106759.5s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.374s, learning 0.166s)
               Value function loss: 9.5360
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 373.37
               Mean episode length: 249.71
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 10.54s
                        Total time: 18372.34s
                               ETA: 1106705.4s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.670s, learning 0.182s)
               Value function loss: 10.7442
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 370.37
               Mean episode length: 248.91
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 10.85s
                        Total time: 18383.19s
                               ETA: 1106670.1s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.473s, learning 0.159s)
               Value function loss: 12.0894
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 374.29
               Mean episode length: 249.05
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 10.63s
                        Total time: 18393.82s
                               ETA: 1106621.6s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.749s, learning 0.209s)
               Value function loss: 10.7935
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 383.13
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 10.96s
                        Total time: 18404.78s
                               ETA: 1106592.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.681s, learning 0.172s)
               Value function loss: 12.6529
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 380.58
               Mean episode length: 249.28
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 10.85s
                        Total time: 18415.63s
                               ETA: 1106557.7s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.400s, learning 0.172s)
               Value function loss: 9.2599
                    Surrogate loss: -0.0220
             Mean action noise std: 0.75
                       Mean reward: 378.44
               Mean episode length: 249.27
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 10.57s
                        Total time: 18426.20s
                               ETA: 1106505.7s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.808s, learning 0.166s)
               Value function loss: 16.0291
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 386.13
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 10.97s
                        Total time: 18437.17s
                               ETA: 1106477.9s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.671s, learning 0.195s)
               Value function loss: 14.4990
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 399.69
               Mean episode length: 249.65
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 10.87s
                        Total time: 18448.04s
                               ETA: 1106443.7s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.899s, learning 0.165s)
               Value function loss: 14.6422
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 399.02
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 11.06s
                        Total time: 18459.10s
                               ETA: 1106421.4s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1586 steps/s (collection: 10.142s, learning 0.186s)
               Value function loss: 16.7455
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 389.60
               Mean episode length: 249.09
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 10.33s
                        Total time: 18469.43s
                               ETA: 1106355.0s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.839s, learning 0.239s)
               Value function loss: 18.0111
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 406.14
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 11.08s
                        Total time: 18480.51s
                               ETA: 1106333.6s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.597s, learning 0.193s)
               Value function loss: 15.5657
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 383.57
               Mean episode length: 248.24
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 10.79s
                        Total time: 18491.30s
                               ETA: 1106294.9s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.652s, learning 0.177s)
               Value function loss: 16.0596
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 400.10
               Mean episode length: 249.53
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 10.83s
                        Total time: 18502.13s
                               ETA: 1106258.6s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.680s, learning 0.218s)
               Value function loss: 16.0050
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 400.24
               Mean episode length: 249.60
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 10.90s
                        Total time: 18513.03s
                               ETA: 1106226.5s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.658s, learning 0.162s)
               Value function loss: 16.5550
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 391.05
               Mean episode length: 248.98
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 10.82s
                        Total time: 18523.85s
                               ETA: 1106189.8s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.168s)
               Value function loss: 17.6006
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 409.45
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 10.98s
                        Total time: 18534.83s
                               ETA: 1106162.8s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.903s, learning 0.197s)
               Value function loss: 16.8848
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 403.31
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 11.10s
                        Total time: 18545.93s
                               ETA: 1106142.8s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.843s, learning 0.162s)
               Value function loss: 14.0702
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 395.80
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 11.01s
                        Total time: 18556.94s
                               ETA: 1106117.2s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.553s, learning 0.178s)
               Value function loss: 12.6939
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 398.08
               Mean episode length: 249.72
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 10.73s
                        Total time: 18567.67s
                               ETA: 1106075.3s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.757s, learning 0.166s)
               Value function loss: 12.1110
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 401.32
               Mean episode length: 249.91
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 10.92s
                        Total time: 18578.59s
                               ETA: 1106044.7s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.165s)
               Value function loss: 12.8445
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 402.47
               Mean episode length: 249.91
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 10.76s
                        Total time: 18589.35s
                               ETA: 1106004.6s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.778s, learning 0.162s)
               Value function loss: 9.8745
                    Surrogate loss: -0.0213
             Mean action noise std: 0.74
                       Mean reward: 403.94
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 10.94s
                        Total time: 18600.29s
                               ETA: 1105975.2s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.652s, learning 0.178s)
               Value function loss: 7.2052
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 396.20
               Mean episode length: 249.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 10.83s
                        Total time: 18611.12s
                               ETA: 1105939.2s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.625s, learning 0.165s)
               Value function loss: 6.3633
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 386.71
               Mean episode length: 246.17
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 10.79s
                        Total time: 18621.91s
                               ETA: 1105900.9s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.647s, learning 0.182s)
               Value function loss: 6.1904
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 385.92
               Mean episode length: 246.16
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 10.83s
                        Total time: 18632.74s
                               ETA: 1105865.0s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.649s, learning 0.175s)
               Value function loss: 6.5587
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 383.06
               Mean episode length: 247.04
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 10.82s
                        Total time: 18643.56s
                               ETA: 1105828.8s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.502s, learning 0.163s)
               Value function loss: 7.7181
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 378.48
               Mean episode length: 247.59
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 10.67s
                        Total time: 18654.23s
                               ETA: 1105783.2s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.744s, learning 0.167s)
               Value function loss: 9.0423
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 362.45
               Mean episode length: 245.96
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 10.91s
                        Total time: 18665.14s
                               ETA: 1105752.2s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.628s, learning 0.163s)
               Value function loss: 6.8052
                    Surrogate loss: 0.0068
             Mean action noise std: 0.74
                       Mean reward: 355.73
               Mean episode length: 243.28
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 10.79s
                        Total time: 18675.93s
                               ETA: 1105714.1s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.618s, learning 0.168s)
               Value function loss: 5.5865
                    Surrogate loss: -0.0242
             Mean action noise std: 0.74
                       Mean reward: 347.57
               Mean episode length: 241.59
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 10.79s
                        Total time: 18686.72s
                               ETA: 1105675.8s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.655s, learning 0.262s)
               Value function loss: 6.7472
                    Surrogate loss: -0.0228
             Mean action noise std: 0.74
                       Mean reward: 343.24
               Mean episode length: 241.39
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 10.92s
                        Total time: 18697.64s
                               ETA: 1105645.2s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.747s, learning 0.191s)
               Value function loss: 7.3275
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 346.46
               Mean episode length: 243.38
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 10.94s
                        Total time: 18708.57s
                               ETA: 1105616.0s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.750s, learning 0.207s)
               Value function loss: 8.4481
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 350.44
               Mean episode length: 245.08
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 10.96s
                        Total time: 18719.53s
                               ETA: 1105587.9s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.902s, learning 0.164s)
               Value function loss: 8.0621
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 338.74
               Mean episode length: 243.76
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 11.07s
                        Total time: 18730.60s
                               ETA: 1105566.1s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.910s, learning 0.172s)
               Value function loss: 7.9609
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 316.01
               Mean episode length: 239.62
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 11.08s
                        Total time: 18741.68s
                               ETA: 1105545.4s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.734s, learning 0.184s)
               Value function loss: 8.8690
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 335.97
               Mean episode length: 240.57
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 10.92s
                        Total time: 18752.60s
                               ETA: 1105515.0s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.163s)
               Value function loss: 8.0713
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 330.45
               Mean episode length: 241.20
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 10.76s
                        Total time: 18763.36s
                               ETA: 1105475.3s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.680s, learning 0.168s)
               Value function loss: 9.1537
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 318.47
               Mean episode length: 241.03
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 10.85s
                        Total time: 18774.20s
                               ETA: 1105440.9s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.465s, learning 0.163s)
               Value function loss: 9.6468
                    Surrogate loss: -0.0243
             Mean action noise std: 0.74
                       Mean reward: 326.99
               Mean episode length: 243.90
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 10.63s
                        Total time: 18784.83s
                               ETA: 1105393.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.801s, learning 0.162s)
               Value function loss: 10.7284
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 349.16
               Mean episode length: 245.17
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 10.96s
                        Total time: 18795.80s
                               ETA: 1105365.9s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.489s, learning 0.168s)
               Value function loss: 9.1059
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 343.84
               Mean episode length: 246.15
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 10.66s
                        Total time: 18806.45s
                               ETA: 1105320.3s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.845s, learning 0.157s)
               Value function loss: 10.1716
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 317.53
               Mean episode length: 242.41
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 11.00s
                        Total time: 18817.45s
                               ETA: 1105295.0s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.745s, learning 0.160s)
               Value function loss: 11.1190
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 336.53
               Mean episode length: 245.56
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 10.90s
                        Total time: 18828.36s
                               ETA: 1105264.0s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.703s, learning 0.180s)
               Value function loss: 10.3723
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 334.33
               Mean episode length: 246.54
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 10.88s
                        Total time: 18839.24s
                               ETA: 1105231.8s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.432s, learning 0.157s)
               Value function loss: 9.9057
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 330.23
               Mean episode length: 244.17
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 10.59s
                        Total time: 18849.83s
                               ETA: 1105182.4s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.757s, learning 0.167s)
               Value function loss: 9.5983
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 331.51
               Mean episode length: 247.03
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 10.92s
                        Total time: 18860.76s
                               ETA: 1105152.6s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.537s, learning 0.160s)
               Value function loss: 9.2896
                    Surrogate loss: -0.0230
             Mean action noise std: 0.74
                       Mean reward: 334.96
               Mean episode length: 246.37
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 10.70s
                        Total time: 18871.45s
                               ETA: 1105109.6s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.492s, learning 0.223s)
               Value function loss: 8.9060
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 339.70
               Mean episode length: 248.27
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 10.71s
                        Total time: 18882.17s
                               ETA: 1105067.7s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.019s, learning 0.209s)
               Value function loss: 8.0335
                    Surrogate loss: -0.0225
             Mean action noise std: 0.74
                       Mean reward: 347.20
               Mean episode length: 248.57
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 11.23s
                        Total time: 18893.40s
                               ETA: 1105055.8s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.571s, learning 0.191s)
               Value function loss: 6.9980
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 342.38
               Mean episode length: 247.77
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 10.76s
                        Total time: 18904.16s
                               ETA: 1105016.6s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1606 steps/s (collection: 10.041s, learning 0.160s)
               Value function loss: 8.0899
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 340.08
               Mean episode length: 247.86
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 10.20s
                        Total time: 18914.36s
                               ETA: 1104944.7s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.672s, learning 0.192s)
               Value function loss: 8.8551
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 342.66
               Mean episode length: 247.48
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 10.86s
                        Total time: 18925.22s
                               ETA: 1104911.7s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.817s, learning 0.187s)
               Value function loss: 5.4959
                    Surrogate loss: -0.0254
             Mean action noise std: 0.74
                       Mean reward: 341.03
               Mean episode length: 247.97
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 11.00s
                        Total time: 18936.23s
                               ETA: 1104886.8s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.683s, learning 0.161s)
               Value function loss: 5.0550
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 333.96
               Mean episode length: 247.04
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 10.84s
                        Total time: 18947.07s
                               ETA: 1104852.6s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.783s, learning 0.164s)
               Value function loss: 3.9552
                    Surrogate loss: -0.0260
             Mean action noise std: 0.74
                       Mean reward: 338.06
               Mean episode length: 247.03
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 10.95s
                        Total time: 18958.02s
                               ETA: 1104824.4s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.611s, learning 0.191s)
               Value function loss: 4.9742
                    Surrogate loss: -0.0227
             Mean action noise std: 0.74
                       Mean reward: 352.63
               Mean episode length: 248.31
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 10.80s
                        Total time: 18968.82s
                               ETA: 1104787.8s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.170s)
               Value function loss: 4.7732
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 350.05
               Mean episode length: 248.46
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 10.76s
                        Total time: 18979.58s
                               ETA: 1104748.8s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.165s)
               Value function loss: 4.5747
                    Surrogate loss: -0.0258
             Mean action noise std: 0.74
                       Mean reward: 343.24
               Mean episode length: 247.20
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 10.80s
                        Total time: 18990.38s
                               ETA: 1104712.0s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.165s)
               Value function loss: 5.1870
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 338.69
               Mean episode length: 246.40
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 10.77s
                        Total time: 19001.15s
                               ETA: 1104673.5s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1571 steps/s (collection: 10.263s, learning 0.162s)
               Value function loss: 6.7645
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 348.41
               Mean episode length: 245.82
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 10.43s
                        Total time: 19011.57s
                               ETA: 1104615.2s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.621s, learning 0.176s)
               Value function loss: 4.5324
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 343.44
               Mean episode length: 245.04
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 10.80s
                        Total time: 19022.37s
                               ETA: 1104578.4s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.649s, learning 0.185s)
               Value function loss: 6.4266
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 340.08
               Mean episode length: 245.18
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 10.83s
                        Total time: 19033.20s
                               ETA: 1104543.8s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.489s, learning 0.177s)
               Value function loss: 6.5418
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 348.13
               Mean episode length: 245.06
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 10.67s
                        Total time: 19043.87s
                               ETA: 1104499.5s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.699s, learning 0.162s)
               Value function loss: 6.9642
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 350.14
               Mean episode length: 245.80
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 10.86s
                        Total time: 19054.73s
                               ETA: 1104466.5s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.166s)
               Value function loss: 8.8616
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 353.49
               Mean episode length: 245.37
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 10.71s
                        Total time: 19065.44s
                               ETA: 1104425.1s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.633s, learning 0.194s)
               Value function loss: 9.5641
                    Surrogate loss: -0.0258
             Mean action noise std: 0.74
                       Mean reward: 358.72
               Mean episode length: 245.41
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 10.83s
                        Total time: 19076.27s
                               ETA: 1104390.3s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.733s, learning 0.192s)
               Value function loss: 7.7894
                    Surrogate loss: -0.0261
             Mean action noise std: 0.74
                       Mean reward: 365.49
               Mean episode length: 247.17
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 10.93s
                        Total time: 19087.20s
                               ETA: 1104361.2s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.843s, learning 0.263s)
               Value function loss: 9.4364
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 364.01
               Mean episode length: 245.86
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 11.11s
                        Total time: 19098.30s
                               ETA: 1104342.5s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.625s, learning 0.187s)
               Value function loss: 9.2931
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 353.49
               Mean episode length: 244.89
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 10.81s
                        Total time: 19109.12s
                               ETA: 1104306.9s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.842s, learning 0.208s)
               Value function loss: 9.8790
                    Surrogate loss: -0.0228
             Mean action noise std: 0.74
                       Mean reward: 354.68
               Mean episode length: 244.81
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 11.05s
                        Total time: 19120.17s
                               ETA: 1104285.1s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.558s, learning 0.173s)
               Value function loss: 11.1946
                    Surrogate loss: -0.0214
             Mean action noise std: 0.74
                       Mean reward: 345.47
               Mean episode length: 245.06
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 10.73s
                        Total time: 19130.90s
                               ETA: 1104244.8s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.549s, learning 0.267s)
               Value function loss: 10.5971
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 378.20
               Mean episode length: 248.01
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 10.82s
                        Total time: 19141.71s
                               ETA: 1104209.5s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.522s, learning 0.162s)
               Value function loss: 12.7773
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 354.87
               Mean episode length: 245.94
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 10.68s
                        Total time: 19152.40s
                               ETA: 1104166.5s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.507s, learning 0.166s)
               Value function loss: 10.5961
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 368.49
               Mean episode length: 247.98
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 10.67s
                        Total time: 19163.07s
                               ETA: 1104123.0s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.468s, learning 0.170s)
               Value function loss: 12.7566
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 368.17
               Mean episode length: 247.10
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 10.64s
                        Total time: 19173.71s
                               ETA: 1104077.5s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.732s, learning 0.161s)
               Value function loss: 11.7035
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 368.01
               Mean episode length: 246.65
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 10.89s
                        Total time: 19184.60s
                               ETA: 1104046.8s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.849s, learning 0.176s)
               Value function loss: 10.3090
                    Surrogate loss: -0.0257
             Mean action noise std: 0.74
                       Mean reward: 358.68
               Mean episode length: 246.55
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 11.03s
                        Total time: 19195.63s
                               ETA: 1104023.7s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.727s, learning 0.181s)
               Value function loss: 9.1893
                    Surrogate loss: -0.0267
             Mean action noise std: 0.74
                       Mean reward: 373.55
               Mean episode length: 248.33
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 10.91s
                        Total time: 19206.53s
                               ETA: 1103993.8s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.573s, learning 0.229s)
               Value function loss: 9.5355
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 371.73
               Mean episode length: 248.66
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 10.80s
                        Total time: 19217.34s
                               ETA: 1103957.9s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.162s)
               Value function loss: 9.6873
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 373.92
               Mean episode length: 247.92
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 11.05s
                        Total time: 19228.38s
                               ETA: 1103936.0s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.161s)
               Value function loss: 7.8541
                    Surrogate loss: -0.0252
             Mean action noise std: 0.74
                       Mean reward: 370.80
               Mean episode length: 248.69
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 10.85s
                        Total time: 19239.24s
                               ETA: 1103903.1s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.761s, learning 0.157s)
               Value function loss: 9.5028
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 393.21
               Mean episode length: 249.18
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 10.92s
                        Total time: 19250.15s
                               ETA: 1103873.9s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.550s, learning 0.162s)
               Value function loss: 9.9558
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 394.12
               Mean episode length: 249.79
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 10.71s
                        Total time: 19260.87s
                               ETA: 1103832.9s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.167s)
               Value function loss: 8.0585
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 394.24
               Mean episode length: 247.20
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 10.70s
                        Total time: 19271.56s
                               ETA: 1103791.0s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.160s)
               Value function loss: 6.1861
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 387.14
               Mean episode length: 247.29
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 10.85s
                        Total time: 19282.41s
                               ETA: 1103757.7s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.681s, learning 0.157s)
               Value function loss: 5.4343
                    Surrogate loss: -0.0262
             Mean action noise std: 0.74
                       Mean reward: 389.24
               Mean episode length: 248.16
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 10.84s
                        Total time: 19293.24s
                               ETA: 1103724.0s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.720s, learning 0.196s)
               Value function loss: 4.7703
                    Surrogate loss: -0.0243
             Mean action noise std: 0.74
                       Mean reward: 384.04
               Mean episode length: 247.63
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 10.92s
                        Total time: 19304.16s
                               ETA: 1103694.8s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.663s, learning 0.180s)
               Value function loss: 4.3982
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: 381.60
               Mean episode length: 247.56
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 10.84s
                        Total time: 19315.00s
                               ETA: 1103661.5s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.508s, learning 0.240s)
               Value function loss: 4.8317
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 369.68
               Mean episode length: 247.37
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 10.75s
                        Total time: 19325.75s
                               ETA: 1103622.8s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.646s, learning 0.189s)
               Value function loss: 5.9894
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 373.44
               Mean episode length: 247.87
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 10.83s
                        Total time: 19336.59s
                               ETA: 1103589.1s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.392s, learning 0.170s)
               Value function loss: 6.1768
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 378.20
               Mean episode length: 246.64
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 10.56s
                        Total time: 19347.15s
                               ETA: 1103539.8s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.168s)
               Value function loss: 5.0820
                    Surrogate loss: -0.0199
             Mean action noise std: 0.74
                       Mean reward: 384.47
               Mean episode length: 247.21
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 11.08s
                        Total time: 19358.23s
                               ETA: 1103520.1s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.474s, learning 0.169s)
               Value function loss: 5.2633
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 382.12
               Mean episode length: 246.73
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 10.64s
                        Total time: 19368.87s
                               ETA: 1103475.5s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.075s, learning 0.178s)
               Value function loss: 5.6661
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 373.35
               Mean episode length: 246.91
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 11.25s
                        Total time: 19380.12s
                               ETA: 1103465.7s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.666s, learning 0.207s)
               Value function loss: 8.1589
                    Surrogate loss: -0.0242
             Mean action noise std: 0.74
                       Mean reward: 368.93
               Mean episode length: 244.23
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 10.87s
                        Total time: 19391.00s
                               ETA: 1103434.2s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.707s, learning 0.174s)
               Value function loss: 8.4918
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 368.00
               Mean episode length: 243.66
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 10.88s
                        Total time: 19401.88s
                               ETA: 1103403.3s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.436s, learning 0.168s)
               Value function loss: 9.9314
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 369.17
               Mean episode length: 245.24
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 10.60s
                        Total time: 19412.48s
                               ETA: 1103356.6s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.918s, learning 0.208s)
               Value function loss: 9.2587
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 375.89
               Mean episode length: 246.33
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 11.13s
                        Total time: 19423.61s
                               ETA: 1103339.6s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.677s, learning 0.172s)
               Value function loss: 11.0319
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 383.96
               Mean episode length: 246.14
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 10.85s
                        Total time: 19434.46s
                               ETA: 1103306.8s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.378s, learning 0.159s)
               Value function loss: 11.3117
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 388.80
               Mean episode length: 246.79
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 10.54s
                        Total time: 19444.99s
                               ETA: 1103256.4s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.694s, learning 0.159s)
               Value function loss: 9.1185
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 369.66
               Mean episode length: 243.80
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 10.85s
                        Total time: 19455.85s
                               ETA: 1103224.0s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.560s, learning 0.191s)
               Value function loss: 9.5655
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 379.57
               Mean episode length: 245.59
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 10.75s
                        Total time: 19466.60s
                               ETA: 1103185.9s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.656s, learning 0.184s)
               Value function loss: 8.2899
                    Surrogate loss: -0.0235
             Mean action noise std: 0.74
                       Mean reward: 383.04
               Mean episode length: 245.75
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 10.84s
                        Total time: 19477.44s
                               ETA: 1103152.7s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.714s, learning 0.156s)
               Value function loss: 11.1618
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 384.97
               Mean episode length: 245.69
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 10.87s
                        Total time: 19488.31s
                               ETA: 1103121.4s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.689s, learning 0.160s)
               Value function loss: 10.3835
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 388.05
               Mean episode length: 245.47
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 10.85s
                        Total time: 19499.16s
                               ETA: 1103088.8s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.700s, learning 0.190s)
               Value function loss: 12.1363
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 395.49
               Mean episode length: 246.83
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 10.89s
                        Total time: 19510.05s
                               ETA: 1103058.6s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.589s, learning 0.196s)
               Value function loss: 11.4925
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 400.59
               Mean episode length: 248.18
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 10.78s
                        Total time: 19520.83s
                               ETA: 1103022.5s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.003s, learning 0.260s)
               Value function loss: 9.9343
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 395.22
               Mean episode length: 248.17
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 11.26s
                        Total time: 19532.10s
                               ETA: 1103013.4s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.823s, learning 0.211s)
               Value function loss: 10.5393
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 391.91
               Mean episode length: 246.54
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 11.03s
                        Total time: 19543.13s
                               ETA: 1102991.4s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.741s, learning 0.185s)
               Value function loss: 10.0721
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: 390.14
               Mean episode length: 246.93
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 10.93s
                        Total time: 19554.06s
                               ETA: 1102963.3s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.952s, learning 0.213s)
               Value function loss: 12.1087
                    Surrogate loss: 0.0079
             Mean action noise std: 0.74
                       Mean reward: 396.82
               Mean episode length: 249.96
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 11.16s
                        Total time: 19565.22s
                               ETA: 1102948.6s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.907s, learning 0.169s)
               Value function loss: 9.3878
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 395.12
               Mean episode length: 248.08
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 11.08s
                        Total time: 19576.30s
                               ETA: 1102929.0s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.818s, learning 0.186s)
               Value function loss: 9.2922
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 392.09
               Mean episode length: 245.91
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 11.00s
                        Total time: 19587.30s
                               ETA: 1102905.3s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.479s, learning 0.170s)
               Value function loss: 9.7003
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 392.54
               Mean episode length: 245.15
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 10.65s
                        Total time: 19597.95s
                               ETA: 1102861.6s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.986s, learning 0.186s)
               Value function loss: 9.2108
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 394.48
               Mean episode length: 247.57
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 11.17s
                        Total time: 19609.12s
                               ETA: 1102847.4s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.358s, learning 0.161s)
               Value function loss: 7.0822
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 381.94
               Mean episode length: 241.43
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 10.52s
                        Total time: 19619.64s
                               ETA: 1102796.5s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.774s, learning 0.164s)
               Value function loss: 6.0125
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 358.06
               Mean episode length: 236.22
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 10.94s
                        Total time: 19630.58s
                               ETA: 1102769.3s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.488s, learning 0.164s)
               Value function loss: 5.5273
                    Surrogate loss: -0.0223
             Mean action noise std: 0.74
                       Mean reward: 350.34
               Mean episode length: 233.68
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 10.65s
                        Total time: 19641.23s
                               ETA: 1102726.0s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.598s, learning 0.169s)
               Value function loss: 5.7954
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 328.57
               Mean episode length: 228.04
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 10.77s
                        Total time: 19652.00s
                               ETA: 1102689.1s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.644s, learning 0.170s)
               Value function loss: 5.7384
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 320.83
               Mean episode length: 225.42
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 10.81s
                        Total time: 19662.81s
                               ETA: 1102655.0s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.643s, learning 0.160s)
               Value function loss: 7.9343
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 333.62
               Mean episode length: 229.37
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 10.80s
                        Total time: 19673.61s
                               ETA: 1102620.3s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.872s, learning 0.286s)
               Value function loss: 9.0478
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 332.78
               Mean episode length: 228.24
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 11.16s
                        Total time: 19684.77s
                               ETA: 1102605.4s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.717s, learning 0.189s)
               Value function loss: 6.7491
                    Surrogate loss: 0.0153
             Mean action noise std: 0.74
                       Mean reward: 341.32
               Mean episode length: 230.27
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 10.91s
                        Total time: 19695.68s
                               ETA: 1102576.4s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.731s, learning 0.187s)
               Value function loss: 5.6506
                    Surrogate loss: -0.0217
             Mean action noise std: 0.74
                       Mean reward: 336.92
               Mean episode length: 230.76
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 10.92s
                        Total time: 19706.60s
                               ETA: 1102548.2s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.412s, learning 0.162s)
               Value function loss: 5.5562
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 325.36
               Mean episode length: 230.32
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 10.57s
                        Total time: 19717.17s
                               ETA: 1102500.7s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.817s, learning 0.171s)
               Value function loss: 6.9131
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 343.91
               Mean episode length: 235.74
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 10.99s
                        Total time: 19728.16s
                               ETA: 1102476.4s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.558s, learning 0.168s)
               Value function loss: 7.7691
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 361.14
               Mean episode length: 238.84
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 10.73s
                        Total time: 19738.89s
                               ETA: 1102437.5s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.886s, learning 0.290s)
               Value function loss: 7.3482
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 356.45
               Mean episode length: 237.48
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 11.18s
                        Total time: 19750.06s
                               ETA: 1102423.7s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.656s, learning 0.184s)
               Value function loss: 7.7975
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 356.81
               Mean episode length: 239.96
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 10.84s
                        Total time: 19760.90s
                               ETA: 1102391.2s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.358s, learning 0.180s)
               Value function loss: 7.9175
                    Surrogate loss: -0.0214
             Mean action noise std: 0.74
                       Mean reward: 359.39
               Mean episode length: 237.07
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 10.54s
                        Total time: 19771.44s
                               ETA: 1102341.9s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.871s, learning 0.180s)
               Value function loss: 8.8185
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 369.77
               Mean episode length: 240.25
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 11.05s
                        Total time: 19782.49s
                               ETA: 1102321.2s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.452s, learning 0.159s)
               Value function loss: 9.6377
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 382.15
               Mean episode length: 243.38
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 10.61s
                        Total time: 19793.10s
                               ETA: 1102276.0s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.786s, learning 0.181s)
               Value function loss: 10.0896
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 371.99
               Mean episode length: 241.34
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 10.97s
                        Total time: 19804.07s
                               ETA: 1102250.7s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.739s, learning 0.169s)
               Value function loss: 9.9505
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 373.53
               Mean episode length: 242.62
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 10.91s
                        Total time: 19814.98s
                               ETA: 1102222.1s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.535s, learning 0.158s)
               Value function loss: 10.5251
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 384.16
               Mean episode length: 243.59
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 10.69s
                        Total time: 19825.67s
                               ETA: 1102181.5s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.522s, learning 0.187s)
               Value function loss: 9.6895
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 392.90
               Mean episode length: 242.30
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 10.71s
                        Total time: 19836.38s
                               ETA: 1102141.9s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.854s, learning 0.159s)
               Value function loss: 8.5207
                    Surrogate loss: -0.0248
             Mean action noise std: 0.74
                       Mean reward: 391.43
               Mean episode length: 244.26
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 11.01s
                        Total time: 19847.39s
                               ETA: 1102119.3s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.726s, learning 0.189s)
               Value function loss: 9.7142
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 402.83
               Mean episode length: 248.64
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 10.91s
                        Total time: 19858.31s
                               ETA: 1102091.1s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.643s, learning 0.171s)
               Value function loss: 8.0829
                    Surrogate loss: -0.0218
             Mean action noise std: 0.74
                       Mean reward: 385.92
               Mean episode length: 243.58
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 10.81s
                        Total time: 19869.12s
                               ETA: 1102057.4s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.736s, learning 0.161s)
               Value function loss: 9.6584
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 384.48
               Mean episode length: 246.89
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 10.90s
                        Total time: 19880.02s
                               ETA: 1102028.3s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.475s, learning 0.159s)
               Value function loss: 7.7524
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 378.59
               Mean episode length: 243.56
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 10.63s
                        Total time: 19890.65s
                               ETA: 1101984.7s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.478s, learning 0.199s)
               Value function loss: 9.0352
                    Surrogate loss: -0.0228
             Mean action noise std: 0.74
                       Mean reward: 379.38
               Mean episode length: 241.94
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 10.68s
                        Total time: 19901.33s
                               ETA: 1101943.5s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.166s)
               Value function loss: 6.7381
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 388.03
               Mean episode length: 246.37
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 10.89s
                        Total time: 19912.22s
                               ETA: 1101914.1s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.805s, learning 0.160s)
               Value function loss: 7.6753
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 380.53
               Mean episode length: 246.06
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 10.96s
                        Total time: 19923.18s
                               ETA: 1101888.9s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.783s, learning 0.165s)
               Value function loss: 6.5320
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 388.00
               Mean episode length: 248.63
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 10.95s
                        Total time: 19934.13s
                               ETA: 1101862.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.775s, learning 0.162s)
               Value function loss: 6.7944
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 377.51
               Mean episode length: 244.15
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 10.94s
                        Total time: 19945.07s
                               ETA: 1101836.0s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.755s, learning 0.191s)
               Value function loss: 5.9952
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 387.22
               Mean episode length: 245.86
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 10.95s
                        Total time: 19956.01s
                               ETA: 1101809.8s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.658s, learning 0.235s)
               Value function loss: 5.4555
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 392.10
               Mean episode length: 246.28
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 10.89s
                        Total time: 19966.91s
                               ETA: 1101780.7s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.775s, learning 0.170s)
               Value function loss: 4.6570
                    Surrogate loss: -0.0243
             Mean action noise std: 0.74
                       Mean reward: 380.95
               Mean episode length: 244.26
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 10.94s
                        Total time: 19977.85s
                               ETA: 1101754.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.477s, learning 0.166s)
               Value function loss: 5.0423
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 387.48
               Mean episode length: 245.56
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 10.64s
                        Total time: 19988.50s
                               ETA: 1101711.6s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.753s, learning 0.165s)
               Value function loss: 5.8323
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 386.69
               Mean episode length: 245.35
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 10.92s
                        Total time: 19999.41s
                               ETA: 1101683.9s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.487s, learning 0.165s)
               Value function loss: 6.0678
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 378.43
               Mean episode length: 244.03
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 10.65s
                        Total time: 20010.07s
                               ETA: 1101641.6s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1572 steps/s (collection: 10.230s, learning 0.191s)
               Value function loss: 5.8850
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 379.37
               Mean episode length: 243.91
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 10.42s
                        Total time: 20020.49s
                               ETA: 1101586.7s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.913s, learning 0.161s)
               Value function loss: 5.3024
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 379.80
               Mean episode length: 243.65
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 11.07s
                        Total time: 20031.56s
                               ETA: 1101567.6s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.760s, learning 0.170s)
               Value function loss: 5.9502
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 379.65
               Mean episode length: 242.59
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 10.93s
                        Total time: 20042.49s
                               ETA: 1101540.7s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.515s, learning 0.179s)
               Value function loss: 5.8307
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 379.23
               Mean episode length: 241.68
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 10.69s
                        Total time: 20053.19s
                               ETA: 1101500.8s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.591s, learning 0.193s)
               Value function loss: 6.4628
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 379.31
               Mean episode length: 241.80
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 10.78s
                        Total time: 20063.97s
                               ETA: 1101466.0s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.343s, learning 0.170s)
               Value function loss: 7.8185
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 391.15
               Mean episode length: 244.61
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 10.51s
                        Total time: 20074.48s
                               ETA: 1101416.3s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.408s, learning 0.157s)
               Value function loss: 8.2641
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 399.11
               Mean episode length: 246.99
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 10.57s
                        Total time: 20085.05s
                               ETA: 1101369.5s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.422s, learning 0.166s)
               Value function loss: 7.1889
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 400.77
               Mean episode length: 247.23
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 10.59s
                        Total time: 20095.64s
                               ETA: 1101323.9s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.661s, learning 0.164s)
               Value function loss: 8.3466
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 398.14
               Mean episode length: 246.80
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 10.82s
                        Total time: 20106.46s
                               ETA: 1101291.4s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.398s, learning 0.162s)
               Value function loss: 10.2817
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 400.52
               Mean episode length: 247.24
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 10.56s
                        Total time: 20117.02s
                               ETA: 1101244.4s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.766s, learning 0.161s)
               Value function loss: 11.3200
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 400.99
               Mean episode length: 246.64
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 10.93s
                        Total time: 20127.95s
                               ETA: 1101217.5s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.572s, learning 0.167s)
               Value function loss: 10.7623
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 402.08
               Mean episode length: 247.16
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 10.74s
                        Total time: 20138.69s
                               ETA: 1101180.3s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.165s)
               Value function loss: 10.1181
                    Surrogate loss: -0.0026
             Mean action noise std: 0.74
                       Mean reward: 389.16
               Mean episode length: 244.20
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 10.88s
                        Total time: 20149.56s
                               ETA: 1101150.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.171s)
               Value function loss: 8.3397
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 403.13
               Mean episode length: 249.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 10.76s
                        Total time: 20160.33s
                               ETA: 1101115.0s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.520s, learning 0.166s)
               Value function loss: 12.3071
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 404.26
               Mean episode length: 248.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 10.69s
                        Total time: 20171.01s
                               ETA: 1101075.0s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.379s, learning 0.164s)
               Value function loss: 9.3397
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 393.78
               Mean episode length: 247.30
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 10.54s
                        Total time: 20181.56s
                               ETA: 1101027.2s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.742s, learning 0.257s)
               Value function loss: 9.5623
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 392.71
               Mean episode length: 247.22
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 11.00s
                        Total time: 20192.55s
                               ETA: 1101004.4s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.884s, learning 0.166s)
               Value function loss: 8.7159
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 392.17
               Mean episode length: 248.69
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 11.05s
                        Total time: 20203.60s
                               ETA: 1100984.3s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.636s, learning 0.183s)
               Value function loss: 11.0869
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 401.52
               Mean episode length: 249.19
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 10.82s
                        Total time: 20214.42s
                               ETA: 1100951.7s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.689s, learning 0.166s)
               Value function loss: 7.8940
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 394.82
               Mean episode length: 247.77
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 10.85s
                        Total time: 20225.28s
                               ETA: 1100921.1s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.523s, learning 0.181s)
               Value function loss: 8.5076
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 398.73
               Mean episode length: 248.06
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 10.70s
                        Total time: 20235.98s
                               ETA: 1100882.3s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.446s, learning 0.163s)
               Value function loss: 7.7796
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 405.45
               Mean episode length: 248.86
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 10.61s
                        Total time: 20246.59s
                               ETA: 1100838.3s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.564s, learning 0.171s)
               Value function loss: 9.2533
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 399.10
               Mean episode length: 248.29
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 10.74s
                        Total time: 20257.33s
                               ETA: 1100801.3s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.444s, learning 0.182s)
               Value function loss: 7.2696
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 407.30
               Mean episode length: 248.61
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 10.63s
                        Total time: 20267.95s
                               ETA: 1100758.4s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.755s, learning 0.165s)
               Value function loss: 9.0442
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 405.95
               Mean episode length: 248.73
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 10.92s
                        Total time: 20278.87s
                               ETA: 1100731.4s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.617s, learning 0.167s)
               Value function loss: 7.2609
                    Surrogate loss: -0.0204
             Mean action noise std: 0.74
                       Mean reward: 406.00
               Mean episode length: 248.26
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 10.78s
                        Total time: 20289.66s
                               ETA: 1100697.1s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.657s, learning 0.165s)
               Value function loss: 5.5038
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 406.23
               Mean episode length: 248.26
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 10.82s
                        Total time: 20300.48s
                               ETA: 1100664.9s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.560s, learning 0.165s)
               Value function loss: 4.6600
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 398.73
               Mean episode length: 245.71
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 10.72s
                        Total time: 20311.20s
                               ETA: 1100627.4s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.437s, learning 0.160s)
               Value function loss: 6.6245
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 395.35
               Mean episode length: 244.40
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 10.60s
                        Total time: 20321.80s
                               ETA: 1100583.0s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.908s, learning 0.164s)
               Value function loss: 5.7733
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 399.01
               Mean episode length: 246.15
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 11.07s
                        Total time: 20332.87s
                               ETA: 1100564.4s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.163s)
               Value function loss: 5.9982
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 399.52
               Mean episode length: 246.81
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 10.96s
                        Total time: 20343.83s
                               ETA: 1100539.8s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.871s, learning 0.163s)
               Value function loss: 6.4910
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 404.08
               Mean episode length: 249.16
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 11.03s
                        Total time: 20354.87s
                               ETA: 1100519.1s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.875s, learning 0.162s)
               Value function loss: 4.9678
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 409.39
               Mean episode length: 249.04
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 11.04s
                        Total time: 20365.90s
                               ETA: 1100498.6s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.171s)
               Value function loss: 5.8944
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 397.99
               Mean episode length: 245.68
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 10.78s
                        Total time: 20376.68s
                               ETA: 1100464.0s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.712s, learning 0.190s)
               Value function loss: 7.7624
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 387.30
               Mean episode length: 243.86
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 10.90s
                        Total time: 20387.58s
                               ETA: 1100436.2s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.702s, learning 0.167s)
               Value function loss: 7.2728
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 387.40
               Mean episode length: 243.63
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 10.87s
                        Total time: 20398.45s
                               ETA: 1100406.7s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.676s, learning 0.174s)
               Value function loss: 8.7869
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 402.26
               Mean episode length: 246.98
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 10.85s
                        Total time: 20409.30s
                               ETA: 1100376.2s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.436s, learning 0.177s)
               Value function loss: 7.6480
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 409.90
               Mean episode length: 249.33
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 10.61s
                        Total time: 20419.91s
                               ETA: 1100333.0s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.757s, learning 0.162s)
               Value function loss: 10.1717
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 402.68
               Mean episode length: 248.68
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 10.92s
                        Total time: 20430.83s
                               ETA: 1100306.2s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.527s, learning 0.185s)
               Value function loss: 8.4241
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 399.02
               Mean episode length: 247.80
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 10.71s
                        Total time: 20441.54s
                               ETA: 1100268.4s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.567s, learning 0.167s)
               Value function loss: 9.8328
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 400.02
               Mean episode length: 248.23
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 10.73s
                        Total time: 20452.28s
                               ETA: 1100231.8s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.558s, learning 0.165s)
               Value function loss: 11.4489
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 406.43
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 10.72s
                        Total time: 20463.00s
                               ETA: 1100194.5s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.938s, learning 0.197s)
               Value function loss: 11.6164
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 409.02
               Mean episode length: 249.94
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 11.14s
                        Total time: 20474.14s
                               ETA: 1100179.5s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.788s, learning 0.177s)
               Value function loss: 14.1009
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 411.42
               Mean episode length: 249.25
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 10.96s
                        Total time: 20485.10s
                               ETA: 1100155.3s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.157s)
               Value function loss: 14.0733
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 404.17
               Mean episode length: 248.77
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 11.04s
                        Total time: 20496.14s
                               ETA: 1100135.2s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1576 steps/s (collection: 10.219s, learning 0.173s)
               Value function loss: 17.5162
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 410.60
               Mean episode length: 249.07
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 10.39s
                        Total time: 20506.54s
                               ETA: 1100080.4s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.881s, learning 0.193s)
               Value function loss: 22.3153
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 408.32
               Mean episode length: 249.85
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 11.07s
                        Total time: 20517.61s
                               ETA: 1100062.1s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.762s, learning 0.190s)
               Value function loss: 19.3014
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 418.26
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 10.95s
                        Total time: 20528.56s
                               ETA: 1100037.3s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.582s, learning 0.166s)
               Value function loss: 18.1244
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 417.64
               Mean episode length: 249.66
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 10.75s
                        Total time: 20539.31s
                               ETA: 1100001.6s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.497s, learning 0.190s)
               Value function loss: 23.1563
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 410.65
               Mean episode length: 249.23
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 10.69s
                        Total time: 20550.00s
                               ETA: 1099962.6s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.007s, learning 0.191s)
               Value function loss: 15.1476
                    Surrogate loss: 0.0065
             Mean action noise std: 0.74
                       Mean reward: 407.20
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 11.20s
                        Total time: 20561.19s
                               ETA: 1099951.1s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.522s, learning 0.196s)
               Value function loss: 16.1369
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 413.27
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 10.72s
                        Total time: 20571.91s
                               ETA: 1099913.8s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.321s, learning 0.162s)
               Value function loss: 15.2605
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 414.75
               Mean episode length: 249.82
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 10.48s
                        Total time: 20582.40s
                               ETA: 1099864.1s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.666s, learning 0.179s)
               Value function loss: 16.0592
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 415.52
               Mean episode length: 248.78
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 10.84s
                        Total time: 20593.24s
                               ETA: 1099833.7s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.430s, learning 0.185s)
               Value function loss: 16.3832
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 416.06
               Mean episode length: 249.86
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 10.62s
                        Total time: 20603.86s
                               ETA: 1099791.0s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.849s, learning 0.166s)
               Value function loss: 15.9625
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 414.37
               Mean episode length: 249.86
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.01s
                        Total time: 20614.87s
                               ETA: 1099769.7s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.525s, learning 0.173s)
               Value function loss: 14.9214
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 414.75
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 10.70s
                        Total time: 20625.57s
                               ETA: 1099731.6s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.486s, learning 0.160s)
               Value function loss: 14.0651
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 415.67
               Mean episode length: 249.01
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 10.65s
                        Total time: 20636.21s
                               ETA: 1099690.7s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.805s, learning 0.161s)
               Value function loss: 10.0679
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 415.27
               Mean episode length: 249.62
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 10.97s
                        Total time: 20647.18s
                               ETA: 1099666.9s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.866s, learning 0.164s)
               Value function loss: 10.6311
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 414.42
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 11.03s
                        Total time: 20658.21s
                               ETA: 1099646.4s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.739s, learning 0.185s)
               Value function loss: 9.8654
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 408.49
               Mean episode length: 247.27
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 10.92s
                        Total time: 20669.13s
                               ETA: 1099620.4s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.809s, learning 0.176s)
               Value function loss: 11.6321
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 404.95
               Mean episode length: 246.55
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 10.98s
                        Total time: 20680.12s
                               ETA: 1099597.6s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.896s, learning 0.169s)
               Value function loss: 12.0438
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 409.39
               Mean episode length: 247.79
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 11.07s
                        Total time: 20691.18s
                               ETA: 1099579.1s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.922s, learning 0.166s)
               Value function loss: 12.2297
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 413.80
               Mean episode length: 248.80
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 11.09s
                        Total time: 20702.27s
                               ETA: 1099561.8s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.645s, learning 0.164s)
               Value function loss: 12.8824
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 416.25
               Mean episode length: 248.53
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 10.81s
                        Total time: 20713.08s
                               ETA: 1099529.7s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.810s, learning 0.163s)
               Value function loss: 13.0856
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 413.23
               Mean episode length: 248.53
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 10.97s
                        Total time: 20724.05s
                               ETA: 1099506.3s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.448s, learning 0.167s)
               Value function loss: 11.4501
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 414.31
               Mean episode length: 248.38
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 10.61s
                        Total time: 20734.67s
                               ETA: 1099464.0s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.913s, learning 0.189s)
               Value function loss: 12.8824
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 414.57
               Mean episode length: 248.38
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 11.10s
                        Total time: 20745.77s
                               ETA: 1099447.4s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.748s, learning 0.168s)
               Value function loss: 15.1600
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 419.26
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 10.92s
                        Total time: 20756.69s
                               ETA: 1099421.1s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.712s, learning 0.160s)
               Value function loss: 15.6070
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 419.51
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 10.87s
                        Total time: 20767.56s
                               ETA: 1099392.4s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1571 steps/s (collection: 10.239s, learning 0.186s)
               Value function loss: 15.0309
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 416.78
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 10.42s
                        Total time: 20777.98s
                               ETA: 1099340.1s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.671s, learning 0.208s)
               Value function loss: 16.8499
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 421.70
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 10.88s
                        Total time: 20788.86s
                               ETA: 1099311.9s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.569s, learning 0.173s)
               Value function loss: 21.3192
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 419.46
               Mean episode length: 249.48
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 10.74s
                        Total time: 20799.60s
                               ETA: 1099276.4s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.696s, learning 0.161s)
               Value function loss: 16.7601
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 424.48
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 10.86s
                        Total time: 20810.46s
                               ETA: 1099247.1s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.663s, learning 0.168s)
               Value function loss: 24.4352
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 418.49
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 10.83s
                        Total time: 20821.29s
                               ETA: 1099216.4s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.689s, learning 0.225s)
               Value function loss: 16.8139
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 420.95
               Mean episode length: 249.67
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 10.91s
                        Total time: 20832.21s
                               ETA: 1099190.0s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.638s, learning 0.193s)
               Value function loss: 25.0597
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 423.53
               Mean episode length: 249.58
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 10.83s
                        Total time: 20843.04s
                               ETA: 1099159.4s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.593s, learning 0.161s)
               Value function loss: 19.1770
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 415.52
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 10.75s
                        Total time: 20853.79s
                               ETA: 1099124.6s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.679s, learning 0.163s)
               Value function loss: 18.2943
                    Surrogate loss: -0.0212
             Mean action noise std: 0.74
                       Mean reward: 418.87
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 10.84s
                        Total time: 20864.63s
                               ETA: 1099094.6s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 779 steps/s (collection: 20.836s, learning 0.171s)
               Value function loss: 19.5119
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 424.39
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 21.01s
                        Total time: 20885.64s
                               ETA: 1099599.7s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 786 steps/s (collection: 20.649s, learning 0.191s)
               Value function loss: 18.3017
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 424.33
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 20.84s
                        Total time: 20906.48s
                               ETA: 1100095.6s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 780 steps/s (collection: 20.811s, learning 0.179s)
               Value function loss: 16.0439
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 421.12
               Mean episode length: 249.35
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 20.99s
                        Total time: 20927.47s
                               ETA: 1100598.7s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 785 steps/s (collection: 20.660s, learning 0.193s)
               Value function loss: 15.3151
                    Surrogate loss: -0.0234
             Mean action noise std: 0.74
                       Mean reward: 418.92
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 20.85s
                        Total time: 20948.32s
                               ETA: 1101094.1s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.421s, learning 0.169s)
               Value function loss: 15.8380
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 416.20
               Mean episode length: 248.18
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 20.59s
                        Total time: 20968.91s
                               ETA: 1101575.1s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 785 steps/s (collection: 20.684s, learning 0.168s)
               Value function loss: 13.8620
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 413.43
               Mean episode length: 248.18
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 20.85s
                        Total time: 20989.76s
                               ETA: 1102069.3s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 789 steps/s (collection: 20.592s, learning 0.168s)
               Value function loss: 12.9700
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 423.03
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 20.76s
                        Total time: 21010.52s
                               ETA: 1102558.1s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 793 steps/s (collection: 20.487s, learning 0.173s)
               Value function loss: 13.3991
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 424.50
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 20.66s
                        Total time: 21031.18s
                               ETA: 1103041.1s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.432s, learning 0.174s)
               Value function loss: 12.5968
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 423.24
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 20.61s
                        Total time: 21051.79s
                               ETA: 1103520.9s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 789 steps/s (collection: 20.601s, learning 0.158s)
               Value function loss: 11.7188
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 413.63
               Mean episode length: 249.31
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 20.76s
                        Total time: 21072.55s
                               ETA: 1104008.1s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 762 steps/s (collection: 21.204s, learning 0.286s)
               Value function loss: 10.3504
                    Surrogate loss: -0.0224
             Mean action noise std: 0.74
                       Mean reward: 418.66
               Mean episode length: 248.99
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 21.49s
                        Total time: 21094.04s
                               ETA: 1104532.9s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.765s, learning 0.161s)
               Value function loss: 8.1018
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 422.37
               Mean episode length: 248.90
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 20.93s
                        Total time: 21114.96s
                               ETA: 1105027.7s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 787 steps/s (collection: 20.641s, learning 0.169s)
               Value function loss: 8.4562
                    Surrogate loss: -0.0230
             Mean action noise std: 0.74
                       Mean reward: 418.25
               Mean episode length: 248.90
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 20.81s
                        Total time: 21135.77s
                               ETA: 1105515.9s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 789 steps/s (collection: 20.580s, learning 0.175s)
               Value function loss: 9.9912
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 423.53
               Mean episode length: 249.54
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 20.76s
                        Total time: 21156.53s
                               ETA: 1106000.7s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 777 steps/s (collection: 20.894s, learning 0.189s)
               Value function loss: 8.0717
                    Surrogate loss: -0.0229
             Mean action noise std: 0.74
                       Mean reward: 425.63
               Mean episode length: 249.20
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 21.08s
                        Total time: 21177.61s
                               ETA: 1106502.1s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 786 steps/s (collection: 20.641s, learning 0.178s)
               Value function loss: 8.6517
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 425.54
               Mean episode length: 249.66
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 20.82s
                        Total time: 21198.43s
                               ETA: 1106989.1s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.732s, learning 0.175s)
               Value function loss: 9.0507
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 424.20
               Mean episode length: 248.93
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 20.91s
                        Total time: 21219.34s
                               ETA: 1107480.2s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.925s, learning 0.166s)
               Value function loss: 8.3233
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 415.72
               Mean episode length: 246.68
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 21.09s
                        Total time: 21240.43s
                               ETA: 1107980.3s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.746s, learning 0.176s)
               Value function loss: 7.7746
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 411.86
               Mean episode length: 246.91
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 20.92s
                        Total time: 21261.35s
                               ETA: 1108471.1s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 774 steps/s (collection: 20.988s, learning 0.164s)
               Value function loss: 10.3365
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 412.91
               Mean episode length: 249.22
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 21.15s
                        Total time: 21282.50s
                               ETA: 1108973.3s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 810 steps/s (collection: 20.045s, learning 0.178s)
               Value function loss: 10.3028
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 409.21
               Mean episode length: 247.76
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 20.22s
                        Total time: 21302.73s
                               ETA: 1109426.5s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 782 steps/s (collection: 20.700s, learning 0.227s)
               Value function loss: 11.3058
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 410.52
               Mean episode length: 247.97
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 20.93s
                        Total time: 21323.65s
                               ETA: 1109915.9s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 799 steps/s (collection: 20.341s, learning 0.162s)
               Value function loss: 9.7209
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 408.47
               Mean episode length: 248.37
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 20.50s
                        Total time: 21344.16s
                               ETA: 1110382.7s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 784 steps/s (collection: 20.677s, learning 0.195s)
               Value function loss: 12.8888
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 408.39
               Mean episode length: 247.86
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 20.87s
                        Total time: 21365.03s
                               ETA: 1110868.2s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 797 steps/s (collection: 20.353s, learning 0.187s)
               Value function loss: 13.0774
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 406.24
               Mean episode length: 247.95
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 20.54s
                        Total time: 21385.57s
                               ETA: 1111335.9s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 781 steps/s (collection: 20.808s, learning 0.164s)
               Value function loss: 12.8989
                    Surrogate loss: -0.0225
             Mean action noise std: 0.74
                       Mean reward: 418.24
               Mean episode length: 248.59
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 20.97s
                        Total time: 21406.54s
                               ETA: 1111825.5s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 777 steps/s (collection: 20.894s, learning 0.188s)
               Value function loss: 14.5096
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 413.08
               Mean episode length: 247.82
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 21.08s
                        Total time: 21427.62s
                               ETA: 1112320.3s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.741s, learning 0.173s)
               Value function loss: 14.1878
                    Surrogate loss: -0.0212
             Mean action noise std: 0.74
                       Mean reward: 412.56
               Mean episode length: 249.22
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 20.91s
                        Total time: 21448.54s
                               ETA: 1112805.8s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.942s, learning 0.160s)
               Value function loss: 14.4543
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 414.79
               Mean episode length: 248.76
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 21.10s
                        Total time: 21469.64s
                               ETA: 1113300.6s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 785 steps/s (collection: 20.677s, learning 0.170s)
               Value function loss: 13.4241
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 411.67
               Mean episode length: 249.28
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 20.85s
                        Total time: 21490.48s
                               ETA: 1113781.6s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 790 steps/s (collection: 20.555s, learning 0.163s)
               Value function loss: 11.0307
                    Surrogate loss: -0.0224
             Mean action noise std: 0.74
                       Mean reward: 408.15
               Mean episode length: 248.48
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 20.72s
                        Total time: 21511.20s
                               ETA: 1114255.3s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.431s, learning 0.172s)
               Value function loss: 14.4837
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 402.10
               Mean episode length: 248.73
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 20.60s
                        Total time: 21531.81s
                               ETA: 1114722.6s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 794 steps/s (collection: 20.448s, learning 0.168s)
               Value function loss: 13.8410
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 404.69
               Mean episode length: 249.47
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 20.62s
                        Total time: 21552.42s
                               ETA: 1115190.1s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 791 steps/s (collection: 20.526s, learning 0.170s)
               Value function loss: 16.3797
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 405.11
               Mean episode length: 247.85
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 20.70s
                        Total time: 21573.12s
                               ETA: 1115661.2s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.441s, learning 0.168s)
               Value function loss: 13.1709
                    Surrogate loss: -0.0246
             Mean action noise std: 0.74
                       Mean reward: 415.44
               Mean episode length: 247.15
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 20.61s
                        Total time: 21593.73s
                               ETA: 1116127.2s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 778 steps/s (collection: 20.879s, learning 0.167s)
               Value function loss: 14.0305
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 416.55
               Mean episode length: 247.74
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 21.05s
                        Total time: 21614.77s
                               ETA: 1116615.3s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 787 steps/s (collection: 20.633s, learning 0.175s)
               Value function loss: 13.1611
                    Surrogate loss: -0.0201
             Mean action noise std: 0.74
                       Mean reward: 407.88
               Mean episode length: 248.25
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 20.81s
                        Total time: 21635.58s
                               ETA: 1117090.6s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.185s, learning 0.183s)
               Value function loss: 14.2789
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 407.35
               Mean episode length: 247.51
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 16.37s
                        Total time: 21651.95s
                               ETA: 1117336.2s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.722s, learning 0.169s)
               Value function loss: 9.7406
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 411.17
               Mean episode length: 248.20
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 10.89s
                        Total time: 21662.84s
                               ETA: 1117299.1s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.841s, learning 0.184s)
               Value function loss: 10.9630
                    Surrogate loss: -0.0213
             Mean action noise std: 0.74
                       Mean reward: 411.51
               Mean episode length: 248.27
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 11.02s
                        Total time: 21673.86s
                               ETA: 1117268.9s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.766s, learning 0.185s)
               Value function loss: 10.2850
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 401.04
               Mean episode length: 244.83
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 10.95s
                        Total time: 21684.82s
                               ETA: 1117235.0s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.843s, learning 0.168s)
               Value function loss: 8.4398
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 399.11
               Mean episode length: 246.33
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 11.01s
                        Total time: 21695.83s
                               ETA: 1117204.1s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.481s, learning 0.186s)
               Value function loss: 7.0391
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 386.28
               Mean episode length: 243.53
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 10.67s
                        Total time: 21706.49s
                               ETA: 1117155.6s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.394s, learning 0.160s)
               Value function loss: 8.1018
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 377.80
               Mean episode length: 241.61
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 10.55s
                        Total time: 21717.05s
                               ETA: 1117101.3s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.583s, learning 0.159s)
               Value function loss: 9.2387
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 384.70
               Mean episode length: 244.06
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 10.74s
                        Total time: 21727.79s
                               ETA: 1117056.7s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1565 steps/s (collection: 10.294s, learning 0.172s)
               Value function loss: 8.6557
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 388.34
               Mean episode length: 245.33
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 10.47s
                        Total time: 21738.26s
                               ETA: 1116997.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.641s, learning 0.187s)
               Value function loss: 8.3976
                    Surrogate loss: -0.0246
             Mean action noise std: 0.74
                       Mean reward: 385.16
               Mean episode length: 242.58
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 10.83s
                        Total time: 21749.08s
                               ETA: 1116957.8s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1569 steps/s (collection: 10.279s, learning 0.162s)
               Value function loss: 8.2207
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 374.87
               Mean episode length: 239.04
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 10.44s
                        Total time: 21759.52s
                               ETA: 1116897.8s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.736s, learning 0.181s)
               Value function loss: 9.9004
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: 351.10
               Mean episode length: 235.23
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 10.92s
                        Total time: 21770.44s
                               ETA: 1116862.3s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.629s, learning 0.185s)
               Value function loss: 8.4321
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 341.51
               Mean episode length: 235.57
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 10.81s
                        Total time: 21781.26s
                               ETA: 1116821.7s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.337s, learning 0.213s)
               Value function loss: 9.3803
                    Surrogate loss: -0.0245
             Mean action noise std: 0.74
                       Mean reward: 348.01
               Mean episode length: 237.73
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 10.55s
                        Total time: 21791.81s
                               ETA: 1116767.4s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.620s, learning 0.160s)
               Value function loss: 9.7018
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 356.04
               Mean episode length: 234.99
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 10.78s
                        Total time: 21802.59s
                               ETA: 1116725.0s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.604s, learning 0.213s)
               Value function loss: 8.7838
                    Surrogate loss: -0.0217
             Mean action noise std: 0.74
                       Mean reward: 361.81
               Mean episode length: 238.74
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 10.82s
                        Total time: 21813.40s
                               ETA: 1116684.6s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.901s, learning 0.174s)
               Value function loss: 9.9602
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 369.67
               Mean episode length: 241.10
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 11.07s
                        Total time: 21824.48s
                               ETA: 1116657.3s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.758s, learning 0.177s)
               Value function loss: 9.7623
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 342.77
               Mean episode length: 232.02
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 10.93s
                        Total time: 21835.41s
                               ETA: 1116622.9s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.708s, learning 0.217s)
               Value function loss: 10.1118
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 357.00
               Mean episode length: 236.84
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 10.92s
                        Total time: 21846.34s
                               ETA: 1116588.0s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.869s, learning 0.178s)
               Value function loss: 11.4097
                    Surrogate loss: -0.0232
             Mean action noise std: 0.74
                       Mean reward: 339.46
               Mean episode length: 237.66
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 11.05s
                        Total time: 21857.38s
                               ETA: 1116559.4s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.687s, learning 0.165s)
               Value function loss: 10.8961
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 340.38
               Mean episode length: 235.35
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 10.85s
                        Total time: 21868.23s
                               ETA: 1116520.8s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.560s, learning 0.176s)
               Value function loss: 11.7448
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 338.88
               Mean episode length: 235.64
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 10.74s
                        Total time: 21878.97s
                               ETA: 1116476.4s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.408s, learning 0.193s)
               Value function loss: 10.4699
                    Surrogate loss: -0.0213
             Mean action noise std: 0.74
                       Mean reward: 335.07
               Mean episode length: 234.24
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 10.60s
                        Total time: 21889.57s
                               ETA: 1116425.1s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.666s, learning 0.161s)
               Value function loss: 11.3611
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 346.88
               Mean episode length: 234.76
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 10.83s
                        Total time: 21900.40s
                               ETA: 1116385.3s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.687s, learning 0.173s)
               Value function loss: 10.9237
                    Surrogate loss: -0.0249
             Mean action noise std: 0.74
                       Mean reward: 345.45
               Mean episode length: 235.18
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 10.86s
                        Total time: 21911.26s
                               ETA: 1116347.3s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.507s, learning 0.189s)
               Value function loss: 10.7451
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 344.42
               Mean episode length: 237.57
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 10.70s
                        Total time: 21921.95s
                               ETA: 1116301.0s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.821s, learning 0.164s)
               Value function loss: 9.8138
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 329.16
               Mean episode length: 234.99
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 10.98s
                        Total time: 21932.94s
                               ETA: 1116269.4s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.910s, learning 0.177s)
               Value function loss: 9.9059
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 330.59
               Mean episode length: 235.51
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 11.09s
                        Total time: 21944.03s
                               ETA: 1116242.9s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1582 steps/s (collection: 10.187s, learning 0.166s)
               Value function loss: 7.2880
                    Surrogate loss: -0.0259
             Mean action noise std: 0.74
                       Mean reward: 329.33
               Mean episode length: 239.22
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 10.35s
                        Total time: 21954.38s
                               ETA: 1116179.3s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.552s, learning 0.176s)
               Value function loss: 7.3992
                    Surrogate loss: -0.0210
             Mean action noise std: 0.74
                       Mean reward: 346.90
               Mean episode length: 242.26
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 10.73s
                        Total time: 21965.11s
                               ETA: 1116134.7s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.652s, learning 0.167s)
               Value function loss: 6.7784
                    Surrogate loss: -0.0258
             Mean action noise std: 0.74
                       Mean reward: 317.13
               Mean episode length: 238.48
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 10.82s
                        Total time: 21975.93s
                               ETA: 1116094.8s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.547s, learning 0.171s)
               Value function loss: 8.7360
                    Surrogate loss: -0.0212
             Mean action noise std: 0.74
                       Mean reward: 309.74
               Mean episode length: 235.15
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 10.72s
                        Total time: 21986.64s
                               ETA: 1116049.7s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.569s, learning 0.200s)
               Value function loss: 7.0109
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 307.94
               Mean episode length: 238.15
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 10.77s
                        Total time: 21997.41s
                               ETA: 1116007.3s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.907s, learning 0.167s)
               Value function loss: 7.9434
                    Surrogate loss: -0.0273
             Mean action noise std: 0.74
                       Mean reward: 297.52
               Mean episode length: 235.54
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 11.07s
                        Total time: 22008.49s
                               ETA: 1115980.5s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.163s)
               Value function loss: 6.8806
                    Surrogate loss: -0.0283
             Mean action noise std: 0.74
                       Mean reward: 290.53
               Mean episode length: 234.34
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 10.95s
                        Total time: 22019.43s
                               ETA: 1115947.1s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.870s, learning 0.165s)
               Value function loss: 6.3256
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 292.87
               Mean episode length: 225.47
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 11.03s
                        Total time: 22030.47s
                               ETA: 1115918.2s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.768s, learning 0.165s)
               Value function loss: 6.5877
                    Surrogate loss: -0.0250
             Mean action noise std: 0.74
                       Mean reward: 295.52
               Mean episode length: 223.69
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 10.93s
                        Total time: 22041.40s
                               ETA: 1115884.2s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.369s, learning 0.165s)
               Value function loss: 6.8964
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 293.55
               Mean episode length: 229.41
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 10.53s
                        Total time: 22051.93s
                               ETA: 1115830.1s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.789s, learning 0.162s)
               Value function loss: 6.2885
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 283.55
               Mean episode length: 226.10
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 10.95s
                        Total time: 22062.88s
                               ETA: 1115797.1s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.517s, learning 0.162s)
               Value function loss: 6.2255
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 280.66
               Mean episode length: 222.22
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 10.68s
                        Total time: 22073.56s
                               ETA: 1115750.4s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.426s, learning 0.168s)
               Value function loss: 7.2717
                    Surrogate loss: -0.0229
             Mean action noise std: 0.74
                       Mean reward: 279.97
               Mean episode length: 219.46
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 10.59s
                        Total time: 22084.16s
                               ETA: 1115699.4s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.551s, learning 0.162s)
               Value function loss: 6.7567
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 277.19
               Mean episode length: 221.53
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 10.71s
                        Total time: 22094.87s
                               ETA: 1115654.4s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.171s)
               Value function loss: 7.4040
                    Surrogate loss: -0.0228
             Mean action noise std: 0.74
                       Mean reward: 288.41
               Mean episode length: 222.61
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 10.78s
                        Total time: 22105.65s
                               ETA: 1115612.7s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.161s)
               Value function loss: 7.0110
                    Surrogate loss: -0.0279
             Mean action noise std: 0.74
                       Mean reward: 294.21
               Mean episode length: 223.56
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 11.14s
                        Total time: 22116.79s
                               ETA: 1115589.3s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.012s, learning 0.273s)
               Value function loss: 7.5324
                    Surrogate loss: -0.0214
             Mean action noise std: 0.74
                       Mean reward: 289.31
               Mean episode length: 223.05
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 11.29s
                        Total time: 22128.07s
                               ETA: 1115573.3s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.751s, learning 0.177s)
               Value function loss: 7.9790
                    Surrogate loss: -0.0204
             Mean action noise std: 0.74
                       Mean reward: 283.54
               Mean episode length: 228.11
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 10.93s
                        Total time: 22139.00s
                               ETA: 1115539.3s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.803s, learning 0.185s)
               Value function loss: 7.9341
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 285.00
               Mean episode length: 226.72
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 10.99s
                        Total time: 22149.99s
                               ETA: 1115508.3s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.714s, learning 0.169s)
               Value function loss: 7.8395
                    Surrogate loss: -0.0262
             Mean action noise std: 0.74
                       Mean reward: 285.53
               Mean episode length: 229.25
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 10.88s
                        Total time: 22160.87s
                               ETA: 1115472.1s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.585s, learning 0.172s)
               Value function loss: 7.7860
                    Surrogate loss: -0.0264
             Mean action noise std: 0.74
                       Mean reward: 272.01
               Mean episode length: 224.42
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 10.76s
                        Total time: 22171.63s
                               ETA: 1115429.6s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.753s, learning 0.167s)
               Value function loss: 8.0288
                    Surrogate loss: -0.0254
             Mean action noise std: 0.74
                       Mean reward: 298.89
               Mean episode length: 228.98
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 10.92s
                        Total time: 22182.55s
                               ETA: 1115395.3s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.658s, learning 0.163s)
               Value function loss: 9.5255
                    Surrogate loss: -0.0265
             Mean action noise std: 0.74
                       Mean reward: 307.29
               Mean episode length: 234.67
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 10.82s
                        Total time: 22193.37s
                               ETA: 1115356.0s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.858s, learning 0.161s)
               Value function loss: 9.3465
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 294.98
               Mean episode length: 232.24
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 11.02s
                        Total time: 22204.39s
                               ETA: 1115326.8s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.165s)
               Value function loss: 10.3978
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 308.35
               Mean episode length: 238.07
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 10.86s
                        Total time: 22215.24s
                               ETA: 1115289.4s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.495s, learning 0.158s)
               Value function loss: 8.8961
                    Surrogate loss: -0.0246
             Mean action noise std: 0.74
                       Mean reward: 322.39
               Mean episode length: 241.51
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 10.65s
                        Total time: 22225.90s
                               ETA: 1115241.8s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.843s, learning 0.169s)
               Value function loss: 9.8164
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 321.79
               Mean episode length: 240.73
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 11.01s
                        Total time: 22236.91s
                               ETA: 1115212.2s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.836s, learning 0.169s)
               Value function loss: 8.9565
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 335.14
               Mean episode length: 240.88
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 11.01s
                        Total time: 22247.91s
                               ETA: 1115182.4s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.685s, learning 0.177s)
               Value function loss: 8.5009
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 329.35
               Mean episode length: 240.82
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 10.86s
                        Total time: 22258.78s
                               ETA: 1115145.4s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.642s, learning 0.164s)
               Value function loss: 7.2823
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 343.21
               Mean episode length: 243.62
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 10.81s
                        Total time: 22269.58s
                               ETA: 1115105.6s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.538s, learning 0.179s)
               Value function loss: 10.1270
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 313.85
               Mean episode length: 238.71
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 10.72s
                        Total time: 22280.30s
                               ETA: 1115061.3s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.937s, learning 0.173s)
               Value function loss: 8.3209
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 316.48
               Mean episode length: 238.91
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 11.11s
                        Total time: 22291.41s
                               ETA: 1115036.8s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.526s, learning 0.168s)
               Value function loss: 7.3262
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 308.22
               Mean episode length: 239.52
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 10.69s
                        Total time: 22302.10s
                               ETA: 1114991.4s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.614s, learning 0.167s)
               Value function loss: 7.6972
                    Surrogate loss: -0.0232
             Mean action noise std: 0.74
                       Mean reward: 320.49
               Mean episode length: 239.17
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 10.78s
                        Total time: 22312.88s
                               ETA: 1114950.5s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.701s, learning 0.180s)
               Value function loss: 8.1646
                    Surrogate loss: -0.0238
             Mean action noise std: 0.74
                       Mean reward: 327.12
               Mean episode length: 237.89
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 10.88s
                        Total time: 22323.76s
                               ETA: 1114914.5s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.859s, learning 0.160s)
               Value function loss: 6.4503
                    Surrogate loss: -0.0239
             Mean action noise std: 0.74
                       Mean reward: 316.95
               Mean episode length: 238.90
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 11.02s
                        Total time: 22334.78s
                               ETA: 1114885.6s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.816s, learning 0.164s)
               Value function loss: 10.6887
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 313.99
               Mean episode length: 233.28
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 10.98s
                        Total time: 22345.76s
                               ETA: 1114854.7s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1611 steps/s (collection: 9.995s, learning 0.172s)
               Value function loss: 8.3797
                    Surrogate loss: -0.0245
             Mean action noise std: 0.74
                       Mean reward: 323.31
               Mean episode length: 234.40
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 10.17s
                        Total time: 22355.93s
                               ETA: 1114783.2s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.762s, learning 0.169s)
               Value function loss: 8.0591
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 330.46
               Mean episode length: 233.34
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 10.93s
                        Total time: 22366.86s
                               ETA: 1114749.9s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.631s, learning 0.186s)
               Value function loss: 7.2301
                    Surrogate loss: -0.0262
             Mean action noise std: 0.74
                       Mean reward: 310.88
               Mean episode length: 230.02
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 10.82s
                        Total time: 22377.68s
                               ETA: 1114710.9s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.742s, learning 0.160s)
               Value function loss: 8.0051
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 313.19
               Mean episode length: 232.15
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 10.90s
                        Total time: 22388.58s
                               ETA: 1114676.2s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.345s, learning 0.187s)
               Value function loss: 8.4795
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 333.82
               Mean episode length: 236.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 10.53s
                        Total time: 22399.11s
                               ETA: 1114623.1s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.169s)
               Value function loss: 9.3973
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 345.30
               Mean episode length: 235.67
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 10.73s
                        Total time: 22409.84s
                               ETA: 1114579.7s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.775s, learning 0.166s)
               Value function loss: 9.1317
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 338.29
               Mean episode length: 235.22
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 10.94s
                        Total time: 22420.78s
                               ETA: 1114547.0s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.166s)
               Value function loss: 10.0312
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 336.40
               Mean episode length: 234.17
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 11.42s
                        Total time: 22432.20s
                               ETA: 1114538.3s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.799s, learning 0.169s)
               Value function loss: 9.8223
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 344.58
               Mean episode length: 235.07
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 10.97s
                        Total time: 22443.17s
                               ETA: 1114507.0s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.163s)
               Value function loss: 10.2990
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 339.99
               Mean episode length: 233.76
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 10.86s
                        Total time: 22454.03s
                               ETA: 1114470.1s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.641s, learning 0.165s)
               Value function loss: 8.4557
                    Surrogate loss: -0.0241
             Mean action noise std: 0.74
                       Mean reward: 338.10
               Mean episode length: 234.98
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 10.81s
                        Total time: 22464.83s
                               ETA: 1114430.8s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.755s, learning 0.191s)
               Value function loss: 10.3160
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 338.52
               Mean episode length: 236.17
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 10.95s
                        Total time: 22475.78s
                               ETA: 1114398.5s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.510s, learning 0.162s)
               Value function loss: 11.3138
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 335.48
               Mean episode length: 235.97
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 10.67s
                        Total time: 22486.45s
                               ETA: 1114352.6s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.536s, learning 0.171s)
               Value function loss: 11.2731
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 351.59
               Mean episode length: 241.44
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 10.71s
                        Total time: 22497.16s
                               ETA: 1114308.5s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.926s, learning 0.208s)
               Value function loss: 10.7993
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: 348.86
               Mean episode length: 239.87
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 11.13s
                        Total time: 22508.29s
                               ETA: 1114285.5s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.855s, learning 0.162s)
               Value function loss: 9.7288
                    Surrogate loss: -0.0279
             Mean action noise std: 0.74
                       Mean reward: 343.09
               Mean episode length: 236.37
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 11.02s
                        Total time: 22519.31s
                               ETA: 1114256.8s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.770s, learning 0.160s)
               Value function loss: 11.8656
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 341.68
               Mean episode length: 231.70
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 10.93s
                        Total time: 22530.24s
                               ETA: 1114223.8s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.841s, learning 0.161s)
               Value function loss: 10.3476
                    Surrogate loss: -0.0274
             Mean action noise std: 0.74
                       Mean reward: 336.61
               Mean episode length: 233.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 11.00s
                        Total time: 22541.24s
                               ETA: 1114194.3s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.171s)
               Value function loss: 12.6094
                    Surrogate loss: -0.0252
             Mean action noise std: 0.74
                       Mean reward: 347.98
               Mean episode length: 238.29
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 10.68s
                        Total time: 22551.92s
                               ETA: 1114149.0s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.165s)
               Value function loss: 9.0189
                    Surrogate loss: -0.0238
             Mean action noise std: 0.74
                       Mean reward: 346.75
               Mean episode length: 237.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 10.95s
                        Total time: 22562.87s
                               ETA: 1114116.9s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.700s, learning 0.163s)
               Value function loss: 15.0914
                    Surrogate loss: -0.0226
             Mean action noise std: 0.74
                       Mean reward: 355.68
               Mean episode length: 238.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 10.86s
                        Total time: 22573.73s
                               ETA: 1114080.7s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.581s, learning 0.160s)
               Value function loss: 11.7682
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 342.18
               Mean episode length: 234.85
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 10.74s
                        Total time: 22584.47s
                               ETA: 1114038.5s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.763s, learning 0.194s)
               Value function loss: 10.7723
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 356.54
               Mean episode length: 236.92
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 10.96s
                        Total time: 22595.43s
                               ETA: 1114006.9s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.793s, learning 0.173s)
               Value function loss: 9.3609
                    Surrogate loss: -0.0247
             Mean action noise std: 0.74
                       Mean reward: 351.82
               Mean episode length: 240.52
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 10.97s
                        Total time: 22606.39s
                               ETA: 1113975.8s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.707s, learning 0.171s)
               Value function loss: 10.3296
                    Surrogate loss: -0.0244
             Mean action noise std: 0.74
                       Mean reward: 342.50
               Mean episode length: 231.88
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 10.88s
                        Total time: 22617.27s
                               ETA: 1113940.4s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.882s, learning 0.275s)
               Value function loss: 11.0984
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 358.79
               Mean episode length: 235.59
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 11.16s
                        Total time: 22628.43s
                               ETA: 1113918.9s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.953s, learning 0.161s)
               Value function loss: 9.1961
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 362.70
               Mean episode length: 238.31
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 11.11s
                        Total time: 22639.54s
                               ETA: 1113895.1s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.678s, learning 0.171s)
               Value function loss: 9.8991
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 348.38
               Mean episode length: 235.97
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 10.85s
                        Total time: 22650.39s
                               ETA: 1113858.4s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.317s, learning 0.212s)
               Value function loss: 10.1808
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 338.81
               Mean episode length: 229.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 10.53s
                        Total time: 22660.92s
                               ETA: 1113806.0s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.504s, learning 0.186s)
               Value function loss: 10.1791
                    Surrogate loss: -0.0202
             Mean action noise std: 0.74
                       Mean reward: 358.87
               Mean episode length: 234.12
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 10.69s
                        Total time: 22671.61s
                               ETA: 1113761.4s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.161s)
               Value function loss: 9.2157
                    Surrogate loss: -0.0246
             Mean action noise std: 0.74
                       Mean reward: 354.10
               Mean episode length: 232.08
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 10.89s
                        Total time: 22682.50s
                               ETA: 1113726.6s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1572 steps/s (collection: 10.233s, learning 0.186s)
               Value function loss: 9.0134
                    Surrogate loss: -0.0191
             Mean action noise std: 0.74
                       Mean reward: 346.19
               Mean episode length: 231.80
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 10.42s
                        Total time: 22692.92s
                               ETA: 1113668.9s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.637s, learning 0.162s)
               Value function loss: 9.2569
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 350.92
               Mean episode length: 233.71
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 10.80s
                        Total time: 22703.72s
                               ETA: 1113629.8s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.561s, learning 0.168s)
               Value function loss: 8.2149
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 373.79
               Mean episode length: 236.25
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 10.73s
                        Total time: 22714.45s
                               ETA: 1113587.4s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.916s, learning 0.192s)
               Value function loss: 9.1407
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 367.20
               Mean episode length: 233.48
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 11.11s
                        Total time: 22725.55s
                               ETA: 1113563.5s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.792s, learning 0.168s)
               Value function loss: 8.3882
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 360.28
               Mean episode length: 231.88
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 10.96s
                        Total time: 22736.51s
                               ETA: 1113532.4s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.715s, learning 0.174s)
               Value function loss: 8.3282
                    Surrogate loss: -0.0200
             Mean action noise std: 0.74
                       Mean reward: 360.44
               Mean episode length: 235.07
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 10.89s
                        Total time: 22747.40s
                               ETA: 1113497.8s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.735s, learning 0.179s)
               Value function loss: 9.3845
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 349.86
               Mean episode length: 232.49
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 10.91s
                        Total time: 22758.32s
                               ETA: 1113464.6s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.387s, learning 0.165s)
               Value function loss: 7.9443
                    Surrogate loss: -0.0237
             Mean action noise std: 0.74
                       Mean reward: 346.25
               Mean episode length: 231.33
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 10.55s
                        Total time: 22768.87s
                               ETA: 1113413.6s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.345s, learning 0.169s)
               Value function loss: 8.8621
                    Surrogate loss: -0.0224
             Mean action noise std: 0.74
                       Mean reward: 343.50
               Mean episode length: 230.97
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 10.51s
                        Total time: 22779.38s
                               ETA: 1113360.8s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.655s, learning 0.162s)
               Value function loss: 11.0780
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 348.70
               Mean episode length: 234.90
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 10.82s
                        Total time: 22790.20s
                               ETA: 1113322.8s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.707s, learning 0.208s)
               Value function loss: 7.3783
                    Surrogate loss: -0.0207
             Mean action noise std: 0.74
                       Mean reward: 347.48
               Mean episode length: 234.28
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 10.91s
                        Total time: 22801.11s
                               ETA: 1113289.7s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.826s, learning 0.172s)
               Value function loss: 10.0919
                    Surrogate loss: -0.0245
             Mean action noise std: 0.74
                       Mean reward: 349.79
               Mean episode length: 232.74
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 11.00s
                        Total time: 22812.11s
                               ETA: 1113260.6s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1576 steps/s (collection: 10.226s, learning 0.169s)
               Value function loss: 13.9506
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 376.96
               Mean episode length: 239.16
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 10.39s
                        Total time: 22822.51s
                               ETA: 1113202.1s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.420s, learning 0.165s)
               Value function loss: 12.1825
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 367.78
               Mean episode length: 237.63
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 10.58s
                        Total time: 22833.09s
                               ETA: 1113152.9s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.714s, learning 0.169s)
               Value function loss: 11.3067
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 366.92
               Mean episode length: 234.15
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 10.88s
                        Total time: 22843.97s
                               ETA: 1113118.3s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.492s, learning 0.189s)
               Value function loss: 12.3954
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 372.46
               Mean episode length: 236.74
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 10.68s
                        Total time: 22854.65s
                               ETA: 1113073.9s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.630s, learning 0.216s)
               Value function loss: 13.9904
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 378.11
               Mean episode length: 240.74
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 10.85s
                        Total time: 22865.50s
                               ETA: 1113037.6s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.758s, learning 0.172s)
               Value function loss: 13.0878
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 370.83
               Mean episode length: 238.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 10.93s
                        Total time: 22876.43s
                               ETA: 1113005.4s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.493s, learning 0.196s)
               Value function loss: 13.7542
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 382.65
               Mean episode length: 243.98
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 10.69s
                        Total time: 22887.12s
                               ETA: 1112961.5s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.564s, learning 0.168s)
               Value function loss: 11.3141
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 355.16
               Mean episode length: 234.18
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 10.73s
                        Total time: 22897.85s
                               ETA: 1112919.7s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.629s, learning 0.270s)
               Value function loss: 13.2319
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 387.20
               Mean episode length: 242.84
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 10.90s
                        Total time: 22908.75s
                               ETA: 1112886.0s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.690s, learning 0.163s)
               Value function loss: 11.6785
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 377.40
               Mean episode length: 241.68
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 10.85s
                        Total time: 22919.60s
                               ETA: 1112850.2s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.159s)
               Value function loss: 10.4776
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 377.46
               Mean episode length: 236.49
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 10.85s
                        Total time: 22930.46s
                               ETA: 1112814.2s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.798s, learning 0.233s)
               Value function loss: 9.1760
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 375.92
               Mean episode length: 242.18
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 11.03s
                        Total time: 22941.49s
                               ETA: 1112787.0s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.583s, learning 0.160s)
               Value function loss: 9.8876
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 376.08
               Mean episode length: 240.89
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 10.74s
                        Total time: 22952.23s
                               ETA: 1112745.9s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.678s, learning 0.167s)
               Value function loss: 10.8702
                    Surrogate loss: -0.0231
             Mean action noise std: 0.74
                       Mean reward: 374.54
               Mean episode length: 240.38
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 10.84s
                        Total time: 22963.07s
                               ETA: 1112709.7s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.663s, learning 0.245s)
               Value function loss: 9.5949
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 378.41
               Mean episode length: 241.12
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 10.91s
                        Total time: 22973.98s
                               ETA: 1112676.6s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.937s, learning 0.165s)
               Value function loss: 10.6004
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 371.92
               Mean episode length: 240.38
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 11.10s
                        Total time: 22985.08s
                               ETA: 1112652.9s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.638s, learning 0.213s)
               Value function loss: 9.3836
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 373.16
               Mean episode length: 241.83
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 10.85s
                        Total time: 22995.93s
                               ETA: 1112617.1s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.587s, learning 0.181s)
               Value function loss: 10.5322
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 369.13
               Mean episode length: 235.71
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 10.77s
                        Total time: 23006.70s
                               ETA: 1112577.4s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1560 steps/s (collection: 10.335s, learning 0.165s)
               Value function loss: 8.4473
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 351.69
               Mean episode length: 228.09
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 10.50s
                        Total time: 23017.20s
                               ETA: 1112524.6s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.888s, learning 0.176s)
               Value function loss: 10.0973
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 360.08
               Mean episode length: 228.88
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 11.06s
                        Total time: 23028.27s
                               ETA: 1112499.2s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.772s, learning 0.185s)
               Value function loss: 9.4083
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 350.65
               Mean episode length: 228.09
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 10.96s
                        Total time: 23039.22s
                               ETA: 1112468.7s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.513s, learning 0.161s)
               Value function loss: 10.0104
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 350.56
               Mean episode length: 223.92
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 10.67s
                        Total time: 23049.90s
                               ETA: 1112424.5s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.474s, learning 0.158s)
               Value function loss: 8.5254
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 353.51
               Mean episode length: 222.80
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 10.63s
                        Total time: 23060.53s
                               ETA: 1112378.3s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.718s, learning 0.166s)
               Value function loss: 10.4899
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 350.18
               Mean episode length: 220.63
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 10.88s
                        Total time: 23071.41s
                               ETA: 1112344.2s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.107s, learning 0.157s)
               Value function loss: 9.4232
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 350.03
               Mean episode length: 221.60
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 11.26s
                        Total time: 23082.68s
                               ETA: 1112328.5s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.960s, learning 0.165s)
               Value function loss: 10.6904
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 350.68
               Mean episode length: 223.39
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 11.13s
                        Total time: 23093.80s
                               ETA: 1112306.2s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.660s, learning 0.166s)
               Value function loss: 9.9556
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 352.83
               Mean episode length: 222.31
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 10.83s
                        Total time: 23104.63s
                               ETA: 1112269.4s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.690s, learning 0.217s)
               Value function loss: 9.3979
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 337.35
               Mean episode length: 214.10
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 10.91s
                        Total time: 23115.54s
                               ETA: 1112236.5s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.944s, learning 0.170s)
               Value function loss: 10.4601
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 326.43
               Mean episode length: 210.42
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 11.11s
                        Total time: 23126.65s
                               ETA: 1112213.6s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.812s, learning 0.159s)
               Value function loss: 9.9989
                    Surrogate loss: 0.0078
             Mean action noise std: 0.73
                       Mean reward: 338.96
               Mean episode length: 216.36
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 10.97s
                        Total time: 23137.62s
                               ETA: 1112183.9s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.920s, learning 0.165s)
               Value function loss: 8.7462
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 320.11
               Mean episode length: 210.38
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 11.08s
                        Total time: 23148.71s
                               ETA: 1112159.6s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.558s, learning 0.187s)
               Value function loss: 10.8931
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 353.59
               Mean episode length: 225.37
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 10.75s
                        Total time: 23159.45s
                               ETA: 1112119.1s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.714s, learning 0.162s)
               Value function loss: 9.8790
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 328.21
               Mean episode length: 211.67
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 10.88s
                        Total time: 23170.33s
                               ETA: 1112084.9s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.561s, learning 0.174s)
               Value function loss: 9.0563
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 341.33
               Mean episode length: 217.60
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 10.74s
                        Total time: 23181.06s
                               ETA: 1112043.9s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.640s, learning 0.213s)
               Value function loss: 9.7367
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 336.78
               Mean episode length: 215.43
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 10.85s
                        Total time: 23191.92s
                               ETA: 1112008.6s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.596s, learning 0.190s)
               Value function loss: 11.0782
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 338.62
               Mean episode length: 215.70
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 10.79s
                        Total time: 23202.70s
                               ETA: 1111970.1s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.809s, learning 0.166s)
               Value function loss: 10.3029
                    Surrogate loss: -0.0217
             Mean action noise std: 0.73
                       Mean reward: 340.61
               Mean episode length: 216.05
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 10.97s
                        Total time: 23213.67s
                               ETA: 1111940.7s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.490s, learning 0.159s)
               Value function loss: 9.1853
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 344.47
               Mean episode length: 217.52
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 10.65s
                        Total time: 23224.32s
                               ETA: 1111895.7s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.705s, learning 0.173s)
               Value function loss: 12.3938
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 344.98
               Mean episode length: 218.75
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 10.88s
                        Total time: 23235.20s
                               ETA: 1111861.7s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.166s)
               Value function loss: 11.8417
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 329.55
               Mean episode length: 210.87
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 10.76s
                        Total time: 23245.96s
                               ETA: 1111822.2s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.427s, learning 0.160s)
               Value function loss: 11.1730
                    Surrogate loss: -0.0212
             Mean action noise std: 0.73
                       Mean reward: 323.02
               Mean episode length: 208.74
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 10.59s
                        Total time: 23256.55s
                               ETA: 1111774.4s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.654s, learning 0.171s)
               Value function loss: 11.5858
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 322.40
               Mean episode length: 207.80
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 10.82s
                        Total time: 23267.38s
                               ETA: 1111737.9s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.513s, learning 0.166s)
               Value function loss: 11.0923
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 311.79
               Mean episode length: 204.95
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 10.68s
                        Total time: 23278.06s
                               ETA: 1111694.6s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.447s, learning 0.182s)
               Value function loss: 10.4703
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 315.59
               Mean episode length: 204.59
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 10.63s
                        Total time: 23288.68s
                               ETA: 1111648.8s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.719s, learning 0.160s)
               Value function loss: 12.1558
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 310.03
               Mean episode length: 199.63
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 10.88s
                        Total time: 23299.56s
                               ETA: 1111615.0s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.686s, learning 0.223s)
               Value function loss: 10.4207
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 300.05
               Mean episode length: 195.55
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 10.91s
                        Total time: 23310.47s
                               ETA: 1111582.7s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.751s, learning 0.163s)
               Value function loss: 9.8803
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 304.84
               Mean episode length: 198.39
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 10.91s
                        Total time: 23321.39s
                               ETA: 1111550.6s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.593s, learning 0.174s)
               Value function loss: 9.1086
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 320.98
               Mean episode length: 204.46
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 10.77s
                        Total time: 23332.15s
                               ETA: 1111511.6s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1463 steps/s (collection: 10.990s, learning 0.208s)
               Value function loss: 9.4201
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 304.22
               Mean episode length: 195.81
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 11.20s
                        Total time: 23343.35s
                               ETA: 1111493.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.627s, learning 0.175s)
               Value function loss: 10.5893
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 314.17
               Mean episode length: 199.85
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 10.80s
                        Total time: 23354.15s
                               ETA: 1111455.7s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.550s, learning 0.163s)
               Value function loss: 10.6285
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 305.18
               Mean episode length: 197.57
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 10.71s
                        Total time: 23364.87s
                               ETA: 1111414.2s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.598s, learning 0.219s)
               Value function loss: 9.6527
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 280.21
               Mean episode length: 188.43
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 10.82s
                        Total time: 23375.68s
                               ETA: 1111377.6s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.569s, learning 0.249s)
               Value function loss: 10.4659
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 299.96
               Mean episode length: 193.99
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 10.82s
                        Total time: 23386.50s
                               ETA: 1111341.1s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.904s, learning 0.170s)
               Value function loss: 9.0882
                    Surrogate loss: 0.0050
             Mean action noise std: 0.73
                       Mean reward: 290.52
               Mean episode length: 191.08
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 11.07s
                        Total time: 23397.58s
                               ETA: 1111316.8s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.552s, learning 0.168s)
               Value function loss: 10.4663
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 266.35
               Mean episode length: 181.62
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 10.72s
                        Total time: 23408.30s
                               ETA: 1111275.7s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.467s, learning 0.171s)
               Value function loss: 9.8943
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 276.42
               Mean episode length: 185.23
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 10.64s
                        Total time: 23418.93s
                               ETA: 1111230.7s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.167s)
               Value function loss: 11.5147
                    Surrogate loss: 0.0094
             Mean action noise std: 0.73
                       Mean reward: 295.70
               Mean episode length: 192.28
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 10.76s
                        Total time: 23429.70s
                               ETA: 1111191.7s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.370s, learning 0.163s)
               Value function loss: 11.6408
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: 265.71
               Mean episode length: 183.95
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 10.53s
                        Total time: 23440.23s
                               ETA: 1111141.8s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.833s, learning 0.160s)
               Value function loss: 10.8686
                    Surrogate loss: 0.0061
             Mean action noise std: 0.73
                       Mean reward: 282.94
               Mean episode length: 188.33
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 10.99s
                        Total time: 23451.22s
                               ETA: 1111113.8s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.641s, learning 0.167s)
               Value function loss: 10.9451
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 284.92
               Mean episode length: 187.22
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 10.81s
                        Total time: 23462.03s
                               ETA: 1111076.9s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.674s, learning 0.171s)
               Value function loss: 12.2724
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 263.03
               Mean episode length: 177.60
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 10.84s
                        Total time: 23472.88s
                               ETA: 1111041.9s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.601s, learning 0.173s)
               Value function loss: 11.2499
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 264.02
               Mean episode length: 179.73
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 10.77s
                        Total time: 23483.65s
                               ETA: 1111003.5s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.390s, learning 0.163s)
               Value function loss: 10.4925
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 282.11
               Mean episode length: 186.14
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 10.55s
                        Total time: 23494.20s
                               ETA: 1110954.7s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.160s)
               Value function loss: 9.4212
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 263.37
               Mean episode length: 176.86
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 10.85s
                        Total time: 23505.05s
                               ETA: 1110919.7s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.828s, learning 0.170s)
               Value function loss: 9.7704
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 284.11
               Mean episode length: 185.82
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 11.00s
                        Total time: 23516.04s
                               ETA: 1110892.0s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.426s, learning 0.193s)
               Value function loss: 10.3379
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 271.41
               Mean episode length: 180.90
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 10.62s
                        Total time: 23526.66s
                               ETA: 1110846.5s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.634s, learning 0.164s)
               Value function loss: 9.1159
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 256.58
               Mean episode length: 174.95
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 10.80s
                        Total time: 23537.46s
                               ETA: 1110809.4s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.875s, learning 0.166s)
               Value function loss: 9.6783
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 279.51
               Mean episode length: 184.86
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 11.04s
                        Total time: 23548.50s
                               ETA: 1110783.8s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.352s, learning 0.158s)
               Value function loss: 9.1030
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 253.63
               Mean episode length: 172.94
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 10.51s
                        Total time: 23559.01s
                               ETA: 1110733.2s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.619s, learning 0.218s)
               Value function loss: 8.3499
                    Surrogate loss: -0.0227
             Mean action noise std: 0.73
                       Mean reward: 249.42
               Mean episode length: 172.36
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 10.84s
                        Total time: 23569.85s
                               ETA: 1110698.0s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.503s, learning 0.181s)
               Value function loss: 8.6740
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 248.82
               Mean episode length: 172.47
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 10.68s
                        Total time: 23580.53s
                               ETA: 1110655.6s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.162s)
               Value function loss: 8.0830
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 250.43
               Mean episode length: 174.94
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 10.78s
                        Total time: 23591.31s
                               ETA: 1110617.6s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.787s, learning 0.162s)
               Value function loss: 8.9709
                    Surrogate loss: -0.0226
             Mean action noise std: 0.73
                       Mean reward: 253.23
               Mean episode length: 173.63
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 10.95s
                        Total time: 23602.26s
                               ETA: 1110587.8s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.754s, learning 0.211s)
               Value function loss: 8.7984
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 264.21
               Mean episode length: 178.24
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 10.96s
                        Total time: 23613.22s
                               ETA: 1110558.7s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.473s, learning 0.167s)
               Value function loss: 9.3248
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 228.69
               Mean episode length: 163.77
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 10.64s
                        Total time: 23623.86s
                               ETA: 1110514.4s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.161s)
               Value function loss: 8.3948
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 238.87
               Mean episode length: 165.79
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 11.05s
                        Total time: 23634.91s
                               ETA: 1110489.2s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.463s, learning 0.163s)
               Value function loss: 8.4588
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 244.51
               Mean episode length: 169.80
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 10.63s
                        Total time: 23645.54s
                               ETA: 1110444.3s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.489s, learning 0.189s)
               Value function loss: 9.6594
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 235.42
               Mean episode length: 167.34
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 10.68s
                        Total time: 23656.21s
                               ETA: 1110401.8s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.669s, learning 0.170s)
               Value function loss: 8.3711
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 240.61
               Mean episode length: 168.37
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 10.84s
                        Total time: 23667.05s
                               ETA: 1110366.9s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.664s, learning 0.169s)
               Value function loss: 9.3512
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 207.66
               Mean episode length: 153.60
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 10.83s
                        Total time: 23677.89s
                               ETA: 1110331.8s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.108s, learning 0.210s)
               Value function loss: 8.4780
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 217.69
               Mean episode length: 157.36
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 10.32s
                        Total time: 23688.20s
                               ETA: 1110272.6s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.316s, learning 0.171s)
               Value function loss: 8.0901
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 209.07
               Mean episode length: 153.29
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 10.49s
                        Total time: 23698.69s
                               ETA: 1110221.3s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.720s, learning 0.162s)
               Value function loss: 8.1787
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 216.61
               Mean episode length: 157.54
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 10.88s
                        Total time: 23709.57s
                               ETA: 1110188.6s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.591s, learning 0.159s)
               Value function loss: 6.3925
                    Surrogate loss: -0.0259
             Mean action noise std: 0.73
                       Mean reward: 214.84
               Mean episode length: 156.84
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 10.75s
                        Total time: 23720.32s
                               ETA: 1110149.6s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.726s, learning 0.208s)
               Value function loss: 6.9900
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 202.31
               Mean episode length: 152.01
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 10.93s
                        Total time: 23731.26s
                               ETA: 1110119.4s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.568s, learning 0.180s)
               Value function loss: 7.7260
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 223.80
               Mean episode length: 160.72
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 10.75s
                        Total time: 23742.00s
                               ETA: 1110080.4s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.381s, learning 0.189s)
               Value function loss: 7.6960
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 223.34
               Mean episode length: 161.40
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 10.57s
                        Total time: 23752.57s
                               ETA: 1110033.2s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.693s, learning 0.251s)
               Value function loss: 6.3793
                    Surrogate loss: -0.0252
             Mean action noise std: 0.73
                       Mean reward: 223.15
               Mean episode length: 162.12
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 10.94s
                        Total time: 23763.52s
                               ETA: 1110003.4s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.874s, learning 0.202s)
               Value function loss: 6.9057
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 226.84
               Mean episode length: 161.84
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 11.08s
                        Total time: 23774.59s
                               ETA: 1109979.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.655s, learning 0.208s)
               Value function loss: 7.2368
                    Surrogate loss: -0.0218
             Mean action noise std: 0.73
                       Mean reward: 217.39
               Mean episode length: 157.60
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 10.86s
                        Total time: 23785.46s
                               ETA: 1109946.4s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.704s, learning 0.188s)
               Value function loss: 8.3545
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 228.07
               Mean episode length: 163.53
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 10.89s
                        Total time: 23796.35s
                               ETA: 1109914.3s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.658s, learning 0.164s)
               Value function loss: 7.3684
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 240.90
               Mean episode length: 168.67
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 10.82s
                        Total time: 23807.17s
                               ETA: 1109879.0s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.729s, learning 0.184s)
               Value function loss: 7.3676
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 246.54
               Mean episode length: 171.43
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 10.91s
                        Total time: 23818.08s
                               ETA: 1109847.9s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.553s, learning 0.166s)
               Value function loss: 7.3073
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 241.78
               Mean episode length: 168.67
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 10.72s
                        Total time: 23828.80s
                               ETA: 1109807.8s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.325s, learning 0.162s)
               Value function loss: 7.0445
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 227.03
               Mean episode length: 164.74
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 10.49s
                        Total time: 23839.29s
                               ETA: 1109756.9s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.534s, learning 0.182s)
               Value function loss: 6.8704
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 233.04
               Mean episode length: 166.35
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 10.72s
                        Total time: 23850.01s
                               ETA: 1109716.7s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.741s, learning 0.160s)
               Value function loss: 7.4079
                    Surrogate loss: -0.0227
             Mean action noise std: 0.73
                       Mean reward: 234.42
               Mean episode length: 167.29
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 10.90s
                        Total time: 23860.91s
                               ETA: 1109685.2s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.487s, learning 0.160s)
               Value function loss: 6.5363
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 231.29
               Mean episode length: 164.90
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 10.65s
                        Total time: 23871.55s
                               ETA: 1109641.8s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.628s, learning 0.161s)
               Value function loss: 8.1387
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 252.26
               Mean episode length: 173.52
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 10.79s
                        Total time: 23882.34s
                               ETA: 1109605.2s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.615s, learning 0.160s)
               Value function loss: 7.9571
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 242.99
               Mean episode length: 170.48
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 10.78s
                        Total time: 23893.12s
                               ETA: 1109567.8s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.743s, learning 0.214s)
               Value function loss: 7.7893
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 269.67
               Mean episode length: 183.76
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 10.96s
                        Total time: 23904.08s
                               ETA: 1109539.0s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.499s, learning 0.186s)
               Value function loss: 6.5914
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 260.27
               Mean episode length: 180.13
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 10.68s
                        Total time: 23914.76s
                               ETA: 1109497.5s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.470s, learning 0.162s)
               Value function loss: 8.1469
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 260.30
               Mean episode length: 177.75
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 10.63s
                        Total time: 23925.39s
                               ETA: 1109453.6s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.710s, learning 0.187s)
               Value function loss: 8.9207
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 257.50
               Mean episode length: 176.78
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 10.90s
                        Total time: 23936.29s
                               ETA: 1109422.1s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.162s)
               Value function loss: 9.5397
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 280.20
               Mean episode length: 185.63
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 10.76s
                        Total time: 23947.05s
                               ETA: 1109384.4s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.827s, learning 0.175s)
               Value function loss: 9.3891
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 290.14
               Mean episode length: 191.01
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 11.00s
                        Total time: 23958.06s
                               ETA: 1109357.7s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.554s, learning 0.169s)
               Value function loss: 8.8853
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 267.11
               Mean episode length: 182.45
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 10.72s
                        Total time: 23968.78s
                               ETA: 1109318.2s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.687s, learning 0.173s)
               Value function loss: 9.0697
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 283.58
               Mean episode length: 188.55
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 10.86s
                        Total time: 23979.64s
                               ETA: 1109285.0s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.658s, learning 0.170s)
               Value function loss: 9.4149
                    Surrogate loss: 0.0040
             Mean action noise std: 0.73
                       Mean reward: 285.59
               Mean episode length: 190.02
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 10.83s
                        Total time: 23990.47s
                               ETA: 1109250.3s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.691s, learning 0.163s)
               Value function loss: 8.9824
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: 290.21
               Mean episode length: 191.51
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 10.85s
                        Total time: 24001.32s
                               ETA: 1109216.8s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.781s, learning 0.185s)
               Value function loss: 7.1428
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 274.00
               Mean episode length: 185.48
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 10.97s
                        Total time: 24012.29s
                               ETA: 1109188.6s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.978s, learning 0.185s)
               Value function loss: 7.7023
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 292.11
               Mean episode length: 191.52
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 11.16s
                        Total time: 24023.45s
                               ETA: 1109169.5s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.764s, learning 0.197s)
               Value function loss: 8.6850
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 308.68
               Mean episode length: 201.43
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 10.96s
                        Total time: 24034.41s
                               ETA: 1109141.0s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.466s, learning 0.190s)
               Value function loss: 8.7225
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 301.23
               Mean episode length: 200.38
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 10.66s
                        Total time: 24045.07s
                               ETA: 1109098.6s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.530s, learning 0.162s)
               Value function loss: 8.6852
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 305.45
               Mean episode length: 201.75
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 10.69s
                        Total time: 24055.76s
                               ETA: 1109057.7s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.168s)
               Value function loss: 8.0874
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 279.39
               Mean episode length: 191.89
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 10.77s
                        Total time: 24066.53s
                               ETA: 1109020.6s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.534s, learning 0.164s)
               Value function loss: 8.6184
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 299.38
               Mean episode length: 200.38
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 10.70s
                        Total time: 24077.23s
                               ETA: 1108980.1s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.747s, learning 0.165s)
               Value function loss: 10.0360
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 306.68
               Mean episode length: 202.55
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 10.91s
                        Total time: 24088.14s
                               ETA: 1108949.5s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.548s, learning 0.192s)
               Value function loss: 8.2249
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 314.69
               Mean episode length: 207.66
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 10.74s
                        Total time: 24098.88s
                               ETA: 1108911.1s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.570s, learning 0.164s)
               Value function loss: 7.9137
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 317.58
               Mean episode length: 206.04
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 10.73s
                        Total time: 24109.61s
                               ETA: 1108872.3s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.165s)
               Value function loss: 8.5481
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 318.40
               Mean episode length: 207.08
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 11.03s
                        Total time: 24120.64s
                               ETA: 1108847.1s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.851s, learning 0.172s)
               Value function loss: 8.0705
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 325.91
               Mean episode length: 208.45
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 11.02s
                        Total time: 24131.67s
                               ETA: 1108821.7s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.976s, learning 0.159s)
               Value function loss: 7.0852
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 328.71
               Mean episode length: 208.94
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 11.13s
                        Total time: 24142.80s
                               ETA: 1108801.4s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.167s)
               Value function loss: 8.8956
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 328.98
               Mean episode length: 213.72
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 10.75s
                        Total time: 24153.55s
                               ETA: 1108763.7s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.643s, learning 0.184s)
               Value function loss: 8.3888
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 329.54
               Mean episode length: 212.90
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 10.83s
                        Total time: 24164.38s
                               ETA: 1108729.4s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.861s, learning 0.169s)
               Value function loss: 7.7342
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 342.83
               Mean episode length: 214.42
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 11.03s
                        Total time: 24175.41s
                               ETA: 1108704.3s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.731s, learning 0.189s)
               Value function loss: 7.7514
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 353.04
               Mean episode length: 219.45
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 10.92s
                        Total time: 24186.33s
                               ETA: 1108674.3s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.917s, learning 0.179s)
               Value function loss: 7.8929
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 309.73
               Mean episode length: 207.35
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 11.10s
                        Total time: 24197.43s
                               ETA: 1108652.3s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.883s, learning 0.164s)
               Value function loss: 7.2021
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 306.55
               Mean episode length: 204.88
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 11.05s
                        Total time: 24208.48s
                               ETA: 1108628.1s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1566 steps/s (collection: 10.286s, learning 0.173s)
               Value function loss: 8.1119
                    Surrogate loss: -0.0222
             Mean action noise std: 0.73
                       Mean reward: 320.92
               Mean episode length: 214.75
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 10.46s
                        Total time: 24218.93s
                               ETA: 1108577.0s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.722s, learning 0.286s)
               Value function loss: 8.6973
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 312.37
               Mean episode length: 212.02
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 11.01s
                        Total time: 24229.94s
                               ETA: 1108551.0s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.730s, learning 0.166s)
               Value function loss: 7.2232
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 328.65
               Mean episode length: 211.65
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 10.90s
                        Total time: 24240.84s
                               ETA: 1108520.0s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.874s, learning 0.164s)
               Value function loss: 8.1188
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 306.28
               Mean episode length: 203.40
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 11.04s
                        Total time: 24251.88s
                               ETA: 1108495.4s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.866s, learning 0.255s)
               Value function loss: 8.4114
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 329.94
               Mean episode length: 217.85
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 11.12s
                        Total time: 24263.00s
                               ETA: 1108474.6s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.947s, learning 0.169s)
               Value function loss: 8.9871
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 329.80
               Mean episode length: 218.89
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 11.12s
                        Total time: 24274.11s
                               ETA: 1108453.7s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.961s, learning 0.162s)
               Value function loss: 8.5428
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 306.69
               Mean episode length: 209.65
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 11.12s
                        Total time: 24285.24s
                               ETA: 1108433.0s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.792s, learning 0.253s)
               Value function loss: 8.3999
                    Surrogate loss: -0.0219
             Mean action noise std: 0.73
                       Mean reward: 316.89
               Mean episode length: 207.74
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 11.04s
                        Total time: 24296.28s
                               ETA: 1108408.8s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.862s, learning 0.172s)
               Value function loss: 9.4845
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 310.30
               Mean episode length: 208.40
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 11.03s
                        Total time: 24307.31s
                               ETA: 1108384.1s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.347s, learning 0.171s)
               Value function loss: 10.4442
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 321.15
               Mean episode length: 212.12
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 10.52s
                        Total time: 24317.83s
                               ETA: 1108335.9s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.741s, learning 0.233s)
               Value function loss: 9.4813
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 330.07
               Mean episode length: 215.49
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 10.97s
                        Total time: 24328.81s
                               ETA: 1108308.5s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.628s, learning 0.171s)
               Value function loss: 8.6189
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 332.95
               Mean episode length: 215.24
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 10.80s
                        Total time: 24339.60s
                               ETA: 1108273.1s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.513s, learning 0.166s)
               Value function loss: 8.2939
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 321.51
               Mean episode length: 214.54
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 10.68s
                        Total time: 24350.28s
                               ETA: 1108232.4s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.679s, learning 0.267s)
               Value function loss: 7.8985
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 318.07
               Mean episode length: 213.08
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 10.95s
                        Total time: 24361.23s
                               ETA: 1108203.8s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.784s, learning 0.158s)
               Value function loss: 9.6827
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 341.53
               Mean episode length: 219.24
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 10.94s
                        Total time: 24372.17s
                               ETA: 1108175.1s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.954s, learning 0.165s)
               Value function loss: 8.0501
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 334.90
               Mean episode length: 214.80
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 11.12s
                        Total time: 24383.29s
                               ETA: 1108154.4s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.408s, learning 0.160s)
               Value function loss: 8.0394
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 315.23
               Mean episode length: 211.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 10.57s
                        Total time: 24393.86s
                               ETA: 1108108.7s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.588s, learning 0.163s)
               Value function loss: 8.7877
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 317.94
               Mean episode length: 209.04
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 10.75s
                        Total time: 24404.61s
                               ETA: 1108071.3s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.584s, learning 0.162s)
               Value function loss: 8.3675
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 329.00
               Mean episode length: 214.66
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 10.75s
                        Total time: 24415.36s
                               ETA: 1108033.7s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.576s, learning 0.175s)
               Value function loss: 10.7513
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 347.67
               Mean episode length: 217.97
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 10.75s
                        Total time: 24426.11s
                               ETA: 1107996.4s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.666s, learning 0.162s)
               Value function loss: 9.0716
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 332.18
               Mean episode length: 217.63
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 10.83s
                        Total time: 24436.94s
                               ETA: 1107962.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.677s, learning 0.187s)
               Value function loss: 8.3505
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 304.45
               Mean episode length: 206.16
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 10.86s
                        Total time: 24447.80s
                               ETA: 1107930.4s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.768s, learning 0.161s)
               Value function loss: 7.7228
                    Surrogate loss: -0.0247
             Mean action noise std: 0.73
                       Mean reward: 319.51
               Mean episode length: 208.42
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 10.93s
                        Total time: 24458.73s
                               ETA: 1107901.2s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.549s, learning 0.167s)
               Value function loss: 7.2896
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 326.99
               Mean episode length: 209.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 10.72s
                        Total time: 24469.45s
                               ETA: 1107862.4s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.899s, learning 0.185s)
               Value function loss: 6.4129
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 307.75
               Mean episode length: 206.41
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 11.08s
                        Total time: 24480.53s
                               ETA: 1107840.2s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.807s, learning 0.175s)
               Value function loss: 7.8794
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 304.74
               Mean episode length: 205.74
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 10.98s
                        Total time: 24491.51s
                               ETA: 1107813.5s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.743s, learning 0.187s)
               Value function loss: 8.2927
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 320.35
               Mean episode length: 214.54
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 10.93s
                        Total time: 24502.44s
                               ETA: 1107784.4s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.436s, learning 0.168s)
               Value function loss: 7.5539
                    Surrogate loss: -0.0222
             Mean action noise std: 0.73
                       Mean reward: 308.08
               Mean episode length: 206.48
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 10.60s
                        Total time: 24513.05s
                               ETA: 1107740.6s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.902s, learning 0.194s)
               Value function loss: 6.5429
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 302.80
               Mean episode length: 203.92
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 11.10s
                        Total time: 24524.14s
                               ETA: 1107719.0s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.344s, learning 0.160s)
               Value function loss: 6.6321
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 328.22
               Mean episode length: 213.80
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 10.50s
                        Total time: 24534.65s
                               ETA: 1107670.8s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.704s, learning 0.182s)
               Value function loss: 7.1672
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 309.18
               Mean episode length: 208.25
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 10.89s
                        Total time: 24545.53s
                               ETA: 1107639.8s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.743s, learning 0.174s)
               Value function loss: 5.0832
                    Surrogate loss: -0.0231
             Mean action noise std: 0.73
                       Mean reward: 289.02
               Mean episode length: 201.94
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 10.92s
                        Total time: 24556.45s
                               ETA: 1107610.3s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.686s, learning 0.214s)
               Value function loss: 7.3888
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 293.92
               Mean episode length: 203.47
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 10.90s
                        Total time: 24567.35s
                               ETA: 1107580.0s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.600s, learning 0.177s)
               Value function loss: 5.6130
                    Surrogate loss: -0.0273
             Mean action noise std: 0.73
                       Mean reward: 308.65
               Mean episode length: 208.11
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 10.78s
                        Total time: 24578.13s
                               ETA: 1107544.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.163s)
               Value function loss: 6.6877
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 309.79
               Mean episode length: 209.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 10.99s
                        Total time: 24589.12s
                               ETA: 1107518.0s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.513s, learning 0.161s)
               Value function loss: 7.4787
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 312.02
               Mean episode length: 209.63
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 10.67s
                        Total time: 24599.79s
                               ETA: 1107477.6s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.674s, learning 0.182s)
               Value function loss: 8.1775
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 322.32
               Mean episode length: 213.26
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 10.86s
                        Total time: 24610.65s
                               ETA: 1107445.3s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.845s, learning 0.166s)
               Value function loss: 8.1432
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 332.51
               Mean episode length: 217.17
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 11.01s
                        Total time: 24621.66s
                               ETA: 1107420.1s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.162s)
               Value function loss: 6.4727
                    Surrogate loss: -0.0247
             Mean action noise std: 0.73
                       Mean reward: 309.13
               Mean episode length: 207.21
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 10.85s
                        Total time: 24632.51s
                               ETA: 1107387.5s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.159s)
               Value function loss: 8.0748
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 319.10
               Mean episode length: 211.49
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 10.91s
                        Total time: 24643.42s
                               ETA: 1107357.7s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.876s, learning 0.258s)
               Value function loss: 10.2072
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 326.82
               Mean episode length: 215.83
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 11.13s
                        Total time: 24654.55s
                               ETA: 1107338.0s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.570s, learning 0.193s)
               Value function loss: 7.9801
                    Surrogate loss: -0.0233
             Mean action noise std: 0.73
                       Mean reward: 333.20
               Mean episode length: 214.69
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 10.76s
                        Total time: 24665.31s
                               ETA: 1107301.6s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.757s, learning 0.158s)
               Value function loss: 8.4445
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 326.68
               Mean episode length: 216.82
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 10.92s
                        Total time: 24676.23s
                               ETA: 1107272.2s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.642s, learning 0.207s)
               Value function loss: 8.1427
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 338.85
               Mean episode length: 218.65
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 10.85s
                        Total time: 24687.08s
                               ETA: 1107239.8s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.604s, learning 0.166s)
               Value function loss: 9.2853
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 339.21
               Mean episode length: 220.19
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 10.77s
                        Total time: 24697.85s
                               ETA: 1107203.8s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.093s, learning 0.190s)
               Value function loss: 9.1173
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 328.01
               Mean episode length: 217.66
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.28s
                        Total time: 24709.13s
                               ETA: 1107190.9s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.704s, learning 0.159s)
               Value function loss: 7.7379
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 330.22
               Mean episode length: 215.74
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 10.86s
                        Total time: 24719.99s
                               ETA: 1107159.2s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.921s, learning 0.174s)
               Value function loss: 9.6907
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 334.25
               Mean episode length: 212.17
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 11.10s
                        Total time: 24731.09s
                               ETA: 1107137.9s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.589s, learning 0.164s)
               Value function loss: 11.4973
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 330.21
               Mean episode length: 215.33
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 10.75s
                        Total time: 24741.84s
                               ETA: 1107101.2s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.888s, learning 0.170s)
               Value function loss: 8.5142
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 317.43
               Mean episode length: 211.23
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 11.06s
                        Total time: 24752.90s
                               ETA: 1107078.3s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.167s)
               Value function loss: 12.1001
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 350.00
               Mean episode length: 220.93
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 10.71s
                        Total time: 24763.62s
                               ETA: 1107040.0s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.652s, learning 0.160s)
               Value function loss: 8.4712
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 354.14
               Mean episode length: 222.09
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 10.81s
                        Total time: 24774.43s
                               ETA: 1107006.1s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.560s, learning 0.161s)
               Value function loss: 8.0147
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 302.73
               Mean episode length: 205.90
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 10.72s
                        Total time: 24785.15s
                               ETA: 1106968.1s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.745s, learning 0.160s)
               Value function loss: 8.1870
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 296.53
               Mean episode length: 203.36
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 10.91s
                        Total time: 24796.05s
                               ETA: 1106938.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1596 steps/s (collection: 10.093s, learning 0.167s)
               Value function loss: 7.7038
                    Surrogate loss: -0.0218
             Mean action noise std: 0.73
                       Mean reward: 323.49
               Mean episode length: 210.55
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 10.26s
                        Total time: 24806.31s
                               ETA: 1106879.9s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.052s, learning 0.184s)
               Value function loss: 8.8238
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 313.98
               Mean episode length: 210.15
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 11.24s
                        Total time: 24817.55s
                               ETA: 1106864.9s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.165s)
               Value function loss: 7.9642
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 275.52
               Mean episode length: 195.62
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 10.87s
                        Total time: 24828.42s
                               ETA: 1106833.7s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.040s, learning 0.270s)
               Value function loss: 7.3279
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 293.67
               Mean episode length: 203.65
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 11.31s
                        Total time: 24839.73s
                               ETA: 1106822.1s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.909s, learning 0.162s)
               Value function loss: 9.0250
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 308.47
               Mean episode length: 208.15
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 11.07s
                        Total time: 24850.80s
                               ETA: 1106799.8s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.662s, learning 0.190s)
               Value function loss: 6.8634
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 315.87
               Mean episode length: 207.33
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 10.85s
                        Total time: 24861.65s
                               ETA: 1106767.8s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.562s, learning 0.213s)
               Value function loss: 6.2395
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 282.69
               Mean episode length: 196.99
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 10.78s
                        Total time: 24872.43s
                               ETA: 1106732.5s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.463s, learning 0.162s)
               Value function loss: 6.7225
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 262.79
               Mean episode length: 190.36
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 10.63s
                        Total time: 24883.05s
                               ETA: 1106690.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.160s)
               Value function loss: 7.1337
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 253.60
               Mean episode length: 186.07
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 10.85s
                        Total time: 24893.90s
                               ETA: 1106658.5s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.593s, learning 0.181s)
               Value function loss: 6.1084
                    Surrogate loss: -0.0214
             Mean action noise std: 0.73
                       Mean reward: 271.08
               Mean episode length: 190.32
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 10.77s
                        Total time: 24904.68s
                               ETA: 1106623.1s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.847s, learning 0.160s)
               Value function loss: 6.4787
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 279.22
               Mean episode length: 192.58
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 11.01s
                        Total time: 24915.68s
                               ETA: 1106598.1s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.589s, learning 0.156s)
               Value function loss: 6.7266
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 265.51
               Mean episode length: 188.76
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 10.75s
                        Total time: 24926.43s
                               ETA: 1106561.5s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.799s, learning 0.206s)
               Value function loss: 7.9784
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 258.11
               Mean episode length: 186.29
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 11.01s
                        Total time: 24937.44s
                               ETA: 1106536.5s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.944s, learning 0.189s)
               Value function loss: 6.9812
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 266.17
               Mean episode length: 187.72
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 11.13s
                        Total time: 24948.57s
                               ETA: 1106517.1s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.639s, learning 0.168s)
               Value function loss: 7.5804
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 265.78
               Mean episode length: 190.04
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 10.81s
                        Total time: 24959.38s
                               ETA: 1106483.3s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.831s, learning 0.167s)
               Value function loss: 7.3839
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 271.15
               Mean episode length: 190.48
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 11.00s
                        Total time: 24970.37s
                               ETA: 1106458.0s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.485s, learning 0.166s)
               Value function loss: 7.3282
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 251.76
               Mean episode length: 185.12
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 10.65s
                        Total time: 24981.03s
                               ETA: 1106417.3s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.814s, learning 0.174s)
               Value function loss: 9.0667
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 287.95
               Mean episode length: 198.07
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 10.99s
                        Total time: 24992.01s
                               ETA: 1106391.6s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.952s, learning 0.190s)
               Value function loss: 7.8200
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 289.55
               Mean episode length: 197.64
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 11.14s
                        Total time: 25003.16s
                               ETA: 1106372.6s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.664s, learning 0.166s)
               Value function loss: 8.9352
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 268.17
               Mean episode length: 190.76
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 10.83s
                        Total time: 25013.98s
                               ETA: 1106339.9s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.432s, learning 0.179s)
               Value function loss: 9.0939
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 282.35
               Mean episode length: 196.99
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 10.61s
                        Total time: 25024.60s
                               ETA: 1106297.6s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.967s, learning 0.166s)
               Value function loss: 7.9105
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 306.30
               Mean episode length: 202.62
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 11.13s
                        Total time: 25035.73s
                               ETA: 1106278.3s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.564s, learning 0.159s)
               Value function loss: 6.6033
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 297.95
               Mean episode length: 198.18
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 10.72s
                        Total time: 25046.45s
                               ETA: 1106240.9s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.815s, learning 0.157s)
               Value function loss: 6.7925
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 298.55
               Mean episode length: 198.68
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 10.97s
                        Total time: 25057.42s
                               ETA: 1106214.5s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.640s, learning 0.251s)
               Value function loss: 6.7229
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 297.66
               Mean episode length: 199.93
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 10.89s
                        Total time: 25068.31s
                               ETA: 1106184.6s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.067s, learning 0.161s)
               Value function loss: 9.4467
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 301.62
               Mean episode length: 200.81
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.23s
                        Total time: 25079.54s
                               ETA: 1106169.6s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.762s, learning 0.160s)
               Value function loss: 8.6427
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 322.06
               Mean episode length: 209.27
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 10.92s
                        Total time: 25090.46s
                               ETA: 1106141.0s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.566s, learning 0.160s)
               Value function loss: 9.3620
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 318.59
               Mean episode length: 206.84
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 10.73s
                        Total time: 25101.19s
                               ETA: 1106103.9s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.665s, learning 0.160s)
               Value function loss: 7.1228
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 315.64
               Mean episode length: 207.69
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 10.82s
                        Total time: 25112.01s
                               ETA: 1106071.1s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.610s, learning 0.162s)
               Value function loss: 8.5217
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 322.38
               Mean episode length: 212.66
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 10.77s
                        Total time: 25122.79s
                               ETA: 1106036.0s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.407s, learning 0.179s)
               Value function loss: 9.0749
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 340.55
               Mean episode length: 216.38
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 10.59s
                        Total time: 25133.37s
                               ETA: 1105992.8s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.524s, learning 0.157s)
               Value function loss: 9.1710
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 324.18
               Mean episode length: 212.34
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 10.68s
                        Total time: 25144.05s
                               ETA: 1105953.8s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.682s, learning 0.174s)
               Value function loss: 9.8508
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 334.30
               Mean episode length: 214.48
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 10.86s
                        Total time: 25154.91s
                               ETA: 1105922.5s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.691s, learning 0.167s)
               Value function loss: 8.6529
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 324.54
               Mean episode length: 213.36
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 10.86s
                        Total time: 25165.77s
                               ETA: 1105891.3s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.734s, learning 0.160s)
               Value function loss: 8.9796
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 347.02
               Mean episode length: 221.25
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 10.89s
                        Total time: 25176.66s
                               ETA: 1105861.7s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.620s, learning 0.181s)
               Value function loss: 9.8112
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 348.05
               Mean episode length: 221.41
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 10.80s
                        Total time: 25187.46s
                               ETA: 1105828.0s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.797s, learning 0.177s)
               Value function loss: 9.1711
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 344.93
               Mean episode length: 222.58
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 10.97s
                        Total time: 25198.44s
                               ETA: 1105802.0s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.173s)
               Value function loss: 8.8408
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 354.59
               Mean episode length: 224.30
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 11.01s
                        Total time: 25209.45s
                               ETA: 1105777.5s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.510s, learning 0.171s)
               Value function loss: 9.3231
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 345.19
               Mean episode length: 219.98
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 10.68s
                        Total time: 25220.13s
                               ETA: 1105738.6s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.044s, learning 0.161s)
               Value function loss: 10.1997
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 348.86
               Mean episode length: 222.53
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 11.21s
                        Total time: 25231.33s
                               ETA: 1105722.8s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.966s, learning 0.169s)
               Value function loss: 10.3760
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 358.78
               Mean episode length: 229.86
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 11.13s
                        Total time: 25242.47s
                               ETA: 1105703.8s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.517s, learning 0.162s)
               Value function loss: 9.9586
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 361.78
               Mean episode length: 229.42
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 10.68s
                        Total time: 25253.15s
                               ETA: 1105664.9s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.683s, learning 0.160s)
               Value function loss: 10.3266
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 379.53
               Mean episode length: 235.20
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 10.84s
                        Total time: 25263.99s
                               ETA: 1105633.2s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.749s, learning 0.183s)
               Value function loss: 9.5484
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 365.06
               Mean episode length: 231.67
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 10.93s
                        Total time: 25274.92s
                               ETA: 1105605.4s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.454s, learning 0.167s)
               Value function loss: 9.3018
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 374.59
               Mean episode length: 232.36
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 10.62s
                        Total time: 25285.54s
                               ETA: 1105564.0s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.321s, learning 0.190s)
               Value function loss: 10.7874
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 385.31
               Mean episode length: 234.22
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 10.51s
                        Total time: 25296.05s
                               ETA: 1105517.8s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.537s, learning 0.170s)
               Value function loss: 10.7827
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 383.42
               Mean episode length: 236.38
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 10.71s
                        Total time: 25306.76s
                               ETA: 1105480.3s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.573s, learning 0.167s)
               Value function loss: 7.6679
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 366.01
               Mean episode length: 231.85
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 10.74s
                        Total time: 25317.50s
                               ETA: 1105444.2s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.735s, learning 0.174s)
               Value function loss: 10.9220
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 370.05
               Mean episode length: 233.63
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 10.91s
                        Total time: 25328.41s
                               ETA: 1105415.5s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.529s, learning 0.161s)
               Value function loss: 10.9619
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 387.58
               Mean episode length: 240.53
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 10.69s
                        Total time: 25339.10s
                               ETA: 1105377.2s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.570s, learning 0.175s)
               Value function loss: 9.4191
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 375.13
               Mean episode length: 234.71
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 10.74s
                        Total time: 25349.84s
                               ETA: 1105341.4s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.456s, learning 0.190s)
               Value function loss: 9.0647
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 370.18
               Mean episode length: 232.60
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 10.65s
                        Total time: 25360.49s
                               ETA: 1105301.3s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.612s, learning 0.162s)
               Value function loss: 8.6202
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 379.91
               Mean episode length: 239.56
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 10.77s
                        Total time: 25371.26s
                               ETA: 1105266.8s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.491s, learning 0.170s)
               Value function loss: 9.3743
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 379.50
               Mean episode length: 239.46
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 10.66s
                        Total time: 25381.93s
                               ETA: 1105227.4s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.521s, learning 0.185s)
               Value function loss: 10.6002
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 385.83
               Mean episode length: 241.06
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 10.71s
                        Total time: 25392.63s
                               ETA: 1105190.0s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.925s, learning 0.163s)
               Value function loss: 9.1430
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 378.90
               Mean episode length: 235.21
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 11.09s
                        Total time: 25403.72s
                               ETA: 1105169.2s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.605s, learning 0.161s)
               Value function loss: 12.3190
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 378.12
               Mean episode length: 234.03
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 10.77s
                        Total time: 25414.49s
                               ETA: 1105134.4s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.728s, learning 0.164s)
               Value function loss: 10.1106
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 391.30
               Mean episode length: 238.81
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 10.89s
                        Total time: 25425.38s
                               ETA: 1105105.2s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.167s)
               Value function loss: 11.5664
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 402.32
               Mean episode length: 243.75
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 10.88s
                        Total time: 25436.26s
                               ETA: 1105075.3s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.533s, learning 0.171s)
               Value function loss: 11.4962
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 384.87
               Mean episode length: 238.77
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 10.70s
                        Total time: 25446.96s
                               ETA: 1105037.9s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.652s, learning 0.162s)
               Value function loss: 10.7712
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 377.84
               Mean episode length: 235.27
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 10.81s
                        Total time: 25457.77s
                               ETA: 1105005.3s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.359s, learning 0.164s)
               Value function loss: 14.7912
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 389.09
               Mean episode length: 239.58
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 10.52s
                        Total time: 25468.30s
                               ETA: 1104960.1s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.477s, learning 0.222s)
               Value function loss: 15.5433
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 385.41
               Mean episode length: 238.49
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 10.70s
                        Total time: 25479.00s
                               ETA: 1104922.6s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.711s, learning 0.157s)
               Value function loss: 13.2905
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 390.95
               Mean episode length: 243.56
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 10.87s
                        Total time: 25489.86s
                               ETA: 1104892.4s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.760s, learning 0.212s)
               Value function loss: 14.1350
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 389.31
               Mean episode length: 239.49
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 10.97s
                        Total time: 25500.84s
                               ETA: 1104866.7s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1560 steps/s (collection: 10.337s, learning 0.162s)
               Value function loss: 13.7622
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 398.09
               Mean episode length: 240.24
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 10.50s
                        Total time: 25511.34s
                               ETA: 1104820.5s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.724s, learning 0.181s)
               Value function loss: 18.5143
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 402.18
               Mean episode length: 243.79
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 10.91s
                        Total time: 25522.24s
                               ETA: 1104792.0s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.702s, learning 0.182s)
               Value function loss: 16.8292
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 395.35
               Mean episode length: 241.65
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 10.88s
                        Total time: 25533.12s
                               ETA: 1104762.6s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.584s, learning 0.161s)
               Value function loss: 14.1661
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 394.31
               Mean episode length: 241.53
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 10.75s
                        Total time: 25543.87s
                               ETA: 1104727.1s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.987s, learning 0.188s)
               Value function loss: 14.8657
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 401.19
               Mean episode length: 243.99
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.17s
                        Total time: 25555.04s
                               ETA: 1104710.3s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.791s, learning 0.167s)
               Value function loss: 19.7067
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 405.33
               Mean episode length: 242.36
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 10.96s
                        Total time: 25566.00s
                               ETA: 1104684.1s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.779s, learning 0.163s)
               Value function loss: 14.5501
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 413.59
               Mean episode length: 244.84
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 10.94s
                        Total time: 25576.95s
                               ETA: 1104657.3s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.704s, learning 0.256s)
               Value function loss: 12.2803
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 409.22
               Mean episode length: 244.24
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 10.96s
                        Total time: 25587.91s
                               ETA: 1104631.2s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.608s, learning 0.162s)
               Value function loss: 12.9331
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 409.60
               Mean episode length: 245.97
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 10.77s
                        Total time: 25598.68s
                               ETA: 1104597.0s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.725s, learning 0.186s)
               Value function loss: 13.6564
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 417.58
               Mean episode length: 247.97
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 10.91s
                        Total time: 25609.59s
                               ETA: 1104568.8s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.770s, learning 0.171s)
               Value function loss: 13.5862
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 401.70
               Mean episode length: 241.53
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 10.94s
                        Total time: 25620.53s
                               ETA: 1104542.0s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.969s, learning 0.187s)
               Value function loss: 11.8072
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 395.15
               Mean episode length: 240.01
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 11.16s
                        Total time: 25631.68s
                               ETA: 1104524.4s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.441s, learning 0.174s)
               Value function loss: 13.9189
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 384.40
               Mean episode length: 240.06
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 10.61s
                        Total time: 25642.30s
                               ETA: 1104483.5s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.785s, learning 0.164s)
               Value function loss: 11.9151
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 381.04
               Mean episode length: 238.84
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 10.95s
                        Total time: 25653.25s
                               ETA: 1104457.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.726s, learning 0.163s)
               Value function loss: 13.1985
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 382.26
               Mean episode length: 238.74
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 10.89s
                        Total time: 25664.14s
                               ETA: 1104428.0s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.812s, learning 0.165s)
               Value function loss: 13.0201
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 392.28
               Mean episode length: 240.12
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 10.98s
                        Total time: 25675.11s
                               ETA: 1104402.8s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.532s, learning 0.167s)
               Value function loss: 12.0932
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 375.15
               Mean episode length: 237.07
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 10.70s
                        Total time: 25685.81s
                               ETA: 1104365.6s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.387s, learning 0.166s)
               Value function loss: 11.2198
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 378.26
               Mean episode length: 240.08
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 10.55s
                        Total time: 25696.36s
                               ETA: 1104322.2s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.805s, learning 0.162s)
               Value function loss: 9.9163
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 378.39
               Mean episode length: 239.47
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 10.97s
                        Total time: 25707.33s
                               ETA: 1104296.5s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.676s, learning 0.161s)
               Value function loss: 11.4022
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 368.40
               Mean episode length: 236.74
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 10.84s
                        Total time: 25718.17s
                               ETA: 1104265.4s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.543s, learning 0.171s)
               Value function loss: 9.9932
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 374.10
               Mean episode length: 235.23
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 10.71s
                        Total time: 25728.88s
                               ETA: 1104228.9s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.603s, learning 0.170s)
               Value function loss: 9.4878
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 365.53
               Mean episode length: 234.21
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 10.77s
                        Total time: 25739.65s
                               ETA: 1104195.0s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.741s, learning 0.195s)
               Value function loss: 11.3730
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 385.17
               Mean episode length: 241.88
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 10.94s
                        Total time: 25750.59s
                               ETA: 1104168.1s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.816s, learning 0.168s)
               Value function loss: 9.5595
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 384.07
               Mean episode length: 242.88
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 10.98s
                        Total time: 25761.57s
                               ETA: 1104143.3s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.997s, learning 0.178s)
               Value function loss: 9.1622
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 373.28
               Mean episode length: 237.11
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 11.18s
                        Total time: 25772.75s
                               ETA: 1104126.7s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.592s, learning 0.163s)
               Value function loss: 10.4631
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 374.09
               Mean episode length: 238.70
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 10.75s
                        Total time: 25783.50s
                               ETA: 1104092.1s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.767s, learning 0.170s)
               Value function loss: 7.3567
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 371.40
               Mean episode length: 237.77
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 10.94s
                        Total time: 25794.44s
                               ETA: 1104065.4s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.727s, learning 0.176s)
               Value function loss: 10.8758
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 373.17
               Mean episode length: 238.57
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 10.90s
                        Total time: 25805.35s
                               ETA: 1104037.2s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.162s)
               Value function loss: 8.6297
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 383.66
               Mean episode length: 241.78
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 10.87s
                        Total time: 25816.22s
                               ETA: 1104007.7s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.694s, learning 0.169s)
               Value function loss: 10.6496
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: 376.84
               Mean episode length: 240.22
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 10.86s
                        Total time: 25827.08s
                               ETA: 1103977.8s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.555s, learning 0.172s)
               Value function loss: 11.3618
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 369.63
               Mean episode length: 239.38
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 10.73s
                        Total time: 25837.81s
                               ETA: 1103942.1s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.692s, learning 0.157s)
               Value function loss: 10.5802
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 367.24
               Mean episode length: 239.87
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 10.85s
                        Total time: 25848.66s
                               ETA: 1103911.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.754s, learning 0.193s)
               Value function loss: 10.3480
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 377.08
               Mean episode length: 242.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 10.95s
                        Total time: 25859.60s
                               ETA: 1103885.4s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.860s, learning 0.172s)
               Value function loss: 9.6054
                    Surrogate loss: 0.0037
             Mean action noise std: 0.73
                       Mean reward: 362.87
               Mean episode length: 237.88
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 11.03s
                        Total time: 25870.64s
                               ETA: 1103862.8s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.509s, learning 0.186s)
               Value function loss: 11.3091
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 373.40
               Mean episode length: 241.55
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 10.70s
                        Total time: 25881.33s
                               ETA: 1103825.8s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.030s, learning 0.188s)
               Value function loss: 10.9936
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 373.66
               Mean episode length: 242.47
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 11.22s
                        Total time: 25892.55s
                               ETA: 1103811.1s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.464s, learning 0.162s)
               Value function loss: 9.0885
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: 380.61
               Mean episode length: 243.56
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 10.63s
                        Total time: 25903.17s
                               ETA: 1103771.2s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.708s, learning 0.171s)
               Value function loss: 8.9940
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 372.52
               Mean episode length: 239.65
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 10.88s
                        Total time: 25914.05s
                               ETA: 1103742.1s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.619s, learning 0.160s)
               Value function loss: 7.9935
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 367.96
               Mean episode length: 239.34
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 10.78s
                        Total time: 25924.83s
                               ETA: 1103708.8s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.781s, learning 0.179s)
               Value function loss: 8.0012
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 378.90
               Mean episode length: 244.22
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 10.96s
                        Total time: 25935.79s
                               ETA: 1103683.2s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.700s, learning 0.166s)
               Value function loss: 7.8074
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 375.64
               Mean episode length: 241.36
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 10.87s
                        Total time: 25946.66s
                               ETA: 1103653.6s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.900s, learning 0.164s)
               Value function loss: 8.4359
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 383.05
               Mean episode length: 243.51
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 11.06s
                        Total time: 25957.72s
                               ETA: 1103632.5s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.767s, learning 0.188s)
               Value function loss: 9.3368
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 378.32
               Mean episode length: 244.34
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 10.96s
                        Total time: 25968.68s
                               ETA: 1103606.7s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.538s, learning 0.162s)
               Value function loss: 8.8013
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 390.82
               Mean episode length: 246.66
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 10.70s
                        Total time: 25979.38s
                               ETA: 1103570.1s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.166s)
               Value function loss: 8.0450
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 391.51
               Mean episode length: 247.27
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 11.02s
                        Total time: 25990.40s
                               ETA: 1103547.0s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.163s)
               Value function loss: 8.3469
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 373.23
               Mean episode length: 242.02
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 10.70s
                        Total time: 26001.09s
                               ETA: 1103510.3s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.601s, learning 0.168s)
               Value function loss: 9.9319
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 380.04
               Mean episode length: 242.91
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 10.77s
                        Total time: 26011.86s
                               ETA: 1103476.7s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1581 steps/s (collection: 10.200s, learning 0.161s)
               Value function loss: 11.9199
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 387.93
               Mean episode length: 245.58
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 10.36s
                        Total time: 26022.22s
                               ETA: 1103425.8s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.897s, learning 0.188s)
               Value function loss: 10.6897
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 390.81
               Mean episode length: 245.26
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 11.09s
                        Total time: 26033.31s
                               ETA: 1103405.7s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.583s, learning 0.167s)
               Value function loss: 8.8699
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 388.69
               Mean episode length: 245.97
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 10.75s
                        Total time: 26044.06s
                               ETA: 1103371.3s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.637s, learning 0.171s)
               Value function loss: 12.2282
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: 396.42
               Mean episode length: 247.89
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 10.81s
                        Total time: 26054.87s
                               ETA: 1103339.5s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.845s, learning 0.266s)
               Value function loss: 10.0591
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 379.65
               Mean episode length: 245.41
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 11.11s
                        Total time: 26065.98s
                               ETA: 1103320.5s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.683s, learning 0.167s)
               Value function loss: 9.7850
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 385.61
               Mean episode length: 245.47
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 10.85s
                        Total time: 26076.83s
                               ETA: 1103290.4s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.668s, learning 0.165s)
               Value function loss: 10.4324
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 385.10
               Mean episode length: 245.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 10.83s
                        Total time: 26087.66s
                               ETA: 1103259.7s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.725s, learning 0.171s)
               Value function loss: 8.8759
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 384.04
               Mean episode length: 247.08
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 10.90s
                        Total time: 26098.56s
                               ETA: 1103231.6s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.843s, learning 0.161s)
               Value function loss: 10.6221
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 385.66
               Mean episode length: 246.91
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 11.00s
                        Total time: 26109.56s
                               ETA: 1103208.1s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.161s)
               Value function loss: 12.9067
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 382.73
               Mean episode length: 246.88
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 10.82s
                        Total time: 26120.39s
                               ETA: 1103177.0s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.594s, learning 0.160s)
               Value function loss: 8.7829
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 380.24
               Mean episode length: 245.79
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 10.75s
                        Total time: 26131.14s
                               ETA: 1103142.9s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.992s, learning 0.163s)
               Value function loss: 9.2974
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 381.99
               Mean episode length: 245.94
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 11.16s
                        Total time: 26142.30s
                               ETA: 1103125.8s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.609s, learning 0.188s)
               Value function loss: 9.5188
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 392.89
               Mean episode length: 248.71
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 10.80s
                        Total time: 26153.09s
                               ETA: 1103093.6s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.861s, learning 0.180s)
               Value function loss: 10.5537
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 403.29
               Mean episode length: 248.53
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 11.04s
                        Total time: 26164.13s
                               ETA: 1103071.7s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.601s, learning 0.161s)
               Value function loss: 13.6291
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 400.76
               Mean episode length: 249.56
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 10.76s
                        Total time: 26174.89s
                               ETA: 1103038.1s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.592s, learning 0.162s)
               Value function loss: 13.8383
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 398.84
               Mean episode length: 248.89
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 10.75s
                        Total time: 26185.65s
                               ETA: 1103004.1s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.698s, learning 0.159s)
               Value function loss: 10.9995
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 398.16
               Mean episode length: 248.70
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 10.86s
                        Total time: 26196.51s
                               ETA: 1102974.5s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.805s, learning 0.163s)
               Value function loss: 13.2061
                    Surrogate loss: 0.0058
             Mean action noise std: 0.73
                       Mean reward: 397.06
               Mean episode length: 248.21
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 10.97s
                        Total time: 26207.47s
                               ETA: 1102949.6s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.481s, learning 0.187s)
               Value function loss: 12.6954
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 400.18
               Mean episode length: 248.63
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 10.67s
                        Total time: 26218.14s
                               ETA: 1102912.1s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.784s, learning 0.161s)
               Value function loss: 12.2908
                    Surrogate loss: 0.0043
             Mean action noise std: 0.73
                       Mean reward: 404.39
               Mean episode length: 248.88
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 10.95s
                        Total time: 26229.09s
                               ETA: 1102886.3s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.033s, learning 0.162s)
               Value function loss: 13.2668
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 403.40
               Mean episode length: 249.04
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.19s
                        Total time: 26240.28s
                               ETA: 1102870.9s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.745s, learning 0.167s)
               Value function loss: 15.4384
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 402.93
               Mean episode length: 248.67
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 10.91s
                        Total time: 26251.19s
                               ETA: 1102843.7s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.777s, learning 0.171s)
               Value function loss: 12.8913
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 404.89
               Mean episode length: 248.64
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 10.95s
                        Total time: 26262.14s
                               ETA: 1102818.0s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.456s, learning 0.183s)
               Value function loss: 13.9851
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 413.54
               Mean episode length: 249.65
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 10.64s
                        Total time: 26272.78s
                               ETA: 1102779.4s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.619s, learning 0.183s)
               Value function loss: 13.7643
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 411.77
               Mean episode length: 249.42
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 10.80s
                        Total time: 26283.58s
                               ETA: 1102747.6s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.612s, learning 0.210s)
               Value function loss: 14.3409
                    Surrogate loss: 0.0122
             Mean action noise std: 0.73
                       Mean reward: 406.40
               Mean episode length: 249.64
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 10.82s
                        Total time: 26294.41s
                               ETA: 1102716.7s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.578s, learning 0.159s)
               Value function loss: 14.5869
                    Surrogate loss: 0.0054
             Mean action noise std: 0.73
                       Mean reward: 408.74
               Mean episode length: 249.14
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 10.74s
                        Total time: 26305.14s
                               ETA: 1102682.2s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.540s, learning 0.161s)
               Value function loss: 14.8155
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 416.25
               Mean episode length: 249.34
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 10.70s
                        Total time: 26315.84s
                               ETA: 1102646.2s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.986s, learning 0.185s)
               Value function loss: 12.5535
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 416.22
               Mean episode length: 249.41
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 11.17s
                        Total time: 26327.01s
                               ETA: 1102630.0s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.446s, learning 0.184s)
               Value function loss: 12.7129
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 408.70
               Mean episode length: 249.55
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 10.63s
                        Total time: 26337.64s
                               ETA: 1102591.1s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.777s, learning 0.169s)
               Value function loss: 16.3727
                    Surrogate loss: 0.0051
             Mean action noise std: 0.73
                       Mean reward: 403.10
               Mean episode length: 248.37
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 10.95s
                        Total time: 26348.59s
                               ETA: 1102565.4s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.688s, learning 0.184s)
               Value function loss: 14.5652
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 405.67
               Mean episode length: 248.84
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 10.87s
                        Total time: 26359.46s
                               ETA: 1102536.7s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.738s, learning 0.222s)
               Value function loss: 15.7466
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 407.68
               Mean episode length: 248.38
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 10.96s
                        Total time: 26370.42s
                               ETA: 1102511.7s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.587s, learning 0.258s)
               Value function loss: 14.6698
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 420.93
               Mean episode length: 249.65
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 10.85s
                        Total time: 26381.27s
                               ETA: 1102481.9s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.694s, learning 0.173s)
               Value function loss: 16.1343
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 413.06
               Mean episode length: 248.35
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 10.87s
                        Total time: 26392.13s
                               ETA: 1102453.0s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.571s, learning 0.168s)
               Value function loss: 15.0928
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 410.71
               Mean episode length: 248.60
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 10.74s
                        Total time: 26402.87s
                               ETA: 1102418.7s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.745s, learning 0.169s)
               Value function loss: 12.5539
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: 406.94
               Mean episode length: 248.13
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 10.91s
                        Total time: 26413.79s
                               ETA: 1102391.8s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.744s, learning 0.172s)
               Value function loss: 14.1252
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 411.73
               Mean episode length: 247.69
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 10.92s
                        Total time: 26424.70s
                               ETA: 1102365.0s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.680s, learning 0.166s)
               Value function loss: 14.6368
                    Surrogate loss: 0.0096
             Mean action noise std: 0.73
                       Mean reward: 411.39
               Mean episode length: 246.03
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 10.85s
                        Total time: 26435.55s
                               ETA: 1102335.3s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.613s, learning 0.166s)
               Value function loss: 12.2439
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 410.41
               Mean episode length: 245.88
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 10.78s
                        Total time: 26446.33s
                               ETA: 1102302.8s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.869s, learning 0.246s)
               Value function loss: 15.6649
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 409.54
               Mean episode length: 247.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 11.12s
                        Total time: 26457.44s
                               ETA: 1102284.4s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.649s, learning 0.189s)
               Value function loss: 13.1709
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 412.05
               Mean episode length: 247.85
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 10.84s
                        Total time: 26468.28s
                               ETA: 1102254.4s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.900s, learning 0.178s)
               Value function loss: 11.8673
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 403.32
               Mean episode length: 246.62
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 11.08s
                        Total time: 26479.36s
                               ETA: 1102234.4s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.931s, learning 0.249s)
               Value function loss: 12.4472
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 409.59
               Mean episode length: 246.98
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 11.18s
                        Total time: 26490.54s
                               ETA: 1102218.6s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.898s, learning 0.162s)
               Value function loss: 11.9686
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 408.58
               Mean episode length: 247.47
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 11.06s
                        Total time: 26501.60s
                               ETA: 1102197.9s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.684s, learning 0.184s)
               Value function loss: 12.3754
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 408.33
               Mean episode length: 247.64
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 10.87s
                        Total time: 26512.47s
                               ETA: 1102169.2s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.397s, learning 0.219s)
               Value function loss: 12.1427
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 399.25
               Mean episode length: 245.13
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 10.62s
                        Total time: 26523.08s
                               ETA: 1102130.1s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.440s, learning 0.170s)
               Value function loss: 11.6961
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 404.85
               Mean episode length: 247.42
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 10.61s
                        Total time: 26533.69s
                               ETA: 1102090.7s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.841s, learning 0.161s)
               Value function loss: 14.0578
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 410.54
               Mean episode length: 247.61
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 11.00s
                        Total time: 26544.70s
                               ETA: 1102067.6s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.770s, learning 0.169s)
               Value function loss: 12.0474
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 416.45
               Mean episode length: 248.25
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 10.94s
                        Total time: 26555.64s
                               ETA: 1102041.9s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.440s, learning 0.258s)
               Value function loss: 13.8177
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 412.62
               Mean episode length: 249.30
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 10.70s
                        Total time: 26566.33s
                               ETA: 1102006.3s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.475s, learning 0.167s)
               Value function loss: 14.7521
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 407.61
               Mean episode length: 248.04
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 10.64s
                        Total time: 26576.98s
                               ETA: 1101968.3s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.753s, learning 0.288s)
               Value function loss: 14.6294
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 410.07
               Mean episode length: 248.97
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 11.04s
                        Total time: 26588.02s
                               ETA: 1101946.9s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.966s, learning 0.167s)
               Value function loss: 10.3150
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 408.57
               Mean episode length: 249.08
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 11.13s
                        Total time: 26599.15s
                               ETA: 1101929.3s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.041s, learning 0.165s)
               Value function loss: 11.9329
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 412.99
               Mean episode length: 249.08
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 11.21s
                        Total time: 26610.36s
                               ETA: 1101914.8s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.431s, learning 0.162s)
               Value function loss: 12.8290
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 416.70
               Mean episode length: 249.69
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 10.59s
                        Total time: 26620.95s
                               ETA: 1101874.8s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.668s, learning 0.186s)
               Value function loss: 14.5244
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 421.83
               Mean episode length: 249.22
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 10.85s
                        Total time: 26631.80s
                               ETA: 1101845.7s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.727s, learning 0.161s)
               Value function loss: 18.8017
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 417.12
               Mean episode length: 248.39
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 10.89s
                        Total time: 26642.69s
                               ETA: 1101818.0s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.721s, learning 0.172s)
               Value function loss: 18.1086
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 418.92
               Mean episode length: 248.54
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 10.89s
                        Total time: 26653.58s
                               ETA: 1101790.5s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.040s, learning 0.162s)
               Value function loss: 18.6267
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 423.19
               Mean episode length: 249.52
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 11.20s
                        Total time: 26664.79s
                               ETA: 1101775.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.761s, learning 0.188s)
               Value function loss: 22.6246
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 426.35
               Mean episode length: 249.23
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 10.95s
                        Total time: 26675.73s
                               ETA: 1101750.7s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.634s, learning 0.165s)
               Value function loss: 24.4576
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 427.29
               Mean episode length: 249.60
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 10.80s
                        Total time: 26686.53s
                               ETA: 1101719.4s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.823s, learning 0.182s)
               Value function loss: 26.3025
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 425.16
               Mean episode length: 248.56
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 11.00s
                        Total time: 26697.54s
                               ETA: 1101696.6s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.559s, learning 0.247s)
               Value function loss: 22.7311
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 430.41
               Mean episode length: 249.28
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 10.81s
                        Total time: 26708.34s
                               ETA: 1101665.6s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.594s, learning 0.159s)
               Value function loss: 20.3904
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 429.17
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 10.75s
                        Total time: 26719.10s
                               ETA: 1101632.4s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.651s, learning 0.193s)
               Value function loss: 19.6178
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 420.00
               Mean episode length: 249.91
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 10.84s
                        Total time: 26729.94s
                               ETA: 1101603.0s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.528s, learning 0.172s)
               Value function loss: 22.0177
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 425.99
               Mean episode length: 249.91
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 10.70s
                        Total time: 26740.64s
                               ETA: 1101567.7s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.952s, learning 0.191s)
               Value function loss: 20.7987
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 430.84
               Mean episode length: 249.86
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 11.14s
                        Total time: 26751.78s
                               ETA: 1101550.7s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.555s, learning 0.176s)
               Value function loss: 18.4373
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 432.95
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 10.73s
                        Total time: 26762.51s
                               ETA: 1101516.7s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.674s, learning 0.176s)
               Value function loss: 22.7312
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 430.07
               Mean episode length: 250.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 10.85s
                        Total time: 26773.36s
                               ETA: 1101487.6s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.804s, learning 0.260s)
               Value function loss: 17.8042
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 422.06
               Mean episode length: 249.52
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 11.06s
                        Total time: 26784.43s
                               ETA: 1101467.3s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.835s, learning 0.174s)
               Value function loss: 17.6568
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 417.70
               Mean episode length: 249.52
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 11.01s
                        Total time: 26795.44s
                               ETA: 1101444.8s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.705s, learning 0.178s)
               Value function loss: 19.7582
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 419.25
               Mean episode length: 250.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 10.88s
                        Total time: 26806.32s
                               ETA: 1101417.1s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.625s, learning 0.171s)
               Value function loss: 15.5697
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 416.96
               Mean episode length: 250.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 10.80s
                        Total time: 26817.12s
                               ETA: 1101385.9s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.558s, learning 0.182s)
               Value function loss: 17.5545
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 404.95
               Mean episode length: 250.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 10.74s
                        Total time: 26827.86s
                               ETA: 1101352.4s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.500s, learning 0.162s)
               Value function loss: 15.3457
                    Surrogate loss: -0.0197
             Mean action noise std: 0.73
                       Mean reward: 409.88
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 10.66s
                        Total time: 26838.52s
                               ETA: 1101315.7s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.672s, learning 0.168s)
               Value function loss: 17.0529
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 409.44
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 10.84s
                        Total time: 26849.36s
                               ETA: 1101286.2s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.675s, learning 0.255s)
               Value function loss: 16.8735
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 407.19
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 10.93s
                        Total time: 26860.29s
                               ETA: 1101260.6s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.798s, learning 0.174s)
               Value function loss: 15.2607
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 411.56
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 10.97s
                        Total time: 26871.26s
                               ETA: 1101236.6s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.462s, learning 0.161s)
               Value function loss: 15.4767
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 410.26
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 10.62s
                        Total time: 26881.88s
                               ETA: 1101198.3s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.688s, learning 0.165s)
               Value function loss: 14.5722
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 412.92
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 10.85s
                        Total time: 26892.74s
                               ETA: 1101169.6s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.734s, learning 0.187s)
               Value function loss: 15.5991
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 423.95
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 10.92s
                        Total time: 26903.66s
                               ETA: 1101143.6s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.461s, learning 0.189s)
               Value function loss: 14.5379
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 415.90
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 10.65s
                        Total time: 26914.31s
                               ETA: 1101106.5s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1572 steps/s (collection: 10.253s, learning 0.163s)
               Value function loss: 15.6308
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 423.55
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 10.42s
                        Total time: 26924.72s
                               ETA: 1101059.9s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.802s, learning 0.278s)
               Value function loss: 14.5031
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 433.00
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 11.08s
                        Total time: 26935.80s
                               ETA: 1101040.4s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.593s, learning 0.194s)
               Value function loss: 13.3624
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 429.14
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 10.79s
                        Total time: 26946.59s
                               ETA: 1101009.0s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.649s, learning 0.167s)
               Value function loss: 16.2611
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 420.93
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 10.82s
                        Total time: 26957.41s
                               ETA: 1100978.8s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.693s, learning 0.167s)
               Value function loss: 14.0635
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 430.54
               Mean episode length: 249.56
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 10.86s
                        Total time: 26968.27s
                               ETA: 1100950.4s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.658s, learning 0.167s)
               Value function loss: 16.1449
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 436.15
               Mean episode length: 249.56
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 10.82s
                        Total time: 26979.09s
                               ETA: 1100920.6s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.171s)
               Value function loss: 16.8128
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 443.08
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 10.76s
                        Total time: 26989.85s
                               ETA: 1100888.3s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.648s, learning 0.177s)
               Value function loss: 15.5445
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 431.34
               Mean episode length: 249.21
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 10.82s
                        Total time: 27000.68s
                               ETA: 1100858.5s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.474s, learning 0.159s)
               Value function loss: 16.7414
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 435.48
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 10.63s
                        Total time: 27011.31s
                               ETA: 1100820.9s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.725s, learning 0.192s)
               Value function loss: 18.2515
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 441.35
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 10.92s
                        Total time: 27022.23s
                               ETA: 1100794.9s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.494s, learning 0.194s)
               Value function loss: 20.8725
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 443.24
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 10.69s
                        Total time: 27032.92s
                               ETA: 1100759.6s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.661s, learning 0.171s)
               Value function loss: 20.1415
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 440.08
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 10.83s
                        Total time: 27043.75s
                               ETA: 1100730.2s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.866s, learning 0.198s)
               Value function loss: 19.8606
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 444.52
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 11.06s
                        Total time: 27054.81s
                               ETA: 1100710.2s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.690s, learning 0.168s)
               Value function loss: 18.9082
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 437.00
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 10.86s
                        Total time: 27065.67s
                               ETA: 1100681.9s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.631s, learning 0.171s)
               Value function loss: 21.5752
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 446.63
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 10.80s
                        Total time: 27076.47s
                               ETA: 1100651.2s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.465s, learning 0.164s)
               Value function loss: 22.6108
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 441.00
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 10.63s
                        Total time: 27087.10s
                               ETA: 1100613.6s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.490s, learning 0.189s)
               Value function loss: 19.8559
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 440.90
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 10.68s
                        Total time: 27097.78s
                               ETA: 1100578.1s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.524s, learning 0.166s)
               Value function loss: 23.1228
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 442.47
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 10.69s
                        Total time: 27108.47s
                               ETA: 1100542.9s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.638s, learning 0.207s)
               Value function loss: 22.0032
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 441.51
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 10.84s
                        Total time: 27119.31s
                               ETA: 1100514.2s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.364s, learning 0.165s)
               Value function loss: 19.0901
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 442.75
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 10.53s
                        Total time: 27129.84s
                               ETA: 1100472.6s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.850s, learning 0.178s)
               Value function loss: 24.0960
                    Surrogate loss: 0.0029
             Mean action noise std: 0.73
                       Mean reward: 442.60
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 11.03s
                        Total time: 27140.87s
                               ETA: 1100451.3s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.507s, learning 0.195s)
               Value function loss: 17.0237
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 443.86
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 10.70s
                        Total time: 27151.57s
                               ETA: 1100416.7s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.636s, learning 0.176s)
               Value function loss: 18.8225
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 446.85
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 10.81s
                        Total time: 27162.38s
                               ETA: 1100386.6s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.666s, learning 0.210s)
               Value function loss: 17.2575
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 446.82
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 10.88s
                        Total time: 27173.26s
                               ETA: 1100359.2s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.758s, learning 0.177s)
               Value function loss: 18.3782
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 447.30
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 10.94s
                        Total time: 27184.20s
                               ETA: 1100334.2s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.613s, learning 0.182s)
               Value function loss: 22.2819
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 448.08
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 10.80s
                        Total time: 27194.99s
                               ETA: 1100303.5s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.488s, learning 0.163s)
               Value function loss: 22.6277
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 446.47
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 10.65s
                        Total time: 27205.64s
                               ETA: 1100267.0s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.952s, learning 0.189s)
               Value function loss: 20.0323
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 444.34
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 11.14s
                        Total time: 27216.78s
                               ETA: 1100250.3s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.689s, learning 0.193s)
               Value function loss: 22.8581
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 448.60
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 10.88s
                        Total time: 27227.66s
                               ETA: 1100223.1s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.901s, learning 0.190s)
               Value function loss: 20.8009
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 448.17
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 11.09s
                        Total time: 27238.75s
                               ETA: 1100204.4s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.884s, learning 0.263s)
               Value function loss: 17.5536
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 446.66
               Mean episode length: 249.97
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 11.15s
                        Total time: 27249.90s
                               ETA: 1100188.0s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.606s, learning 0.286s)
               Value function loss: 20.5011
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 448.04
               Mean episode length: 249.97
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 10.89s
                        Total time: 27260.79s
                               ETA: 1100161.3s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.495s, learning 0.171s)
               Value function loss: 19.7100
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 448.26
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 10.67s
                        Total time: 27271.46s
                               ETA: 1100125.5s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.484s, learning 0.165s)
               Value function loss: 17.4116
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 448.15
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 10.65s
                        Total time: 27282.11s
                               ETA: 1100089.1s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.469s, learning 0.169s)
               Value function loss: 17.1524
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 444.48
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 10.64s
                        Total time: 27292.75s
                               ETA: 1100052.2s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.694s, learning 0.164s)
               Value function loss: 17.1266
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 442.41
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 10.86s
                        Total time: 27303.61s
                               ETA: 1100024.2s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.960s, learning 0.157s)
               Value function loss: 18.9162
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 447.43
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 11.12s
                        Total time: 27314.72s
                               ETA: 1100006.6s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.460s, learning 0.172s)
               Value function loss: 20.7849
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 444.39
               Mean episode length: 249.97
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 10.63s
                        Total time: 27325.35s
                               ETA: 1099969.5s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.283s, learning 0.250s)
               Value function loss: 16.4839
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 444.25
               Mean episode length: 249.97
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 10.53s
                        Total time: 27335.89s
                               ETA: 1099928.5s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.443s, learning 0.176s)
               Value function loss: 18.6961
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 439.39
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 10.62s
                        Total time: 27346.51s
                               ETA: 1099891.0s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.493s, learning 0.214s)
               Value function loss: 19.7217
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 451.26
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 10.71s
                        Total time: 27357.21s
                               ETA: 1099857.0s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.865s, learning 0.159s)
               Value function loss: 18.8555
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 449.79
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 11.02s
                        Total time: 27368.24s
                               ETA: 1099835.7s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.557s, learning 0.169s)
               Value function loss: 19.2043
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 446.36
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 10.73s
                        Total time: 27378.97s
                               ETA: 1099802.6s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1563 steps/s (collection: 10.321s, learning 0.160s)
               Value function loss: 17.6109
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 440.93
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 10.48s
                        Total time: 27389.45s
                               ETA: 1099759.6s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.642s, learning 0.158s)
               Value function loss: 15.8499
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 437.36
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 10.80s
                        Total time: 27400.25s
                               ETA: 1099729.4s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.634s, learning 0.158s)
               Value function loss: 19.2315
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 438.44
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 10.79s
                        Total time: 27411.04s
                               ETA: 1099698.9s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.598s, learning 0.196s)
               Value function loss: 20.9032
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 445.48
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 10.79s
                        Total time: 27421.83s
                               ETA: 1099668.5s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.809s, learning 0.261s)
               Value function loss: 17.4214
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 444.31
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 11.07s
                        Total time: 27432.90s
                               ETA: 1099649.2s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.842s, learning 0.188s)
               Value function loss: 18.9412
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 445.86
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 11.03s
                        Total time: 27443.93s
                               ETA: 1099628.3s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.591s, learning 0.187s)
               Value function loss: 19.2934
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 443.89
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 10.78s
                        Total time: 27454.71s
                               ETA: 1099597.3s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.582s, learning 0.166s)
               Value function loss: 15.2277
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 446.18
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 10.75s
                        Total time: 27465.46s
                               ETA: 1099565.1s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.109s, learning 0.185s)
               Value function loss: 20.4673
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 447.26
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 11.29s
                        Total time: 27476.75s
                               ETA: 1099554.8s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.804s, learning 0.196s)
               Value function loss: 14.9466
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 448.58
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 11.00s
                        Total time: 27487.75s
                               ETA: 1099532.7s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.399s, learning 0.191s)
               Value function loss: 14.3966
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 447.11
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 10.59s
                        Total time: 27498.35s
                               ETA: 1099494.3s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.566s, learning 0.188s)
               Value function loss: 13.1859
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 442.74
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 10.75s
                        Total time: 27509.10s
                               ETA: 1099462.4s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1455 steps/s (collection: 10.993s, learning 0.261s)
               Value function loss: 15.8263
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 442.87
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 11.25s
                        Total time: 27520.35s
                               ETA: 1099450.5s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.764s, learning 0.163s)
               Value function loss: 18.5391
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 448.65
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 10.93s
                        Total time: 27531.28s
                               ETA: 1099425.6s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.547s, learning 0.161s)
               Value function loss: 21.1835
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 444.84
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 10.71s
                        Total time: 27541.99s
                               ETA: 1099391.9s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.512s, learning 0.160s)
               Value function loss: 15.7534
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 441.72
               Mean episode length: 249.95
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 10.67s
                        Total time: 27552.66s
                               ETA: 1099356.8s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.871s, learning 0.169s)
               Value function loss: 18.1300
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 443.10
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 11.04s
                        Total time: 27563.70s
                               ETA: 1099336.4s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.673s, learning 0.191s)
               Value function loss: 16.6485
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 447.14
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 10.86s
                        Total time: 27574.56s
                               ETA: 1099309.0s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.167s)
               Value function loss: 16.2466
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 447.97
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 10.77s
                        Total time: 27585.34s
                               ETA: 1099277.9s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.502s, learning 0.182s)
               Value function loss: 18.3204
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 445.58
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 10.68s
                        Total time: 27596.02s
                               ETA: 1099243.3s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.626s, learning 0.185s)
               Value function loss: 17.1419
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 438.54
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 10.81s
                        Total time: 27606.83s
                               ETA: 1099213.8s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.167s)
               Value function loss: 13.7767
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 440.56
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 11.02s
                        Total time: 27617.85s
                               ETA: 1099192.6s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.565s, learning 0.173s)
               Value function loss: 14.2593
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 443.87
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 10.74s
                        Total time: 27628.59s
                               ETA: 1099160.2s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.576s, learning 0.159s)
               Value function loss: 15.0025
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 438.73
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 10.73s
                        Total time: 27639.32s
                               ETA: 1099127.8s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.385s, learning 0.173s)
               Value function loss: 14.9037
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 436.67
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 10.56s
                        Total time: 27649.88s
                               ETA: 1099088.3s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1569 steps/s (collection: 10.273s, learning 0.168s)
               Value function loss: 15.9872
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 437.64
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 10.44s
                        Total time: 27660.32s
                               ETA: 1099044.2s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.908s, learning 0.194s)
               Value function loss: 16.0387
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 436.46
               Mean episode length: 249.70
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 11.10s
                        Total time: 27671.42s
                               ETA: 1099026.4s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.422s, learning 0.171s)
               Value function loss: 13.4783
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 433.03
               Mean episode length: 249.69
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 10.59s
                        Total time: 27682.02s
                               ETA: 1098988.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.931s, learning 0.166s)
               Value function loss: 16.4812
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 433.18
               Mean episode length: 249.69
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 11.10s
                        Total time: 27693.11s
                               ETA: 1098970.4s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.816s, learning 0.164s)
               Value function loss: 15.9472
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 432.86
               Mean episode length: 249.88
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 10.98s
                        Total time: 27704.09s
                               ETA: 1098947.8s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.576s, learning 0.198s)
               Value function loss: 16.0036
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 435.84
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 10.77s
                        Total time: 27714.87s
                               ETA: 1098917.1s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.619s, learning 0.178s)
               Value function loss: 16.7579
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 437.45
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 10.80s
                        Total time: 27725.67s
                               ETA: 1098887.2s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.938s, learning 0.166s)
               Value function loss: 17.3361
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 439.99
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 11.10s
                        Total time: 27736.77s
                               ETA: 1098869.5s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.678s, learning 0.164s)
               Value function loss: 19.9428
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 439.31
               Mean episode length: 249.64
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 10.84s
                        Total time: 27747.61s
                               ETA: 1098841.5s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.645s, learning 0.192s)
               Value function loss: 19.6754
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 437.74
               Mean episode length: 249.08
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 10.84s
                        Total time: 27758.45s
                               ETA: 1098813.2s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.762s, learning 0.163s)
               Value function loss: 15.6004
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 436.10
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 10.92s
                        Total time: 27769.37s
                               ETA: 1098788.5s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.713s, learning 0.173s)
               Value function loss: 15.4118
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 443.47
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 10.89s
                        Total time: 27780.26s
                               ETA: 1098762.2s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.612s, learning 0.170s)
               Value function loss: 18.1200
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 438.63
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 10.78s
                        Total time: 27791.04s
                               ETA: 1098731.8s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.474s, learning 0.176s)
               Value function loss: 14.1171
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 425.01
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 10.65s
                        Total time: 27801.69s
                               ETA: 1098696.2s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.535s, learning 0.170s)
               Value function loss: 20.2123
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 437.41
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 10.70s
                        Total time: 27812.40s
                               ETA: 1098662.8s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.644s, learning 0.185s)
               Value function loss: 17.3032
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 441.99
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 10.83s
                        Total time: 27823.23s
                               ETA: 1098634.4s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.169s)
               Value function loss: 15.7914
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 445.34
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 10.95s
                        Total time: 27834.18s
                               ETA: 1098610.8s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.998s, learning 0.168s)
               Value function loss: 15.3324
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 441.69
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 11.17s
                        Total time: 27845.34s
                               ETA: 1098595.6s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.570s, learning 0.218s)
               Value function loss: 18.8265
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 448.40
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 10.79s
                        Total time: 27856.13s
                               ETA: 1098565.5s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.704s, learning 0.172s)
               Value function loss: 21.5095
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 450.04
               Mean episode length: 249.79
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 10.88s
                        Total time: 27867.01s
                               ETA: 1098539.0s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.494s, learning 0.211s)
               Value function loss: 20.9084
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 448.60
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 10.70s
                        Total time: 27877.71s
                               ETA: 1098505.7s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.802s, learning 0.254s)
               Value function loss: 17.2004
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 445.40
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 11.06s
                        Total time: 27888.77s
                               ETA: 1098486.2s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.683s, learning 0.160s)
               Value function loss: 20.4200
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 445.94
               Mean episode length: 249.82
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 10.84s
                        Total time: 27899.61s
                               ETA: 1098458.4s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1567 steps/s (collection: 10.286s, learning 0.169s)
               Value function loss: 20.3086
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 445.74
               Mean episode length: 249.14
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 10.45s
                        Total time: 27910.06s
                               ETA: 1098415.3s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.653s, learning 0.165s)
               Value function loss: 21.8098
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 444.72
               Mean episode length: 249.92
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 10.82s
                        Total time: 27920.88s
                               ETA: 1098386.5s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.636s, learning 0.160s)
               Value function loss: 20.7190
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 447.06
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 10.80s
                        Total time: 27931.68s
                               ETA: 1098356.9s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.545s, learning 0.207s)
               Value function loss: 20.9841
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 445.62
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 10.75s
                        Total time: 27942.43s
                               ETA: 1098325.6s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.590s, learning 0.160s)
               Value function loss: 14.8318
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 443.49
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 10.75s
                        Total time: 27953.18s
                               ETA: 1098294.2s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.166s)
               Value function loss: 16.8646
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 444.34
               Mean episode length: 249.87
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 10.95s
                        Total time: 27964.13s
                               ETA: 1098270.6s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.657s, learning 0.164s)
               Value function loss: 16.3956
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 445.20
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 10.82s
                        Total time: 27974.95s
                               ETA: 1098242.0s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.832s, learning 0.237s)
               Value function loss: 17.1796
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 442.97
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 11.07s
                        Total time: 27986.02s
                               ETA: 1098223.2s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.938s, learning 0.181s)
               Value function loss: 17.7646
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 451.85
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 11.12s
                        Total time: 27997.14s
                               ETA: 1098206.3s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.746s, learning 0.170s)
               Value function loss: 16.8688
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 457.56
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 10.92s
                        Total time: 28008.05s
                               ETA: 1098181.5s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.437s, learning 0.164s)
               Value function loss: 16.6717
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 448.59
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 10.60s
                        Total time: 28018.65s
                               ETA: 1098144.3s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.627s, learning 0.218s)
               Value function loss: 18.9331
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 449.92
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 10.84s
                        Total time: 28029.50s
                               ETA: 1098116.7s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.586s, learning 0.159s)
               Value function loss: 17.5820
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 445.90
               Mean episode length: 249.37
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 10.75s
                        Total time: 28040.24s
                               ETA: 1098085.2s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.929s, learning 0.169s)
               Value function loss: 20.5962
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 442.75
               Mean episode length: 249.37
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 11.10s
                        Total time: 28051.34s
                               ETA: 1098067.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.733s, learning 0.177s)
               Value function loss: 19.7087
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 448.28
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 10.91s
                        Total time: 28062.25s
                               ETA: 1098042.6s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.693s, learning 0.169s)
               Value function loss: 18.4419
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 450.26
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 10.86s
                        Total time: 28073.11s
                               ETA: 1098015.7s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.557s, learning 0.176s)
               Value function loss: 21.0238
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 448.26
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 10.73s
                        Total time: 28083.85s
                               ETA: 1097983.8s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.802s, learning 0.167s)
               Value function loss: 19.0340
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 449.53
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 10.97s
                        Total time: 28094.82s
                               ETA: 1097961.2s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.371s, learning 0.166s)
               Value function loss: 23.1588
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 451.70
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 10.54s
                        Total time: 28105.35s
                               ETA: 1097921.6s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.670s, learning 0.188s)
               Value function loss: 18.3354
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 453.97
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 10.86s
                        Total time: 28116.21s
                               ETA: 1097894.7s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.727s, learning 0.208s)
               Value function loss: 22.8901
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 453.16
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 10.94s
                        Total time: 28127.15s
                               ETA: 1097870.8s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.796s, learning 0.165s)
               Value function loss: 19.7383
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 445.80
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 10.96s
                        Total time: 28138.11s
                               ETA: 1097847.8s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.693s, learning 0.249s)
               Value function loss: 18.5848
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 450.31
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 10.94s
                        Total time: 28149.05s
                               ETA: 1097824.2s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.802s, learning 0.155s)
               Value function loss: 22.4582
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 458.77
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 10.96s
                        Total time: 28160.01s
                               ETA: 1097801.1s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.576s, learning 0.180s)
               Value function loss: 15.6076
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 459.46
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 10.76s
                        Total time: 28170.76s
                               ETA: 1097770.3s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.306s, learning 0.208s)
               Value function loss: 16.3009
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 460.72
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 10.51s
                        Total time: 28181.28s
                               ETA: 1097730.0s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.972s, learning 0.167s)
               Value function loss: 17.5528
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 456.79
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 11.14s
                        Total time: 28192.42s
                               ETA: 1097714.0s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.633s, learning 0.165s)
               Value function loss: 19.1709
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 456.40
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 10.80s
                        Total time: 28203.21s
                               ETA: 1097684.8s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.783s, learning 0.172s)
               Value function loss: 17.1297
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 454.73
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 10.96s
                        Total time: 28214.17s
                               ETA: 1097661.8s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.445s, learning 0.160s)
               Value function loss: 17.3472
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 452.88
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 10.61s
                        Total time: 28224.77s
                               ETA: 1097625.1s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.729s, learning 0.169s)
               Value function loss: 16.7240
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 454.25
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 10.90s
                        Total time: 28235.67s
                               ETA: 1097599.8s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.714s, learning 0.170s)
               Value function loss: 20.6372
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 453.08
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 10.88s
                        Total time: 28246.56s
                               ETA: 1097574.0s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.715s, learning 0.173s)
               Value function loss: 18.4207
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 456.96
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 10.89s
                        Total time: 28257.44s
                               ETA: 1097548.4s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.534s, learning 0.186s)
               Value function loss: 15.3040
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 455.36
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 10.72s
                        Total time: 28268.16s
                               ETA: 1097516.2s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.682s, learning 0.165s)
               Value function loss: 18.8361
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 454.46
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 10.85s
                        Total time: 28279.01s
                               ETA: 1097489.1s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.554s, learning 0.158s)
               Value function loss: 16.0495
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 455.24
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 10.71s
                        Total time: 28289.72s
                               ETA: 1097456.6s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.527s, learning 0.187s)
               Value function loss: 13.7357
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 455.81
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 10.71s
                        Total time: 28300.44s
                               ETA: 1097424.3s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.653s, learning 0.172s)
               Value function loss: 19.0088
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 457.59
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 10.82s
                        Total time: 28311.26s
                               ETA: 1097396.3s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.596s, learning 0.211s)
               Value function loss: 13.7839
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 456.44
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 10.81s
                        Total time: 28322.07s
                               ETA: 1097367.6s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.629s, learning 0.192s)
               Value function loss: 14.7940
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 455.62
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 10.82s
                        Total time: 28332.89s
                               ETA: 1097339.5s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.823s, learning 0.164s)
               Value function loss: 17.4321
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 458.69
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 10.99s
                        Total time: 28343.88s
                               ETA: 1097317.8s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.626s, learning 0.173s)
               Value function loss: 17.0019
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 455.91
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 10.80s
                        Total time: 28354.68s
                               ETA: 1097288.8s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.789s, learning 0.190s)
               Value function loss: 18.6968
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 455.90
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 10.98s
                        Total time: 28365.65s
                               ETA: 1097266.8s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.946s, learning 0.170s)
               Value function loss: 20.8188
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 457.37
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 11.12s
                        Total time: 28376.77s
                               ETA: 1097250.1s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.358s, learning 0.164s)
               Value function loss: 20.7942
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 456.53
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 10.52s
                        Total time: 28387.29s
                               ETA: 1097210.5s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.568s, learning 0.188s)
               Value function loss: 19.7078
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 456.86
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 10.76s
                        Total time: 28398.05s
                               ETA: 1097179.9s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.689s, learning 0.187s)
               Value function loss: 18.1418
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 455.34
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 10.88s
                        Total time: 28408.92s
                               ETA: 1097154.0s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.537s, learning 0.171s)
               Value function loss: 17.3438
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 456.37
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 10.71s
                        Total time: 28419.63s
                               ETA: 1097121.6s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.560s, learning 0.163s)
               Value function loss: 19.6804
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 457.10
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 10.72s
                        Total time: 28430.35s
                               ETA: 1097089.8s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.383s, learning 0.170s)
               Value function loss: 19.2089
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 457.53
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 10.55s
                        Total time: 28440.91s
                               ETA: 1097051.5s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.167s)
               Value function loss: 17.1261
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 460.31
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 10.70s
                        Total time: 28451.61s
                               ETA: 1097018.8s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.893s, learning 0.167s)
               Value function loss: 19.5272
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 457.28
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 11.06s
                        Total time: 28462.67s
                               ETA: 1097000.1s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.667s, learning 0.180s)
               Value function loss: 17.3786
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 453.08
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 10.85s
                        Total time: 28473.51s
                               ETA: 1096973.1s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.395s, learning 0.158s)
               Value function loss: 16.3107
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 454.12
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 10.55s
                        Total time: 28484.07s
                               ETA: 1096934.8s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.402s, learning 0.163s)
               Value function loss: 19.3399
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 457.12
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 10.57s
                        Total time: 28494.63s
                               ETA: 1096897.1s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.649s, learning 0.161s)
               Value function loss: 15.1629
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 455.21
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 10.81s
                        Total time: 28505.44s
                               ETA: 1096868.7s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.499s, learning 0.236s)
               Value function loss: 15.5714
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 456.57
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 10.73s
                        Total time: 28516.18s
                               ETA: 1096837.5s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.745s, learning 0.163s)
               Value function loss: 13.2058
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 456.95
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 10.91s
                        Total time: 28527.09s
                               ETA: 1096813.0s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.774s, learning 0.233s)
               Value function loss: 14.4641
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 456.62
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 11.01s
                        Total time: 28538.09s
                               ETA: 1096792.3s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.827s, learning 0.164s)
               Value function loss: 17.2336
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 456.50
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 10.99s
                        Total time: 28549.08s
                               ETA: 1096771.0s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.570s, learning 0.170s)
               Value function loss: 17.5287
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 456.33
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 10.74s
                        Total time: 28559.82s
                               ETA: 1096740.0s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.568s, learning 0.170s)
               Value function loss: 15.2637
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 457.40
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 10.74s
                        Total time: 28570.56s
                               ETA: 1096709.0s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.660s, learning 0.163s)
               Value function loss: 16.6044
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 461.70
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 10.82s
                        Total time: 28581.39s
                               ETA: 1096681.3s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.902s, learning 0.168s)
               Value function loss: 14.9866
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 459.77
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 11.07s
                        Total time: 28592.46s
                               ETA: 1096663.0s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.689s, learning 0.182s)
               Value function loss: 12.9634
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 457.69
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 10.87s
                        Total time: 28603.33s
                               ETA: 1096637.1s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.634s, learning 0.159s)
               Value function loss: 15.3683
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 458.85
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 10.79s
                        Total time: 28614.12s
                               ETA: 1096608.2s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.677s, learning 0.164s)
               Value function loss: 13.6001
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 455.96
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 10.84s
                        Total time: 28624.96s
                               ETA: 1096581.3s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.700s, learning 0.283s)
               Value function loss: 14.3733
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 454.67
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 10.98s
                        Total time: 28635.94s
                               ETA: 1096559.7s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.886s, learning 0.163s)
               Value function loss: 13.6298
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 459.12
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 11.05s
                        Total time: 28646.99s
                               ETA: 1096540.7s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.754s, learning 0.171s)
               Value function loss: 12.2184
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 462.49
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 10.92s
                        Total time: 28657.92s
                               ETA: 1096516.9s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.748s, learning 0.209s)
               Value function loss: 16.6964
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 461.62
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 10.96s
                        Total time: 28668.87s
                               ETA: 1096494.4s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.601s, learning 0.173s)
               Value function loss: 17.1757
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 461.57
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 10.77s
                        Total time: 28679.65s
                               ETA: 1096464.9s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.537s, learning 0.214s)
               Value function loss: 15.5577
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 462.11
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 10.75s
                        Total time: 28690.40s
                               ETA: 1096434.5s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.575s, learning 0.161s)
               Value function loss: 16.8917
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 461.83
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 10.74s
                        Total time: 28701.13s
                               ETA: 1096403.6s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.342s, learning 0.189s)
               Value function loss: 17.9784
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 459.02
               Mean episode length: 250.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 10.53s
                        Total time: 28711.66s
                               ETA: 1096364.8s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.763s, learning 0.165s)
               Value function loss: 17.2363
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 458.76
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 10.93s
                        Total time: 28722.59s
                               ETA: 1096341.3s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.805s, learning 0.158s)
               Value function loss: 17.0649
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 462.79
               Mean episode length: 250.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 10.96s
                        Total time: 28733.56s
                               ETA: 1096319.0s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.579s, learning 0.256s)
               Value function loss: 16.7972
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 459.23
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 10.83s
                        Total time: 28744.39s
                               ETA: 1096291.9s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.693s, learning 0.188s)
               Value function loss: 13.2781
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 460.71
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 10.88s
                        Total time: 28755.27s
                               ETA: 1096266.6s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.464s, learning 0.171s)
               Value function loss: 20.5905
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 460.64
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 10.64s
                        Total time: 28765.91s
                               ETA: 1096231.9s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.779s, learning 0.162s)
               Value function loss: 19.4684
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 460.50
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 10.94s
                        Total time: 28776.85s
                               ETA: 1096208.9s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.675s, learning 0.161s)
               Value function loss: 16.0499
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 460.29
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 10.84s
                        Total time: 28787.68s
                               ETA: 1096181.8s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.700s, learning 0.160s)
               Value function loss: 15.8135
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 458.98
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 10.86s
                        Total time: 28798.54s
                               ETA: 1096155.8s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.601s, learning 0.165s)
               Value function loss: 15.2720
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 459.12
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 10.77s
                        Total time: 28809.31s
                               ETA: 1096126.1s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.838s, learning 0.157s)
               Value function loss: 14.2111
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 458.47
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 11.00s
                        Total time: 28820.30s
                               ETA: 1096105.2s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.642s, learning 0.168s)
               Value function loss: 19.0425
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 458.11
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 10.81s
                        Total time: 28831.11s
                               ETA: 1096077.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.831s, learning 0.173s)
               Value function loss: 14.5755
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 457.69
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 11.00s
                        Total time: 28842.12s
                               ETA: 1096056.7s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.531s, learning 0.174s)
               Value function loss: 16.6834
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 457.40
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 10.71s
                        Total time: 28852.82s
                               ETA: 1096024.8s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.708s, learning 0.161s)
               Value function loss: 11.7614
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 455.84
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 10.87s
                        Total time: 28863.69s
                               ETA: 1095999.2s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.522s, learning 0.162s)
               Value function loss: 13.7895
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 457.33
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 10.68s
                        Total time: 28874.38s
                               ETA: 1095966.5s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.649s, learning 0.165s)
               Value function loss: 18.9708
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 456.78
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 10.81s
                        Total time: 28885.19s
                               ETA: 1095938.8s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.743s, learning 0.237s)
               Value function loss: 19.9993
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 456.32
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 10.98s
                        Total time: 28896.17s
                               ETA: 1095917.4s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.870s, learning 0.255s)
               Value function loss: 17.9445
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 454.97
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 11.13s
                        Total time: 28907.30s
                               ETA: 1095901.5s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.797s, learning 0.188s)
               Value function loss: 19.1139
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 451.17
               Mean episode length: 249.46
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 10.99s
                        Total time: 28918.28s
                               ETA: 1095880.2s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.823s, learning 0.186s)
               Value function loss: 18.7090
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 454.46
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 11.01s
                        Total time: 28929.29s
                               ETA: 1095859.9s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.918s, learning 0.172s)
               Value function loss: 19.8790
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 454.81
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 11.09s
                        Total time: 28940.38s
                               ETA: 1095842.7s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.658s, learning 0.186s)
               Value function loss: 19.5243
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 454.55
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 10.84s
                        Total time: 28951.22s
                               ETA: 1095816.2s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.949s, learning 0.162s)
               Value function loss: 20.4519
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 456.60
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 11.11s
                        Total time: 28962.33s
                               ETA: 1095799.8s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.701s, learning 0.165s)
               Value function loss: 19.1320
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 461.16
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 10.87s
                        Total time: 28973.20s
                               ETA: 1095774.1s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.587s, learning 0.182s)
               Value function loss: 16.6063
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 450.63
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 10.77s
                        Total time: 28983.97s
                               ETA: 1095744.8s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.717s, learning 0.159s)
               Value function loss: 18.7047
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 450.37
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 10.88s
                        Total time: 28994.85s
                               ETA: 1095719.5s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.698s, learning 0.171s)
               Value function loss: 17.3863
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 457.30
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 10.87s
                        Total time: 29005.71s
                               ETA: 1095693.9s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.790s, learning 0.161s)
               Value function loss: 20.6631
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 460.52
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 10.95s
                        Total time: 29016.67s
                               ETA: 1095671.5s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.564s, learning 0.167s)
               Value function loss: 18.3565
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 461.09
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 10.73s
                        Total time: 29027.40s
                               ETA: 1095640.8s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.731s, learning 0.170s)
               Value function loss: 19.7757
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 460.42
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 10.90s
                        Total time: 29038.30s
                               ETA: 1095616.5s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.670s, learning 0.162s)
               Value function loss: 22.7121
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 455.51
               Mean episode length: 250.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 10.83s
                        Total time: 29049.13s
                               ETA: 1095589.7s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.979s, learning 0.169s)
               Value function loss: 21.4273
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 457.13
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 11.15s
                        Total time: 29060.28s
                               ETA: 1095574.7s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.451s, learning 0.165s)
               Value function loss: 22.6393
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 454.51
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 10.62s
                        Total time: 29070.89s
                               ETA: 1095539.7s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.840s, learning 0.159s)
               Value function loss: 23.5241
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 452.17
               Mean episode length: 250.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 11.00s
                        Total time: 29081.89s
                               ETA: 1095519.1s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.606s, learning 0.285s)
               Value function loss: 23.3833
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 455.69
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 10.89s
                        Total time: 29092.78s
                               ETA: 1095494.5s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.896s, learning 0.159s)
               Value function loss: 26.4821
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 453.48
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 11.06s
                        Total time: 29103.84s
                               ETA: 1095476.1s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.867s, learning 0.169s)
               Value function loss: 26.1785
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 458.83
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 11.04s
                        Total time: 29114.87s
                               ETA: 1095456.9s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.617s, learning 0.178s)
               Value function loss: 24.1586
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 458.64
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 10.79s
                        Total time: 29125.67s
                               ETA: 1095428.7s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.435s, learning 0.159s)
               Value function loss: 23.4727
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 453.48
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 10.59s
                        Total time: 29136.26s
                               ETA: 1095393.0s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1565 steps/s (collection: 10.305s, learning 0.160s)
               Value function loss: 25.7884
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 459.45
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 10.47s
                        Total time: 29146.73s
                               ETA: 1095352.4s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.466s, learning 0.160s)
               Value function loss: 20.3592
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 456.68
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 10.63s
                        Total time: 29157.35s
                               ETA: 1095318.0s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.725s, learning 0.188s)
               Value function loss: 24.7079
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 457.04
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 10.91s
                        Total time: 29168.27s
                               ETA: 1095294.3s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.680s, learning 0.170s)
               Value function loss: 23.8625
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 460.30
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 10.85s
                        Total time: 29179.12s
                               ETA: 1095268.2s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.789s, learning 0.159s)
               Value function loss: 20.0166
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 458.12
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 10.95s
                        Total time: 29190.06s
                               ETA: 1095245.8s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.695s, learning 0.197s)
               Value function loss: 20.3246
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 460.42
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 10.89s
                        Total time: 29200.96s
                               ETA: 1095221.4s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.532s, learning 0.159s)
               Value function loss: 21.5668
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 458.90
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 10.69s
                        Total time: 29211.65s
                               ETA: 1095189.4s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.703s, learning 0.162s)
               Value function loss: 28.1198
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 459.53
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 10.86s
                        Total time: 29222.51s
                               ETA: 1095163.9s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.822s, learning 0.190s)
               Value function loss: 25.0925
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 462.22
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 11.01s
                        Total time: 29233.52s
                               ETA: 1095144.0s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.790s, learning 0.183s)
               Value function loss: 22.8680
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 451.62
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 10.97s
                        Total time: 29244.50s
                               ETA: 1095122.6s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.595s, learning 0.182s)
               Value function loss: 28.0675
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 455.34
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 10.78s
                        Total time: 29255.27s
                               ETA: 1095093.9s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.870s, learning 0.161s)
               Value function loss: 25.3941
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 453.91
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 11.03s
                        Total time: 29266.30s
                               ETA: 1095074.7s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.637s, learning 0.168s)
               Value function loss: 27.1681
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 456.04
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 10.80s
                        Total time: 29277.11s
                               ETA: 1095047.1s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.663s, learning 0.160s)
               Value function loss: 21.9221
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 448.16
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 10.82s
                        Total time: 29287.93s
                               ETA: 1095020.1s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.688s, learning 0.160s)
               Value function loss: 22.2416
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 454.68
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 10.85s
                        Total time: 29298.78s
                               ETA: 1094994.2s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.773s, learning 0.186s)
               Value function loss: 17.7221
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 455.08
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 10.96s
                        Total time: 29309.74s
                               ETA: 1094972.3s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.827s, learning 0.164s)
               Value function loss: 19.6305
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 452.70
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 10.99s
                        Total time: 29320.73s
                               ETA: 1094951.7s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.736s, learning 0.167s)
               Value function loss: 19.6681
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 451.29
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 10.90s
                        Total time: 29331.63s
                               ETA: 1094927.8s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.912s, learning 0.166s)
               Value function loss: 18.8846
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 452.07
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 11.08s
                        Total time: 29342.71s
                               ETA: 1094910.4s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.795s, learning 0.164s)
               Value function loss: 19.5930
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 451.73
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 10.96s
                        Total time: 29353.67s
                               ETA: 1094888.6s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.164s)
               Value function loss: 18.1581
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 452.59
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 10.77s
                        Total time: 29364.44s
                               ETA: 1094859.6s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.603s, learning 0.165s)
               Value function loss: 16.7368
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 453.77
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 10.77s
                        Total time: 29375.21s
                               ETA: 1094830.7s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.426s, learning 0.171s)
               Value function loss: 18.6327
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 447.03
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 10.60s
                        Total time: 29385.80s
                               ETA: 1094795.5s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.510s, learning 0.189s)
               Value function loss: 20.3465
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 444.80
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 10.70s
                        Total time: 29396.50s
                               ETA: 1094764.0s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.064s, learning 0.164s)
               Value function loss: 19.3886
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 446.36
               Mean episode length: 250.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 11.23s
                        Total time: 29407.73s
                               ETA: 1094752.3s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.070s, learning 0.171s)
               Value function loss: 18.0675
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 441.74
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 11.24s
                        Total time: 29418.97s
                               ETA: 1094741.0s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.549s, learning 0.180s)
               Value function loss: 17.2245
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 446.23
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 10.73s
                        Total time: 29429.70s
                               ETA: 1094710.7s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.668s, learning 0.164s)
               Value function loss: 17.2296
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 452.67
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 10.83s
                        Total time: 29440.53s
                               ETA: 1094684.3s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.469s, learning 0.166s)
               Value function loss: 16.7104
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 452.92
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 10.63s
                        Total time: 29451.17s
                               ETA: 1094650.5s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.734s, learning 0.284s)
               Value function loss: 19.8587
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 449.27
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 11.02s
                        Total time: 29462.19s
                               ETA: 1094631.0s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.742s, learning 0.182s)
               Value function loss: 16.0550
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 455.72
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 10.92s
                        Total time: 29473.11s
                               ETA: 1094608.0s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.451s, learning 0.168s)
               Value function loss: 18.5101
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 453.79
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 10.62s
                        Total time: 29483.73s
                               ETA: 1094573.6s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.778s, learning 0.175s)
               Value function loss: 17.0548
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 451.60
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 10.95s
                        Total time: 29494.68s
                               ETA: 1094551.8s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.778s, learning 0.170s)
               Value function loss: 15.8203
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 456.16
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 10.95s
                        Total time: 29505.63s
                               ETA: 1094529.7s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.668s, learning 0.171s)
               Value function loss: 19.5934
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 452.72
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 10.84s
                        Total time: 29516.47s
                               ETA: 1094503.6s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1560 steps/s (collection: 10.323s, learning 0.174s)
               Value function loss: 12.5618
                    Surrogate loss: -0.0209
             Mean action noise std: 0.73
                       Mean reward: 453.02
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 10.50s
                        Total time: 29526.97s
                               ETA: 1094464.8s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.577s, learning 0.164s)
               Value function loss: 18.4617
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 456.79
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 10.74s
                        Total time: 29537.71s
                               ETA: 1094435.1s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.646s, learning 0.161s)
               Value function loss: 17.7671
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 456.29
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 10.81s
                        Total time: 29548.52s
                               ETA: 1094407.8s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.595s, learning 0.224s)
               Value function loss: 21.4188
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 455.34
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 10.82s
                        Total time: 29559.34s
                               ETA: 1094381.0s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.490s, learning 0.170s)
               Value function loss: 19.4305
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 457.86
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 10.66s
                        Total time: 29570.00s
                               ETA: 1094348.4s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.461s, learning 0.188s)
               Value function loss: 18.3474
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 451.65
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 10.65s
                        Total time: 29580.65s
                               ETA: 1094315.3s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.627s, learning 0.166s)
               Value function loss: 20.1692
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 448.29
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 10.79s
                        Total time: 29591.44s
                               ETA: 1094287.6s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.616s, learning 0.171s)
               Value function loss: 21.8182
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 456.36
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 10.79s
                        Total time: 29602.23s
                               ETA: 1094259.6s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.623s, learning 0.162s)
               Value function loss: 21.6240
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 454.66
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 10.79s
                        Total time: 29613.01s
                               ETA: 1094231.7s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.484s, learning 0.163s)
               Value function loss: 17.9644
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 456.14
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 10.65s
                        Total time: 29623.66s
                               ETA: 1094198.6s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.799s, learning 0.186s)
               Value function loss: 21.7993
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 446.91
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 10.98s
                        Total time: 29634.64s
                               ETA: 1094178.0s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.169s)
               Value function loss: 19.2505
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 444.46
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 10.77s
                        Total time: 29645.42s
                               ETA: 1094149.6s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.696s, learning 0.211s)
               Value function loss: 15.6207
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 443.25
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 10.91s
                        Total time: 29656.32s
                               ETA: 1094126.2s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.530s, learning 0.162s)
               Value function loss: 20.0549
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 446.75
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 10.69s
                        Total time: 29667.02s
                               ETA: 1094094.8s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.619s, learning 0.157s)
               Value function loss: 16.9701
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 444.80
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 10.78s
                        Total time: 29677.79s
                               ETA: 1094066.6s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.572s, learning 0.338s)
               Value function loss: 18.5841
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 451.50
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 10.91s
                        Total time: 29688.70s
                               ETA: 1094043.3s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.860s, learning 0.167s)
               Value function loss: 20.4425
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 446.75
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 11.03s
                        Total time: 29699.73s
                               ETA: 1094024.3s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.701s, learning 0.192s)
               Value function loss: 19.0882
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 445.45
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 10.89s
                        Total time: 29710.62s
                               ETA: 1094000.4s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.560s, learning 0.221s)
               Value function loss: 21.1095
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 445.01
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 10.78s
                        Total time: 29721.40s
                               ETA: 1093972.4s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.592s, learning 0.170s)
               Value function loss: 22.1044
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 446.03
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 10.76s
                        Total time: 29732.17s
                               ETA: 1093943.7s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.463s, learning 0.168s)
               Value function loss: 24.1179
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 442.06
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 10.63s
                        Total time: 29742.80s
                               ETA: 1093910.2s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.639s, learning 0.175s)
               Value function loss: 21.1519
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 445.89
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 10.81s
                        Total time: 29753.61s
                               ETA: 1093883.4s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.592s, learning 0.159s)
               Value function loss: 19.1512
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 445.50
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 10.75s
                        Total time: 29764.36s
                               ETA: 1093854.4s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.342s, learning 0.162s)
               Value function loss: 18.1695
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 448.24
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 10.50s
                        Total time: 29774.87s
                               ETA: 1093816.3s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.586s, learning 0.166s)
               Value function loss: 21.6939
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 449.49
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 10.75s
                        Total time: 29785.62s
                               ETA: 1093787.3s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.821s, learning 0.166s)
               Value function loss: 19.2120
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 446.44
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 10.99s
                        Total time: 29796.61s
                               ETA: 1093766.9s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.507s, learning 0.174s)
               Value function loss: 19.5956
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 446.53
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 10.68s
                        Total time: 29807.29s
                               ETA: 1093735.3s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1576 steps/s (collection: 10.232s, learning 0.159s)
               Value function loss: 21.8032
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 451.47
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 10.39s
                        Total time: 29817.68s
                               ETA: 1093693.1s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.866s, learning 0.185s)
               Value function loss: 19.8344
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 449.12
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 11.05s
                        Total time: 29828.73s
                               ETA: 1093675.1s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.757s, learning 0.159s)
               Value function loss: 18.8085
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 444.78
               Mean episode length: 249.82
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 10.92s
                        Total time: 29839.65s
                               ETA: 1093652.2s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.589s, learning 0.243s)
               Value function loss: 21.6485
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 445.94
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 10.83s
                        Total time: 29850.48s
                               ETA: 1093626.2s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.489s, learning 0.176s)
               Value function loss: 15.9981
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 446.86
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 10.66s
                        Total time: 29861.14s
                               ETA: 1093594.1s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.443s, learning 0.210s)
               Value function loss: 17.9547
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 451.37
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 10.65s
                        Total time: 29871.80s
                               ETA: 1093561.6s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.648s, learning 0.215s)
               Value function loss: 14.6992
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 449.36
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 10.86s
                        Total time: 29882.66s
                               ETA: 1093536.8s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.571s, learning 0.172s)
               Value function loss: 17.5067
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 447.87
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 10.74s
                        Total time: 29893.40s
                               ETA: 1093507.6s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.673s, learning 0.167s)
               Value function loss: 21.5400
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 440.36
               Mean episode length: 249.98
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 10.84s
                        Total time: 29904.24s
                               ETA: 1093482.0s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.697s, learning 0.164s)
               Value function loss: 20.8505
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 445.64
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 10.86s
                        Total time: 29915.10s
                               ETA: 1093457.1s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.451s, learning 0.163s)
               Value function loss: 18.0725
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 448.62
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 10.61s
                        Total time: 29925.72s
                               ETA: 1093423.2s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.548s, learning 0.166s)
               Value function loss: 20.7142
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 447.97
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 10.71s
                        Total time: 29936.43s
                               ETA: 1093393.0s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.843s, learning 0.159s)
               Value function loss: 19.5635
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 445.19
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 11.00s
                        Total time: 29947.43s
                               ETA: 1093373.3s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.628s, learning 0.171s)
               Value function loss: 17.6211
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 445.45
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 10.80s
                        Total time: 29958.23s
                               ETA: 1093346.3s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.463s, learning 0.164s)
               Value function loss: 18.5871
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 444.73
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 10.63s
                        Total time: 29968.86s
                               ETA: 1093312.9s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.791s, learning 0.162s)
               Value function loss: 19.3356
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 448.01
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 10.95s
                        Total time: 29979.81s
                               ETA: 1093291.5s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.615s, learning 0.160s)
               Value function loss: 16.8631
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 449.26
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 10.77s
                        Total time: 29990.58s
                               ETA: 1093263.5s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.836s, learning 0.164s)
               Value function loss: 17.8775
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 447.96
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 11.00s
                        Total time: 30001.59s
                               ETA: 1093243.8s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.664s, learning 0.157s)
               Value function loss: 17.5569
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 449.70
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 10.82s
                        Total time: 30012.41s
                               ETA: 1093217.6s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.769s, learning 0.171s)
               Value function loss: 18.6655
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 449.88
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 10.94s
                        Total time: 30023.35s
                               ETA: 1093195.8s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.374s, learning 0.190s)
               Value function loss: 19.4741
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 451.14
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 10.56s
                        Total time: 30033.91s
                               ETA: 1093160.2s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.511s, learning 0.161s)
               Value function loss: 18.3966
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 447.66
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 10.67s
                        Total time: 30044.58s
                               ETA: 1093128.6s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.636s, learning 0.161s)
               Value function loss: 19.0238
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 444.24
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 10.80s
                        Total time: 30055.38s
                               ETA: 1093101.6s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.639s, learning 0.161s)
               Value function loss: 20.4005
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 445.74
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 10.80s
                        Total time: 30066.18s
                               ETA: 1093074.7s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.622s, learning 0.187s)
               Value function loss: 17.8673
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 447.17
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 10.81s
                        Total time: 30076.99s
                               ETA: 1093048.1s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.726s, learning 0.186s)
               Value function loss: 20.5692
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 445.50
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 10.91s
                        Total time: 30087.90s
                               ETA: 1093025.3s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.436s, learning 0.161s)
               Value function loss: 17.3749
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 445.77
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 10.60s
                        Total time: 30098.50s
                               ETA: 1092991.0s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.754s, learning 0.206s)
               Value function loss: 15.2518
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 442.56
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 10.96s
                        Total time: 30109.46s
                               ETA: 1092970.0s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.758s, learning 0.183s)
               Value function loss: 19.9734
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 448.04
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 10.94s
                        Total time: 30120.40s
                               ETA: 1092948.2s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.460s, learning 0.187s)
               Value function loss: 18.2077
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 449.42
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 10.65s
                        Total time: 30131.05s
                               ETA: 1092915.8s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.727s, learning 0.219s)
               Value function loss: 16.2259
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 448.39
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 10.95s
                        Total time: 30141.99s
                               ETA: 1092894.3s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.714s, learning 0.160s)
               Value function loss: 16.9322
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 443.61
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 10.87s
                        Total time: 30152.87s
                               ETA: 1092870.1s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.893s, learning 0.188s)
               Value function loss: 18.6376
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 442.57
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 11.08s
                        Total time: 30163.95s
                               ETA: 1092853.5s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.616s, learning 0.161s)
               Value function loss: 15.7264
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 445.52
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 10.78s
                        Total time: 30174.72s
                               ETA: 1092825.9s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.697s, learning 0.171s)
               Value function loss: 18.7882
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 443.63
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 10.87s
                        Total time: 30185.59s
                               ETA: 1092801.6s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.779s, learning 0.284s)
               Value function loss: 13.3369
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 439.88
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 11.06s
                        Total time: 30196.66s
                               ETA: 1092784.3s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.531s, learning 0.167s)
               Value function loss: 14.7202
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 442.69
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 10.70s
                        Total time: 30207.35s
                               ETA: 1092753.8s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.786s, learning 0.177s)
               Value function loss: 12.6829
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 446.21
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 10.96s
                        Total time: 30218.32s
                               ETA: 1092733.0s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.547s, learning 0.172s)
               Value function loss: 14.4095
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 443.82
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 10.72s
                        Total time: 30229.04s
                               ETA: 1092703.3s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.871s, learning 0.209s)
               Value function loss: 18.4499
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 446.43
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 11.08s
                        Total time: 30240.12s
                               ETA: 1092686.7s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.483s, learning 0.169s)
               Value function loss: 19.9412
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 445.71
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 10.65s
                        Total time: 30250.77s
                               ETA: 1092654.6s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.724s, learning 0.187s)
               Value function loss: 15.7949
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 445.70
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 10.91s
                        Total time: 30261.68s
                               ETA: 1092631.9s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.551s, learning 0.170s)
               Value function loss: 17.6704
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 447.70
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 10.72s
                        Total time: 30272.40s
                               ETA: 1092602.3s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.517s, learning 0.172s)
               Value function loss: 16.8462
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 445.94
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 10.69s
                        Total time: 30283.09s
                               ETA: 1092571.6s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.478s, learning 0.160s)
               Value function loss: 16.4884
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 443.38
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 10.64s
                        Total time: 30293.73s
                               ETA: 1092539.1s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.714s, learning 0.169s)
               Value function loss: 17.5490
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 448.00
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 10.88s
                        Total time: 30304.61s
                               ETA: 1092515.4s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1566 steps/s (collection: 10.303s, learning 0.158s)
               Value function loss: 18.2490
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 443.77
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 10.46s
                        Total time: 30315.07s
                               ETA: 1092476.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.700s, learning 0.166s)
               Value function loss: 16.2602
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 443.35
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 10.87s
                        Total time: 30325.94s
                               ETA: 1092452.3s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.711s, learning 0.180s)
               Value function loss: 15.7751
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 442.77
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 10.89s
                        Total time: 30336.83s
                               ETA: 1092428.9s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.559s, learning 0.176s)
               Value function loss: 17.1785
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 443.39
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 10.73s
                        Total time: 30347.56s
                               ETA: 1092400.0s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.451s, learning 0.184s)
               Value function loss: 16.5094
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 442.46
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 10.63s
                        Total time: 30358.20s
                               ETA: 1092367.4s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.917s, learning 0.175s)
               Value function loss: 19.5992
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 448.57
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 11.09s
                        Total time: 30369.29s
                               ETA: 1092351.4s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.324s, learning 0.164s)
               Value function loss: 19.3639
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 446.86
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 10.49s
                        Total time: 30379.78s
                               ETA: 1092313.6s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.451s, learning 0.190s)
               Value function loss: 20.2697
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 447.43
               Mean episode length: 249.64
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 10.64s
                        Total time: 30390.42s
                               ETA: 1092281.3s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.467s, learning 0.167s)
               Value function loss: 19.1475
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 449.73
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 10.63s
                        Total time: 30401.05s
                               ETA: 1092248.8s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.684s, learning 0.175s)
               Value function loss: 20.3281
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 446.91
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 10.86s
                        Total time: 30411.91s
                               ETA: 1092224.4s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.693s, learning 0.186s)
               Value function loss: 18.5536
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 444.03
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 10.88s
                        Total time: 30422.79s
                               ETA: 1092200.7s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.658s, learning 0.160s)
               Value function loss: 19.0826
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 449.37
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 10.82s
                        Total time: 30433.61s
                               ETA: 1092174.8s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.654s, learning 0.163s)
               Value function loss: 18.5532
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 450.57
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 10.82s
                        Total time: 30444.43s
                               ETA: 1092148.9s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.708s, learning 0.160s)
               Value function loss: 21.2622
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 445.44
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 10.87s
                        Total time: 30455.30s
                               ETA: 1092124.9s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.473s, learning 0.169s)
               Value function loss: 20.2704
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 448.48
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 10.64s
                        Total time: 30465.94s
                               ETA: 1092092.7s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.726s, learning 0.164s)
               Value function loss: 18.3946
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 446.50
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 10.89s
                        Total time: 30476.83s
                               ETA: 1092069.5s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.787s, learning 0.161s)
               Value function loss: 18.8319
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 444.97
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 10.95s
                        Total time: 30487.78s
                               ETA: 1092048.4s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.753s, learning 0.161s)
               Value function loss: 19.9394
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 445.35
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 10.91s
                        Total time: 30498.69s
                               ETA: 1092026.0s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.780s, learning 0.184s)
               Value function loss: 16.4286
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 446.16
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 10.96s
                        Total time: 30509.66s
                               ETA: 1092005.5s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.655s, learning 0.198s)
               Value function loss: 21.0539
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 446.44
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 10.85s
                        Total time: 30520.51s
                               ETA: 1091980.9s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.846s, learning 0.263s)
               Value function loss: 18.6683
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 446.76
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 11.11s
                        Total time: 30531.62s
                               ETA: 1091965.5s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.676s, learning 0.157s)
               Value function loss: 16.1645
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 447.17
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 10.83s
                        Total time: 30542.45s
                               ETA: 1091940.3s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.841s, learning 0.168s)
               Value function loss: 14.8821
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 448.95
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 11.01s
                        Total time: 30553.46s
                               ETA: 1091921.4s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.544s, learning 0.182s)
               Value function loss: 17.0578
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 445.24
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 10.73s
                        Total time: 30564.19s
                               ETA: 1091892.3s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.643s, learning 0.186s)
               Value function loss: 19.7211
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 447.14
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 10.83s
                        Total time: 30575.01s
                               ETA: 1091867.0s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.485s, learning 0.169s)
               Value function loss: 19.0421
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 448.00
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 10.65s
                        Total time: 30585.67s
                               ETA: 1091835.4s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.465s, learning 0.189s)
               Value function loss: 16.3802
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 446.05
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 10.65s
                        Total time: 30596.32s
                               ETA: 1091803.8s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.922s, learning 0.165s)
               Value function loss: 20.1200
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 445.54
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 11.09s
                        Total time: 30607.41s
                               ETA: 1091787.7s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.506s, learning 0.166s)
               Value function loss: 16.9191
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 435.82
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 10.67s
                        Total time: 30618.08s
                               ETA: 1091756.9s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.781s, learning 0.178s)
               Value function loss: 20.3010
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 443.28
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 10.96s
                        Total time: 30629.04s
                               ETA: 1091736.2s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.788s, learning 0.165s)
               Value function loss: 18.9469
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 445.06
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 10.95s
                        Total time: 30639.99s
                               ETA: 1091715.3s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.823s, learning 0.171s)
               Value function loss: 17.5378
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 443.56
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 10.99s
                        Total time: 30650.99s
                               ETA: 1091695.9s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1584 steps/s (collection: 10.177s, learning 0.167s)
               Value function loss: 15.6177
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 443.28
               Mean episode length: 249.33
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 10.34s
                        Total time: 30661.33s
                               ETA: 1091653.4s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.165s)
               Value function loss: 15.8964
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 439.37
               Mean episode length: 249.33
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 10.67s
                        Total time: 30672.01s
                               ETA: 1091622.6s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.605s, learning 0.211s)
               Value function loss: 17.3865
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 442.03
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 10.82s
                        Total time: 30682.82s
                               ETA: 1091597.0s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.463s, learning 0.260s)
               Value function loss: 17.8384
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 448.12
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 10.72s
                        Total time: 30693.55s
                               ETA: 1091568.0s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.630s, learning 0.170s)
               Value function loss: 17.1722
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 447.42
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 10.80s
                        Total time: 30704.34s
                               ETA: 1091541.7s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.027s, learning 0.176s)
               Value function loss: 17.2551
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 449.34
               Mean episode length: 249.71
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 11.20s
                        Total time: 30715.55s
                               ETA: 1091529.8s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.465s, learning 0.173s)
               Value function loss: 15.5170
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 438.01
               Mean episode length: 249.71
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 10.64s
                        Total time: 30726.19s
                               ETA: 1091497.8s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1219 steps/s (collection: 13.248s, learning 0.190s)
               Value function loss: 17.3007
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 437.17
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 13.44s
                        Total time: 30739.62s
                               ETA: 1091565.3s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 797 steps/s (collection: 20.375s, learning 0.161s)
               Value function loss: 18.5031
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 441.44
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 20.54s
                        Total time: 30760.16s
                               ETA: 1091884.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 803 steps/s (collection: 20.203s, learning 0.192s)
               Value function loss: 18.2355
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 435.98
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 20.40s
                        Total time: 30780.56s
                               ETA: 1092198.8s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 794 steps/s (collection: 20.446s, learning 0.174s)
               Value function loss: 17.5257
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 437.05
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 20.62s
                        Total time: 30801.17s
                               ETA: 1092520.6s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.931s, learning 0.166s)
               Value function loss: 15.6776
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 439.57
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 21.10s
                        Total time: 30822.27s
                               ETA: 1092859.1s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 775 steps/s (collection: 20.958s, learning 0.168s)
               Value function loss: 17.1890
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 439.29
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 21.13s
                        Total time: 30843.40s
                               ETA: 1093198.3s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 806 steps/s (collection: 20.145s, learning 0.178s)
               Value function loss: 16.9031
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 443.07
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 20.32s
                        Total time: 30863.72s
                               ETA: 1093508.9s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 794 steps/s (collection: 20.429s, learning 0.200s)
               Value function loss: 17.5265
                    Surrogate loss: -0.0192
             Mean action noise std: 0.73
                       Mean reward: 440.21
               Mean episode length: 249.96
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 20.63s
                        Total time: 30884.35s
                               ETA: 1093830.1s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 788 steps/s (collection: 20.595s, learning 0.178s)
               Value function loss: 14.6848
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 440.19
               Mean episode length: 249.96
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 20.77s
                        Total time: 30905.12s
                               ETA: 1094156.1s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 789 steps/s (collection: 20.598s, learning 0.163s)
               Value function loss: 17.3181
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 440.60
               Mean episode length: 249.88
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 20.76s
                        Total time: 30925.88s
                               ETA: 1094481.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 768 steps/s (collection: 21.111s, learning 0.209s)
               Value function loss: 15.0570
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 441.53
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 21.32s
                        Total time: 30947.20s
                               ETA: 1094826.3s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 784 steps/s (collection: 20.731s, learning 0.164s)
               Value function loss: 14.2987
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 439.52
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 20.89s
                        Total time: 30968.10s
                               ETA: 1095155.8s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.933s, learning 0.180s)
               Value function loss: 17.3185
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 439.23
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 21.11s
                        Total time: 30989.21s
                               ETA: 1095492.8s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 787 steps/s (collection: 20.634s, learning 0.161s)
               Value function loss: 13.2023
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 434.15
               Mean episode length: 249.99
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 20.80s
                        Total time: 31010.01s
                               ETA: 1095818.3s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.741s, learning 0.163s)
               Value function loss: 13.9455
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 432.45
               Mean episode length: 249.99
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 20.90s
                        Total time: 31030.91s
                               ETA: 1096147.4s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 780 steps/s (collection: 20.812s, learning 0.174s)
               Value function loss: 13.2709
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 432.22
               Mean episode length: 249.87
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 20.99s
                        Total time: 31051.90s
                               ETA: 1096479.2s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 787 steps/s (collection: 20.640s, learning 0.172s)
               Value function loss: 16.8991
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 434.73
               Mean episode length: 249.85
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 20.81s
                        Total time: 31072.71s
                               ETA: 1096804.6s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.742s, learning 0.168s)
               Value function loss: 15.0962
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 428.08
               Mean episode length: 248.46
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 20.91s
                        Total time: 31093.62s
                               ETA: 1097133.1s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 793 steps/s (collection: 20.482s, learning 0.170s)
               Value function loss: 14.8330
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 435.89
               Mean episode length: 249.69
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 20.65s
                        Total time: 31114.27s
                               ETA: 1097452.3s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 778 steps/s (collection: 20.878s, learning 0.162s)
               Value function loss: 16.1651
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 443.95
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 21.04s
                        Total time: 31135.31s
                               ETA: 1097784.9s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 786 steps/s (collection: 20.678s, learning 0.163s)
               Value function loss: 14.3579
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 444.39
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 20.84s
                        Total time: 31156.15s
                               ETA: 1098110.3s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 794 steps/s (collection: 20.433s, learning 0.176s)
               Value function loss: 14.4872
                    Surrogate loss: -0.0192
             Mean action noise std: 0.73
                       Mean reward: 428.29
               Mean episode length: 248.54
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 20.61s
                        Total time: 31176.76s
                               ETA: 1098427.2s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.426s, learning 0.166s)
               Value function loss: 13.3602
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 419.00
               Mean episode length: 248.59
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 20.59s
                        Total time: 31197.35s
                               ETA: 1098743.3s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 786 steps/s (collection: 20.655s, learning 0.172s)
               Value function loss: 14.4212
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 426.94
               Mean episode length: 249.36
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 20.83s
                        Total time: 31218.18s
                               ETA: 1099067.5s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 776 steps/s (collection: 20.934s, learning 0.168s)
               Value function loss: 12.6129
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 421.35
               Mean episode length: 248.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 21.10s
                        Total time: 31239.28s
                               ETA: 1099401.1s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 785 steps/s (collection: 20.682s, learning 0.167s)
               Value function loss: 10.8781
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 421.94
               Mean episode length: 248.31
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 20.85s
                        Total time: 31260.13s
                               ETA: 1099725.5s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 772 steps/s (collection: 21.056s, learning 0.166s)
               Value function loss: 12.0908
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 429.47
               Mean episode length: 249.37
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 21.22s
                        Total time: 31281.35s
                               ETA: 1100062.8s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.420s, learning 0.170s)
               Value function loss: 11.0715
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 425.79
               Mean episode length: 248.12
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 20.59s
                        Total time: 31301.94s
                               ETA: 1100377.6s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.760s, learning 0.158s)
               Value function loss: 13.2215
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 428.57
               Mean episode length: 248.81
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 20.92s
                        Total time: 31322.86s
                               ETA: 1100703.6s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 793 steps/s (collection: 20.482s, learning 0.169s)
               Value function loss: 13.1612
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 421.43
               Mean episode length: 248.72
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 20.65s
                        Total time: 31343.51s
                               ETA: 1101020.1s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 777 steps/s (collection: 20.891s, learning 0.179s)
               Value function loss: 12.3271
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 419.58
               Mean episode length: 247.20
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 21.07s
                        Total time: 31364.58s
                               ETA: 1101351.0s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 779 steps/s (collection: 20.850s, learning 0.170s)
               Value function loss: 14.5694
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 421.04
               Mean episode length: 247.06
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 21.02s
                        Total time: 31385.60s
                               ETA: 1101679.9s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 801 steps/s (collection: 20.274s, learning 0.173s)
               Value function loss: 14.1951
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 422.50
               Mean episode length: 248.32
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 20.45s
                        Total time: 31406.05s
                               ETA: 1101988.4s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 791 steps/s (collection: 20.461s, learning 0.227s)
               Value function loss: 15.7492
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 423.58
               Mean episode length: 246.45
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 20.69s
                        Total time: 31426.74s
                               ETA: 1102305.2s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 795 steps/s (collection: 20.443s, learning 0.164s)
               Value function loss: 15.2655
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 419.54
               Mean episode length: 248.21
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 20.61s
                        Total time: 31447.34s
                               ETA: 1102618.9s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 783 steps/s (collection: 20.736s, learning 0.183s)
               Value function loss: 13.1399
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 424.06
               Mean episode length: 248.68
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 20.92s
                        Total time: 31468.26s
                               ETA: 1102943.3s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 779 steps/s (collection: 20.854s, learning 0.173s)
               Value function loss: 13.5512
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 414.80
               Mean episode length: 246.97
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 21.03s
                        Total time: 31489.29s
                               ETA: 1103271.2s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 781 steps/s (collection: 20.785s, learning 0.170s)
               Value function loss: 15.4600
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 431.21
               Mean episode length: 248.15
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 20.95s
                        Total time: 31510.24s
                               ETA: 1103596.3s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1104 steps/s (collection: 14.663s, learning 0.172s)
               Value function loss: 14.7400
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 429.96
               Mean episode length: 247.16
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 14.83s
                        Total time: 31525.08s
                               ETA: 1103706.9s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.169s)
               Value function loss: 13.6932
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 414.93
               Mean episode length: 245.32
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 10.79s
                        Total time: 31535.86s
                               ETA: 1103675.7s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.443s, learning 0.176s)
               Value function loss: 13.5855
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 424.42
               Mean episode length: 248.07
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 10.62s
                        Total time: 31546.48s
                               ETA: 1103638.7s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.872s, learning 0.167s)
               Value function loss: 14.4427
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 429.63
               Mean episode length: 248.95
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 11.04s
                        Total time: 31557.52s
                               ETA: 1103616.4s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.730s, learning 0.189s)
               Value function loss: 13.7683
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 424.16
               Mean episode length: 248.17
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 10.92s
                        Total time: 31568.44s
                               ETA: 1103589.9s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.597s, learning 0.175s)
               Value function loss: 14.1453
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 428.58
               Mean episode length: 249.11
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 10.77s
                        Total time: 31579.21s
                               ETA: 1103558.3s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.602s, learning 0.162s)
               Value function loss: 13.7260
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 434.20
               Mean episode length: 248.99
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 10.76s
                        Total time: 31589.97s
                               ETA: 1103526.5s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.466s, learning 0.163s)
               Value function loss: 13.3948
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 429.29
               Mean episode length: 248.59
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 10.63s
                        Total time: 31600.60s
                               ETA: 1103489.9s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.321s, learning 0.250s)
               Value function loss: 12.7976
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 426.07
               Mean episode length: 248.63
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 10.57s
                        Total time: 31611.17s
                               ETA: 1103451.3s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.489s, learning 0.203s)
               Value function loss: 11.7447
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 415.84
               Mean episode length: 249.06
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 10.69s
                        Total time: 31621.87s
                               ETA: 1103417.0s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.955s, learning 0.175s)
               Value function loss: 17.3870
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 427.21
               Mean episode length: 249.32
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 11.13s
                        Total time: 31633.00s
                               ETA: 1103398.0s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1542 steps/s (collection: 10.462s, learning 0.162s)
               Value function loss: 15.1727
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 426.43
               Mean episode length: 248.85
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 10.62s
                        Total time: 31643.62s
                               ETA: 1103361.3s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.643s, learning 0.168s)
               Value function loss: 12.7931
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 425.43
               Mean episode length: 248.69
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 10.81s
                        Total time: 31654.43s
                               ETA: 1103331.1s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.584s, learning 0.193s)
               Value function loss: 15.7002
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 420.68
               Mean episode length: 247.56
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 10.78s
                        Total time: 31665.21s
                               ETA: 1103299.8s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.769s, learning 0.186s)
               Value function loss: 15.4169
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 415.54
               Mean episode length: 247.77
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 10.96s
                        Total time: 31676.16s
                               ETA: 1103274.7s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.651s, learning 0.161s)
               Value function loss: 13.0764
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 421.71
               Mean episode length: 249.35
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 10.81s
                        Total time: 31686.98s
                               ETA: 1103244.7s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.478s, learning 0.166s)
               Value function loss: 15.8123
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 434.39
               Mean episode length: 249.08
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 10.64s
                        Total time: 31697.62s
                               ETA: 1103208.8s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.605s, learning 0.173s)
               Value function loss: 14.0763
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 432.85
               Mean episode length: 248.25
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 10.78s
                        Total time: 31708.40s
                               ETA: 1103177.6s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.830s, learning 0.171s)
               Value function loss: 11.1654
                    Surrogate loss: -0.0197
             Mean action noise std: 0.73
                       Mean reward: 423.76
               Mean episode length: 247.51
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 11.00s
                        Total time: 31719.40s
                               ETA: 1103154.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.646s, learning 0.196s)
               Value function loss: 11.6929
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 420.75
               Mean episode length: 247.99
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 10.84s
                        Total time: 31730.24s
                               ETA: 1103125.2s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.666s, learning 0.176s)
               Value function loss: 11.5027
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 425.03
               Mean episode length: 248.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 10.84s
                        Total time: 31741.08s
                               ETA: 1103096.2s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.691s, learning 0.166s)
               Value function loss: 13.0631
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 427.44
               Mean episode length: 248.44
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 10.86s
                        Total time: 31751.94s
                               ETA: 1103067.8s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.744s, learning 0.162s)
               Value function loss: 13.0588
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 425.16
               Mean episode length: 249.15
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 10.91s
                        Total time: 31762.84s
                               ETA: 1103041.1s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.507s, learning 0.187s)
               Value function loss: 11.4654
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 416.31
               Mean episode length: 247.90
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 10.69s
                        Total time: 31773.54s
                               ETA: 1103007.0s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.596s, learning 0.161s)
               Value function loss: 12.0898
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 404.37
               Mean episode length: 248.23
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 10.76s
                        Total time: 31784.29s
                               ETA: 1102975.2s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.615s, learning 0.170s)
               Value function loss: 12.8940
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 405.13
               Mean episode length: 248.46
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 10.78s
                        Total time: 31795.08s
                               ETA: 1102944.3s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.518s, learning 0.187s)
               Value function loss: 14.3512
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 417.49
               Mean episode length: 248.79
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 10.71s
                        Total time: 31805.78s
                               ETA: 1102910.7s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.852s, learning 0.187s)
               Value function loss: 13.1655
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 415.96
               Mean episode length: 248.37
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 11.04s
                        Total time: 31816.82s
                               ETA: 1102888.7s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.773s, learning 0.273s)
               Value function loss: 12.2370
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 404.03
               Mean episode length: 246.83
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 11.05s
                        Total time: 31827.87s
                               ETA: 1102866.9s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.815s, learning 0.196s)
               Value function loss: 11.1673
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 412.62
               Mean episode length: 247.76
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 11.01s
                        Total time: 31838.88s
                               ETA: 1102843.9s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1576 steps/s (collection: 10.226s, learning 0.169s)
               Value function loss: 16.4003
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 425.85
               Mean episode length: 249.15
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 10.40s
                        Total time: 31849.28s
                               ETA: 1102799.6s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.702s, learning 0.163s)
               Value function loss: 16.2654
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 416.10
               Mean episode length: 248.46
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 10.87s
                        Total time: 31860.14s
                               ETA: 1102771.6s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.831s, learning 0.165s)
               Value function loss: 14.5440
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 406.03
               Mean episode length: 247.94
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 11.00s
                        Total time: 31871.14s
                               ETA: 1102748.2s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.040s, learning 0.162s)
               Value function loss: 11.6081
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 403.58
               Mean episode length: 247.63
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 11.20s
                        Total time: 31882.34s
                               ETA: 1102731.8s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1567 steps/s (collection: 10.289s, learning 0.164s)
               Value function loss: 16.0572
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 415.96
               Mean episode length: 249.52
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 10.45s
                        Total time: 31892.79s
                               ETA: 1102689.6s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.535s, learning 0.177s)
               Value function loss: 12.9285
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 414.32
               Mean episode length: 246.43
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 10.71s
                        Total time: 31903.50s
                               ETA: 1102656.4s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.874s, learning 0.162s)
               Value function loss: 17.3745
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 417.97
               Mean episode length: 248.56
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 11.04s
                        Total time: 31914.54s
                               ETA: 1102634.3s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1584 steps/s (collection: 10.180s, learning 0.162s)
               Value function loss: 12.4461
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 418.29
               Mean episode length: 248.64
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 10.34s
                        Total time: 31924.88s
                               ETA: 1102588.3s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.657s, learning 0.172s)
               Value function loss: 13.3103
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 415.94
               Mean episode length: 248.15
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 10.83s
                        Total time: 31935.71s
                               ETA: 1102559.2s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.528s, learning 0.182s)
               Value function loss: 11.0785
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 403.32
               Mean episode length: 246.64
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 10.71s
                        Total time: 31946.42s
                               ETA: 1102525.9s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.722s, learning 0.170s)
               Value function loss: 14.2503
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 403.11
               Mean episode length: 246.41
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 10.89s
                        Total time: 31957.31s
                               ETA: 1102499.0s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.705s, learning 0.161s)
               Value function loss: 15.8152
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 409.13
               Mean episode length: 246.29
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 10.87s
                        Total time: 31968.18s
                               ETA: 1102471.1s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1572 steps/s (collection: 10.250s, learning 0.172s)
               Value function loss: 17.4873
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 413.15
               Mean episode length: 247.26
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 10.42s
                        Total time: 31978.60s
                               ETA: 1102428.0s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.779s, learning 0.165s)
               Value function loss: 12.2143
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 410.74
               Mean episode length: 246.81
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 10.94s
                        Total time: 31989.54s
                               ETA: 1102402.8s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.778s, learning 0.165s)
               Value function loss: 15.2086
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 412.55
               Mean episode length: 246.90
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 10.94s
                        Total time: 32000.49s
                               ETA: 1102377.7s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.672s, learning 0.164s)
               Value function loss: 14.7786
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 410.30
               Mean episode length: 245.53
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 10.84s
                        Total time: 32011.32s
                               ETA: 1102348.9s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.165s)
               Value function loss: 13.3168
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 414.40
               Mean episode length: 246.93
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 10.87s
                        Total time: 32022.19s
                               ETA: 1102321.2s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.348s, learning 0.160s)
               Value function loss: 11.9091
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 414.87
               Mean episode length: 248.15
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 10.51s
                        Total time: 32032.70s
                               ETA: 1102281.1s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.723s, learning 0.157s)
               Value function loss: 14.4884
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 409.05
               Mean episode length: 247.21
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 10.88s
                        Total time: 32043.58s
                               ETA: 1102253.8s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.873s, learning 0.167s)
               Value function loss: 16.3231
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 394.76
               Mean episode length: 242.74
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 11.04s
                        Total time: 32054.62s
                               ETA: 1102232.0s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.704s, learning 0.166s)
               Value function loss: 13.3058
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 384.39
               Mean episode length: 239.93
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 10.87s
                        Total time: 32065.49s
                               ETA: 1102204.5s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.694s, learning 0.189s)
               Value function loss: 11.3200
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 399.09
               Mean episode length: 243.58
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 10.88s
                        Total time: 32076.37s
                               ETA: 1102177.3s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.655s, learning 0.169s)
               Value function loss: 13.6679
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 387.94
               Mean episode length: 240.72
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 10.82s
                        Total time: 32087.20s
                               ETA: 1102148.2s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.739s, learning 0.182s)
               Value function loss: 13.8567
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 384.83
               Mean episode length: 240.48
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 10.92s
                        Total time: 32098.12s
                               ETA: 1102122.4s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.505s, learning 0.189s)
               Value function loss: 12.4150
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 383.74
               Mean episode length: 241.27
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 10.69s
                        Total time: 32108.81s
                               ETA: 1102088.9s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.736s, learning 0.159s)
               Value function loss: 11.9164
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 387.43
               Mean episode length: 242.58
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 10.90s
                        Total time: 32119.71s
                               ETA: 1102062.2s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.761s, learning 0.171s)
               Value function loss: 12.2166
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 385.12
               Mean episode length: 242.70
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 10.93s
                        Total time: 32130.64s
                               ETA: 1102036.8s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.688s, learning 0.268s)
               Value function loss: 11.2849
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 389.05
               Mean episode length: 241.38
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 10.96s
                        Total time: 32141.60s
                               ETA: 1102012.2s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.062s, learning 0.193s)
               Value function loss: 11.3100
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 381.34
               Mean episode length: 241.55
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 11.25s
                        Total time: 32152.85s
                               ETA: 1101997.9s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.601s, learning 0.266s)
               Value function loss: 11.1681
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 373.25
               Mean episode length: 239.15
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 10.87s
                        Total time: 32163.72s
                               ETA: 1101970.3s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.612s, learning 0.193s)
               Value function loss: 11.4973
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 369.83
               Mean episode length: 242.07
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 10.81s
                        Total time: 32174.52s
                               ETA: 1101940.6s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.576s, learning 0.167s)
               Value function loss: 11.1090
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 377.16
               Mean episode length: 241.11
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 10.74s
                        Total time: 32185.27s
                               ETA: 1101908.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.483s, learning 0.172s)
               Value function loss: 11.2301
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 375.51
               Mean episode length: 242.76
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 10.65s
                        Total time: 32195.92s
                               ETA: 1101873.9s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.282s, learning 0.174s)
               Value function loss: 11.9896
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 376.64
               Mean episode length: 242.95
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 11.46s
                        Total time: 32207.38s
                               ETA: 1101866.6s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.385s, learning 0.179s)
               Value function loss: 8.7605
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 385.18
               Mean episode length: 244.31
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 10.56s
                        Total time: 32217.94s
                               ETA: 1101828.7s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.575s, learning 0.170s)
               Value function loss: 11.1242
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 382.47
               Mean episode length: 244.08
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 10.74s
                        Total time: 32228.69s
                               ETA: 1101797.0s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.454s, learning 0.186s)
               Value function loss: 11.3997
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 380.59
               Mean episode length: 243.08
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 10.64s
                        Total time: 32239.33s
                               ETA: 1101761.7s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.641s, learning 0.174s)
               Value function loss: 11.8939
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 380.32
               Mean episode length: 245.87
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 10.82s
                        Total time: 32250.14s
                               ETA: 1101732.4s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.681s, learning 0.161s)
               Value function loss: 9.6626
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 376.44
               Mean episode length: 244.64
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 10.84s
                        Total time: 32260.98s
                               ETA: 1101704.1s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.731s, learning 0.173s)
               Value function loss: 13.5229
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 387.86
               Mean episode length: 243.74
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 10.90s
                        Total time: 32271.89s
                               ETA: 1101677.9s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.674s, learning 0.188s)
               Value function loss: 10.2663
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 375.61
               Mean episode length: 242.16
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 10.86s
                        Total time: 32282.75s
                               ETA: 1101650.3s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.726s, learning 0.172s)
               Value function loss: 14.4905
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 379.31
               Mean episode length: 243.53
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 10.90s
                        Total time: 32293.65s
                               ETA: 1101623.9s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.808s, learning 0.183s)
               Value function loss: 14.3041
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 398.78
               Mean episode length: 246.69
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 10.99s
                        Total time: 32304.64s
                               ETA: 1101600.7s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.503s, learning 0.259s)
               Value function loss: 13.2117
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 390.09
               Mean episode length: 245.39
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 10.76s
                        Total time: 32315.40s
                               ETA: 1101569.7s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.654s, learning 0.172s)
               Value function loss: 12.8533
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 391.38
               Mean episode length: 246.35
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 10.83s
                        Total time: 32326.23s
                               ETA: 1101540.9s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.403s, learning 0.162s)
               Value function loss: 13.7721
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 392.69
               Mean episode length: 246.09
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 10.56s
                        Total time: 32336.79s
                               ETA: 1101503.2s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.590s, learning 0.186s)
               Value function loss: 13.2101
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 397.21
               Mean episode length: 246.82
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 10.78s
                        Total time: 32347.57s
                               ETA: 1101472.7s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.413s, learning 0.202s)
               Value function loss: 15.7099
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 393.93
               Mean episode length: 244.87
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 10.61s
                        Total time: 32358.18s
                               ETA: 1101436.7s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.715s, learning 0.181s)
               Value function loss: 14.4828
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 390.90
               Mean episode length: 244.37
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 10.90s
                        Total time: 32369.08s
                               ETA: 1101410.3s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.377s, learning 0.158s)
               Value function loss: 14.4084
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 381.58
               Mean episode length: 245.78
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 10.54s
                        Total time: 32379.61s
                               ETA: 1101371.7s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.581s, learning 0.180s)
               Value function loss: 16.6803
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 396.69
               Mean episode length: 245.95
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 10.76s
                        Total time: 32390.38s
                               ETA: 1101340.8s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.858s, learning 0.169s)
               Value function loss: 15.0620
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 397.45
               Mean episode length: 245.18
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 11.03s
                        Total time: 32401.40s
                               ETA: 1101318.9s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.401s, learning 0.165s)
               Value function loss: 12.6052
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 396.80
               Mean episode length: 247.96
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 10.57s
                        Total time: 32411.97s
                               ETA: 1101281.4s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.576s, learning 0.160s)
               Value function loss: 16.4419
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 399.68
               Mean episode length: 249.05
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 10.74s
                        Total time: 32422.70s
                               ETA: 1101249.6s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.603s, learning 0.159s)
               Value function loss: 18.6096
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 395.59
               Mean episode length: 246.73
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 10.76s
                        Total time: 32433.47s
                               ETA: 1101218.7s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.625s, learning 0.161s)
               Value function loss: 15.9980
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 407.66
               Mean episode length: 247.96
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 10.79s
                        Total time: 32444.25s
                               ETA: 1101188.7s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1610 steps/s (collection: 9.909s, learning 0.262s)
               Value function loss: 17.4893
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 404.19
               Mean episode length: 246.18
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 10.17s
                        Total time: 32454.42s
                               ETA: 1101137.9s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1575 steps/s (collection: 10.240s, learning 0.160s)
               Value function loss: 17.3934
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 390.76
               Mean episode length: 243.19
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 10.40s
                        Total time: 32464.82s
                               ETA: 1101094.8s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.729s, learning 0.161s)
               Value function loss: 17.2371
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 398.32
               Mean episode length: 245.86
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 10.89s
                        Total time: 32475.71s
                               ETA: 1101068.4s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1569 steps/s (collection: 10.257s, learning 0.183s)
               Value function loss: 18.1769
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 407.76
               Mean episode length: 246.37
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 10.44s
                        Total time: 32486.15s
                               ETA: 1101026.7s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.505s, learning 0.167s)
               Value function loss: 13.7521
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 412.43
               Mean episode length: 248.11
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 10.67s
                        Total time: 32496.82s
                               ETA: 1100992.9s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.618s, learning 0.160s)
               Value function loss: 15.5627
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 396.64
               Mean episode length: 247.69
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 10.78s
                        Total time: 32507.60s
                               ETA: 1100962.7s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.506s, learning 0.160s)
               Value function loss: 13.0841
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 387.74
               Mean episode length: 247.23
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 10.67s
                        Total time: 32518.27s
                               ETA: 1100928.7s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.706s, learning 0.186s)
               Value function loss: 14.5394
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 399.95
               Mean episode length: 248.53
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 10.89s
                        Total time: 32529.16s
                               ETA: 1100902.4s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.839s, learning 0.159s)
               Value function loss: 16.7030
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 389.16
               Mean episode length: 247.61
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 11.00s
                        Total time: 32540.16s
                               ETA: 1100879.7s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.558s, learning 0.157s)
               Value function loss: 10.8043
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 389.64
               Mean episode length: 247.73
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 10.72s
                        Total time: 32550.87s
                               ETA: 1100847.5s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.568s, learning 0.168s)
               Value function loss: 10.6973
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 382.36
               Mean episode length: 246.70
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 10.74s
                        Total time: 32561.61s
                               ETA: 1100815.9s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.678s, learning 0.165s)
               Value function loss: 12.2804
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 381.21
               Mean episode length: 247.37
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 10.84s
                        Total time: 32572.45s
                               ETA: 1100788.0s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.613s, learning 0.180s)
               Value function loss: 11.3954
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 385.09
               Mean episode length: 247.39
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 10.79s
                        Total time: 32583.25s
                               ETA: 1100758.4s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.494s, learning 0.159s)
               Value function loss: 10.7535
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 382.79
               Mean episode length: 246.04
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 10.65s
                        Total time: 32593.90s
                               ETA: 1100724.1s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.665s, learning 0.166s)
               Value function loss: 12.3528
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 387.11
               Mean episode length: 246.07
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 10.83s
                        Total time: 32604.73s
                               ETA: 1100695.8s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.478s, learning 0.158s)
               Value function loss: 14.0584
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 398.72
               Mean episode length: 247.30
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 10.64s
                        Total time: 32615.37s
                               ETA: 1100660.9s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.907s, learning 0.183s)
               Value function loss: 12.4415
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 393.87
               Mean episode length: 248.16
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 11.09s
                        Total time: 32626.45s
                               ETA: 1100641.4s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.639s, learning 0.159s)
               Value function loss: 12.6925
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 384.27
               Mean episode length: 247.19
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 10.80s
                        Total time: 32637.25s
                               ETA: 1100612.0s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.538s, learning 0.161s)
               Value function loss: 11.6929
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 391.03
               Mean episode length: 248.28
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 10.70s
                        Total time: 32647.95s
                               ETA: 1100579.3s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.581s, learning 0.197s)
               Value function loss: 11.6371
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 382.24
               Mean episode length: 247.82
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 10.78s
                        Total time: 32658.73s
                               ETA: 1100549.3s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.693s, learning 0.167s)
               Value function loss: 10.1313
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 378.54
               Mean episode length: 246.95
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 10.86s
                        Total time: 32669.59s
                               ETA: 1100522.1s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.851s, learning 0.158s)
               Value function loss: 12.5971
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 387.29
               Mean episode length: 248.34
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 11.01s
                        Total time: 32680.60s
                               ETA: 1100499.9s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.802s, learning 0.166s)
               Value function loss: 13.1925
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 386.02
               Mean episode length: 247.65
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 10.97s
                        Total time: 32691.57s
                               ETA: 1100476.4s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.414s, learning 0.162s)
               Value function loss: 10.1551
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 375.13
               Mean episode length: 245.62
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 10.58s
                        Total time: 32702.14s
                               ETA: 1100439.6s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.901s, learning 0.186s)
               Value function loss: 14.3272
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 383.75
               Mean episode length: 242.58
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 11.09s
                        Total time: 32713.23s
                               ETA: 1100420.1s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.492s, learning 0.163s)
               Value function loss: 11.7169
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 387.34
               Mean episode length: 245.51
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 10.65s
                        Total time: 32723.89s
                               ETA: 1100386.0s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.534s, learning 0.185s)
               Value function loss: 11.3873
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 380.38
               Mean episode length: 245.94
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 10.72s
                        Total time: 32734.61s
                               ETA: 1100354.1s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.599s, learning 0.207s)
               Value function loss: 12.2968
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 379.69
               Mean episode length: 247.32
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 10.81s
                        Total time: 32745.41s
                               ETA: 1100325.2s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.480s, learning 0.158s)
               Value function loss: 9.8802
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 387.43
               Mean episode length: 247.13
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 10.64s
                        Total time: 32756.05s
                               ETA: 1100290.6s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.626s, learning 0.165s)
               Value function loss: 15.1441
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 389.58
               Mean episode length: 247.29
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 10.79s
                        Total time: 32766.84s
                               ETA: 1100261.2s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.498s, learning 0.174s)
               Value function loss: 12.5554
                    Surrogate loss: -0.0208
             Mean action noise std: 0.73
                       Mean reward: 389.66
               Mean episode length: 246.07
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 10.67s
                        Total time: 32777.51s
                               ETA: 1100227.7s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.566s, learning 0.162s)
               Value function loss: 13.4181
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 392.04
               Mean episode length: 247.27
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 10.73s
                        Total time: 32788.24s
                               ETA: 1100196.2s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.464s, learning 0.167s)
               Value function loss: 13.5343
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 385.10
               Mean episode length: 247.30
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 10.63s
                        Total time: 32798.87s
                               ETA: 1100161.4s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.528s, learning 0.161s)
               Value function loss: 13.9040
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 378.66
               Mean episode length: 244.56
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 10.69s
                        Total time: 32809.56s
                               ETA: 1100128.6s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.492s, learning 0.197s)
               Value function loss: 12.7961
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 390.98
               Mean episode length: 246.92
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 10.69s
                        Total time: 32820.25s
                               ETA: 1100095.8s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.722s, learning 0.188s)
               Value function loss: 13.1332
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 390.50
               Mean episode length: 245.69
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 10.91s
                        Total time: 32831.16s
                               ETA: 1100070.5s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.756s, learning 0.160s)
               Value function loss: 14.8834
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 399.19
               Mean episode length: 246.88
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 10.92s
                        Total time: 32842.08s
                               ETA: 1100045.3s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.656s, learning 0.182s)
               Value function loss: 10.3532
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 380.23
               Mean episode length: 244.03
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 10.84s
                        Total time: 32852.91s
                               ETA: 1100017.5s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.504s, learning 0.258s)
               Value function loss: 12.0576
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 369.52
               Mean episode length: 243.67
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 10.76s
                        Total time: 32863.68s
                               ETA: 1099987.2s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.639s, learning 0.161s)
               Value function loss: 10.4532
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 359.76
               Mean episode length: 240.92
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 10.80s
                        Total time: 32874.48s
                               ETA: 1099958.2s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1564 steps/s (collection: 10.310s, learning 0.163s)
               Value function loss: 9.4740
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 374.15
               Mean episode length: 245.51
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 10.47s
                        Total time: 32884.95s
                               ETA: 1099918.3s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.323s, learning 0.165s)
               Value function loss: 10.2482
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 382.01
               Mean episode length: 245.30
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 10.49s
                        Total time: 32895.44s
                               ETA: 1099878.9s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.670s, learning 0.157s)
               Value function loss: 9.6373
                    Surrogate loss: -0.0192
             Mean action noise std: 0.73
                       Mean reward: 374.94
               Mean episode length: 245.10
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 10.83s
                        Total time: 32906.26s
                               ETA: 1099850.8s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.469s, learning 0.191s)
               Value function loss: 9.3944
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 378.04
               Mean episode length: 247.43
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 10.66s
                        Total time: 32916.92s
                               ETA: 1099817.2s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.667s, learning 0.267s)
               Value function loss: 9.8452
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 379.08
               Mean episode length: 248.26
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 10.93s
                        Total time: 32927.86s
                               ETA: 1099792.8s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.592s, learning 0.160s)
               Value function loss: 7.5193
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 350.64
               Mean episode length: 244.54
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 10.75s
                        Total time: 32938.61s
                               ETA: 1099762.3s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.728s, learning 0.161s)
               Value function loss: 11.6146
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 366.20
               Mean episode length: 245.30
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 10.89s
                        Total time: 32949.50s
                               ETA: 1099736.3s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.490s, learning 0.156s)
               Value function loss: 9.7363
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 376.56
               Mean episode length: 245.10
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 10.65s
                        Total time: 32960.15s
                               ETA: 1099702.3s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.616s, learning 0.164s)
               Value function loss: 10.5123
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 373.05
               Mean episode length: 246.25
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 10.78s
                        Total time: 32970.93s
                               ETA: 1099672.7s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.486s, learning 0.217s)
               Value function loss: 12.5637
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 372.59
               Mean episode length: 246.41
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 10.70s
                        Total time: 32981.63s
                               ETA: 1099640.7s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.541s, learning 0.163s)
               Value function loss: 12.5135
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 375.58
               Mean episode length: 246.74
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 10.70s
                        Total time: 32992.34s
                               ETA: 1099608.6s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.160s)
               Value function loss: 10.2434
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 368.88
               Mean episode length: 245.33
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 10.93s
                        Total time: 33003.27s
                               ETA: 1099584.2s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.536s, learning 0.162s)
               Value function loss: 12.3436
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 373.02
               Mean episode length: 246.17
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 10.70s
                        Total time: 33013.97s
                               ETA: 1099551.9s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.928s, learning 0.185s)
               Value function loss: 12.9792
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 381.19
               Mean episode length: 247.08
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 11.11s
                        Total time: 33025.08s
                               ETA: 1099533.5s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.807s, learning 0.175s)
               Value function loss: 10.3435
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 379.47
               Mean episode length: 244.89
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 10.98s
                        Total time: 33036.06s
                               ETA: 1099510.8s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.620s, learning 0.159s)
               Value function loss: 11.5206
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 375.59
               Mean episode length: 246.35
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 10.78s
                        Total time: 33046.84s
                               ETA: 1099481.3s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.481s, learning 0.178s)
               Value function loss: 11.6957
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 379.34
               Mean episode length: 245.40
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 10.66s
                        Total time: 33057.50s
                               ETA: 1099447.8s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.425s, learning 0.161s)
               Value function loss: 11.2444
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 372.61
               Mean episode length: 243.60
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 10.59s
                        Total time: 33068.08s
                               ETA: 1099411.9s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.503s, learning 0.170s)
               Value function loss: 12.1788
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 372.04
               Mean episode length: 245.19
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 10.67s
                        Total time: 33078.76s
                               ETA: 1099378.9s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.653s, learning 0.190s)
               Value function loss: 8.6470
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 365.34
               Mean episode length: 244.88
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 10.84s
                        Total time: 33089.60s
                               ETA: 1099351.6s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.724s, learning 0.171s)
               Value function loss: 12.4002
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 367.89
               Mean episode length: 244.15
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 10.90s
                        Total time: 33100.50s
                               ETA: 1099326.0s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.922s, learning 0.257s)
               Value function loss: 12.0957
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 375.61
               Mean episode length: 246.81
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 11.18s
                        Total time: 33111.68s
                               ETA: 1099309.9s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.683s, learning 0.161s)
               Value function loss: 10.3084
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 383.34
               Mean episode length: 245.09
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 10.84s
                        Total time: 33122.52s
                               ETA: 1099282.6s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.763s, learning 0.168s)
               Value function loss: 12.1489
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 378.33
               Mean episode length: 246.32
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 10.93s
                        Total time: 33133.45s
                               ETA: 1099258.3s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.721s, learning 0.170s)
               Value function loss: 11.7102
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 368.59
               Mean episode length: 245.44
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 10.89s
                        Total time: 33144.34s
                               ETA: 1099232.6s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.562s, learning 0.166s)
               Value function loss: 9.3473
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 383.39
               Mean episode length: 247.46
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 10.73s
                        Total time: 33155.07s
                               ETA: 1099201.5s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.695s, learning 0.160s)
               Value function loss: 10.6960
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 375.38
               Mean episode length: 245.38
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 10.85s
                        Total time: 33165.92s
                               ETA: 1099174.6s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.589s, learning 0.168s)
               Value function loss: 9.2163
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 369.45
               Mean episode length: 245.68
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 10.76s
                        Total time: 33176.68s
                               ETA: 1099144.5s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.032s, learning 0.162s)
               Value function loss: 9.3244
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 391.76
               Mean episode length: 248.91
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 11.19s
                        Total time: 33187.87s
                               ETA: 1099129.0s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.920s, learning 0.168s)
               Value function loss: 9.3927
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 386.23
               Mean episode length: 248.44
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 11.09s
                        Total time: 33198.96s
                               ETA: 1099109.9s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.593s, learning 0.181s)
               Value function loss: 8.2797
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 373.00
               Mean episode length: 247.51
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 10.77s
                        Total time: 33209.74s
                               ETA: 1099080.4s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.837s, learning 0.168s)
               Value function loss: 13.1441
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 379.04
               Mean episode length: 247.10
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 11.00s
                        Total time: 33220.74s
                               ETA: 1099058.5s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.577s, learning 0.166s)
               Value function loss: 6.9402
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 382.04
               Mean episode length: 247.52
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 10.74s
                        Total time: 33231.49s
                               ETA: 1099028.1s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.839s, learning 0.162s)
               Value function loss: 12.6881
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 375.37
               Mean episode length: 245.77
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 11.00s
                        Total time: 33242.49s
                               ETA: 1099006.1s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.662s, learning 0.170s)
               Value function loss: 8.3462
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 373.25
               Mean episode length: 247.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 10.83s
                        Total time: 33253.32s
                               ETA: 1098978.6s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.546s, learning 0.174s)
               Value function loss: 9.8999
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 392.15
               Mean episode length: 247.93
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 10.72s
                        Total time: 33264.04s
                               ETA: 1098947.4s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.680s, learning 0.167s)
               Value function loss: 8.7898
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 402.41
               Mean episode length: 248.87
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 10.85s
                        Total time: 33274.89s
                               ETA: 1098920.4s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.678s, learning 0.166s)
               Value function loss: 9.9396
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 393.67
               Mean episode length: 249.06
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 10.84s
                        Total time: 33285.73s
                               ETA: 1098893.3s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.655s, learning 0.256s)
               Value function loss: 11.1705
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 395.41
               Mean episode length: 249.32
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 10.91s
                        Total time: 33296.64s
                               ETA: 1098868.4s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.970s, learning 0.160s)
               Value function loss: 11.9146
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 399.46
               Mean episode length: 248.79
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 11.13s
                        Total time: 33307.77s
                               ETA: 1098850.8s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.565s, learning 0.212s)
               Value function loss: 13.5568
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 408.13
               Mean episode length: 248.95
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 10.78s
                        Total time: 33318.55s
                               ETA: 1098821.5s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.625s, learning 0.170s)
               Value function loss: 13.4590
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 402.58
               Mean episode length: 248.53
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 10.80s
                        Total time: 33329.34s
                               ETA: 1098792.8s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.552s, learning 0.165s)
               Value function loss: 14.3006
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 411.40
               Mean episode length: 249.53
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 10.72s
                        Total time: 33340.06s
                               ETA: 1098761.6s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.587s, learning 0.178s)
               Value function loss: 11.5107
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 404.23
               Mean episode length: 249.26
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 10.77s
                        Total time: 33350.83s
                               ETA: 1098732.0s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.406s, learning 0.159s)
               Value function loss: 12.5110
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 414.50
               Mean episode length: 248.94
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 10.57s
                        Total time: 33361.39s
                               ETA: 1098695.8s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.645s, learning 0.163s)
               Value function loss: 14.3490
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 414.70
               Mean episode length: 249.56
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 10.81s
                        Total time: 33372.20s
                               ETA: 1098667.6s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.722s, learning 0.208s)
               Value function loss: 13.9945
                    Surrogate loss: -0.0172
             Mean action noise std: 0.72
                       Mean reward: 409.28
               Mean episode length: 248.74
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 10.93s
                        Total time: 33383.13s
                               ETA: 1098643.4s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.831s, learning 0.158s)
               Value function loss: 12.6169
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 412.88
               Mean episode length: 249.56
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 10.99s
                        Total time: 33394.12s
                               ETA: 1098621.2s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.652s, learning 0.166s)
               Value function loss: 12.8079
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 424.41
               Mean episode length: 249.76
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 10.82s
                        Total time: 33404.94s
                               ETA: 1098593.4s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.674s, learning 0.165s)
               Value function loss: 13.2494
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 418.36
               Mean episode length: 249.46
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 10.84s
                        Total time: 33415.77s
                               ETA: 1098566.2s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.630s, learning 0.185s)
               Value function loss: 12.0282
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 416.46
               Mean episode length: 248.25
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 10.81s
                        Total time: 33426.59s
                               ETA: 1098538.3s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.665s, learning 0.170s)
               Value function loss: 17.6534
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 428.43
               Mean episode length: 248.95
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 10.83s
                        Total time: 33437.42s
                               ETA: 1098511.1s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.608s, learning 0.180s)
               Value function loss: 13.6763
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 420.76
               Mean episode length: 248.47
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 10.79s
                        Total time: 33448.21s
                               ETA: 1098482.3s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.497s, learning 0.165s)
               Value function loss: 12.9397
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 414.47
               Mean episode length: 249.49
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 10.66s
                        Total time: 33458.87s
                               ETA: 1098449.4s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1546 steps/s (collection: 10.426s, learning 0.170s)
               Value function loss: 13.8189
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 407.59
               Mean episode length: 248.61
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 10.60s
                        Total time: 33469.47s
                               ETA: 1098414.4s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.745s, learning 0.177s)
               Value function loss: 13.2823
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 406.99
               Mean episode length: 248.83
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 10.92s
                        Total time: 33480.39s
                               ETA: 1098390.0s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.676s, learning 0.159s)
               Value function loss: 11.7966
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 410.92
               Mean episode length: 249.78
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 10.83s
                        Total time: 33491.23s
                               ETA: 1098362.8s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.019s, learning 0.168s)
               Value function loss: 12.7830
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 419.77
               Mean episode length: 250.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 11.19s
                        Total time: 33502.41s
                               ETA: 1098347.2s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.786s, learning 0.164s)
               Value function loss: 11.1106
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 409.45
               Mean episode length: 247.26
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 10.95s
                        Total time: 33513.36s
                               ETA: 1098323.8s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.733s, learning 0.173s)
               Value function loss: 13.4304
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 421.41
               Mean episode length: 249.32
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 10.91s
                        Total time: 33524.27s
                               ETA: 1098299.0s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.746s, learning 0.171s)
               Value function loss: 11.9325
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 412.46
               Mean episode length: 248.13
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 10.92s
                        Total time: 33535.19s
                               ETA: 1098274.6s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.487s, learning 0.170s)
               Value function loss: 13.7214
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 412.75
               Mean episode length: 247.97
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 10.66s
                        Total time: 33545.84s
                               ETA: 1098241.6s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.613s, learning 0.175s)
               Value function loss: 11.1378
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 419.50
               Mean episode length: 249.09
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 10.79s
                        Total time: 33556.63s
                               ETA: 1098212.9s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.839s, learning 0.161s)
               Value function loss: 10.2602
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 423.34
               Mean episode length: 249.19
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 11.00s
                        Total time: 33567.63s
                               ETA: 1098191.2s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.983s, learning 0.162s)
               Value function loss: 16.6348
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 429.68
               Mean episode length: 249.82
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 11.15s
                        Total time: 33578.78s
                               ETA: 1098174.3s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.498s, learning 0.166s)
               Value function loss: 11.2158
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 425.12
               Mean episode length: 249.98
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 10.66s
                        Total time: 33589.44s
                               ETA: 1098141.6s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.865s, learning 0.172s)
               Value function loss: 10.8158
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 425.74
               Mean episode length: 249.56
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 11.04s
                        Total time: 33600.48s
                               ETA: 1098121.2s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.546s, learning 0.185s)
               Value function loss: 9.7244
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 420.34
               Mean episode length: 249.19
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 10.73s
                        Total time: 33611.21s
                               ETA: 1098090.7s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.533s, learning 0.165s)
               Value function loss: 14.5088
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 425.86
               Mean episode length: 249.61
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 10.70s
                        Total time: 33621.91s
                               ETA: 1098059.2s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.573s, learning 0.186s)
               Value function loss: 11.8504
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 426.38
               Mean episode length: 249.98
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 10.76s
                        Total time: 33632.67s
                               ETA: 1098029.7s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.898s, learning 0.256s)
               Value function loss: 15.3729
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 417.46
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 11.15s
                        Total time: 33643.82s
                               ETA: 1098013.1s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.807s, learning 0.162s)
               Value function loss: 14.4110
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 426.22
               Mean episode length: 249.24
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 10.97s
                        Total time: 33654.79s
                               ETA: 1097990.4s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.164s)
               Value function loss: 15.1548
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 430.89
               Mean episode length: 249.97
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 10.85s
                        Total time: 33665.64s
                               ETA: 1097963.8s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.762s, learning 0.159s)
               Value function loss: 14.7376
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 429.26
               Mean episode length: 249.59
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 10.92s
                        Total time: 33676.56s
                               ETA: 1097939.6s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.723s, learning 0.164s)
               Value function loss: 12.9285
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 426.36
               Mean episode length: 249.03
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 10.89s
                        Total time: 33687.45s
                               ETA: 1097914.4s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.436s, learning 0.171s)
               Value function loss: 14.5171
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 426.72
               Mean episode length: 249.30
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 10.61s
                        Total time: 33698.06s
                               ETA: 1097879.9s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.718s, learning 0.167s)
               Value function loss: 14.2614
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 428.54
               Mean episode length: 249.67
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 10.89s
                        Total time: 33708.94s
                               ETA: 1097854.6s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.917s, learning 0.162s)
               Value function loss: 14.4375
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 432.20
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 11.08s
                        Total time: 33720.02s
                               ETA: 1097835.6s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.638s, learning 0.163s)
               Value function loss: 14.4781
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 436.39
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 10.80s
                        Total time: 33730.82s
                               ETA: 1097807.5s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.480s, learning 0.161s)
               Value function loss: 13.3621
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 433.86
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 10.64s
                        Total time: 33741.46s
                               ETA: 1097774.3s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.692s, learning 0.165s)
               Value function loss: 12.5233
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 426.40
               Mean episode length: 249.72
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 10.86s
                        Total time: 33752.32s
                               ETA: 1097748.0s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.674s, learning 0.210s)
               Value function loss: 15.3201
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 435.01
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 10.88s
                        Total time: 33763.20s
                               ETA: 1097722.7s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.735s, learning 0.165s)
               Value function loss: 16.1176
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 433.26
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 10.90s
                        Total time: 33774.10s
                               ETA: 1097697.9s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.815s, learning 0.183s)
               Value function loss: 15.5649
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 426.27
               Mean episode length: 249.69
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 11.00s
                        Total time: 33785.10s
                               ETA: 1097676.3s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.673s, learning 0.159s)
               Value function loss: 16.0769
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 425.48
               Mean episode length: 249.80
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 10.83s
                        Total time: 33795.93s
                               ETA: 1097649.4s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.774s, learning 0.177s)
               Value function loss: 13.8527
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 425.21
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 10.95s
                        Total time: 33806.88s
                               ETA: 1097626.3s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.681s, learning 0.167s)
               Value function loss: 16.3710
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 433.98
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 10.85s
                        Total time: 33817.73s
                               ETA: 1097599.8s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.813s, learning 0.264s)
               Value function loss: 16.5791
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 432.82
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 11.08s
                        Total time: 33828.81s
                               ETA: 1097580.8s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.640s, learning 0.173s)
               Value function loss: 11.5274
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 435.71
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 10.81s
                        Total time: 33839.62s
                               ETA: 1097553.3s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.755s, learning 0.184s)
               Value function loss: 16.1512
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 431.96
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 10.94s
                        Total time: 33850.56s
                               ETA: 1097529.8s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.769s, learning 0.185s)
               Value function loss: 14.5534
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 432.46
               Mean episode length: 249.16
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 10.95s
                        Total time: 33861.52s
                               ETA: 1097506.8s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.997s, learning 0.163s)
               Value function loss: 13.4457
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 437.29
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 11.16s
                        Total time: 33872.68s
                               ETA: 1097490.5s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.505s, learning 0.165s)
               Value function loss: 15.5030
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 433.82
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 10.67s
                        Total time: 33883.35s
                               ETA: 1097458.4s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.709s, learning 0.160s)
               Value function loss: 15.1248
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 431.20
               Mean episode length: 249.70
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 10.87s
                        Total time: 33894.21s
                               ETA: 1097432.7s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.580s, learning 0.161s)
               Value function loss: 10.5695
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: 437.63
               Mean episode length: 249.70
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 10.74s
                        Total time: 33904.96s
                               ETA: 1097402.8s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.552s, learning 0.205s)
               Value function loss: 16.2587
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 444.92
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 10.76s
                        Total time: 33915.71s
                               ETA: 1097373.6s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.574s, learning 0.163s)
               Value function loss: 14.8808
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 442.60
               Mean episode length: 249.86
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 10.74s
                        Total time: 33926.45s
                               ETA: 1097343.6s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.591s, learning 0.162s)
               Value function loss: 9.9137
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 440.53
               Mean episode length: 249.86
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 10.75s
                        Total time: 33937.20s
                               ETA: 1097314.3s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.671s, learning 0.183s)
               Value function loss: 13.1752
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 438.36
               Mean episode length: 248.93
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 10.85s
                        Total time: 33948.06s
                               ETA: 1097288.1s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.692s, learning 0.185s)
               Value function loss: 13.7247
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 436.88
               Mean episode length: 248.68
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 10.88s
                        Total time: 33958.93s
                               ETA: 1097262.7s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.845s, learning 0.192s)
               Value function loss: 16.2855
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 435.82
               Mean episode length: 249.53
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 11.04s
                        Total time: 33969.97s
                               ETA: 1097242.5s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.853s, learning 0.235s)
               Value function loss: 15.8043
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 436.10
               Mean episode length: 249.73
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 11.09s
                        Total time: 33981.06s
                               ETA: 1097224.0s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.763s, learning 0.216s)
               Value function loss: 13.7543
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 435.60
               Mean episode length: 249.75
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 10.98s
                        Total time: 33992.04s
                               ETA: 1097201.9s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.722s, learning 0.160s)
               Value function loss: 14.0223
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 426.59
               Mean episode length: 248.48
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 10.88s
                        Total time: 34002.92s
                               ETA: 1097176.8s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.165s)
               Value function loss: 14.3759
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 425.49
               Mean episode length: 248.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 10.85s
                        Total time: 34013.77s
                               ETA: 1097150.5s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.517s, learning 0.159s)
               Value function loss: 15.5486
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 430.71
               Mean episode length: 249.40
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 10.68s
                        Total time: 34024.45s
                               ETA: 1097118.7s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.729s, learning 0.162s)
               Value function loss: 19.4687
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 427.17
               Mean episode length: 249.78
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 10.89s
                        Total time: 34035.34s
                               ETA: 1097093.9s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.869s, learning 0.175s)
               Value function loss: 18.9838
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 429.95
               Mean episode length: 249.21
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 11.04s
                        Total time: 34046.38s
                               ETA: 1097073.9s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.438s, learning 0.161s)
               Value function loss: 14.7155
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 438.22
               Mean episode length: 249.79
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 10.60s
                        Total time: 34056.98s
                               ETA: 1097039.7s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.164s)
               Value function loss: 16.8181
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 440.01
               Mean episode length: 249.79
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 10.83s
                        Total time: 34067.81s
                               ETA: 1097012.8s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.823s, learning 0.193s)
               Value function loss: 14.4333
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 436.79
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 11.02s
                        Total time: 34078.82s
                               ETA: 1096992.0s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.840s, learning 0.263s)
               Value function loss: 16.1942
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 422.51
               Mean episode length: 249.17
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 11.10s
                        Total time: 34089.93s
                               ETA: 1096974.1s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.159s)
               Value function loss: 18.7596
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 428.28
               Mean episode length: 249.19
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 10.73s
                        Total time: 34100.66s
                               ETA: 1096944.1s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.702s, learning 0.212s)
               Value function loss: 15.3128
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 431.00
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 10.91s
                        Total time: 34111.57s
                               ETA: 1096920.1s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.687s, learning 0.162s)
               Value function loss: 21.7826
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 433.40
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 10.85s
                        Total time: 34122.42s
                               ETA: 1096893.9s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.749s, learning 0.187s)
               Value function loss: 19.6578
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 424.31
               Mean episode length: 249.90
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 10.94s
                        Total time: 34133.36s
                               ETA: 1096870.6s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.667s, learning 0.214s)
               Value function loss: 17.8665
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 426.43
               Mean episode length: 249.59
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 10.88s
                        Total time: 34144.24s
                               ETA: 1096845.5s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.668s, learning 0.170s)
               Value function loss: 18.6453
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 435.09
               Mean episode length: 249.87
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 10.84s
                        Total time: 34155.08s
                               ETA: 1096819.0s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.588s, learning 0.224s)
               Value function loss: 20.2211
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 431.33
               Mean episode length: 249.65
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 10.81s
                        Total time: 34165.89s
                               ETA: 1096791.7s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.585s, learning 0.235s)
               Value function loss: 16.6328
                    Surrogate loss: -0.0172
             Mean action noise std: 0.72
                       Mean reward: 435.69
               Mean episode length: 249.88
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 10.82s
                        Total time: 34176.71s
                               ETA: 1096764.8s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1564 steps/s (collection: 10.269s, learning 0.206s)
               Value function loss: 19.1567
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 433.01
               Mean episode length: 247.86
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 10.48s
                        Total time: 34187.18s
                               ETA: 1096726.7s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.780s, learning 0.173s)
               Value function loss: 19.8036
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 437.71
               Mean episode length: 249.62
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 10.95s
                        Total time: 34198.14s
                               ETA: 1096704.0s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.992s, learning 0.170s)
               Value function loss: 16.3122
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 436.33
               Mean episode length: 249.67
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 11.16s
                        Total time: 34209.30s
                               ETA: 1096688.0s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.703s, learning 0.213s)
               Value function loss: 19.5849
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 430.42
               Mean episode length: 249.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 10.92s
                        Total time: 34220.22s
                               ETA: 1096664.1s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.818s, learning 0.170s)
               Value function loss: 19.9730
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 436.84
               Mean episode length: 249.25
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 10.99s
                        Total time: 34231.20s
                               ETA: 1096642.5s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.884s, learning 0.161s)
               Value function loss: 15.7416
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 440.44
               Mean episode length: 249.72
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 11.05s
                        Total time: 34242.25s
                               ETA: 1096622.7s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.159s)
               Value function loss: 18.1850
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 432.93
               Mean episode length: 249.47
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 10.93s
                        Total time: 34253.18s
                               ETA: 1096599.4s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.811s, learning 0.165s)
               Value function loss: 18.4183
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 428.39
               Mean episode length: 248.37
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 10.98s
                        Total time: 34264.16s
                               ETA: 1096577.4s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1458 steps/s (collection: 10.982s, learning 0.251s)
               Value function loss: 13.3023
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 430.44
               Mean episode length: 248.19
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.23s
                        Total time: 34275.39s
                               ETA: 1096563.7s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.885s, learning 0.165s)
               Value function loss: 14.8972
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 440.75
               Mean episode length: 249.19
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 11.05s
                        Total time: 34286.44s
                               ETA: 1096544.1s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.636s, learning 0.174s)
               Value function loss: 17.0191
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 437.94
               Mean episode length: 248.25
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 10.81s
                        Total time: 34297.25s
                               ETA: 1096516.9s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1548 steps/s (collection: 10.419s, learning 0.163s)
               Value function loss: 16.6122
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 434.70
               Mean episode length: 248.63
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 10.58s
                        Total time: 34307.83s
                               ETA: 1096482.4s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.019s, learning 0.173s)
               Value function loss: 16.9898
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 438.59
               Mean episode length: 249.88
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 11.19s
                        Total time: 34319.02s
                               ETA: 1096467.4s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.161s)
               Value function loss: 15.0301
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 440.51
               Mean episode length: 249.48
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 10.91s
                        Total time: 34329.93s
                               ETA: 1096443.4s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.839s, learning 0.169s)
               Value function loss: 17.4950
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 429.88
               Mean episode length: 249.48
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 11.01s
                        Total time: 34340.94s
                               ETA: 1096422.5s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.685s, learning 0.171s)
               Value function loss: 16.1763
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 418.32
               Mean episode length: 247.47
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 10.86s
                        Total time: 34351.80s
                               ETA: 1096396.8s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.762s, learning 0.158s)
               Value function loss: 17.5285
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 433.58
               Mean episode length: 249.52
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 10.92s
                        Total time: 34362.72s
                               ETA: 1096373.1s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.667s, learning 0.157s)
               Value function loss: 20.0321
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 437.11
               Mean episode length: 249.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 10.82s
                        Total time: 34373.54s
                               ETA: 1096346.4s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.629s, learning 0.165s)
               Value function loss: 14.7623
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 423.74
               Mean episode length: 248.16
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 10.79s
                        Total time: 34384.34s
                               ETA: 1096318.7s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.805s, learning 0.273s)
               Value function loss: 14.9655
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 425.82
               Mean episode length: 249.30
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 11.08s
                        Total time: 34395.41s
                               ETA: 1096300.1s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.589s, learning 0.165s)
               Value function loss: 14.4486
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 430.03
               Mean episode length: 249.58
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 10.75s
                        Total time: 34406.17s
                               ETA: 1096271.2s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.589s, learning 0.188s)
               Value function loss: 15.5213
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 429.61
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 10.78s
                        Total time: 34416.95s
                               ETA: 1096243.0s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1575 steps/s (collection: 10.229s, learning 0.167s)
               Value function loss: 13.4191
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 432.08
               Mean episode length: 249.99
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 10.40s
                        Total time: 34427.34s
                               ETA: 1096202.7s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.481s, learning 0.292s)
               Value function loss: 15.6806
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 421.59
               Mean episode length: 249.55
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 10.77s
                        Total time: 34438.11s
                               ETA: 1096174.4s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.776s, learning 0.166s)
               Value function loss: 14.2716
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 415.98
               Mean episode length: 247.93
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 10.94s
                        Total time: 34449.06s
                               ETA: 1096151.5s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.464s, learning 0.162s)
               Value function loss: 20.2400
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 430.92
               Mean episode length: 249.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 10.63s
                        Total time: 34459.68s
                               ETA: 1096118.6s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.710s, learning 0.261s)
               Value function loss: 17.6830
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 427.36
               Mean episode length: 248.42
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 10.97s
                        Total time: 34470.65s
                               ETA: 1096096.7s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.713s, learning 0.159s)
               Value function loss: 13.8624
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 430.40
               Mean episode length: 248.91
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 10.87s
                        Total time: 34481.52s
                               ETA: 1096071.6s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.757s, learning 0.170s)
               Value function loss: 18.3603
                    Surrogate loss: -0.0172
             Mean action noise std: 0.72
                       Mean reward: 436.69
               Mean episode length: 249.78
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 10.93s
                        Total time: 34492.45s
                               ETA: 1096048.2s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.603s, learning 0.161s)
               Value function loss: 18.8686
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 435.23
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 10.76s
                        Total time: 34503.22s
                               ETA: 1096019.8s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.548s, learning 0.209s)
               Value function loss: 13.0581
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 432.93
               Mean episode length: 249.77
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 10.76s
                        Total time: 34513.97s
                               ETA: 1095991.1s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.164s)
               Value function loss: 15.7463
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 428.91
               Mean episode length: 249.87
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 10.76s
                        Total time: 34524.73s
                               ETA: 1095962.5s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.736s, learning 0.168s)
               Value function loss: 15.8646
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 425.52
               Mean episode length: 249.15
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 10.90s
                        Total time: 34535.64s
                               ETA: 1095938.4s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.712s, learning 0.179s)
               Value function loss: 12.7732
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 427.25
               Mean episode length: 248.81
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 10.89s
                        Total time: 34546.53s
                               ETA: 1095914.0s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.426s, learning 0.163s)
               Value function loss: 16.4608
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 427.81
               Mean episode length: 249.66
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 10.59s
                        Total time: 34557.12s
                               ETA: 1095880.0s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.518s, learning 0.174s)
               Value function loss: 16.8240
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 429.74
               Mean episode length: 249.93
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 10.69s
                        Total time: 34567.81s
                               ETA: 1095849.3s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.594s, learning 0.214s)
               Value function loss: 14.8116
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 430.44
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 10.81s
                        Total time: 34578.62s
                               ETA: 1095822.3s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.598s, learning 0.226s)
               Value function loss: 12.7786
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 433.20
               Mean episode length: 249.67
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 10.82s
                        Total time: 34589.44s
                               ETA: 1095795.8s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.823s, learning 0.167s)
               Value function loss: 16.6224
                    Surrogate loss: -0.0195
             Mean action noise std: 0.72
                       Mean reward: 429.49
               Mean episode length: 249.08
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 10.99s
                        Total time: 34600.43s
                               ETA: 1095774.6s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.547s, learning 0.164s)
               Value function loss: 13.2699
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 423.02
               Mean episode length: 249.41
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 10.71s
                        Total time: 34611.14s
                               ETA: 1095744.5s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.759s, learning 0.165s)
               Value function loss: 14.3038
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 429.87
               Mean episode length: 249.49
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 10.92s
                        Total time: 34622.07s
                               ETA: 1095721.2s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.470s, learning 0.178s)
               Value function loss: 12.9539
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 433.75
               Mean episode length: 249.49
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 10.65s
                        Total time: 34632.72s
                               ETA: 1095689.2s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.639s, learning 0.172s)
               Value function loss: 13.0093
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 432.29
               Mean episode length: 249.39
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 10.81s
                        Total time: 34643.53s
                               ETA: 1095662.3s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.674s, learning 0.191s)
               Value function loss: 12.5560
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 428.43
               Mean episode length: 249.39
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 10.87s
                        Total time: 34654.39s
                               ETA: 1095637.1s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.565s, learning 0.161s)
               Value function loss: 15.2918
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 432.05
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 10.73s
                        Total time: 34665.12s
                               ETA: 1095607.6s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.754s, learning 0.167s)
               Value function loss: 16.8449
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 430.28
               Mean episode length: 249.67
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 10.92s
                        Total time: 34676.04s
                               ETA: 1095584.3s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.722s, learning 0.160s)
               Value function loss: 16.8352
                    Surrogate loss: -0.0221
             Mean action noise std: 0.72
                       Mean reward: 427.08
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 10.88s
                        Total time: 34686.92s
                               ETA: 1095559.7s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.724s, learning 0.206s)
               Value function loss: 16.9847
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 432.54
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 10.93s
                        Total time: 34697.85s
                               ETA: 1095536.6s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.752s, learning 0.204s)
               Value function loss: 17.0568
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 429.37
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 10.96s
                        Total time: 34708.81s
                               ETA: 1095514.3s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.889s, learning 0.165s)
               Value function loss: 12.8178
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 415.98
               Mean episode length: 248.72
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 11.05s
                        Total time: 34719.86s
                               ETA: 1095495.2s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.668s, learning 0.266s)
               Value function loss: 15.3472
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 414.47
               Mean episode length: 248.72
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 10.93s
                        Total time: 34730.79s
                               ETA: 1095472.3s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.734s, learning 0.198s)
               Value function loss: 13.6697
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 426.92
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 10.93s
                        Total time: 34741.73s
                               ETA: 1095449.3s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.595s, learning 0.168s)
               Value function loss: 12.5951
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 428.64
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 10.76s
                        Total time: 34752.49s
                               ETA: 1095421.0s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.750s, learning 0.282s)
               Value function loss: 13.3769
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 426.15
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 11.03s
                        Total time: 34763.52s
                               ETA: 1095401.2s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.502s, learning 0.172s)
               Value function loss: 15.5919
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 433.66
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 10.67s
                        Total time: 34774.19s
                               ETA: 1095370.2s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.928s, learning 0.171s)
               Value function loss: 13.4288
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 428.46
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 11.10s
                        Total time: 34785.29s
                               ETA: 1095352.5s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.583s, learning 0.201s)
               Value function loss: 17.1612
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 426.37
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 10.78s
                        Total time: 34796.08s
                               ETA: 1095324.9s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.445s, learning 0.192s)
               Value function loss: 17.0440
                    Surrogate loss: -0.0228
             Mean action noise std: 0.72
                       Mean reward: 430.38
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 10.64s
                        Total time: 34806.71s
                               ETA: 1095292.7s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.722s, learning 0.164s)
               Value function loss: 12.6678
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 428.79
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 10.89s
                        Total time: 34817.60s
                               ETA: 1095268.4s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.705s, learning 0.165s)
               Value function loss: 13.8210
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 429.43
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 10.87s
                        Total time: 34828.47s
                               ETA: 1095243.5s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.517s, learning 0.212s)
               Value function loss: 17.5310
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 429.64
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 10.73s
                        Total time: 34839.20s
                               ETA: 1095214.2s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.538s, learning 0.184s)
               Value function loss: 14.2047
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 429.69
               Mean episode length: 248.84
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 10.72s
                        Total time: 34849.92s
                               ETA: 1095184.8s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.606s, learning 0.159s)
               Value function loss: 14.7839
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 428.54
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 10.77s
                        Total time: 34860.69s
                               ETA: 1095156.7s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.380s, learning 0.170s)
               Value function loss: 16.4908
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 433.83
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 10.55s
                        Total time: 34871.24s
                               ETA: 1095121.8s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.658s, learning 0.174s)
               Value function loss: 16.3371
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 439.35
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 10.83s
                        Total time: 34882.07s
                               ETA: 1095095.8s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.638s, learning 0.182s)
               Value function loss: 15.1882
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 439.81
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 10.82s
                        Total time: 34892.89s
                               ETA: 1095069.5s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.614s, learning 0.159s)
               Value function loss: 15.8321
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 434.28
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 10.77s
                        Total time: 34903.66s
                               ETA: 1095041.6s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.545s, learning 0.163s)
               Value function loss: 14.5660
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 438.53
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 10.71s
                        Total time: 34914.37s
                               ETA: 1095011.8s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.792s, learning 0.158s)
               Value function loss: 10.1775
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 437.99
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 10.95s
                        Total time: 34925.32s
                               ETA: 1094989.6s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.508s, learning 0.168s)
               Value function loss: 15.6024
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 435.20
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 10.68s
                        Total time: 34936.00s
                               ETA: 1094958.7s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.773s, learning 0.163s)
               Value function loss: 12.6716
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 437.88
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 10.94s
                        Total time: 34946.93s
                               ETA: 1094936.1s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.688s, learning 0.159s)
               Value function loss: 12.2392
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 428.68
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 10.85s
                        Total time: 34957.78s
                               ETA: 1094910.7s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.632s, learning 0.170s)
               Value function loss: 13.9210
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 421.88
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 10.80s
                        Total time: 34968.58s
                               ETA: 1094883.8s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1553 steps/s (collection: 10.388s, learning 0.161s)
               Value function loss: 13.1754
                    Surrogate loss: -0.0217
             Mean action noise std: 0.72
                       Mean reward: 427.13
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 10.55s
                        Total time: 34979.13s
                               ETA: 1094849.1s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.654s, learning 0.163s)
               Value function loss: 14.2331
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 428.60
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 10.82s
                        Total time: 34989.95s
                               ETA: 1094822.7s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.618s, learning 0.167s)
               Value function loss: 16.2793
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 434.43
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 10.79s
                        Total time: 35000.73s
                               ETA: 1094795.4s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1602 steps/s (collection: 9.994s, learning 0.230s)
               Value function loss: 15.3111
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 438.70
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 10.22s
                        Total time: 35010.96s
                               ETA: 1094750.5s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.658s, learning 0.213s)
               Value function loss: 16.9808
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 431.63
               Mean episode length: 249.61
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 10.87s
                        Total time: 35021.83s
                               ETA: 1094725.9s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.641s, learning 0.159s)
               Value function loss: 17.0695
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 432.07
               Mean episode length: 249.61
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 10.80s
                        Total time: 35032.63s
                               ETA: 1094699.0s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.651s, learning 0.192s)
               Value function loss: 15.7759
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 424.86
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 10.84s
                        Total time: 35043.47s
                               ETA: 1094673.5s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.550s, learning 0.167s)
               Value function loss: 16.5305
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 427.32
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 10.72s
                        Total time: 35054.19s
                               ETA: 1094644.1s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.695s, learning 0.171s)
               Value function loss: 16.6149
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 431.44
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 10.87s
                        Total time: 35065.05s
                               ETA: 1094619.4s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.599s, learning 0.157s)
               Value function loss: 15.6855
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 426.81
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 10.76s
                        Total time: 35075.81s
                               ETA: 1094591.2s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.375s, learning 0.165s)
               Value function loss: 13.4130
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 417.19
               Mean episode length: 249.83
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 10.54s
                        Total time: 35086.35s
                               ETA: 1094556.3s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.772s, learning 0.276s)
               Value function loss: 14.5935
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 418.99
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 11.05s
                        Total time: 35097.40s
                               ETA: 1094537.3s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.701s, learning 0.168s)
               Value function loss: 16.9644
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 430.94
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 10.87s
                        Total time: 35108.27s
                               ETA: 1094512.6s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.489s, learning 0.159s)
               Value function loss: 17.8927
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 428.20
               Mean episode length: 249.80
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 10.65s
                        Total time: 35118.91s
                               ETA: 1094481.2s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.515s, learning 0.213s)
               Value function loss: 23.6365
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 434.76
               Mean episode length: 249.98
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 10.73s
                        Total time: 35129.64s
                               ETA: 1094452.2s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.790s, learning 0.165s)
               Value function loss: 18.0686
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 436.40
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 10.95s
                        Total time: 35140.60s
                               ETA: 1094430.3s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.593s, learning 0.169s)
               Value function loss: 20.1126
                    Surrogate loss: -0.0172
             Mean action noise std: 0.72
                       Mean reward: 429.08
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 10.76s
                        Total time: 35151.36s
                               ETA: 1094402.4s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.795s, learning 0.162s)
               Value function loss: 15.7681
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 429.29
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 10.96s
                        Total time: 35162.32s
                               ETA: 1094380.5s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.574s, learning 0.183s)
               Value function loss: 21.4540
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 438.12
               Mean episode length: 249.62
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 10.76s
                        Total time: 35173.07s
                               ETA: 1094352.5s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.524s, learning 0.161s)
               Value function loss: 19.4131
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 440.60
               Mean episode length: 250.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 10.69s
                        Total time: 35183.76s
                               ETA: 1094322.2s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.709s, learning 0.165s)
               Value function loss: 16.4235
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 437.02
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 10.87s
                        Total time: 35194.63s
                               ETA: 1094297.8s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.604s, learning 0.166s)
               Value function loss: 16.9015
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 434.89
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 10.77s
                        Total time: 35205.40s
                               ETA: 1094270.2s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.522s, learning 0.170s)
               Value function loss: 21.5178
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 430.74
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 10.69s
                        Total time: 35216.09s
                               ETA: 1094240.2s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.343s, learning 0.213s)
               Value function loss: 16.3365
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 435.55
               Mean episode length: 250.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 10.56s
                        Total time: 35226.65s
                               ETA: 1094206.0s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.677s, learning 0.167s)
               Value function loss: 16.4438
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 432.59
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 10.84s
                        Total time: 35237.50s
                               ETA: 1094180.7s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.797s, learning 0.173s)
               Value function loss: 18.4550
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 433.46
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 10.97s
                        Total time: 35248.47s
                               ETA: 1094159.3s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1540 steps/s (collection: 10.466s, learning 0.167s)
               Value function loss: 11.3679
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 429.65
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 10.63s
                        Total time: 35259.10s
                               ETA: 1094127.5s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.530s, learning 0.187s)
               Value function loss: 17.1796
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 429.02
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 10.72s
                        Total time: 35269.82s
                               ETA: 1094098.4s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1547 steps/s (collection: 10.376s, learning 0.211s)
               Value function loss: 16.9493
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 432.05
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 10.59s
                        Total time: 35280.40s
                               ETA: 1094065.2s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.668s, learning 0.173s)
               Value function loss: 12.7569
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 433.49
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 10.84s
                        Total time: 35291.24s
                               ETA: 1094039.9s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.160s)
               Value function loss: 15.0949
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 432.65
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 10.76s
                        Total time: 35302.00s
                               ETA: 1094012.0s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.861s, learning 0.161s)
               Value function loss: 14.0250
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 424.86
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 11.02s
                        Total time: 35313.03s
                               ETA: 1093992.3s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.562s, learning 0.160s)
               Value function loss: 14.4702
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 430.55
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 10.72s
                        Total time: 35323.75s
                               ETA: 1093963.4s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.523s, learning 0.165s)
               Value function loss: 15.2070
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 427.11
               Mean episode length: 249.97
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 10.69s
                        Total time: 35334.44s
                               ETA: 1093933.4s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.595s, learning 0.183s)
               Value function loss: 15.2722
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 421.83
               Mean episode length: 249.97
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 10.78s
                        Total time: 35345.21s
                               ETA: 1093906.2s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.656s, learning 0.200s)
               Value function loss: 17.3221
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 424.49
               Mean episode length: 249.55
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 10.86s
                        Total time: 35356.07s
                               ETA: 1093881.4s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1619 steps/s (collection: 9.958s, learning 0.162s)
               Value function loss: 15.6374
                    Surrogate loss: -0.0122
             Mean action noise std: 0.72
                       Mean reward: 434.79
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 10.12s
                        Total time: 35366.19s
                               ETA: 1093833.8s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.793s, learning 0.240s)
               Value function loss: 14.3590
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 433.38
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 11.03s
                        Total time: 35377.22s
                               ETA: 1093814.5s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.588s, learning 0.170s)
               Value function loss: 20.1062
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 433.49
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 10.76s
                        Total time: 35387.98s
                               ETA: 1093786.7s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.406s, learning 0.164s)
               Value function loss: 15.4335
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 434.54
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 10.57s
                        Total time: 35398.55s
                               ETA: 1093753.1s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.165s)
               Value function loss: 14.4132
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 427.94
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 10.67s
                        Total time: 35409.23s
                               ETA: 1093722.8s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.519s, learning 0.161s)
               Value function loss: 15.6249
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 426.37
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 10.68s
                        Total time: 35419.91s
                               ETA: 1093692.6s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.734s, learning 0.173s)
               Value function loss: 16.7843
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 435.23
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 10.91s
                        Total time: 35430.81s
                               ETA: 1093669.5s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.736s, learning 0.163s)
               Value function loss: 15.5010
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 441.30
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 10.90s
                        Total time: 35441.71s
                               ETA: 1093646.1s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.690s, learning 0.192s)
               Value function loss: 17.2798
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 434.48
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 10.88s
                        Total time: 35452.59s
                               ETA: 1093622.2s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.506s, learning 0.160s)
               Value function loss: 14.3837
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 432.32
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 10.67s
                        Total time: 35463.26s
                               ETA: 1093591.6s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.778s, learning 0.159s)
               Value function loss: 21.3787
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 433.53
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 10.94s
                        Total time: 35474.20s
                               ETA: 1093569.5s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.827s, learning 0.172s)
               Value function loss: 17.6020
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 437.89
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 11.00s
                        Total time: 35485.20s
                               ETA: 1093549.2s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.647s, learning 0.161s)
               Value function loss: 16.3776
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 440.28
               Mean episode length: 248.23
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 10.81s
                        Total time: 35496.00s
                               ETA: 1093523.0s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.653s, learning 0.163s)
               Value function loss: 16.7926
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 428.91
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 10.82s
                        Total time: 35506.82s
                               ETA: 1093497.1s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.018s, learning 0.198s)
               Value function loss: 18.0489
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 434.86
               Mean episode length: 249.99
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 11.22s
                        Total time: 35518.03s
                               ETA: 1093483.6s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.996s, learning 0.164s)
               Value function loss: 14.7530
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 424.24
               Mean episode length: 249.37
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 11.16s
                        Total time: 35529.19s
                               ETA: 1093468.3s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.721s, learning 0.178s)
               Value function loss: 18.4989
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 427.72
               Mean episode length: 249.27
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 10.90s
                        Total time: 35540.09s
                               ETA: 1093444.9s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.615s, learning 0.161s)
               Value function loss: 19.2155
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 432.32
               Mean episode length: 249.99
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 10.78s
                        Total time: 35550.87s
                               ETA: 1093417.8s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.760s, learning 0.167s)
               Value function loss: 16.4563
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 430.71
               Mean episode length: 249.99
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 10.93s
                        Total time: 35561.80s
                               ETA: 1093395.4s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.706s, learning 0.167s)
               Value function loss: 16.8456
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 439.84
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 10.87s
                        Total time: 35572.67s
                               ETA: 1093371.3s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.745s, learning 0.169s)
               Value function loss: 15.5046
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 437.37
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 10.91s
                        Total time: 35583.58s
                               ETA: 1093348.5s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.617s, learning 0.165s)
               Value function loss: 14.1849
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 437.90
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 10.78s
                        Total time: 35594.36s
                               ETA: 1093321.6s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1562 steps/s (collection: 10.322s, learning 0.161s)
               Value function loss: 14.7353
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 440.38
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 10.48s
                        Total time: 35604.85s
                               ETA: 1093285.5s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.502s, learning 0.168s)
               Value function loss: 15.8860
                    Surrogate loss: -0.0209
             Mean action noise std: 0.72
                       Mean reward: 441.02
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 10.67s
                        Total time: 35615.52s
                               ETA: 1093255.3s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.984s, learning 0.181s)
               Value function loss: 11.7285
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 434.77
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 11.16s
                        Total time: 35626.68s
                               ETA: 1093240.2s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.749s, learning 0.173s)
               Value function loss: 11.9672
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 436.10
               Mean episode length: 249.88
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 10.92s
                        Total time: 35637.60s
                               ETA: 1093217.7s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.545s, learning 0.177s)
               Value function loss: 14.3329
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 434.32
               Mean episode length: 249.88
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 10.72s
                        Total time: 35648.33s
                               ETA: 1093189.0s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.502s, learning 0.161s)
               Value function loss: 13.4244
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 435.10
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 10.66s
                        Total time: 35658.99s
                               ETA: 1093158.5s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.777s, learning 0.208s)
               Value function loss: 17.4758
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 437.96
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 10.99s
                        Total time: 35669.97s
                               ETA: 1093137.9s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.596s, learning 0.170s)
               Value function loss: 15.2440
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 435.91
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 10.77s
                        Total time: 35680.74s
                               ETA: 1093110.7s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.148s, learning 0.172s)
               Value function loss: 18.4034
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 432.94
               Mean episode length: 249.67
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 10.32s
                        Total time: 35691.06s
                               ETA: 1093069.7s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.746s, learning 0.176s)
               Value function loss: 16.9449
                    Surrogate loss: -0.0193
             Mean action noise std: 0.72
                       Mean reward: 428.46
               Mean episode length: 249.67
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 10.92s
                        Total time: 35701.98s
                               ETA: 1093047.2s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.531s, learning 0.165s)
               Value function loss: 16.0255
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 429.61
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 10.70s
                        Total time: 35712.68s
                               ETA: 1093017.9s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.718s, learning 0.160s)
               Value function loss: 20.9181
                    Surrogate loss: -0.0172
             Mean action noise std: 0.72
                       Mean reward: 419.42
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 10.88s
                        Total time: 35723.56s
                               ETA: 1092994.1s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.747s, learning 0.166s)
               Value function loss: 15.5666
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 434.01
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 10.91s
                        Total time: 35734.47s
                               ETA: 1092971.3s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.672s, learning 0.187s)
               Value function loss: 16.3761
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 441.06
               Mean episode length: 249.25
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 10.86s
                        Total time: 35745.33s
                               ETA: 1092947.0s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.713s, learning 0.188s)
               Value function loss: 14.4487
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 434.11
               Mean episode length: 249.25
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 10.90s
                        Total time: 35756.23s
                               ETA: 1092923.9s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.860s, learning 0.278s)
               Value function loss: 16.5747
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 436.86
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 11.14s
                        Total time: 35767.37s
                               ETA: 1092908.0s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.713s, learning 0.172s)
               Value function loss: 14.2452
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 436.98
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 10.89s
                        Total time: 35778.25s
                               ETA: 1092884.5s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.587s, learning 0.188s)
               Value function loss: 16.7708
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 436.62
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 10.77s
                        Total time: 35789.03s
                               ETA: 1092857.6s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.630s, learning 0.221s)
               Value function loss: 16.0503
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 444.12
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 10.85s
                        Total time: 35799.88s
                               ETA: 1092833.1s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.399s, learning 0.211s)
               Value function loss: 21.0816
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 442.44
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 10.61s
                        Total time: 35810.49s
                               ETA: 1092801.1s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.493s, learning 0.187s)
               Value function loss: 16.9342
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 429.59
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 10.68s
                        Total time: 35821.17s
                               ETA: 1092771.3s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.896s, learning 0.206s)
               Value function loss: 13.9662
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 436.01
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 11.10s
                        Total time: 35832.27s
                               ETA: 1092754.4s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.745s, learning 0.243s)
               Value function loss: 16.4423
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 437.35
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 10.99s
                        Total time: 35843.26s
                               ETA: 1092734.1s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.069s, learning 0.187s)
               Value function loss: 17.1314
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 432.84
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 11.26s
                        Total time: 35854.51s
                               ETA: 1092721.9s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.735s, learning 0.158s)
               Value function loss: 14.7028
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 429.63
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 10.89s
                        Total time: 35865.41s
                               ETA: 1092698.6s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.166s)
               Value function loss: 15.4324
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 428.42
               Mean episode length: 247.90
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 10.68s
                        Total time: 35876.08s
                               ETA: 1092668.7s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.708s, learning 0.200s)
               Value function loss: 16.4220
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 439.01
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 10.91s
                        Total time: 35886.99s
                               ETA: 1092646.0s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.353s, learning 0.163s)
               Value function loss: 13.6739
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 439.09
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 10.52s
                        Total time: 35897.51s
                               ETA: 1092611.3s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.495s, learning 0.168s)
               Value function loss: 14.8468
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 433.10
               Mean episode length: 249.67
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 10.66s
                        Total time: 35908.17s
                               ETA: 1092581.1s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1553 steps/s (collection: 10.368s, learning 0.176s)
               Value function loss: 17.0330
                    Surrogate loss: -0.0204
             Mean action noise std: 0.72
                       Mean reward: 442.74
               Mean episode length: 249.67
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 10.54s
                        Total time: 35918.71s
                               ETA: 1092547.3s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.645s, learning 0.202s)
               Value function loss: 13.3606
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 432.63
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 10.85s
                        Total time: 35929.56s
                               ETA: 1092522.7s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.893s, learning 0.189s)
               Value function loss: 10.6327
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 430.64
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 11.08s
                        Total time: 35940.64s
                               ETA: 1092505.3s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1554 steps/s (collection: 10.371s, learning 0.168s)
               Value function loss: 17.9047
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 433.76
               Mean episode length: 248.25
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 10.54s
                        Total time: 35951.18s
                               ETA: 1092471.3s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.613s, learning 0.159s)
               Value function loss: 13.1786
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 437.59
               Mean episode length: 248.25
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 10.77s
                        Total time: 35961.95s
                               ETA: 1092444.5s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.711s, learning 0.216s)
               Value function loss: 14.0105
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 437.06
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 10.93s
                        Total time: 35972.88s
                               ETA: 1092422.4s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.875s, learning 0.166s)
               Value function loss: 11.6826
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 433.96
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 11.04s
                        Total time: 35983.92s
                               ETA: 1092403.7s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.818s, learning 0.166s)
               Value function loss: 14.3072
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 432.60
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 10.98s
                        Total time: 35994.91s
                               ETA: 1092383.4s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.655s, learning 0.176s)
               Value function loss: 14.5780
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 440.88
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 10.83s
                        Total time: 36005.74s
                               ETA: 1092358.3s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.895s, learning 0.161s)
               Value function loss: 16.8668
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 438.30
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 11.06s
                        Total time: 36016.79s
                               ETA: 1092340.1s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.824s, learning 0.167s)
               Value function loss: 16.9576
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 430.91
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 10.99s
                        Total time: 36027.78s
                               ETA: 1092320.0s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.820s, learning 0.171s)
               Value function loss: 17.7162
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 437.91
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 10.99s
                        Total time: 36038.78s
                               ETA: 1092299.9s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.782s, learning 0.161s)
               Value function loss: 20.0818
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 444.33
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 10.94s
                        Total time: 36049.72s
                               ETA: 1092278.3s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.672s, learning 0.162s)
               Value function loss: 17.5043
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 440.65
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 10.83s
                        Total time: 36060.55s
                               ETA: 1092253.4s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1536 steps/s (collection: 10.500s, learning 0.164s)
               Value function loss: 16.6952
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 440.78
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 10.66s
                        Total time: 36071.22s
                               ETA: 1092223.3s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.738s, learning 0.194s)
               Value function loss: 18.1690
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 435.41
               Mean episode length: 249.72
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 10.93s
                        Total time: 36082.15s
                               ETA: 1092201.4s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.743s, learning 0.197s)
               Value function loss: 16.7109
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 440.09
               Mean episode length: 249.72
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 10.94s
                        Total time: 36093.09s
                               ETA: 1092179.8s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.760s, learning 0.197s)
               Value function loss: 13.8035
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 438.58
               Mean episode length: 249.52
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 10.96s
                        Total time: 36104.05s
                               ETA: 1092158.7s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.756s, learning 0.170s)
               Value function loss: 14.7623
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 441.15
               Mean episode length: 249.52
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 10.93s
                        Total time: 36114.97s
                               ETA: 1092136.6s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.753s, learning 0.187s)
               Value function loss: 18.0784
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 441.81
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 10.94s
                        Total time: 36125.91s
                               ETA: 1092114.9s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.523s, learning 0.195s)
               Value function loss: 16.0998
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 446.15
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 10.72s
                        Total time: 36136.63s
                               ETA: 1092086.6s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.727s, learning 0.205s)
               Value function loss: 19.9692
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 443.67
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 10.93s
                        Total time: 36147.56s
                               ETA: 1092064.7s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.924s, learning 0.161s)
               Value function loss: 20.8027
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 442.43
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 11.09s
                        Total time: 36158.65s
                               ETA: 1092047.5s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.786s, learning 0.178s)
               Value function loss: 14.6526
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 438.70
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 10.96s
                        Total time: 36169.61s
                               ETA: 1092026.6s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.766s, learning 0.174s)
               Value function loss: 16.3926
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 443.16
               Mean episode length: 249.51
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 10.94s
                        Total time: 36180.55s
                               ETA: 1092005.0s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.644s, learning 0.191s)
               Value function loss: 17.5784
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 433.86
               Mean episode length: 249.12
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 10.84s
                        Total time: 36191.38s
                               ETA: 1091980.3s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.796s, learning 0.175s)
               Value function loss: 15.4053
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 436.57
               Mean episode length: 249.61
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 10.97s
                        Total time: 36202.36s
                               ETA: 1091959.6s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.641s, learning 0.173s)
               Value function loss: 17.2298
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 440.45
               Mean episode length: 249.88
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 10.81s
                        Total time: 36213.17s
                               ETA: 1091934.2s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.540s, learning 0.164s)
               Value function loss: 14.6802
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 437.26
               Mean episode length: 249.42
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 10.70s
                        Total time: 36223.87s
                               ETA: 1091905.6s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.431s, learning 0.213s)
               Value function loss: 15.9970
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 433.42
               Mean episode length: 249.54
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 10.64s
                        Total time: 36234.52s
                               ETA: 1091875.1s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.861s, learning 0.170s)
               Value function loss: 13.3923
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 430.39
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 11.03s
                        Total time: 36245.55s
                               ETA: 1091856.3s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.541s, learning 0.167s)
               Value function loss: 15.0208
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 432.65
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 10.71s
                        Total time: 36256.26s
                               ETA: 1091827.7s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.838s, learning 0.167s)
               Value function loss: 13.5648
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 430.23
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 11.00s
                        Total time: 36267.26s
                               ETA: 1091808.2s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.711s, learning 0.164s)
               Value function loss: 9.7156
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 430.74
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 10.87s
                        Total time: 36278.14s
                               ETA: 1091784.7s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.717s, learning 0.186s)
               Value function loss: 13.8467
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 433.07
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 10.90s
                        Total time: 36289.04s
                               ETA: 1091762.1s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.495s, learning 0.163s)
               Value function loss: 12.7417
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 431.62
               Mean episode length: 249.65
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 10.66s
                        Total time: 36299.70s
                               ETA: 1091732.1s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.754s, learning 0.189s)
               Value function loss: 10.8009
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 431.19
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 10.94s
                        Total time: 36310.64s
                               ETA: 1091710.6s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.713s, learning 0.159s)
               Value function loss: 10.5993
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 429.64
               Mean episode length: 249.98
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 10.87s
                        Total time: 36321.51s
                               ETA: 1091687.1s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.441s, learning 0.162s)
               Value function loss: 13.1385
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 429.03
               Mean episode length: 249.98
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 10.60s
                        Total time: 36332.12s
                               ETA: 1091655.5s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.749s, learning 0.163s)
               Value function loss: 12.0942
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 429.77
               Mean episode length: 249.44
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 10.91s
                        Total time: 36343.03s
                               ETA: 1091633.2s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.474s, learning 0.168s)
               Value function loss: 13.8434
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 433.39
               Mean episode length: 249.44
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 10.64s
                        Total time: 36353.67s
                               ETA: 1091602.7s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.920s, learning 0.168s)
               Value function loss: 13.0649
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 431.44
               Mean episode length: 248.83
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 11.09s
                        Total time: 36364.76s
                               ETA: 1091585.7s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.758s, learning 0.170s)
               Value function loss: 13.1761
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 429.27
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 10.93s
                        Total time: 36375.69s
                               ETA: 1091563.9s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.602s, learning 0.173s)
               Value function loss: 14.5942
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 439.18
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 10.77s
                        Total time: 36386.46s
                               ETA: 1091537.5s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.773s, learning 0.248s)
               Value function loss: 13.2378
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 436.09
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 11.02s
                        Total time: 36397.48s
                               ETA: 1091518.4s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.895s, learning 0.165s)
               Value function loss: 18.1404
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 432.10
               Mean episode length: 249.24
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 11.06s
                        Total time: 36408.54s
                               ETA: 1091500.6s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.623s, learning 0.165s)
               Value function loss: 15.5915
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 429.37
               Mean episode length: 249.24
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 10.79s
                        Total time: 36419.33s
                               ETA: 1091474.6s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.941s, learning 0.161s)
               Value function loss: 15.4978
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 441.25
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 11.10s
                        Total time: 36430.43s
                               ETA: 1091458.0s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.777s, learning 0.190s)
               Value function loss: 14.8029
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 436.89
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 10.97s
                        Total time: 36441.40s
                               ETA: 1091437.4s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.598s, learning 0.166s)
               Value function loss: 13.1770
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 427.95
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 10.76s
                        Total time: 36452.16s
                               ETA: 1091410.7s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.739s, learning 0.159s)
               Value function loss: 15.3786
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 432.95
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 10.90s
                        Total time: 36463.06s
                               ETA: 1091388.0s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.641s, learning 0.188s)
               Value function loss: 15.4415
                    Surrogate loss: -0.0196
             Mean action noise std: 0.72
                       Mean reward: 438.37
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 10.83s
                        Total time: 36473.89s
                               ETA: 1091363.3s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.569s, learning 0.186s)
               Value function loss: 19.2533
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 436.57
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 10.76s
                        Total time: 36484.64s
                               ETA: 1091336.4s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.869s, learning 0.188s)
               Value function loss: 16.9138
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 436.84
               Mean episode length: 249.27
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 11.06s
                        Total time: 36495.70s
                               ETA: 1091318.5s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.764s, learning 0.174s)
               Value function loss: 16.6877
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 434.52
               Mean episode length: 249.27
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 10.94s
                        Total time: 36506.64s
                               ETA: 1091297.0s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.868s, learning 0.164s)
               Value function loss: 11.7295
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 435.30
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 11.03s
                        Total time: 36517.67s
                               ETA: 1091278.4s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.355s, learning 0.171s)
               Value function loss: 16.0922
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 432.20
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 10.53s
                        Total time: 36528.20s
                               ETA: 1091244.7s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.441s, learning 0.185s)
               Value function loss: 16.0432
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 440.87
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 10.63s
                        Total time: 36538.83s
                               ETA: 1091214.0s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.757s, learning 0.173s)
               Value function loss: 15.1949
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 435.94
               Mean episode length: 249.26
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 10.93s
                        Total time: 36549.76s
                               ETA: 1091192.3s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.881s, learning 0.162s)
               Value function loss: 15.8878
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 430.99
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 11.04s
                        Total time: 36560.80s
                               ETA: 1091174.1s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.512s, learning 0.200s)
               Value function loss: 14.5777
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 433.53
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 10.71s
                        Total time: 36571.51s
                               ETA: 1091145.9s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.711s, learning 0.157s)
               Value function loss: 14.6380
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 434.96
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 10.87s
                        Total time: 36582.38s
                               ETA: 1091122.4s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.367s, learning 0.165s)
               Value function loss: 14.9868
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 429.02
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 10.53s
                        Total time: 36592.91s
                               ETA: 1091088.9s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.615s, learning 0.164s)
               Value function loss: 17.0780
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 427.02
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 10.78s
                        Total time: 36603.69s
                               ETA: 1091062.8s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.809s, learning 0.160s)
               Value function loss: 12.3633
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 429.16
               Mean episode length: 248.25
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 10.97s
                        Total time: 36614.66s
                               ETA: 1091042.4s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.885s, learning 0.172s)
               Value function loss: 14.6568
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 435.41
               Mean episode length: 248.25
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 11.06s
                        Total time: 36625.71s
                               ETA: 1091024.5s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.904s, learning 0.166s)
               Value function loss: 17.8674
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 443.67
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 11.07s
                        Total time: 36636.79s
                               ETA: 1091007.1s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.696s, learning 0.168s)
               Value function loss: 11.5095
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 442.39
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 10.86s
                        Total time: 36647.65s
                               ETA: 1090983.6s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.774s, learning 0.160s)
               Value function loss: 14.1245
                    Surrogate loss: -0.0195
             Mean action noise std: 0.72
                       Mean reward: 432.39
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 10.93s
                        Total time: 36658.58s
                               ETA: 1090962.1s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.758s, learning 0.176s)
               Value function loss: 14.4848
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 441.61
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 10.93s
                        Total time: 36669.52s
                               ETA: 1090940.7s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.573s, learning 0.158s)
               Value function loss: 16.5143
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 441.59
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 10.73s
                        Total time: 36680.25s
                               ETA: 1090913.3s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.860s, learning 0.161s)
               Value function loss: 20.2295
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 441.40
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 11.02s
                        Total time: 36691.27s
                               ETA: 1090894.4s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1557 steps/s (collection: 10.360s, learning 0.161s)
               Value function loss: 15.2637
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 438.91
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 10.52s
                        Total time: 36701.79s
                               ETA: 1090860.7s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.653s, learning 0.169s)
               Value function loss: 19.2470
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 436.35
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 10.82s
                        Total time: 36712.61s
                               ETA: 1090836.0s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.658s, learning 0.168s)
               Value function loss: 16.2761
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 431.93
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 10.83s
                        Total time: 36723.44s
                               ETA: 1090811.3s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.520s, learning 0.188s)
               Value function loss: 15.7088
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 432.39
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 10.71s
                        Total time: 36734.15s
                               ETA: 1090783.2s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.519s, learning 0.174s)
               Value function loss: 22.7815
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 441.81
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 10.69s
                        Total time: 36744.84s
                               ETA: 1090754.6s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.668s, learning 0.217s)
               Value function loss: 18.6582
                    Surrogate loss: -0.0190
             Mean action noise std: 0.72
                       Mean reward: 443.97
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 10.88s
                        Total time: 36755.72s
                               ETA: 1090731.8s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.888s, learning 0.166s)
               Value function loss: 16.0155
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 442.52
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 11.05s
                        Total time: 36766.78s
                               ETA: 1090714.0s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.729s, learning 0.173s)
               Value function loss: 16.3426
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 437.19
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 10.90s
                        Total time: 36777.68s
                               ETA: 1090691.7s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.072s, learning 0.168s)
               Value function loss: 17.0758
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 428.96
               Mean episode length: 247.84
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 11.24s
                        Total time: 36788.92s
                               ETA: 1090679.4s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.162s)
               Value function loss: 15.4576
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 435.85
               Mean episode length: 247.84
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 10.76s
                        Total time: 36799.68s
                               ETA: 1090652.8s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.661s, learning 0.166s)
               Value function loss: 19.1794
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 439.74
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 10.83s
                        Total time: 36810.51s
                               ETA: 1090628.2s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.587s, learning 0.163s)
               Value function loss: 13.8121
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 439.54
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 10.75s
                        Total time: 36821.26s
                               ETA: 1090601.4s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.802s, learning 0.166s)
               Value function loss: 20.0079
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 438.77
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 10.97s
                        Total time: 36832.23s
                               ETA: 1090581.1s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.538s, learning 0.215s)
               Value function loss: 14.9638
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 437.33
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 10.75s
                        Total time: 36842.98s
                               ETA: 1090554.4s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.780s, learning 0.172s)
               Value function loss: 11.7539
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 432.50
               Mean episode length: 250.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 10.95s
                        Total time: 36853.93s
                               ETA: 1090533.6s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1582 steps/s (collection: 10.188s, learning 0.165s)
               Value function loss: 16.6018
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 434.30
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 10.35s
                        Total time: 36864.28s
                               ETA: 1090495.2s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.936s, learning 0.179s)
               Value function loss: 15.2725
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 427.57
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 11.11s
                        Total time: 36875.40s
                               ETA: 1090479.2s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.843s, learning 0.182s)
               Value function loss: 16.1820
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 429.36
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 11.03s
                        Total time: 36886.42s
                               ETA: 1090460.6s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.535s, learning 0.166s)
               Value function loss: 16.9313
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: 440.53
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 10.70s
                        Total time: 36897.13s
                               ETA: 1090432.4s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.668s, learning 0.163s)
               Value function loss: 19.8575
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 445.16
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 10.83s
                        Total time: 36907.96s
                               ETA: 1090408.0s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.512s, learning 0.188s)
               Value function loss: 14.2587
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 444.64
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 10.70s
                        Total time: 36918.66s
                               ETA: 1090379.8s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.396s, learning 0.163s)
               Value function loss: 13.4145
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 439.12
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 10.56s
                        Total time: 36929.22s
                               ETA: 1090347.5s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.774s, learning 0.173s)
               Value function loss: 15.0063
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 435.83
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 10.95s
                        Total time: 36940.16s
                               ETA: 1090326.6s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.770s, learning 0.173s)
               Value function loss: 11.9752
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 437.77
               Mean episode length: 250.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 10.94s
                        Total time: 36951.11s
                               ETA: 1090305.6s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.663s, learning 0.185s)
               Value function loss: 11.2265
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 429.69
               Mean episode length: 250.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 10.85s
                        Total time: 36961.95s
                               ETA: 1090281.8s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1602 steps/s (collection: 10.058s, learning 0.165s)
               Value function loss: 15.3910
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 426.22
               Mean episode length: 250.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 10.22s
                        Total time: 36972.18s
                               ETA: 1090239.6s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.725s, learning 0.166s)
               Value function loss: 12.2505
                    Surrogate loss: -0.0207
             Mean action noise std: 0.72
                       Mean reward: 432.98
               Mean episode length: 248.82
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 10.89s
                        Total time: 36983.07s
                               ETA: 1090217.1s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.626s, learning 0.172s)
               Value function loss: 11.9060
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 434.98
               Mean episode length: 248.82
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 10.80s
                        Total time: 36993.86s
                               ETA: 1090191.8s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.859s, learning 0.218s)
               Value function loss: 10.9424
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 431.48
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 11.08s
                        Total time: 37004.94s
                               ETA: 1090174.8s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.846s, learning 0.242s)
               Value function loss: 12.9436
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 434.80
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 11.09s
                        Total time: 37016.03s
                               ETA: 1090158.1s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.804s, learning 0.207s)
               Value function loss: 17.5198
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 440.16
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 11.01s
                        Total time: 37027.04s
                               ETA: 1090139.2s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.781s, learning 0.162s)
               Value function loss: 13.6514
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 430.31
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 10.94s
                        Total time: 37037.98s
                               ETA: 1090118.2s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.613s, learning 0.174s)
               Value function loss: 19.1436
                    Surrogate loss: -0.0195
             Mean action noise std: 0.72
                       Mean reward: 430.23
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 10.79s
                        Total time: 37048.77s
                               ETA: 1090092.7s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.724s, learning 0.171s)
               Value function loss: 16.9209
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 435.06
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 10.90s
                        Total time: 37059.67s
                               ETA: 1090070.4s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.746s, learning 0.165s)
               Value function loss: 16.1866
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 433.78
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 10.91s
                        Total time: 37070.58s
                               ETA: 1090048.5s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.591s, learning 0.186s)
               Value function loss: 18.9920
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 443.59
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 10.78s
                        Total time: 37081.35s
                               ETA: 1090022.7s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.407s, learning 0.162s)
               Value function loss: 15.9639
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 436.08
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 10.57s
                        Total time: 37091.92s
                               ETA: 1089990.8s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.763s, learning 0.159s)
               Value function loss: 16.0347
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 440.18
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 10.92s
                        Total time: 37102.84s
                               ETA: 1089969.3s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.744s, learning 0.160s)
               Value function loss: 15.9943
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 445.08
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 10.90s
                        Total time: 37113.75s
                               ETA: 1089947.3s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.510s, learning 0.160s)
               Value function loss: 19.0734
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 443.70
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 10.67s
                        Total time: 37124.42s
                               ETA: 1089918.4s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.486s, learning 0.160s)
               Value function loss: 14.6396
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 434.88
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 10.65s
                        Total time: 37135.06s
                               ETA: 1089888.8s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.622s, learning 0.161s)
               Value function loss: 18.9168
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 440.15
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 10.78s
                        Total time: 37145.85s
                               ETA: 1089863.2s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.506s, learning 0.165s)
               Value function loss: 17.4749
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 451.49
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 10.67s
                        Total time: 37156.52s
                               ETA: 1089834.4s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.005s, learning 0.165s)
               Value function loss: 24.0434
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 447.14
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 11.17s
                        Total time: 37167.69s
                               ETA: 1089820.2s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.615s, learning 0.187s)
               Value function loss: 18.2622
                    Surrogate loss: 0.0073
             Mean action noise std: 0.72
                       Mean reward: 449.07
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 10.80s
                        Total time: 37178.49s
                               ETA: 1089795.2s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.574s, learning 0.164s)
               Value function loss: 12.3429
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 443.43
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 10.74s
                        Total time: 37189.23s
                               ETA: 1089768.3s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.904s, learning 0.220s)
               Value function loss: 17.6188
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 445.55
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 11.12s
                        Total time: 37200.35s
                               ETA: 1089752.8s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.523s, learning 0.191s)
               Value function loss: 20.7293
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 442.96
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 10.71s
                        Total time: 37211.07s
                               ETA: 1089725.3s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.841s, learning 0.174s)
               Value function loss: 16.3690
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 443.36
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 11.02s
                        Total time: 37222.08s
                               ETA: 1089706.6s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1537 steps/s (collection: 10.490s, learning 0.169s)
               Value function loss: 18.6674
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 444.06
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 10.66s
                        Total time: 37232.74s
                               ETA: 1089677.4s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.652s, learning 0.164s)
               Value function loss: 17.6534
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 447.20
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 10.82s
                        Total time: 37243.56s
                               ETA: 1089652.9s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.635s, learning 0.169s)
               Value function loss: 14.9405
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 443.06
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 10.80s
                        Total time: 37254.36s
                               ETA: 1089628.0s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.802s, learning 0.167s)
               Value function loss: 16.3285
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 446.28
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 10.97s
                        Total time: 37265.33s
                               ETA: 1089608.0s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.514s, learning 0.158s)
               Value function loss: 17.8489
                    Surrogate loss: -0.0192
             Mean action noise std: 0.72
                       Mean reward: 448.05
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 10.67s
                        Total time: 37276.00s
                               ETA: 1089579.3s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.571s, learning 0.170s)
               Value function loss: 16.6931
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 445.55
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 10.74s
                        Total time: 37286.74s
                               ETA: 1089552.7s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.958s, learning 0.167s)
               Value function loss: 13.1939
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 444.90
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 11.13s
                        Total time: 37297.87s
                               ETA: 1089537.2s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.577s, learning 0.161s)
               Value function loss: 20.6420
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 442.81
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 10.74s
                        Total time: 37308.61s
                               ETA: 1089510.5s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.696s, learning 0.224s)
               Value function loss: 14.1300
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 445.66
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 10.92s
                        Total time: 37319.53s
                               ETA: 1089489.1s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1533 steps/s (collection: 10.504s, learning 0.182s)
               Value function loss: 14.3296
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 445.55
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 10.69s
                        Total time: 37330.21s
                               ETA: 1089460.8s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.803s, learning 0.180s)
               Value function loss: 14.5509
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 445.09
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 10.98s
                        Total time: 37341.20s
                               ETA: 1089441.2s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.582s, learning 0.164s)
               Value function loss: 15.4262
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 444.02
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 10.75s
                        Total time: 37351.94s
                               ETA: 1089414.7s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.914s, learning 0.241s)
               Value function loss: 15.2423
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 446.62
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 11.16s
                        Total time: 37363.10s
                               ETA: 1089400.2s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.780s, learning 0.163s)
               Value function loss: 15.9095
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 447.20
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 10.94s
                        Total time: 37374.04s
                               ETA: 1089379.5s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.829s, learning 0.171s)
               Value function loss: 19.1852
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 449.79
               Mean episode length: 250.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 11.00s
                        Total time: 37385.04s
                               ETA: 1089360.4s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1541 steps/s (collection: 10.458s, learning 0.169s)
               Value function loss: 19.8124
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 447.71
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 10.63s
                        Total time: 37395.67s
                               ETA: 1089330.5s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1548 steps/s (collection: 10.414s, learning 0.167s)
               Value function loss: 18.3777
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 446.82
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 10.58s
                        Total time: 37406.25s
                               ETA: 1089299.2s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.555s, learning 0.172s)
               Value function loss: 16.9364
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 441.80
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 10.73s
                        Total time: 37416.97s
                               ETA: 1089272.2s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.624s, learning 0.175s)
               Value function loss: 17.5994
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 448.09
               Mean episode length: 250.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 10.80s
                        Total time: 37427.77s
                               ETA: 1089247.3s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1553 steps/s (collection: 10.378s, learning 0.171s)
               Value function loss: 17.8188
                    Surrogate loss: -0.0199
             Mean action noise std: 0.72
                       Mean reward: 445.00
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 10.55s
                        Total time: 37438.32s
                               ETA: 1089215.2s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.653s, learning 0.170s)
               Value function loss: 16.4781
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 446.32
               Mean episode length: 250.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 10.82s
                        Total time: 37449.14s
                               ETA: 1089191.0s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.780s, learning 0.260s)
               Value function loss: 16.4895
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 447.91
               Mean episode length: 249.27
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 11.04s
                        Total time: 37460.18s
                               ETA: 1089173.2s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.522s, learning 0.268s)
               Value function loss: 18.7620
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 450.54
               Mean episode length: 249.27
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 10.79s
                        Total time: 37470.97s
                               ETA: 1089148.1s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.719s, learning 0.169s)
               Value function loss: 19.1026
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 452.63
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 10.89s
                        Total time: 37481.86s
                               ETA: 1089125.8s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.591s, learning 0.255s)
               Value function loss: 19.4789
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 454.01
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 10.85s
                        Total time: 37492.71s
                               ETA: 1089102.3s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.808s, learning 0.162s)
               Value function loss: 22.6261
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 454.59
               Mean episode length: 250.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 10.97s
                        Total time: 37503.68s
                               ETA: 1089082.5s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.379s, learning 0.190s)
               Value function loss: 21.9608
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 450.54
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 10.57s
                        Total time: 37514.25s
                               ETA: 1089051.0s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.831s, learning 0.180s)
               Value function loss: 17.2955
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 450.08
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 11.01s
                        Total time: 37525.26s
                               ETA: 1089032.3s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.697s, learning 0.190s)
               Value function loss: 18.7470
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 448.06
               Mean episode length: 250.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 10.89s
                        Total time: 37536.14s
                               ETA: 1089010.1s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.512s, learning 0.161s)
               Value function loss: 21.0200
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 448.13
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 10.67s
                        Total time: 37546.82s
                               ETA: 1088981.6s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.798s, learning 0.172s)
               Value function loss: 17.1007
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 448.95
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 10.97s
                        Total time: 37557.79s
                               ETA: 1088961.8s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.829s, learning 0.171s)
               Value function loss: 18.3694
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 442.83
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 11.00s
                        Total time: 37568.79s
                               ETA: 1088942.9s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.354s, learning 0.160s)
               Value function loss: 17.1891
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 444.16
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 10.51s
                        Total time: 37579.30s
                               ETA: 1088909.9s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1543 steps/s (collection: 10.451s, learning 0.164s)
               Value function loss: 17.5726
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 447.17
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 10.61s
                        Total time: 37589.92s
                               ETA: 1088879.8s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.380s, learning 0.184s)
               Value function loss: 18.4949
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 450.63
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 10.56s
                        Total time: 37600.48s
                               ETA: 1088848.2s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.737s, learning 0.160s)
               Value function loss: 18.7081
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 447.26
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 10.90s
                        Total time: 37611.38s
                               ETA: 1088826.3s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.500s, learning 0.171s)
               Value function loss: 15.8405
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 440.42
               Mean episode length: 248.92
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 10.67s
                        Total time: 37622.05s
                               ETA: 1088797.9s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.621s, learning 0.167s)
               Value function loss: 12.4915
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 441.05
               Mean episode length: 248.92
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 10.79s
                        Total time: 37632.84s
                               ETA: 1088772.8s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.935s, learning 0.162s)
               Value function loss: 20.9233
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: 443.83
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 11.10s
                        Total time: 37643.93s
                               ETA: 1088756.7s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.594s, learning 0.161s)
               Value function loss: 18.4359
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 442.81
               Mean episode length: 249.94
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 10.76s
                        Total time: 37654.69s
                               ETA: 1088730.8s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.393s, learning 0.161s)
               Value function loss: 13.3836
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 445.40
               Mean episode length: 249.94
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 10.55s
                        Total time: 37665.24s
                               ETA: 1088699.0s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.663s, learning 0.163s)
               Value function loss: 14.6749
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 445.49
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 10.83s
                        Total time: 37676.07s
                               ETA: 1088675.1s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.631s, learning 0.257s)
               Value function loss: 14.8221
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 439.49
               Mean episode length: 250.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 10.89s
                        Total time: 37686.96s
                               ETA: 1088653.0s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1544 steps/s (collection: 10.437s, learning 0.171s)
               Value function loss: 15.8642
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 436.15
               Mean episode length: 249.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 10.61s
                        Total time: 37697.57s
                               ETA: 1088622.8s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.580s, learning 0.172s)
               Value function loss: 18.8356
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 443.60
               Mean episode length: 249.55
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 10.75s
                        Total time: 37708.32s
                               ETA: 1088596.8s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.589s, learning 0.168s)
               Value function loss: 18.2392
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 438.82
               Mean episode length: 249.39
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 10.76s
                        Total time: 37719.07s
                               ETA: 1088570.9s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.656s, learning 0.161s)
               Value function loss: 21.3830
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 441.70
               Mean episode length: 249.54
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 10.82s
                        Total time: 37729.89s
                               ETA: 1088546.8s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1549 steps/s (collection: 10.401s, learning 0.175s)
               Value function loss: 17.7545
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 435.04
               Mean episode length: 249.89
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 10.58s
                        Total time: 37740.47s
                               ETA: 1088515.7s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.495s, learning 0.194s)
               Value function loss: 15.9242
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 436.82
               Mean episode length: 249.69
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 10.69s
                        Total time: 37751.16s
                               ETA: 1088487.9s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.767s, learning 0.163s)
               Value function loss: 19.9968
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 435.39
               Mean episode length: 247.79
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 10.93s
                        Total time: 37762.09s
                               ETA: 1088467.1s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1553 steps/s (collection: 10.366s, learning 0.181s)
               Value function loss: 18.8011
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 435.81
               Mean episode length: 247.54
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 10.55s
                        Total time: 37772.63s
                               ETA: 1088435.2s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.595s, learning 0.159s)
               Value function loss: 16.5646
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 431.91
               Mean episode length: 248.09
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 10.75s
                        Total time: 37783.39s
                               ETA: 1088409.3s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1561 steps/s (collection: 10.328s, learning 0.164s)
               Value function loss: 16.2375
                    Surrogate loss: -0.0160
             Mean action noise std: 0.72
                       Mean reward: 432.25
               Mean episode length: 248.11
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 10.49s
                        Total time: 37793.88s
                               ETA: 1088375.9s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.701s, learning 0.172s)
               Value function loss: 16.4862
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 430.91
               Mean episode length: 248.67
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 10.87s
                        Total time: 37804.75s
                               ETA: 1088353.4s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1505 steps/s (collection: 10.719s, learning 0.164s)
               Value function loss: 15.3663
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 429.43
               Mean episode length: 249.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 10.88s
                        Total time: 37815.63s
                               ETA: 1088331.3s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.864s, learning 0.167s)
               Value function loss: 15.8159
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 429.27
               Mean episode length: 248.22
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 11.03s
                        Total time: 37826.67s
                               ETA: 1088313.4s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.718s, learning 0.169s)
               Value function loss: 18.8019
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 425.34
               Mean episode length: 248.45
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 10.89s
                        Total time: 37837.55s
                               ETA: 1088291.3s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.535s, learning 0.188s)
               Value function loss: 18.6278
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 421.53
               Mean episode length: 248.83
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 10.72s
                        Total time: 37848.28s
                               ETA: 1088264.6s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.761s, learning 0.191s)
               Value function loss: 15.9567
                    Surrogate loss: -0.0213
             Mean action noise std: 0.72
                       Mean reward: 420.97
               Mean episode length: 247.73
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 10.95s
                        Total time: 37859.23s
                               ETA: 1088244.5s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.715s, learning 0.163s)
               Value function loss: 11.6490
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 412.49
               Mean episode length: 245.93
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 10.88s
                        Total time: 37870.11s
                               ETA: 1088222.2s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.750s, learning 0.174s)
               Value function loss: 15.5615
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 414.61
               Mean episode length: 246.38
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 10.92s
                        Total time: 37881.03s
                               ETA: 1088201.3s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.850s, learning 0.168s)
               Value function loss: 14.0964
                    Surrogate loss: -0.0200
             Mean action noise std: 0.72
                       Mean reward: 414.28
               Mean episode length: 246.93
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 11.02s
                        Total time: 37892.05s
                               ETA: 1088183.0s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.697s, learning 0.168s)
               Value function loss: 12.6787
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 411.54
               Mean episode length: 247.18
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 10.87s
                        Total time: 37902.91s
                               ETA: 1088160.4s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.764s, learning 0.167s)
               Value function loss: 13.0680
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 412.53
               Mean episode length: 247.29
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 10.93s
                        Total time: 37913.84s
                               ETA: 1088139.7s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.509s, learning 0.167s)
               Value function loss: 14.8851
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 414.35
               Mean episode length: 246.32
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 10.68s
                        Total time: 37924.52s
                               ETA: 1088111.7s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1485 steps/s (collection: 10.754s, learning 0.277s)
               Value function loss: 13.4951
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 421.22
               Mean episode length: 246.28
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 11.03s
                        Total time: 37935.55s
                               ETA: 1088093.8s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.968s, learning 0.176s)
               Value function loss: 15.2136
                    Surrogate loss: -0.0179
             Mean action noise std: 0.72
                       Mean reward: 417.07
               Mean episode length: 245.93
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 11.14s
                        Total time: 37946.69s
                               ETA: 1088079.2s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.582s, learning 0.169s)
               Value function loss: 15.8149
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 420.68
               Mean episode length: 247.08
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 10.75s
                        Total time: 37957.45s
                               ETA: 1088053.4s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.694s, learning 0.199s)
               Value function loss: 10.5457
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 409.62
               Mean episode length: 244.93
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 10.89s
                        Total time: 37968.34s
                               ETA: 1088031.6s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1529 steps/s (collection: 10.543s, learning 0.171s)
               Value function loss: 15.9373
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 413.11
               Mean episode length: 245.90
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 10.71s
                        Total time: 37979.05s
                               ETA: 1088004.7s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.531s, learning 0.195s)
               Value function loss: 17.6542
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 413.92
               Mean episode length: 246.27
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 10.73s
                        Total time: 37989.78s
                               ETA: 1087978.1s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.757s, learning 0.167s)
               Value function loss: 11.8404
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 406.48
               Mean episode length: 242.80
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 10.92s
                        Total time: 38000.70s
                               ETA: 1087957.3s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.622s, learning 0.161s)
               Value function loss: 13.3093
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 412.08
               Mean episode length: 243.59
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 10.78s
                        Total time: 38011.49s
                               ETA: 1087932.4s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1534 steps/s (collection: 10.496s, learning 0.179s)
               Value function loss: 11.5019
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 401.54
               Mean episode length: 242.95
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 10.67s
                        Total time: 38022.16s
                               ETA: 1087904.4s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.635s, learning 0.166s)
               Value function loss: 13.3349
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 401.83
               Mean episode length: 243.53
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 10.80s
                        Total time: 38032.96s
                               ETA: 1087880.1s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.345s, learning 0.170s)
               Value function loss: 14.6377
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 396.21
               Mean episode length: 240.36
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 10.51s
                        Total time: 38043.48s
                               ETA: 1087847.5s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.704s, learning 0.276s)
               Value function loss: 12.5548
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 404.72
               Mean episode length: 243.67
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 10.98s
                        Total time: 38054.46s
                               ETA: 1087828.3s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.734s, learning 0.179s)
               Value function loss: 12.7714
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 395.36
               Mean episode length: 246.09
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 10.91s
                        Total time: 38065.37s
                               ETA: 1087807.1s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.738s, learning 0.177s)
               Value function loss: 11.6447
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 407.88
               Mean episode length: 246.79
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 10.91s
                        Total time: 38076.28s
                               ETA: 1087786.1s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.558s, learning 0.173s)
               Value function loss: 12.9427
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 409.80
               Mean episode length: 246.32
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 10.73s
                        Total time: 38087.02s
                               ETA: 1087759.8s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.761s, learning 0.174s)
               Value function loss: 16.6711
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 417.10
               Mean episode length: 247.80
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 10.94s
                        Total time: 38097.95s
                               ETA: 1087739.3s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1521 steps/s (collection: 10.608s, learning 0.163s)
               Value function loss: 15.0188
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 417.61
               Mean episode length: 247.60
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 10.77s
                        Total time: 38108.72s
                               ETA: 1087714.1s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.785s, learning 0.173s)
               Value function loss: 13.3701
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 424.73
               Mean episode length: 247.45
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 10.96s
                        Total time: 38119.68s
                               ETA: 1087694.3s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.465s, learning 0.184s)
               Value function loss: 13.6642
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 424.82
               Mean episode length: 247.61
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 10.65s
                        Total time: 38130.33s
                               ETA: 1087665.7s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.437s, learning 0.162s)
               Value function loss: 15.2221
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 425.98
               Mean episode length: 247.72
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 10.60s
                        Total time: 38140.93s
                               ETA: 1087635.6s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.714s, learning 0.161s)
               Value function loss: 12.3496
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 420.10
               Mean episode length: 247.88
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 10.87s
                        Total time: 38151.80s
                               ETA: 1087613.5s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.542s, learning 0.187s)
               Value function loss: 17.8145
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 418.91
               Mean episode length: 248.04
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 10.73s
                        Total time: 38162.53s
                               ETA: 1087587.1s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.659s, learning 0.171s)
               Value function loss: 13.3266
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 430.61
               Mean episode length: 249.15
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 10.83s
                        Total time: 38173.36s
                               ETA: 1087563.7s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.411s, learning 0.160s)
               Value function loss: 16.5883
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 420.34
               Mean episode length: 247.95
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 10.57s
                        Total time: 38183.93s
                               ETA: 1087532.9s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.489s, learning 0.182s)
               Value function loss: 17.5701
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 418.49
               Mean episode length: 248.83
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 10.67s
                        Total time: 38194.60s
                               ETA: 1087504.9s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.749s, learning 0.170s)
               Value function loss: 16.0253
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 420.70
               Mean episode length: 248.23
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 10.92s
                        Total time: 38205.52s
                               ETA: 1087484.1s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.589s, learning 0.189s)
               Value function loss: 16.7693
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 422.67
               Mean episode length: 248.75
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 10.78s
                        Total time: 38216.30s
                               ETA: 1087459.2s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.828s, learning 0.187s)
               Value function loss: 18.0118
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 421.55
               Mean episode length: 246.23
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 11.01s
                        Total time: 38227.32s
                               ETA: 1087441.1s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.888s, learning 0.179s)
               Value function loss: 17.1445
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 427.56
               Mean episode length: 247.64
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 11.07s
                        Total time: 38238.38s
                               ETA: 1087424.4s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.485s, learning 0.185s)
               Value function loss: 17.3966
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 431.78
               Mean episode length: 248.68
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 10.67s
                        Total time: 38249.05s
                               ETA: 1087396.5s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.575s, learning 0.164s)
               Value function loss: 19.6280
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: 417.18
               Mean episode length: 246.78
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 10.74s
                        Total time: 38259.79s
                               ETA: 1087370.5s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.592s, learning 0.158s)
               Value function loss: 16.9516
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 421.37
               Mean episode length: 247.20
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 10.75s
                        Total time: 38270.54s
                               ETA: 1087344.9s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1514 steps/s (collection: 10.660s, learning 0.161s)
               Value function loss: 18.0999
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 419.26
               Mean episode length: 246.45
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 10.82s
                        Total time: 38281.36s
                               ETA: 1087321.3s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.820s, learning 0.162s)
               Value function loss: 16.6394
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 420.41
               Mean episode length: 246.65
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 10.98s
                        Total time: 38292.35s
                               ETA: 1087302.3s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.332s, learning 0.191s)
               Value function loss: 13.9574
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 418.84
               Mean episode length: 246.47
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 10.52s
                        Total time: 38302.87s
                               ETA: 1087270.2s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.818s, learning 0.165s)
               Value function loss: 13.2359
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 405.64
               Mean episode length: 245.56
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 10.98s
                        Total time: 38313.85s
                               ETA: 1087251.2s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.641s, learning 0.160s)
               Value function loss: 14.6788
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 406.24
               Mean episode length: 245.71
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 10.80s
                        Total time: 38324.65s
                               ETA: 1087227.1s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.831s, learning 0.160s)
               Value function loss: 11.2428
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 418.45
               Mean episode length: 249.09
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 10.99s
                        Total time: 38335.64s
                               ETA: 1087208.3s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.616s, learning 0.196s)
               Value function loss: 13.3304
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 413.50
               Mean episode length: 248.64
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 10.81s
                        Total time: 38346.46s
                               ETA: 1087184.5s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1488 steps/s (collection: 10.844s, learning 0.160s)
               Value function loss: 12.1413
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 414.71
               Mean episode length: 247.49
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 11.00s
                        Total time: 38357.46s
                               ETA: 1087166.1s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.668s, learning 0.163s)
               Value function loss: 12.9767
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 404.82
               Mean episode length: 246.92
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 10.83s
                        Total time: 38368.29s
                               ETA: 1087142.9s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1528 steps/s (collection: 10.551s, learning 0.169s)
               Value function loss: 16.2213
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 416.35
               Mean episode length: 247.97
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 10.72s
                        Total time: 38379.01s
                               ETA: 1087116.4s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.853s, learning 0.217s)
               Value function loss: 13.2325
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 421.63
               Mean episode length: 249.01
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 11.07s
                        Total time: 38390.08s
                               ETA: 1087100.0s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.666s, learning 0.207s)
               Value function loss: 15.0051
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 417.25
               Mean episode length: 248.84
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 10.87s
                        Total time: 38400.96s
                               ETA: 1087077.9s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.622s, learning 0.180s)
               Value function loss: 13.4296
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 411.34
               Mean episode length: 247.52
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 10.80s
                        Total time: 38411.76s
                               ETA: 1087053.9s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.731s, learning 0.171s)
               Value function loss: 14.1749
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 407.40
               Mean episode length: 247.20
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 10.90s
                        Total time: 38422.66s
                               ETA: 1087032.6s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.531s, learning 0.171s)
               Value function loss: 14.5061
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 407.35
               Mean episode length: 247.54
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 10.70s
                        Total time: 38433.36s
                               ETA: 1087005.7s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.603s, learning 0.173s)
               Value function loss: 14.2559
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 415.34
               Mean episode length: 247.85
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 10.78s
                        Total time: 38444.14s
                               ETA: 1086981.0s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.479s, learning 0.171s)
               Value function loss: 16.3358
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 416.73
               Mean episode length: 246.91
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 10.65s
                        Total time: 38454.79s
                               ETA: 1086952.6s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1555 steps/s (collection: 10.368s, learning 0.166s)
               Value function loss: 13.5109
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 416.87
               Mean episode length: 246.76
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 10.53s
                        Total time: 38465.32s
                               ETA: 1086921.0s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.853s, learning 0.171s)
               Value function loss: 14.5249
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 410.92
               Mean episode length: 248.56
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 11.02s
                        Total time: 38476.34s
                               ETA: 1086903.3s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1552 steps/s (collection: 10.386s, learning 0.165s)
               Value function loss: 12.8862
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 406.17
               Mean episode length: 246.99
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 10.55s
                        Total time: 38486.90s
                               ETA: 1086872.2s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.791s, learning 0.168s)
               Value function loss: 12.1748
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 405.67
               Mean episode length: 246.48
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 10.96s
                        Total time: 38497.86s
                               ETA: 1086852.6s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.798s, learning 0.282s)
               Value function loss: 11.0199
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 403.25
               Mean episode length: 247.30
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 11.08s
                        Total time: 38508.94s
                               ETA: 1086836.5s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.867s, learning 0.187s)
               Value function loss: 17.5204
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 398.99
               Mean episode length: 246.32
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 11.05s
                        Total time: 38519.99s
                               ETA: 1086819.6s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.664s, learning 0.164s)
               Value function loss: 15.5552
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 411.05
               Mean episode length: 248.93
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 10.83s
                        Total time: 38530.82s
                               ETA: 1086796.4s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.621s, learning 0.186s)
               Value function loss: 10.3888
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 401.81
               Mean episode length: 248.19
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 10.81s
                        Total time: 38541.62s
                               ETA: 1086772.5s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.600s, learning 0.164s)
               Value function loss: 13.2975
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 406.15
               Mean episode length: 247.83
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 10.76s
                        Total time: 38552.39s
                               ETA: 1086747.5s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.487s, learning 0.160s)
               Value function loss: 16.1867
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 416.92
               Mean episode length: 247.92
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 10.65s
                        Total time: 38563.04s
                               ETA: 1086719.2s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.667s, learning 0.248s)
               Value function loss: 11.7115
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 413.60
               Mean episode length: 247.98
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 10.92s
                        Total time: 38573.95s
                               ETA: 1086698.4s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.521s, learning 0.167s)
               Value function loss: 16.1081
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 402.17
               Mean episode length: 247.32
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 10.69s
                        Total time: 38584.64s
                               ETA: 1086671.3s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1500 steps/s (collection: 10.755s, learning 0.161s)
               Value function loss: 17.6833
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 413.26
               Mean episode length: 247.18
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 10.92s
                        Total time: 38595.56s
                               ETA: 1086650.5s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.586s, learning 0.192s)
               Value function loss: 14.4914
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 417.69
               Mean episode length: 247.36
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 10.78s
                        Total time: 38606.33s
                               ETA: 1086626.0s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.700s, learning 0.179s)
               Value function loss: 12.5789
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 401.11
               Mean episode length: 247.98
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 10.88s
                        Total time: 38617.21s
                               ETA: 1086604.2s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1569 steps/s (collection: 10.258s, learning 0.177s)
               Value function loss: 15.1393
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 414.42
               Mean episode length: 249.34
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 10.44s
                        Total time: 38627.65s
                               ETA: 1086570.0s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1527 steps/s (collection: 10.554s, learning 0.169s)
               Value function loss: 17.0694
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 406.31
               Mean episode length: 244.45
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 10.72s
                        Total time: 38638.37s
                               ETA: 1086543.9s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.482s, learning 0.164s)
               Value function loss: 13.5344
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 405.38
               Mean episode length: 244.81
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 10.65s
                        Total time: 38649.02s
                               ETA: 1086515.6s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.463s, learning 0.281s)
               Value function loss: 17.4452
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 406.71
               Mean episode length: 247.42
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 10.74s
                        Total time: 38659.76s
                               ETA: 1086490.1s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1567 steps/s (collection: 10.278s, learning 0.171s)
               Value function loss: 14.1978
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: 406.42
               Mean episode length: 245.79
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 10.45s
                        Total time: 38670.21s
                               ETA: 1086456.3s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.684s, learning 0.171s)
               Value function loss: 12.9952
                    Surrogate loss: -0.0174
             Mean action noise std: 0.72
                       Mean reward: 412.46
               Mean episode length: 244.36
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 10.86s
                        Total time: 38681.07s
                               ETA: 1086433.9s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1535 steps/s (collection: 10.494s, learning 0.174s)
               Value function loss: 13.0041
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 416.35
               Mean episode length: 247.27
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 10.67s
                        Total time: 38691.73s
                               ETA: 1086406.3s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1516 steps/s (collection: 10.617s, learning 0.189s)
               Value function loss: 14.2980
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 412.60
               Mean episode length: 245.53
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 10.81s
                        Total time: 38702.54s
                               ETA: 1086382.6s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.765s, learning 0.162s)
               Value function loss: 13.0912
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 406.37
               Mean episode length: 244.35
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 10.93s
                        Total time: 38713.47s
                               ETA: 1086362.2s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1493 steps/s (collection: 10.765s, learning 0.203s)
               Value function loss: 14.2283
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 411.70
               Mean episode length: 247.59
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 10.97s
                        Total time: 38724.44s
                               ETA: 1086343.0s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.764s, learning 0.178s)
               Value function loss: 17.2337
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 416.70
               Mean episode length: 246.84
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 10.94s
                        Total time: 38735.38s
                               ETA: 1086323.2s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.523s, learning 0.165s)
               Value function loss: 17.5580
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 418.30
               Mean episode length: 247.39
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 10.69s
                        Total time: 38746.07s
                               ETA: 1086296.1s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.819s, learning 0.164s)
               Value function loss: 15.9828
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 408.68
               Mean episode length: 245.40
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 10.98s
                        Total time: 38757.05s
                               ETA: 1086277.4s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.719s, learning 0.160s)
               Value function loss: 14.0206
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 422.35
               Mean episode length: 248.62
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 10.88s
                        Total time: 38767.93s
                               ETA: 1086255.7s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.665s, learning 0.187s)
               Value function loss: 14.3607
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 408.04
               Mean episode length: 246.19
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 10.85s
                        Total time: 38778.78s
                               ETA: 1086233.3s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.596s, learning 0.166s)
               Value function loss: 15.8793
                    Surrogate loss: -0.0122
             Mean action noise std: 0.72
                       Mean reward: 400.55
               Mean episode length: 247.32
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 10.76s
                        Total time: 38789.54s
                               ETA: 1086208.5s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.416s, learning 0.186s)
               Value function loss: 14.8072
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 416.35
               Mean episode length: 248.48
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 10.60s
                        Total time: 38800.14s
                               ETA: 1086179.1s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1531 steps/s (collection: 10.536s, learning 0.163s)
               Value function loss: 13.7944
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 417.84
               Mean episode length: 249.10
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 10.70s
                        Total time: 38810.84s
                               ETA: 1086152.4s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1512 steps/s (collection: 10.666s, learning 0.166s)
               Value function loss: 14.2284
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 416.73
               Mean episode length: 248.78
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 10.83s
                        Total time: 38821.68s
                               ETA: 1086129.5s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.695s, learning 0.167s)
               Value function loss: 15.0948
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 413.83
               Mean episode length: 248.23
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 10.86s
                        Total time: 38832.54s
                               ETA: 1086107.4s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1502 steps/s (collection: 10.731s, learning 0.175s)
               Value function loss: 16.1328
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 410.74
               Mean episode length: 247.68
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 10.91s
                        Total time: 38843.44s
                               ETA: 1086086.5s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.806s, learning 0.196s)
               Value function loss: 22.2258
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 426.86
               Mean episode length: 248.63
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 11.00s
                        Total time: 38854.44s
                               ETA: 1086068.3s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1517 steps/s (collection: 10.621s, learning 0.173s)
               Value function loss: 19.5324
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 431.51
               Mean episode length: 249.89
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 10.79s
                        Total time: 38865.24s
                               ETA: 1086044.4s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.764s, learning 0.178s)
               Value function loss: 20.7474
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 424.24
               Mean episode length: 246.36
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 10.94s
                        Total time: 38876.18s
                               ETA: 1086024.5s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1503 steps/s (collection: 10.739s, learning 0.159s)
               Value function loss: 19.2162
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 422.49
               Mean episode length: 248.88
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 10.90s
                        Total time: 38887.08s
                               ETA: 1086003.5s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.482s, learning 0.167s)
               Value function loss: 19.5236
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 418.51
               Mean episode length: 247.83
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 10.65s
                        Total time: 38897.73s
                               ETA: 1085975.5s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.766s, learning 0.249s)
               Value function loss: 15.6681
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 415.37
               Mean episode length: 248.80
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 11.01s
                        Total time: 38908.74s
                               ETA: 1085957.7s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1538 steps/s (collection: 10.475s, learning 0.172s)
               Value function loss: 15.6372
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 424.56
               Mean episode length: 249.23
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 10.65s
                        Total time: 38919.39s
                               ETA: 1085929.7s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.609s, learning 0.163s)
               Value function loss: 18.3868
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 429.45
               Mean episode length: 248.69
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 10.77s
                        Total time: 38930.16s
                               ETA: 1085905.2s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.400s, learning 0.160s)
               Value function loss: 17.4210
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 426.65
               Mean episode length: 248.23
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 10.56s
                        Total time: 38940.72s
                               ETA: 1085874.7s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1576 steps/s (collection: 10.204s, learning 0.187s)
               Value function loss: 16.4644
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 427.70
               Mean episode length: 248.98
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 10.39s
                        Total time: 38951.11s
                               ETA: 1085839.6s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.563s, learning 0.171s)
               Value function loss: 14.3336
                    Surrogate loss: -0.0187
             Mean action noise std: 0.72
                       Mean reward: 420.70
               Mean episode length: 248.76
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 10.73s
                        Total time: 38961.85s
                               ETA: 1085814.0s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1513 steps/s (collection: 10.639s, learning 0.188s)
               Value function loss: 14.3051
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 422.23
               Mean episode length: 249.51
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 10.83s
                        Total time: 38972.67s
                               ETA: 1085791.0s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.576s, learning 0.169s)
               Value function loss: 13.3296
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 429.10
               Mean episode length: 249.77
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 10.74s
                        Total time: 38983.42s
                               ETA: 1085765.7s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1532 steps/s (collection: 10.519s, learning 0.171s)
               Value function loss: 14.7325
                    Surrogate loss: -0.0182
             Mean action noise std: 0.72
                       Mean reward: 419.48
               Mean episode length: 247.57
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 10.69s
                        Total time: 38994.10s
                               ETA: 1085738.9s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.899s, learning 0.174s)
               Value function loss: 16.2722
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 424.71
               Mean episode length: 248.64
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 11.07s
                        Total time: 39005.18s
                               ETA: 1085722.8s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1522 steps/s (collection: 10.599s, learning 0.164s)
               Value function loss: 13.8727
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 412.84
               Mean episode length: 248.03
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 10.76s
                        Total time: 39015.94s
                               ETA: 1085698.1s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1525 steps/s (collection: 10.519s, learning 0.219s)
               Value function loss: 14.0070
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 421.01
               Mean episode length: 249.19
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 10.74s
                        Total time: 39026.68s
                               ETA: 1085672.7s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1565 steps/s (collection: 10.277s, learning 0.189s)
               Value function loss: 16.5381
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 425.18
               Mean episode length: 248.59
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 10.47s
                        Total time: 39037.15s
                               ETA: 1085639.8s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.798s, learning 0.163s)
               Value function loss: 15.2273
                    Surrogate loss: -0.0168
             Mean action noise std: 0.72
                       Mean reward: 427.22
               Mean episode length: 249.30
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 10.96s
                        Total time: 39048.11s
                               ETA: 1085620.6s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1550 steps/s (collection: 10.408s, learning 0.161s)
               Value function loss: 15.5706
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 424.93
               Mean episode length: 248.31
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 10.57s
                        Total time: 39058.68s
                               ETA: 1085590.5s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1530 steps/s (collection: 10.540s, learning 0.167s)
               Value function loss: 16.9631
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 427.71
               Mean episode length: 248.97
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 10.71s
                        Total time: 39069.38s
                               ETA: 1085564.3s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.685s, learning 0.161s)
               Value function loss: 18.5715
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 417.23
               Mean episode length: 248.46
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 10.85s
                        Total time: 39080.23s
                               ETA: 1085542.0s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1515 steps/s (collection: 10.627s, learning 0.181s)
               Value function loss: 16.1784
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 415.82
               Mean episode length: 248.29
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 10.81s
                        Total time: 39091.04s
                               ETA: 1085518.5s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1539 steps/s (collection: 10.465s, learning 0.176s)
               Value function loss: 14.3580
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 429.44
               Mean episode length: 249.40
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 10.64s
                        Total time: 39101.68s
                               ETA: 1085490.5s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1575 steps/s (collection: 10.226s, learning 0.171s)
               Value function loss: 14.6775
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 423.55
               Mean episode length: 249.40
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 10.40s
                        Total time: 39112.08s
                               ETA: 1085455.7s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1520 steps/s (collection: 10.608s, learning 0.168s)
               Value function loss: 11.5903
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 414.51
               Mean episode length: 247.33
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 10.78s
                        Total time: 39122.85s
                               ETA: 1085431.4s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.567s, learning 0.214s)
               Value function loss: 13.1039
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 423.69
               Mean episode length: 247.87
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 10.78s
                        Total time: 39133.63s
                               ETA: 1085407.3s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1523 steps/s (collection: 10.600s, learning 0.157s)
               Value function loss: 12.1526
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 421.97
               Mean episode length: 247.71
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 10.76s
                        Total time: 39144.39s
                               ETA: 1085382.5s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.673s, learning 0.165s)
               Value function loss: 11.7288
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 408.42
               Mean episode length: 246.01
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 10.84s
                        Total time: 39155.23s
                               ETA: 1085360.0s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1556 steps/s (collection: 10.362s, learning 0.163s)
               Value function loss: 13.5462
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 410.48
               Mean episode length: 246.64
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 10.53s
                        Total time: 39165.75s
                               ETA: 1085328.8s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.744s, learning 0.259s)
               Value function loss: 12.2210
                    Surrogate loss: -0.0224
             Mean action noise std: 0.72
                       Mean reward: 415.08
               Mean episode length: 247.91
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 11.00s
                        Total time: 39176.76s
                               ETA: 1085310.9s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.756s, learning 0.168s)
               Value function loss: 15.8051
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 411.22
               Mean episode length: 246.85
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 10.92s
                        Total time: 39187.68s
                               ETA: 1085290.7s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1545 steps/s (collection: 10.441s, learning 0.159s)
               Value function loss: 13.1433
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 412.35
               Mean episode length: 246.96
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 10.60s
                        Total time: 39198.28s
                               ETA: 1085261.6s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.782s, learning 0.167s)
               Value function loss: 15.2382
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 419.40
               Mean episode length: 248.39
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 10.95s
                        Total time: 39209.23s
                               ETA: 1085242.2s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1519 steps/s (collection: 10.598s, learning 0.186s)
               Value function loss: 10.9738
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 416.27
               Mean episode length: 249.50
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 10.78s
                        Total time: 39220.01s
                               ETA: 1085218.2s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.588s, learning 0.162s)
               Value function loss: 15.2716
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 420.17
               Mean episode length: 249.76
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 10.75s
                        Total time: 39230.76s
                               ETA: 1085193.3s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1559 steps/s (collection: 10.242s, learning 0.262s)
               Value function loss: 11.5893
                    Surrogate loss: -0.0228
             Mean action noise std: 0.72
                       Mean reward: 419.85
               Mean episode length: 249.31
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 10.50s
                        Total time: 39241.27s
                               ETA: 1085161.6s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1511 steps/s (collection: 10.653s, learning 0.185s)
               Value function loss: 11.2669
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 408.20
               Mean episode length: 246.51
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 10.84s
                        Total time: 39252.11s
                               ETA: 1085139.1s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.679s, learning 0.166s)
               Value function loss: 13.6786
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 421.09
               Mean episode length: 248.60
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 10.85s
                        Total time: 39262.95s
                               ETA: 1085116.9s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1526 steps/s (collection: 10.570s, learning 0.161s)
               Value function loss: 16.5173
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 423.00
               Mean episode length: 249.01
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 10.73s
                        Total time: 39273.68s
                               ETA: 1085091.5s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.819s, learning 0.164s)
               Value function loss: 12.1837
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 416.98
               Mean episode length: 248.47
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 10.98s
                        Total time: 39284.66s
                               ETA: 1085073.0s

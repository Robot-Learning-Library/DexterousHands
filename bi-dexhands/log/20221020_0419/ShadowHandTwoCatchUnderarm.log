Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-1248l427
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandTwoCatchUnderarm_ppo_20221020041912
wandb: ⭐️ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: 🚀 View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/1248l427
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:3
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=3, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=3, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:3', seed=None, sim_device='cuda:3', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandTwoCatchUnderarm', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_two_catch_underarm', 'numEnvs': 2048, 'envSpacing': 0.75, 'episodeLength': 75, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.01, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.01, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.2, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 50.0, 'transition_scale': 0.05, 'orientation_scale': 0.5, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.65, 'fallPenalty': 0.0, 'objectType': 'egg', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandTwoCatchUnderarm', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandTwoCatchUnderarm_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 7129
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:3
Sequential(
  (0): Linear(in_features=446, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=446, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1175 steps/s (collection: 6.266s, learning 7.673s)
               Value function loss: 7.4037
                    Surrogate loss: 0.1080
             Mean action noise std: 0.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 13.94s
                        Total time: 13.94s
                               ETA: 1393812.3s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 2996 steps/s (collection: 4.579s, learning 0.888s)
               Value function loss: 0.4080
                    Surrogate loss: -0.0234
             Mean action noise std: 0.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 5.47s
                        Total time: 19.41s
                               ETA: 970246.6s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 3619 steps/s (collection: 4.331s, learning 0.195s)
               Value function loss: 0.1914
                    Surrogate loss: -0.0395
             Mean action noise std: 0.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 4.53s
                        Total time: 23.93s
                               ETA: 797688.7s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 3089 steps/s (collection: 5.116s, learning 0.187s)
               Value function loss: 0.1338
                    Surrogate loss: -0.0305
             Mean action noise std: 0.80
                       Mean reward: 0.66
               Mean episode length: 29.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 5.30s
                        Total time: 29.23s
                               ETA: 730843.1s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 2254 steps/s (collection: 7.071s, learning 0.198s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0299
             Mean action noise std: 0.80
                       Mean reward: 0.69
               Mean episode length: 32.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 7.27s
                        Total time: 36.50s
                               ETA: 730034.3s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 3139 steps/s (collection: 5.039s, learning 0.179s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0352
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 35.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.22s
                        Total time: 41.72s
                               ETA: 695326.2s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 2273 steps/s (collection: 6.778s, learning 0.430s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0340
             Mean action noise std: 0.80
                       Mean reward: 0.88
               Mean episode length: 40.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 7.21s
                        Total time: 48.93s
                               ETA: 698950.0s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 2716 steps/s (collection: 5.864s, learning 0.167s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0452
             Mean action noise std: 0.80
                       Mean reward: 0.97
               Mean episode length: 43.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 6.03s
                        Total time: 54.96s
                               ETA: 686951.8s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.163s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0283
             Mean action noise std: 0.80
                       Mean reward: 1.05
               Mean episode length: 47.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 8.36s
                        Total time: 63.32s
                               ETA: 703463.8s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 1784 steps/s (collection: 9.016s, learning 0.165s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0213
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 74.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 9.18s
                        Total time: 72.50s
                               ETA: 724906.1s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 1508 steps/s (collection: 10.694s, learning 0.164s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0320
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 74.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 10.86s
                        Total time: 83.36s
                               ETA: 757700.8s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 1210 steps/s (collection: 13.343s, learning 0.189s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0569
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 73.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 13.53s
                        Total time: 96.89s
                               ETA: 807306.2s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 1077 steps/s (collection: 15.018s, learning 0.184s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0411
             Mean action noise std: 0.80
                       Mean reward: 1.65
               Mean episode length: 72.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 15.20s
                        Total time: 112.09s
                               ETA: 862122.1s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.287s, learning 0.167s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0380
             Mean action noise std: 0.80
                       Mean reward: 1.63
               Mean episode length: 71.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 16.45s
                        Total time: 128.54s
                               ETA: 918044.0s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.460s, learning 0.272s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 70.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 16.73s
                        Total time: 145.27s
                               ETA: 968360.2s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 936 steps/s (collection: 17.316s, learning 0.170s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0334
             Mean action noise std: 0.80
                       Mean reward: 1.56
               Mean episode length: 69.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 17.49s
                        Total time: 162.76s
                               ETA: 1017098.8s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 964 steps/s (collection: 16.823s, learning 0.165s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0432
             Mean action noise std: 0.80
                       Mean reward: 1.56
               Mean episode length: 69.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 16.99s
                        Total time: 179.75s
                               ETA: 1057175.9s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.914s, learning 0.180s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0460
             Mean action noise std: 0.80
                       Mean reward: 1.56
               Mean episode length: 68.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 17.09s
                        Total time: 196.84s
                               ETA: 1093388.8s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 918 steps/s (collection: 17.664s, learning 0.178s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 17.84s
                        Total time: 214.68s
                               ETA: 1129716.4s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 942 steps/s (collection: 17.206s, learning 0.172s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 17.38s
                        Total time: 232.06s
                               ETA: 1160090.6s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 947 steps/s (collection: 17.037s, learning 0.249s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0387
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 17.29s
                        Total time: 249.35s
                               ETA: 1187132.3s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.205s, learning 0.167s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0440
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 74.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 17.37s
                        Total time: 266.72s
                               ETA: 1212109.2s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 937 steps/s (collection: 17.301s, learning 0.174s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0404
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 73.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 17.48s
                        Total time: 284.20s
                               ETA: 1235360.7s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 935 steps/s (collection: 17.338s, learning 0.175s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0470
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 72.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 17.51s
                        Total time: 301.71s
                               ETA: 1256830.5s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 953 steps/s (collection: 17.007s, learning 0.173s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0505
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 70.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 17.18s
                        Total time: 318.89s
                               ETA: 1275250.0s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 936 steps/s (collection: 17.316s, learning 0.172s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0507
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 69.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 17.49s
                        Total time: 336.38s
                               ETA: 1293435.8s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 964 steps/s (collection: 16.812s, learning 0.171s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0371
             Mean action noise std: 0.80
                       Mean reward: 1.60
               Mean episode length: 68.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 16.98s
                        Total time: 353.36s
                               ETA: 1308402.7s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.625s, learning 0.190s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0214
             Mean action noise std: 0.80
                       Mean reward: 1.69
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 16.81s
                        Total time: 370.18s
                               ETA: 1321697.1s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.751s, learning 0.204s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0270
             Mean action noise std: 0.80
                       Mean reward: 1.68
               Mean episode length: 74.51
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 16.95s
                        Total time: 387.13s
                               ETA: 1334557.4s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.704s, learning 0.198s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0422
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 73.90
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 16.90s
                        Total time: 404.03s
                               ETA: 1346382.2s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.587s, learning 0.177s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0442
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 73.90
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 16.76s
                        Total time: 420.79s
                               ETA: 1356995.9s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.690s, learning 0.166s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 1.61
               Mean episode length: 71.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 16.86s
                        Total time: 437.65s
                               ETA: 1367237.4s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.142s, learning 0.183s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0473
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 69.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 17.32s
                        Total time: 454.98s
                               ETA: 1378273.9s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.207s, learning 0.201s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0409
             Mean action noise std: 0.80
                       Mean reward: 1.49
               Mean episode length: 65.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 17.41s
                        Total time: 472.38s
                               ETA: 1388907.3s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.043s, learning 0.160s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0519
             Mean action noise std: 0.80
                       Mean reward: 1.45
               Mean episode length: 63.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 17.20s
                        Total time: 489.59s
                               ETA: 1398345.6s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 953 steps/s (collection: 17.018s, learning 0.167s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 1.44
               Mean episode length: 62.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 17.19s
                        Total time: 506.77s
                               ETA: 1407208.8s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.457s, learning 0.168s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0462
             Mean action noise std: 0.80
                       Mean reward: 1.57
               Mean episode length: 67.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 16.63s
                        Total time: 523.40s
                               ETA: 1414080.2s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.564s, learning 0.255s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0196
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 74.68
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.82s
                        Total time: 536.22s
                               ETA: 1410575.9s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.402s, learning 0.177s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0357
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 74.68
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.58s
                        Total time: 544.80s
                               ETA: 1396382.3s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.700s, learning 0.162s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0499
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 74.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.86s
                        Total time: 553.66s
                               ETA: 1383606.1s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1777 steps/s (collection: 9.051s, learning 0.165s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0516
             Mean action noise std: 0.80
                       Mean reward: 1.68
               Mean episode length: 73.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 9.22s
                        Total time: 562.87s
                               ETA: 1372315.4s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.716s, learning 0.181s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0505
             Mean action noise std: 0.80
                       Mean reward: 1.63
               Mean episode length: 72.05
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.90s
                        Total time: 571.77s
                               ETA: 1360801.7s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.517s, learning 0.160s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0491
             Mean action noise std: 0.80
                       Mean reward: 1.64
               Mean episode length: 70.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.68s
                        Total time: 580.45s
                               ETA: 1349311.3s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.667s, learning 0.165s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0503
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 67.18
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.83s
                        Total time: 589.28s
                               ETA: 1338694.9s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.503s, learning 0.174s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0495
             Mean action noise std: 0.80
                       Mean reward: 1.60
               Mean episode length: 68.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 8.68s
                        Total time: 597.96s
                               ETA: 1328207.2s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.891s, learning 0.186s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0510
             Mean action noise std: 0.80
                       Mean reward: 1.59
               Mean episode length: 69.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 9.08s
                        Total time: 607.03s
                               ETA: 1319043.0s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.570s, learning 0.194s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0299
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.76s
                        Total time: 615.80s
                               ETA: 1309604.1s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.559s, learning 0.168s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0314
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.73s
                        Total time: 624.53s
                               ETA: 1300482.5s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.631s, learning 0.256s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0521
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.89s
                        Total time: 633.41s
                               ETA: 1292058.0s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1768 steps/s (collection: 9.045s, learning 0.220s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0531
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 74.27
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 9.27s
                        Total time: 642.68s
                               ETA: 1284726.2s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.668s, learning 0.164s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0565
             Mean action noise std: 0.80
                       Mean reward: 1.65
               Mean episode length: 71.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 8.83s
                        Total time: 651.51s
                               ETA: 1276830.7s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.597s, learning 0.255s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0552
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 66.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.85s
                        Total time: 660.36s
                               ETA: 1269277.2s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.489s, learning 0.184s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0500
             Mean action noise std: 0.80
                       Mean reward: 1.49
               Mean episode length: 64.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 8.67s
                        Total time: 669.03s
                               ETA: 1261672.0s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.619s, learning 0.170s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0546
             Mean action noise std: 0.80
                       Mean reward: 1.61
               Mean episode length: 68.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.79s
                        Total time: 677.82s
                               ETA: 1254564.1s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.511s, learning 0.263s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0515
             Mean action noise std: 0.80
                       Mean reward: 1.63
               Mean episode length: 69.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.77s
                        Total time: 686.60s
                               ETA: 1247686.7s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.807s, learning 0.162s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0518
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 71.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.97s
                        Total time: 695.57s
                               ETA: 1241400.9s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.622s, learning 0.166s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0354
             Mean action noise std: 0.80
                       Mean reward: 1.74
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.79s
                        Total time: 704.35s
                               ETA: 1235018.3s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.529s, learning 0.172s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0471
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 73.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.70s
                        Total time: 713.06s
                               ETA: 1228705.0s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.974s, learning 0.165s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0612
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 72.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 9.14s
                        Total time: 722.19s
                               ETA: 1223348.5s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.748s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0544
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 70.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.91s
                        Total time: 731.10s
                               ETA: 1217789.4s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.428s, learning 0.159s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0511
             Mean action noise std: 0.80
                       Mean reward: 1.54
               Mean episode length: 66.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.59s
                        Total time: 739.69s
                               ETA: 1211882.2s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.552s, learning 0.182s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 1.53
               Mean episode length: 64.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.73s
                        Total time: 748.43s
                               ETA: 1206401.3s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.808s, learning 0.160s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0499
             Mean action noise std: 0.80
                       Mean reward: 1.60
               Mean episode length: 67.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 8.97s
                        Total time: 757.39s
                               ETA: 1201466.8s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1774 steps/s (collection: 9.053s, learning 0.181s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 69.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 9.23s
                        Total time: 766.63s
                               ETA: 1197100.9s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.787s, learning 0.161s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0469
             Mean action noise std: 0.80
                       Mean reward: 1.75
               Mean episode length: 72.30
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.95s
                        Total time: 775.58s
                               ETA: 1192429.4s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.598s, learning 0.174s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0227
             Mean action noise std: 0.80
                       Mean reward: 1.79
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.77s
                        Total time: 784.35s
                               ETA: 1187633.1s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.684s, learning 0.166s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0412
             Mean action noise std: 0.80
                       Mean reward: 1.76
               Mean episode length: 73.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.85s
                        Total time: 793.20s
                               ETA: 1183096.4s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.644s, learning 0.173s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0575
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 71.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.82s
                        Total time: 802.02s
                               ETA: 1178643.9s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1791 steps/s (collection: 8.978s, learning 0.166s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0551
             Mean action noise std: 0.80
                       Mean reward: 1.64
               Mean episode length: 69.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 9.14s
                        Total time: 811.16s
                               ETA: 1174794.0s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.704s, learning 0.165s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0502
             Mean action noise std: 0.80
                       Mean reward: 1.49
               Mean episode length: 63.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.87s
                        Total time: 820.03s
                               ETA: 1170660.5s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.895s, learning 0.166s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0470
             Mean action noise std: 0.80
                       Mean reward: 1.54
               Mean episode length: 63.90
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 9.06s
                        Total time: 829.09s
                               ETA: 1166913.8s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.676s, learning 0.166s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0519
             Mean action noise std: 0.80
                       Mean reward: 1.64
               Mean episode length: 68.51
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.84s
                        Total time: 837.93s
                               ETA: 1162967.0s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.666s, learning 0.188s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 69.36
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.85s
                        Total time: 846.78s
                               ETA: 1159144.2s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.688s, learning 0.161s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0502
             Mean action noise std: 0.80
                       Mean reward: 1.69
               Mean episode length: 70.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.85s
                        Total time: 855.63s
                               ETA: 1155418.2s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.580s, learning 0.186s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0309
             Mean action noise std: 0.80
                       Mean reward: 1.81
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.77s
                        Total time: 864.40s
                               ETA: 1151679.9s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.645s, learning 0.224s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0420
             Mean action noise std: 0.80
                       Mean reward: 1.77
               Mean episode length: 73.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 8.87s
                        Total time: 873.27s
                               ETA: 1148175.8s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.654s, learning 0.169s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0552
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 70.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.82s
                        Total time: 882.09s
                               ETA: 1144703.2s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.615s, learning 0.165s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0486
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 69.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.78s
                        Total time: 890.87s
                               ETA: 1141263.5s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.735s, learning 0.180s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0539
             Mean action noise std: 0.80
                       Mean reward: 1.38
               Mean episode length: 55.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.91s
                        Total time: 899.79s
                               ETA: 1138081.8s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.633s, learning 0.222s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0520
             Mean action noise std: 0.80
                       Mean reward: 1.51
               Mean episode length: 61.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.85s
                        Total time: 908.64s
                               ETA: 1134903.5s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.702s, learning 0.194s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 1.62
               Mean episode length: 67.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 8.90s
                        Total time: 917.54s
                               ETA: 1131855.5s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.687s, learning 0.277s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0404
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 70.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.96s
                        Total time: 926.50s
                               ETA: 1128964.9s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.944s, learning 0.206s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0473
             Mean action noise std: 0.80
                       Mean reward: 1.71
               Mean episode length: 70.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 9.15s
                        Total time: 935.65s
                               ETA: 1126367.4s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.470s, learning 0.177s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0417
             Mean action noise std: 0.80
                       Mean reward: 1.77
               Mean episode length: 72.33
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.65s
                        Total time: 944.30s
                               ETA: 1123232.6s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.714s, learning 0.168s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0356
             Mean action noise std: 0.80
                       Mean reward: 1.80
               Mean episode length: 74.18
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.88s
                        Total time: 953.18s
                               ETA: 1120447.2s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.503s, learning 0.162s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0508
             Mean action noise std: 0.80
                       Mean reward: 1.76
               Mean episode length: 70.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.67s
                        Total time: 961.85s
                               ETA: 1117474.9s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.277s, learning 0.179s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0550
             Mean action noise std: 0.80
                       Mean reward: 1.70
               Mean episode length: 67.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.46s
                        Total time: 970.30s
                               ETA: 1114330.9s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.539s, learning 0.208s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0484
             Mean action noise std: 0.80
                       Mean reward: 1.54
               Mean episode length: 61.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.75s
                        Total time: 979.05s
                               ETA: 1111587.8s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.510s, learning 0.349s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0437
             Mean action noise std: 0.80
                       Mean reward: 1.50
               Mean episode length: 62.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.86s
                        Total time: 987.91s
                               ETA: 1109031.7s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.663s, learning 0.244s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0373
             Mean action noise std: 0.80
                       Mean reward: 1.69
               Mean episode length: 68.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.91s
                        Total time: 996.81s
                               ETA: 1106586.2s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.624s, learning 0.172s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0371
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 69.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.80s
                        Total time: 1005.61s
                               ETA: 1104072.8s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.342s, learning 0.194s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 70.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.54s
                        Total time: 1014.15s
                               ETA: 1101330.0s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.544s, learning 0.270s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 1.73
               Mean episode length: 71.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.81s
                        Total time: 1022.96s
                               ETA: 1098945.9s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.720s, learning 0.263s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 1.84
               Mean episode length: 74.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 4.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.98s
                        Total time: 1031.94s
                               ETA: 1096792.2s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.756s, learning 0.281s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0491
             Mean action noise std: 0.80
                       Mean reward: 1.77
               Mean episode length: 72.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 9.04s
                        Total time: 1040.98s
                               ETA: 1094739.8s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.488s, learning 0.249s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0557
             Mean action noise std: 0.80
                       Mean reward: 1.67
               Mean episode length: 67.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.74s
                        Total time: 1049.72s
                               ETA: 1092418.5s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.717s, learning 0.272s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0523
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 62.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.99s
                        Total time: 1058.71s
                               ETA: 1090402.9s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.554s, learning 0.262s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0451
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 63.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 8.82s
                        Total time: 1067.52s
                               ETA: 1088252.6s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.655s, learning 0.214s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0457
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 66.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.87s
                        Total time: 1076.39s
                               ETA: 1086199.9s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.595s, learning 0.374s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0418
             Mean action noise std: 0.80
                       Mean reward: 1.68
               Mean episode length: 66.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 8.97s
                        Total time: 1085.36s
                               ETA: 1084287.0s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.355s, learning 0.168s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0412
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 69.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.52s
                        Total time: 1093.88s
                               ETA: 1081970.8s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.747s, learning 0.373s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 1.73
               Mean episode length: 69.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 9.12s
                        Total time: 1103.00s
                               ETA: 1080284.1s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.656s, learning 0.259s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0242
             Mean action noise std: 0.80
                       Mean reward: 1.84
               Mean episode length: 75.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.91s
                        Total time: 1111.92s
                               ETA: 1078431.0s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.446s, learning 0.165s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0343
             Mean action noise std: 0.80
                       Mean reward: 1.73
               Mean episode length: 70.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 8.61s
                        Total time: 1120.53s
                               ETA: 1076322.2s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.439s, learning 0.381s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0549
             Mean action noise std: 0.80
                       Mean reward: 1.57
               Mean episode length: 63.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 8.82s
                        Total time: 1129.35s
                               ETA: 1074452.5s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.374s, learning 0.248s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0496
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 62.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.62s
                        Total time: 1137.97s
                               ETA: 1072430.3s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.485s, learning 0.407s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0451
             Mean action noise std: 0.80
                       Mean reward: 1.66
               Mean episode length: 64.55
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.89s
                        Total time: 1146.86s
                               ETA: 1070698.2s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.230s, learning 0.158s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 1.58
               Mean episode length: 64.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.39s
                        Total time: 1155.25s
                               ETA: 1068532.4s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.384s, learning 0.321s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0444
             Mean action noise std: 0.80
                       Mean reward: 1.65
               Mean episode length: 66.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.70s
                        Total time: 1163.96s
                               ETA: 1066696.3s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.775s, learning 0.264s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 1.72
               Mean episode length: 68.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 9.04s
                        Total time: 1173.00s
                               ETA: 1065197.2s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.877s, learning 0.221s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0418
             Mean action noise std: 0.80
                       Mean reward: 1.80
               Mean episode length: 71.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 9.10s
                        Total time: 1182.09s
                               ETA: 1063777.7s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.351s, learning 0.225s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0401
             Mean action noise std: 0.80
                       Mean reward: 1.75
               Mean episode length: 68.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 8.58s
                        Total time: 1190.67s
                               ETA: 1061917.1s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.767s, learning 0.374s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0374
             Mean action noise std: 0.80
                       Mean reward: 1.77
               Mean episode length: 69.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 9.14s
                        Total time: 1199.81s
                               ETA: 1060589.1s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.690s, learning 0.266s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0537
             Mean action noise std: 0.80
                       Mean reward: 1.54
               Mean episode length: 61.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 8.96s
                        Total time: 1208.77s
                               ETA: 1059122.2s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.583s, learning 0.271s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0520
             Mean action noise std: 0.80
                       Mean reward: 1.55
               Mean episode length: 62.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 8.85s
                        Total time: 1217.62s
                               ETA: 1057592.0s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.764s, learning 0.177s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0486
             Mean action noise std: 0.79
                       Mean reward: 1.58
               Mean episode length: 63.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 8.94s
                        Total time: 1226.56s
                               ETA: 1056162.7s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.407s, learning 0.391s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0483
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 65.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 8.80s
                        Total time: 1235.36s
                               ETA: 1054635.7s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1772 steps/s (collection: 8.832s, learning 0.411s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0436
             Mean action noise std: 0.79
                       Mean reward: 1.63
               Mean episode length: 65.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 9.24s
                        Total time: 1244.60s
                               ETA: 1053510.9s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1773 steps/s (collection: 8.861s, learning 0.378s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0424
             Mean action noise std: 0.79
                       Mean reward: 1.73
               Mean episode length: 68.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 9.24s
                        Total time: 1253.84s
                               ETA: 1052402.1s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.543s, learning 0.290s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0394
             Mean action noise std: 0.79
                       Mean reward: 1.80
               Mean episode length: 69.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 8.83s
                        Total time: 1262.67s
                               ETA: 1050973.6s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.729s, learning 0.246s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 1.77
               Mean episode length: 70.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.97s
                        Total time: 1271.65s
                               ETA: 1049685.3s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.698s, learning 0.264s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0322
             Mean action noise std: 0.79
                       Mean reward: 1.83
               Mean episode length: 72.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 5.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.96s
                        Total time: 1280.61s
                               ETA: 1048407.6s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1797 steps/s (collection: 8.814s, learning 0.302s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0456
             Mean action noise std: 0.79
                       Mean reward: 1.57
               Mean episode length: 61.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 9.12s
                        Total time: 1289.72s
                               ETA: 1047276.2s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.719s, learning 0.306s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0459
             Mean action noise std: 0.79
                       Mean reward: 1.64
               Mean episode length: 64.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 9.03s
                        Total time: 1298.75s
                               ETA: 1046089.3s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.609s, learning 0.228s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0472
             Mean action noise std: 0.79
                       Mean reward: 1.62
               Mean episode length: 63.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 8.84s
                        Total time: 1307.59s
                               ETA: 1044770.9s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.566s, learning 0.266s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0441
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 66.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.83s
                        Total time: 1316.42s
                               ETA: 1043470.0s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.787s, learning 0.259s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0439
             Mean action noise std: 0.79
                       Mean reward: 1.69
               Mean episode length: 66.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 9.05s
                        Total time: 1325.46s
                               ETA: 1042357.4s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.296s, learning 0.211s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0432
             Mean action noise std: 0.79
                       Mean reward: 1.73
               Mean episode length: 67.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.51s
                        Total time: 1333.97s
                               ETA: 1040841.1s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.774s, learning 0.168s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0420
             Mean action noise std: 0.79
                       Mean reward: 1.78
               Mean episode length: 69.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.94s
                        Total time: 1342.91s
                               ETA: 1039685.1s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.595s, learning 0.302s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0381
             Mean action noise std: 0.79
                       Mean reward: 1.72
               Mean episode length: 68.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 8.90s
                        Total time: 1351.81s
                               ETA: 1038512.3s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.796s, learning 0.277s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0364
             Mean action noise std: 0.79
                       Mean reward: 1.81
               Mean episode length: 68.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 9.07s
                        Total time: 1360.88s
                               ETA: 1037491.1s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.645s, learning 0.271s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0354
             Mean action noise std: 0.79
                       Mean reward: 1.59
               Mean episode length: 60.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.92s
                        Total time: 1369.80s
                               ETA: 1036366.3s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.696s, learning 0.347s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0427
             Mean action noise std: 0.79
                       Mean reward: 1.55
               Mean episode length: 60.77
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 9.04s
                        Total time: 1378.84s
                               ETA: 1035353.7s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1768 steps/s (collection: 8.932s, learning 0.332s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0436
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 63.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 9.26s
                        Total time: 1388.11s
                               ETA: 1034521.7s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.550s, learning 0.232s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0421
             Mean action noise std: 0.79
                       Mean reward: 1.63
               Mean episode length: 64.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 8.78s
                        Total time: 1396.89s
                               ETA: 1033344.4s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.315s, learning 0.188s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0387
             Mean action noise std: 0.79
                       Mean reward: 1.68
               Mean episode length: 66.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 8.50s
                        Total time: 1405.39s
                               ETA: 1031979.9s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.565s, learning 0.204s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0391
             Mean action noise std: 0.79
                       Mean reward: 1.71
               Mean episode length: 67.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.77s
                        Total time: 1414.16s
                               ETA: 1030829.3s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.559s, learning 0.169s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0382
             Mean action noise std: 0.79
                       Mean reward: 1.73
               Mean episode length: 67.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.73s
                        Total time: 1422.89s
                               ETA: 1029665.7s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.446s, learning 0.262s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0380
             Mean action noise std: 0.79
                       Mean reward: 1.87
               Mean episode length: 71.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 8.71s
                        Total time: 1431.60s
                               ETA: 1028503.9s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.538s, learning 0.333s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0412
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 69.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 8.87s
                        Total time: 1440.47s
                               ETA: 1027475.2s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.568s, learning 0.262s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0467
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 68.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 8.83s
                        Total time: 1449.30s
                               ETA: 1026431.1s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.550s, learning 0.262s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0470
             Mean action noise std: 0.79
                       Mean reward: 1.73
               Mean episode length: 66.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 8.81s
                        Total time: 1458.11s
                               ETA: 1025389.8s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.370s, learning 0.211s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0421
             Mean action noise std: 0.79
                       Mean reward: 1.70
               Mean episode length: 65.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 8.58s
                        Total time: 1466.69s
                               ETA: 1024200.7s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.554s, learning 0.220s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0402
             Mean action noise std: 0.79
                       Mean reward: 1.71
               Mean episode length: 64.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 8.77s
                        Total time: 1475.46s
                               ETA: 1023162.8s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.597s, learning 0.349s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0397
             Mean action noise std: 0.79
                       Mean reward: 1.66
               Mean episode length: 62.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 8.95s
                        Total time: 1484.41s
                               ETA: 1022257.1s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.404s, learning 0.259s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 1.78
               Mean episode length: 68.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.66s
                        Total time: 1493.07s
                               ETA: 1021170.0s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.753s, learning 0.268s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0394
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 67.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 9.02s
                        Total time: 1502.09s
                               ETA: 1020340.9s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.377s, learning 0.337s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0367
             Mean action noise std: 0.79
                       Mean reward: 1.83
               Mean episode length: 70.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 8.71s
                        Total time: 1510.81s
                               ETA: 1019315.6s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.561s, learning 0.196s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0377
             Mean action noise std: 0.79
                       Mean reward: 1.74
               Mean episode length: 67.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.76s
                        Total time: 1519.56s
                               ETA: 1018332.5s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.232s, learning 0.166s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0438
             Mean action noise std: 0.79
                       Mean reward: 1.89
               Mean episode length: 72.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 8.40s
                        Total time: 1527.96s
                               ETA: 1017123.7s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.653s, learning 0.228s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0410
             Mean action noise std: 0.79
                       Mean reward: 1.83
               Mean episode length: 67.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 8.88s
                        Total time: 1536.84s
                               ETA: 1016250.3s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.250s, learning 0.184s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0423
             Mean action noise std: 0.79
                       Mean reward: 1.68
               Mean episode length: 63.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 8.43s
                        Total time: 1545.28s
                               ETA: 1015094.5s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.596s, learning 0.220s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0371
             Mean action noise std: 0.79
                       Mean reward: 1.80
               Mean episode length: 65.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.82s
                        Total time: 1554.09s
                               ETA: 1014203.2s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.165s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0379
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 68.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.63s
                        Total time: 1562.72s
                               ETA: 1013200.4s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.497s, learning 0.220s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0391
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 67.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 8.72s
                        Total time: 1571.44s
                               ETA: 1012268.7s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.642s, learning 0.198s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0390
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 67.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.84s
                        Total time: 1580.28s
                               ETA: 1011427.8s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.570s, learning 0.200s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0382
             Mean action noise std: 0.79
                       Mean reward: 1.80
               Mean episode length: 67.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.77s
                        Total time: 1589.05s
                               ETA: 1010552.7s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.378s, learning 0.164s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0321
             Mean action noise std: 0.79
                       Mean reward: 1.82
               Mean episode length: 68.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.54s
                        Total time: 1597.59s
                               ETA: 1009544.6s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.500s, learning 0.278s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0385
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 67.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.78s
                        Total time: 1606.37s
                               ETA: 1008696.8s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1774 steps/s (collection: 8.933s, learning 0.301s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0382
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 63.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 9.23s
                        Total time: 1615.60s
                               ETA: 1008144.3s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.668s, learning 0.179s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0386
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 60.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.85s
                        Total time: 1624.45s
                               ETA: 1007358.8s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.599s, learning 0.199s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0392
             Mean action noise std: 0.79
                       Mean reward: 1.69
               Mean episode length: 62.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 8.80s
                        Total time: 1633.24s
                               ETA: 1006552.6s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.509s, learning 0.272s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0367
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 62.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.78s
                        Total time: 1642.03s
                               ETA: 1005746.0s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.177s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0358
             Mean action noise std: 0.79
                       Mean reward: 1.84
               Mean episode length: 67.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.56s
                        Total time: 1650.59s
                               ETA: 1004817.1s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.406s, learning 0.253s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0307
             Mean action noise std: 0.79
                       Mean reward: 1.80
               Mean episode length: 66.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.66s
                        Total time: 1659.25s
                               ETA: 1003956.9s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.274s, learning 0.186s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0395
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 65.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.46s
                        Total time: 1667.71s
                               ETA: 1002987.1s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.440s, learning 0.316s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0397
             Mean action noise std: 0.79
                       Mean reward: 1.80
               Mean episode length: 65.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.76s
                        Total time: 1676.47s
                               ETA: 1002205.5s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.162s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0416
             Mean action noise std: 0.79
                       Mean reward: 1.86
               Mean episode length: 67.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 8.59s
                        Total time: 1685.06s
                               ETA: 1001334.9s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.444s, learning 0.276s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0369
             Mean action noise std: 0.79
                       Mean reward: 1.74
               Mean episode length: 63.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 8.72s
                        Total time: 1693.78s
                               ETA: 1000550.4s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.721s, learning 0.178s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0319
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 63.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 8.90s
                        Total time: 1702.67s
                               ETA: 999880.5s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.717s, learning 0.185s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 1.71
               Mean episode length: 60.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.90s
                        Total time: 1711.58s
                               ETA: 999220.7s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.701s, learning 0.186s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 63.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.89s
                        Total time: 1720.46s
                               ETA: 998559.3s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.639s, learning 0.214s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0390
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 62.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 8.85s
                        Total time: 1729.32s
                               ETA: 997886.3s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.346s, learning 0.179s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0375
             Mean action noise std: 0.79
                       Mean reward: 1.78
               Mean episode length: 64.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.52s
                        Total time: 1737.84s
                               ETA: 997031.9s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.668s, learning 0.233s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0403
             Mean action noise std: 0.79
                       Mean reward: 1.78
               Mean episode length: 63.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 8.90s
                        Total time: 1746.74s
                               ETA: 996402.0s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.194s, learning 0.171s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0409
             Mean action noise std: 0.79
                       Mean reward: 1.86
               Mean episode length: 64.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.37s
                        Total time: 1755.11s
                               ETA: 995475.3s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.725s, learning 0.228s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0390
             Mean action noise std: 0.79
                       Mean reward: 1.78
               Mean episode length: 63.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.95s
                        Total time: 1764.06s
                               ETA: 994890.8s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.271s, learning 0.180s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0387
             Mean action noise std: 0.79
                       Mean reward: 1.90
               Mean episode length: 70.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.45s
                        Total time: 1772.51s
                               ETA: 994030.9s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.404s, learning 0.294s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0404
             Mean action noise std: 0.79
                       Mean reward: 1.64
               Mean episode length: 59.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.70s
                        Total time: 1781.21s
                               ETA: 993318.0s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.499s, learning 0.180s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0373
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 63.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.68s
                        Total time: 1789.89s
                               ETA: 992602.5s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.368s, learning 0.229s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0388
             Mean action noise std: 0.79
                       Mean reward: 1.79
               Mean episode length: 64.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 8.60s
                        Total time: 1798.49s
                               ETA: 991850.0s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.676s, learning 0.179s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0436
             Mean action noise std: 0.79
                       Mean reward: 1.95
               Mean episode length: 68.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.85s
                        Total time: 1807.34s
                               ETA: 991246.5s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.592s, learning 0.190s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0404
             Mean action noise std: 0.79
                       Mean reward: 1.86
               Mean episode length: 66.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 8.78s
                        Total time: 1816.12s
                               ETA: 990610.1s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.502s, learning 0.167s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0355
             Mean action noise std: 0.79
                       Mean reward: 1.87
               Mean episode length: 67.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 8.67s
                        Total time: 1824.79s
                               ETA: 989919.4s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.556s, learning 0.189s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0383
             Mean action noise std: 0.79
                       Mean reward: 1.75
               Mean episode length: 61.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 8.74s
                        Total time: 1833.54s
                               ETA: 989276.8s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.582s, learning 0.182s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0379
             Mean action noise std: 0.79
                       Mean reward: 1.82
               Mean episode length: 63.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.76s
                        Total time: 1842.30s
                               ETA: 988651.3s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.361s, learning 0.190s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0373
             Mean action noise std: 0.78
                       Mean reward: 1.78
               Mean episode length: 62.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 8.55s
                        Total time: 1850.85s
                               ETA: 987919.0s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.333s, learning 0.164s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0375
             Mean action noise std: 0.78
                       Mean reward: 1.74
               Mean episode length: 61.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 8.50s
                        Total time: 1859.35s
                               ETA: 987165.6s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.455s, learning 0.185s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0366
             Mean action noise std: 0.78
                       Mean reward: 1.78
               Mean episode length: 61.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 8.64s
                        Total time: 1867.99s
                               ETA: 986495.8s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.744s, learning 0.181s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0380
             Mean action noise std: 0.78
                       Mean reward: 1.83
               Mean episode length: 63.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 8.93s
                        Total time: 1876.91s
                               ETA: 985982.6s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.303s, learning 0.238s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0363
             Mean action noise std: 0.78
                       Mean reward: 1.78
               Mean episode length: 60.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 8.54s
                        Total time: 1885.46s
                               ETA: 985274.1s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1775 steps/s (collection: 8.980s, learning 0.250s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0364
             Mean action noise std: 0.78
                       Mean reward: 1.83
               Mean episode length: 63.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 9.23s
                        Total time: 1894.69s
                               ETA: 984930.4s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.485s, learning 0.177s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0374
             Mean action noise std: 0.78
                       Mean reward: 1.78
               Mean episode length: 61.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 8.66s
                        Total time: 1903.35s
                               ETA: 984296.7s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.188s, learning 0.176s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0364
             Mean action noise std: 0.78
                       Mean reward: 1.89
               Mean episode length: 63.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 8.36s
                        Total time: 1911.71s
                               ETA: 983516.0s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.468s, learning 0.162s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0382
             Mean action noise std: 0.78
                       Mean reward: 1.78
               Mean episode length: 60.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.63s
                        Total time: 1920.34s
                               ETA: 982879.6s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.781s, learning 0.181s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0408
             Mean action noise std: 0.78
                       Mean reward: 1.90
               Mean episode length: 64.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 8.96s
                        Total time: 1929.30s
                               ETA: 982418.8s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.757s, learning 0.205s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0374
             Mean action noise std: 0.78
                       Mean reward: 1.91
               Mean episode length: 65.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.96s
                        Total time: 1938.26s
                               ETA: 981962.2s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.160s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0356
             Mean action noise std: 0.78
                       Mean reward: 1.90
               Mean episode length: 64.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 8.41s
                        Total time: 1946.68s
                               ETA: 981233.3s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.383s, learning 0.359s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0341
             Mean action noise std: 0.78
                       Mean reward: 1.85
               Mean episode length: 62.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.74s
                        Total time: 1955.42s
                               ETA: 980676.7s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.329s, learning 0.186s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0392
             Mean action noise std: 0.78
                       Mean reward: 1.86
               Mean episode length: 62.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.52s
                        Total time: 1963.93s
                               ETA: 980012.9s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.525s, learning 0.267s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0351
             Mean action noise std: 0.78
                       Mean reward: 1.90
               Mean episode length: 65.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.79s
                        Total time: 1972.73s
                               ETA: 979493.0s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.811s, learning 0.327s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0353
             Mean action noise std: 0.78
                       Mean reward: 1.83
               Mean episode length: 61.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 9.14s
                        Total time: 1981.86s
                               ETA: 979149.2s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.526s, learning 0.166s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0379
             Mean action noise std: 0.78
                       Mean reward: 1.91
               Mean episode length: 63.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.69s
                        Total time: 1990.56s
                               ETA: 978589.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.478s, learning 0.171s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0359
             Mean action noise std: 0.78
                       Mean reward: 1.90
               Mean episode length: 62.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.65s
                        Total time: 1999.21s
                               ETA: 978013.6s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.534s, learning 0.161s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0366
             Mean action noise std: 0.78
                       Mean reward: 1.90
               Mean episode length: 62.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 8.70s
                        Total time: 2007.90s
                               ETA: 977466.2s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.562s, learning 0.194s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0346
             Mean action noise std: 0.78
                       Mean reward: 1.91
               Mean episode length: 61.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 8.76s
                        Total time: 2016.66s
                               ETA: 976953.5s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.468s, learning 0.175s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0311
             Mean action noise std: 0.78
                       Mean reward: 1.84
               Mean episode length: 60.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 8.64s
                        Total time: 2025.30s
                               ETA: 976391.3s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.586s, learning 0.186s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0360
             Mean action noise std: 0.78
                       Mean reward: 1.93
               Mean episode length: 63.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.77s
                        Total time: 2034.07s
                               ETA: 975896.1s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.171s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0375
             Mean action noise std: 0.78
                       Mean reward: 1.88
               Mean episode length: 61.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.86s
                        Total time: 2042.94s
                               ETA: 975447.9s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.498s, learning 0.203s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0369
             Mean action noise std: 0.78
                       Mean reward: 1.91
               Mean episode length: 62.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.70s
                        Total time: 2051.64s
                               ETA: 974927.8s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.941s, learning 0.161s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0376
             Mean action noise std: 0.78
                       Mean reward: 1.95
               Mean episode length: 63.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 9.10s
                        Total time: 2060.74s
                               ETA: 974602.4s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.387s, learning 0.280s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0370
             Mean action noise std: 0.78
                       Mean reward: 2.07
               Mean episode length: 66.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.67s
                        Total time: 2069.41s
                               ETA: 974075.3s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.068s, learning 0.184s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0364
             Mean action noise std: 0.78
                       Mean reward: 1.89
               Mean episode length: 60.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.25s
                        Total time: 2077.66s
                               ETA: 973358.5s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.712s, learning 0.229s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0387
             Mean action noise std: 0.78
                       Mean reward: 1.87
               Mean episode length: 60.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.94s
                        Total time: 2086.60s
                               ETA: 972969.4s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.491s, learning 0.198s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0374
             Mean action noise std: 0.78
                       Mean reward: 2.00
               Mean episode length: 64.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.69s
                        Total time: 2095.29s
                               ETA: 972466.8s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.205s, learning 0.181s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0353
             Mean action noise std: 0.78
                       Mean reward: 1.86
               Mean episode length: 59.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.39s
                        Total time: 2103.67s
                               ETA: 971829.0s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.172s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0382
             Mean action noise std: 0.78
                       Mean reward: 1.96
               Mean episode length: 63.02
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.75s
                        Total time: 2112.42s
                               ETA: 971363.9s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.567s, learning 0.245s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0370
             Mean action noise std: 0.78
                       Mean reward: 1.95
               Mean episode length: 61.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.81s
                        Total time: 2121.23s
                               ETA: 970931.7s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.433s, learning 0.194s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0376
             Mean action noise std: 0.78
                       Mean reward: 1.99
               Mean episode length: 62.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.63s
                        Total time: 2129.86s
                               ETA: 970419.2s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.599s, learning 0.174s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0354
             Mean action noise std: 0.78
                       Mean reward: 1.86
               Mean episode length: 59.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.77s
                        Total time: 2138.63s
                               ETA: 969977.7s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.405s, learning 0.183s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0366
             Mean action noise std: 0.78
                       Mean reward: 1.93
               Mean episode length: 60.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.59s
                        Total time: 2147.22s
                               ETA: 969456.3s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.428s, learning 0.193s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0357
             Mean action noise std: 0.78
                       Mean reward: 2.02
               Mean episode length: 62.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.62s
                        Total time: 2155.84s
                               ETA: 968954.5s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.929s, learning 0.178s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0366
             Mean action noise std: 0.78
                       Mean reward: 2.01
               Mean episode length: 63.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.11s
                        Total time: 2163.95s
                               ETA: 968227.0s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.187s, learning 0.224s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0358
             Mean action noise std: 0.78
                       Mean reward: 1.98
               Mean episode length: 62.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 8.41s
                        Total time: 2172.36s
                               ETA: 967641.6s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.766s, learning 0.216s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0348
             Mean action noise std: 0.78
                       Mean reward: 2.03
               Mean episode length: 62.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.98s
                        Total time: 2181.34s
                               ETA: 967314.4s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.378s, learning 0.162s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0361
             Mean action noise std: 0.78
                       Mean reward: 2.03
               Mean episode length: 63.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.54s
                        Total time: 2189.88s
                               ETA: 966794.6s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.454s, learning 0.181s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0372
             Mean action noise std: 0.78
                       Mean reward: 1.95
               Mean episode length: 60.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.63s
                        Total time: 2198.52s
                               ETA: 966321.3s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.420s, learning 0.159s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0359
             Mean action noise std: 0.78
                       Mean reward: 2.00
               Mean episode length: 61.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.58s
                        Total time: 2207.10s
                               ETA: 965827.7s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.524s, learning 0.261s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0350
             Mean action noise std: 0.78
                       Mean reward: 2.06
               Mean episode length: 65.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.78s
                        Total time: 2215.88s
                               ETA: 965427.9s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.268s, learning 0.179s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0386
             Mean action noise std: 0.78
                       Mean reward: 2.07
               Mean episode length: 64.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.45s
                        Total time: 2224.33s
                               ETA: 964885.2s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.751s, learning 0.336s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0323
             Mean action noise std: 0.78
                       Mean reward: 2.03
               Mean episode length: 64.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 9.09s
                        Total time: 2233.42s
                               ETA: 964623.1s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.554s, learning 0.173s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0325
             Mean action noise std: 0.78
                       Mean reward: 2.03
               Mean episode length: 63.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.73s
                        Total time: 2242.14s
                               ETA: 964208.8s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.473s, learning 0.222s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0361
             Mean action noise std: 0.77
                       Mean reward: 2.07
               Mean episode length: 63.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.69s
                        Total time: 2250.84s
                               ETA: 963783.7s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.760s, learning 0.260s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 1.97
               Mean episode length: 61.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 9.02s
                        Total time: 2259.86s
                               ETA: 963501.1s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.613s, learning 0.232s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0357
             Mean action noise std: 0.77
                       Mean reward: 2.05
               Mean episode length: 63.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 8.84s
                        Total time: 2268.70s
                               ETA: 963146.3s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.497s, learning 0.168s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0334
             Mean action noise std: 0.77
                       Mean reward: 2.00
               Mean episode length: 63.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 8.66s
                        Total time: 2277.37s
                               ETA: 962718.4s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.154s, learning 0.284s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0336
             Mean action noise std: 0.77
                       Mean reward: 1.95
               Mean episode length: 61.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.44s
                        Total time: 2285.81s
                               ETA: 962198.6s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.226s, learning 0.168s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0341
             Mean action noise std: 0.77
                       Mean reward: 1.97
               Mean episode length: 62.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.39s
                        Total time: 2294.20s
                               ETA: 961664.9s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.196s, learning 0.172s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0319
             Mean action noise std: 0.77
                       Mean reward: 2.07
               Mean episode length: 65.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 8.37s
                        Total time: 2302.57s
                               ETA: 961124.3s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.129s, learning 0.216s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0355
             Mean action noise std: 0.77
                       Mean reward: 2.07
               Mean episode length: 63.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.34s
                        Total time: 2310.91s
                               ETA: 960578.6s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.917s, learning 0.181s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0342
             Mean action noise std: 0.77
                       Mean reward: 2.05
               Mean episode length: 63.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 9.10s
                        Total time: 2320.01s
                               ETA: 960349.3s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.752s, learning 0.179s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0354
             Mean action noise std: 0.77
                       Mean reward: 2.06
               Mean episode length: 64.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 8.93s
                        Total time: 2328.94s
                               ETA: 960052.7s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.499s, learning 0.162s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0350
             Mean action noise std: 0.77
                       Mean reward: 2.09
               Mean episode length: 64.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 8.66s
                        Total time: 2337.60s
                               ETA: 959647.8s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.168s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0349
             Mean action noise std: 0.77
                       Mean reward: 2.12
               Mean episode length: 64.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 8.58s
                        Total time: 2346.18s
                               ETA: 959214.1s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.260s, learning 0.185s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0352
             Mean action noise std: 0.77
                       Mean reward: 2.09
               Mean episode length: 64.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 8.45s
                        Total time: 2354.63s
                               ETA: 958727.9s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.570s, learning 0.246s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0324
             Mean action noise std: 0.77
                       Mean reward: 2.11
               Mean episode length: 65.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 8.82s
                        Total time: 2363.44s
                               ETA: 958395.9s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.772s, learning 0.225s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0353
             Mean action noise std: 0.77
                       Mean reward: 2.06
               Mean episode length: 63.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 9.00s
                        Total time: 2372.44s
                               ETA: 958140.0s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.294s, learning 0.183s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0341
             Mean action noise std: 0.77
                       Mean reward: 2.10
               Mean episode length: 66.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 8.48s
                        Total time: 2380.92s
                               ETA: 957676.3s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.465s, learning 0.177s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0359
             Mean action noise std: 0.77
                       Mean reward: 2.07
               Mean episode length: 63.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 8.64s
                        Total time: 2389.56s
                               ETA: 957282.9s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.163s, learning 0.224s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0339
             Mean action noise std: 0.77
                       Mean reward: 2.09
               Mean episode length: 64.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 11.39s
                        Total time: 2400.95s
                               ETA: 957987.5s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.461s, learning 0.167s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0345
             Mean action noise std: 0.77
                       Mean reward: 2.10
               Mean episode length: 66.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 16.63s
                        Total time: 2417.57s
                               ETA: 960769.0s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.143s, learning 0.231s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0347
             Mean action noise std: 0.77
                       Mean reward: 2.20
               Mean episode length: 69.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 17.37s
                        Total time: 2434.95s
                               ETA: 963823.8s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.852s, learning 0.177s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0346
             Mean action noise std: 0.77
                       Mean reward: 2.16
               Mean episode length: 67.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 17.03s
                        Total time: 2451.98s
                               ETA: 966718.9s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.877s, learning 0.184s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 2.21
               Mean episode length: 68.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 17.06s
                        Total time: 2469.04s
                               ETA: 969602.9s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.551s, learning 0.184s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0326
             Mean action noise std: 0.77
                       Mean reward: 2.18
               Mean episode length: 69.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 16.74s
                        Total time: 2485.77s
                               ETA: 972337.0s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.845s, learning 0.161s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0331
             Mean action noise std: 0.77
                       Mean reward: 2.14
               Mean episode length: 69.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 17.01s
                        Total time: 2502.78s
                               ETA: 975155.3s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.961s, learning 0.213s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0352
             Mean action noise std: 0.77
                       Mean reward: 2.09
               Mean episode length: 65.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 17.17s
                        Total time: 2519.95s
                               ETA: 978016.4s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.702s, learning 0.177s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0322
             Mean action noise std: 0.77
                       Mean reward: 2.18
               Mean episode length: 68.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 16.88s
                        Total time: 2536.83s
                               ETA: 980741.3s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.194s, learning 0.161s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0315
             Mean action noise std: 0.77
                       Mean reward: 2.23
               Mean episode length: 68.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 16.36s
                        Total time: 2553.19s
                               ETA: 983243.3s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.636s, learning 0.251s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0334
             Mean action noise std: 0.77
                       Mean reward: 2.18
               Mean episode length: 68.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 16.89s
                        Total time: 2570.08s
                               ETA: 985930.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.287s, learning 0.164s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0337
             Mean action noise std: 0.77
                       Mean reward: 2.08
               Mean episode length: 67.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 16.45s
                        Total time: 2586.53s
                               ETA: 988429.5s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.241s, learning 0.193s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0339
             Mean action noise std: 0.77
                       Mean reward: 2.13
               Mean episode length: 67.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 16.43s
                        Total time: 2602.96s
                               ETA: 990903.1s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.266s, learning 0.164s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0339
             Mean action noise std: 0.77
                       Mean reward: 2.16
               Mean episode length: 66.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 16.43s
                        Total time: 2619.39s
                               ETA: 993356.2s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.634s, learning 0.163s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0323
             Mean action noise std: 0.77
                       Mean reward: 2.22
               Mean episode length: 69.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 16.80s
                        Total time: 2636.19s
                               ETA: 995929.5s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.887s, learning 0.188s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0345
             Mean action noise std: 0.77
                       Mean reward: 2.15
               Mean episode length: 68.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 17.07s
                        Total time: 2653.26s
                               ETA: 998587.6s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.637s, learning 0.397s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0353
             Mean action noise std: 0.77
                       Mean reward: 2.13
               Mean episode length: 66.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 17.03s
                        Total time: 2670.30s
                               ETA: 1001210.6s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.765s, learning 0.181s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 2.16
               Mean episode length: 67.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 16.95s
                        Total time: 2687.24s
                               ETA: 1003780.7s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.579s, learning 0.190s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0328
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 69.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 16.77s
                        Total time: 2704.01s
                               ETA: 1006265.7s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.263s, learning 0.165s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0328
             Mean action noise std: 0.77
                       Mean reward: 2.17
               Mean episode length: 68.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 16.43s
                        Total time: 2720.44s
                               ETA: 1008605.8s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.889s, learning 0.170s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0350
             Mean action noise std: 0.77
                       Mean reward: 2.22
               Mean episode length: 69.13
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 17.06s
                        Total time: 2737.50s
                               ETA: 1011161.1s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.576s, learning 0.235s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0365
             Mean action noise std: 0.77
                       Mean reward: 2.17
               Mean episode length: 69.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 16.81s
                        Total time: 2754.31s
                               ETA: 1013606.5s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.604s, learning 0.182s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0344
             Mean action noise std: 0.77
                       Mean reward: 2.23
               Mean episode length: 69.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 16.79s
                        Total time: 2771.10s
                               ETA: 1016024.5s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.657s, learning 0.163s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0342
             Mean action noise std: 0.77
                       Mean reward: 2.21
               Mean episode length: 67.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 16.82s
                        Total time: 2787.92s
                               ETA: 1018436.9s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.207s, learning 0.158s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0347
             Mean action noise std: 0.77
                       Mean reward: 2.15
               Mean episode length: 68.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 16.37s
                        Total time: 2804.28s
                               ETA: 1020666.2s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.718s, learning 0.166s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0341
             Mean action noise std: 0.77
                       Mean reward: 2.22
               Mean episode length: 69.52
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 16.88s
                        Total time: 2821.17s
                               ETA: 1023067.4s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.323s, learning 0.354s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0345
             Mean action noise std: 0.77
                       Mean reward: 2.18
               Mean episode length: 68.33
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 16.68s
                        Total time: 2837.84s
                               ETA: 1025376.3s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.728s, learning 0.206s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0368
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 69.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 16.93s
                        Total time: 2854.78s
                               ETA: 1027760.8s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.943s, learning 0.178s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0354
             Mean action noise std: 0.77
                       Mean reward: 2.22
               Mean episode length: 66.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 16.12s
                        Total time: 2870.90s
                               ETA: 1029836.2s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.483s, learning 0.367s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0341
             Mean action noise std: 0.77
                       Mean reward: 2.11
               Mean episode length: 68.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 16.85s
                        Total time: 2887.75s
                               ETA: 1032157.4s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.926s, learning 0.168s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0338
             Mean action noise std: 0.77
                       Mean reward: 2.19
               Mean episode length: 68.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 17.09s
                        Total time: 2904.84s
                               ETA: 1034548.5s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 949 steps/s (collection: 16.981s, learning 0.277s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0370
             Mean action noise std: 0.77
                       Mean reward: 2.19
               Mean episode length: 68.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 17.26s
                        Total time: 2922.10s
                               ETA: 1036981.2s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.803s, learning 0.203s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0334
             Mean action noise std: 0.77
                       Mean reward: 2.28
               Mean episode length: 68.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 17.01s
                        Total time: 2939.11s
                               ETA: 1039307.3s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.681s, learning 0.164s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0361
             Mean action noise std: 0.77
                       Mean reward: 2.19
               Mean episode length: 69.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 16.85s
                        Total time: 2955.95s
                               ETA: 1041560.0s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.139s, learning 0.191s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0353
             Mean action noise std: 0.77
                       Mean reward: 2.20
               Mean episode length: 66.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 17.33s
                        Total time: 2973.28s
                               ETA: 1043967.1s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.218s, learning 0.168s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 2.20
               Mean episode length: 67.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 16.39s
                        Total time: 2989.67s
                               ETA: 1046026.8s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.876s, learning 0.193s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0361
             Mean action noise std: 0.77
                       Mean reward: 2.28
               Mean episode length: 69.85
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 17.07s
                        Total time: 3006.74s
                               ETA: 1048310.3s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 951 steps/s (collection: 16.971s, learning 0.248s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0332
             Mean action noise std: 0.77
                       Mean reward: 2.18
               Mean episode length: 66.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 17.22s
                        Total time: 3023.96s
                               ETA: 1050629.8s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.754s, learning 0.213s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 68.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 11.97s
                        Total time: 3035.92s
                               ETA: 1051114.6s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.195s, learning 0.159s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 2.13
               Mean episode length: 65.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 8.35s
                        Total time: 3044.28s
                               ETA: 1050349.2s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.636s, learning 0.313s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 2.27
               Mean episode length: 67.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 8.95s
                        Total time: 3053.23s
                               ETA: 1049793.6s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.243s, learning 0.179s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0368
             Mean action noise std: 0.77
                       Mean reward: 2.21
               Mean episode length: 66.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.42s
                        Total time: 3061.65s
                               ETA: 1049061.5s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1781 steps/s (collection: 8.895s, learning 0.302s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0378
             Mean action noise std: 0.77
                       Mean reward: 2.08
               Mean episode length: 62.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 9.20s
                        Total time: 3070.84s
                               ETA: 1048598.9s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.710s, learning 0.159s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0365
             Mean action noise std: 0.77
                       Mean reward: 2.26
               Mean episode length: 66.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 8.87s
                        Total time: 3079.71s
                               ETA: 1048027.8s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.338s, learning 0.301s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0360
             Mean action noise std: 0.77
                       Mean reward: 2.21
               Mean episode length: 64.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 8.64s
                        Total time: 3088.35s
                               ETA: 1047382.6s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.915s, learning 0.177s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0407
             Mean action noise std: 0.77
                       Mean reward: 2.21
               Mean episode length: 64.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 9.09s
                        Total time: 3097.45s
                               ETA: 1046894.8s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.567s, learning 0.187s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0361
             Mean action noise std: 0.77
                       Mean reward: 2.31
               Mean episode length: 67.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 8.75s
                        Total time: 3106.20s
                               ETA: 1046296.1s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.562s, learning 0.340s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 65.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 8.90s
                        Total time: 3115.10s
                               ETA: 1045751.1s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.433s, learning 0.185s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 65.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 8.62s
                        Total time: 3123.72s
                               ETA: 1045114.5s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.573s, learning 0.168s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0367
             Mean action noise std: 0.77
                       Mean reward: 2.29
               Mean episode length: 67.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 8.74s
                        Total time: 3132.46s
                               ETA: 1044523.4s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.588s, learning 0.174s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 2.32
               Mean episode length: 66.57
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.76s
                        Total time: 3141.22s
                               ETA: 1043943.1s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.207s, learning 0.159s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0368
             Mean action noise std: 0.77
                       Mean reward: 2.26
               Mean episode length: 66.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.37s
                        Total time: 3149.59s
                               ETA: 1043235.6s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.557s, learning 0.185s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 2.34
               Mean episode length: 66.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 8.74s
                        Total time: 3158.33s
                               ETA: 1042656.7s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.402s, learning 0.193s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0388
             Mean action noise std: 0.77
                       Mean reward: 2.29
               Mean episode length: 65.96
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.59s
                        Total time: 3166.92s
                               ETA: 1042033.3s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.714s, learning 0.339s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0360
             Mean action noise std: 0.77
                       Mean reward: 2.22
               Mean episode length: 63.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 9.05s
                        Total time: 3175.98s
                               ETA: 1041564.0s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.020s, learning 0.167s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0343
             Mean action noise std: 0.77
                       Mean reward: 2.33
               Mean episode length: 66.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.19s
                        Total time: 3184.16s
                               ETA: 1040814.6s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1757 steps/s (collection: 9.080s, learning 0.244s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0363
             Mean action noise std: 0.77
                       Mean reward: 2.23
               Mean episode length: 65.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 9.32s
                        Total time: 3193.49s
                               ETA: 1040440.3s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.379s, learning 0.180s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0352
             Mean action noise std: 0.77
                       Mean reward: 2.23
               Mean episode length: 64.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.56s
                        Total time: 3202.05s
                               ETA: 1039820.0s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.477s, learning 0.161s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 2.24
               Mean episode length: 64.34
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.64s
                        Total time: 3210.68s
                               ETA: 1039229.4s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.448s, learning 0.175s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0387
             Mean action noise std: 0.77
                       Mean reward: 2.32
               Mean episode length: 64.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.62s
                        Total time: 3219.31s
                               ETA: 1038637.9s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.996s, learning 0.178s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0397
             Mean action noise std: 0.77
                       Mean reward: 2.34
               Mean episode length: 65.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.17s
                        Total time: 3227.48s
                               ETA: 1037905.5s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.183s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0414
             Mean action noise std: 0.77
                       Mean reward: 2.34
               Mean episode length: 66.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.58s
                        Total time: 3236.06s
                               ETA: 1037309.2s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.550s, learning 0.221s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0391
             Mean action noise std: 0.77
                       Mean reward: 2.28
               Mean episode length: 66.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.77s
                        Total time: 3244.84s
                               ETA: 1036776.8s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.151s, learning 0.194s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 2.20
               Mean episode length: 64.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.34s
                        Total time: 3253.18s
                               ETA: 1036111.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.480s, learning 0.204s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 63.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.68s
                        Total time: 3261.86s
                               ETA: 1035558.7s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.619s, learning 0.187s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0363
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 65.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.81s
                        Total time: 3270.67s
                               ETA: 1035047.5s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.708s, learning 0.171s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0360
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 65.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 8.88s
                        Total time: 3279.55s
                               ETA: 1034562.6s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1785 steps/s (collection: 8.922s, learning 0.255s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 63.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 9.18s
                        Total time: 3288.73s
                               ETA: 1034174.6s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.567s, learning 0.273s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 63.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.84s
                        Total time: 3297.57s
                               ETA: 1033683.2s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.655s, learning 0.205s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 65.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.86s
                        Total time: 3306.43s
                               ETA: 1033201.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.359s, learning 0.196s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0373
             Mean action noise std: 0.76
                       Mean reward: 2.17
               Mean episode length: 61.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.56s
                        Total time: 3314.98s
                               ETA: 1032627.0s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.768s, learning 0.174s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 61.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 8.94s
                        Total time: 3323.92s
                               ETA: 1032176.6s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.388s, learning 0.173s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 2.36
               Mean episode length: 63.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.56s
                        Total time: 3332.48s
                               ETA: 1031610.8s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.437s, learning 0.264s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 64.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.70s
                        Total time: 3341.19s
                               ETA: 1031091.8s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.626s, learning 0.196s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 63.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.82s
                        Total time: 3350.01s
                               ETA: 1030613.1s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.607s, learning 0.176s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 63.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 8.78s
                        Total time: 3358.79s
                               ETA: 1030125.5s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.516s, learning 0.174s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 2.35
               Mean episode length: 62.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.69s
                        Total time: 3367.48s
                               ETA: 1029612.6s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.649s, learning 0.297s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0363
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 62.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 8.95s
                        Total time: 3376.43s
                               ETA: 1029180.6s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.540s, learning 0.163s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 64.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.70s
                        Total time: 3385.13s
                               ETA: 1028677.4s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.508s, learning 0.204s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 61.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.71s
                        Total time: 3393.84s
                               ETA: 1028179.8s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.176s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 2.25
               Mean episode length: 63.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.46s
                        Total time: 3402.30s
                               ETA: 1027608.5s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.089s, learning 0.208s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 2.15
               Mean episode length: 61.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.30s
                        Total time: 3410.60s
                               ETA: 1026992.0s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.412s, learning 0.166s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0398
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 61.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.58s
                        Total time: 3419.18s
                               ETA: 1026463.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.203s, learning 0.186s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 63.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.39s
                        Total time: 3427.57s
                               ETA: 1025882.1s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.422s, learning 0.184s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 62.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 8.61s
                        Total time: 3436.17s
                               ETA: 1025368.2s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.449s, learning 0.195s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 62.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 8.64s
                        Total time: 3444.82s
                               ETA: 1024869.0s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.600s, learning 0.368s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 61.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.97s
                        Total time: 3453.79s
                               ETA: 1024468.8s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.896s, learning 0.184s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0381
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 62.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 9.08s
                        Total time: 3462.87s
                               ETA: 1024103.9s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.163s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 2.31
               Mean episode length: 59.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 8.53s
                        Total time: 3471.40s
                               ETA: 1023579.7s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.343s, learning 0.174s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 61.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.52s
                        Total time: 3479.92s
                               ETA: 1023054.2s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.270s, learning 0.184s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 2.22
               Mean episode length: 59.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 8.45s
                        Total time: 3488.37s
                               ETA: 1022512.8s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.682s, learning 0.160s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 60.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 8.84s
                        Total time: 3497.21s
                               ETA: 1022088.2s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.421s, learning 0.221s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 2.31
               Mean episode length: 59.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 8.64s
                        Total time: 3505.85s
                               ETA: 1021607.7s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.531s, learning 0.182s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0358
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 59.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 8.71s
                        Total time: 3514.57s
                               ETA: 1021150.6s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.208s, learning 0.227s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 60.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.43s
                        Total time: 3523.00s
                               ETA: 1020615.3s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.729s, learning 0.197s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 59.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 8.93s
                        Total time: 3531.93s
                               ETA: 1020225.2s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.649s, learning 0.166s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0353
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 60.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.81s
                        Total time: 3540.74s
                               ETA: 1019805.1s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.856s, learning 0.169s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 60.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 9.03s
                        Total time: 3549.77s
                               ETA: 1019447.9s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.726s, learning 0.185s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 58.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.91s
                        Total time: 3558.68s
                               ETA: 1019060.0s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.350s, learning 0.194s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0401
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 61.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 8.54s
                        Total time: 3567.22s
                               ETA: 1018569.4s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.412s, learning 0.179s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 61.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 8.59s
                        Total time: 3575.81s
                               ETA: 1018095.1s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.323s, learning 0.179s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0391
             Mean action noise std: 0.76
                       Mean reward: 2.19
               Mean episode length: 57.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 8.50s
                        Total time: 3584.31s
                               ETA: 1017598.1s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.436s, learning 0.168s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0388
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 61.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 8.60s
                        Total time: 3592.92s
                               ETA: 1017132.8s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.160s, learning 0.160s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0368
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 61.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 8.32s
                        Total time: 3601.24s
                               ETA: 1016589.9s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.479s, learning 0.161s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0411
             Mean action noise std: 0.76
                       Mean reward: 2.20
               Mean episode length: 57.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 8.64s
                        Total time: 3609.88s
                               ETA: 1016140.1s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.506s, learning 0.161s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0392
             Mean action noise std: 0.76
                       Mean reward: 2.19
               Mean episode length: 58.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 8.67s
                        Total time: 3618.55s
                               ETA: 1015700.1s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.514s, learning 0.182s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 59.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 8.70s
                        Total time: 3627.24s
                               ETA: 1015270.8s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.619s, learning 0.167s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 60.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.79s
                        Total time: 3636.03s
                               ETA: 1014869.3s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.722s, learning 0.170s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 59.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.89s
                        Total time: 3644.92s
                               ETA: 1014499.2s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.651s, learning 0.202s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.21
               Mean episode length: 56.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 8.85s
                        Total time: 3653.77s
                               ETA: 1014120.4s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.644s, learning 0.182s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0400
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 61.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 8.83s
                        Total time: 3662.60s
                               ETA: 1013736.0s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.851s, learning 0.189s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.25
               Mean episode length: 58.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 9.04s
                        Total time: 3671.64s
                               ETA: 1013412.7s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.592s, learning 0.173s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0405
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 60.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 8.76s
                        Total time: 3680.40s
                               ETA: 1013015.5s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.286s, learning 0.200s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 2.19
               Mean episode length: 58.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 8.49s
                        Total time: 3688.89s
                               ETA: 1012544.1s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.611s, learning 0.211s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0391
             Mean action noise std: 0.76
                       Mean reward: 2.20
               Mean episode length: 58.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 8.82s
                        Total time: 3697.71s
                               ETA: 1012167.1s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.332s, learning 0.194s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 58.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.53s
                        Total time: 3706.24s
                               ETA: 1011711.2s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1783 steps/s (collection: 8.879s, learning 0.308s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0373
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 60.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 9.19s
                        Total time: 3715.42s
                               ETA: 1011437.8s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.292s, learning 0.195s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 60.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 8.49s
                        Total time: 3723.91s
                               ETA: 1010975.8s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.296s, learning 0.182s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 2.25
               Mean episode length: 61.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.48s
                        Total time: 3732.39s
                               ETA: 1010513.7s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.213s, learning 0.172s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0405
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 59.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.38s
                        Total time: 3740.77s
                               ETA: 1010028.9s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.616s, learning 0.178s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0362
             Mean action noise std: 0.76
                       Mean reward: 2.16
               Mean episode length: 58.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 8.79s
                        Total time: 3749.57s
                               ETA: 1009656.8s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.574s, learning 0.195s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0367
             Mean action noise std: 0.76
                       Mean reward: 2.39
               Mean episode length: 61.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.77s
                        Total time: 3758.34s
                               ETA: 1009280.3s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.421s, learning 0.252s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 2.22
               Mean episode length: 57.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.67s
                        Total time: 3767.01s
                               ETA: 1008879.9s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.763s, learning 0.217s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 2.21
               Mean episode length: 57.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.98s
                        Total time: 3775.99s
                               ETA: 1008563.5s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.723s, learning 0.185s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 57.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.91s
                        Total time: 3784.90s
                               ETA: 1008229.6s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.520s, learning 0.163s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 58.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.68s
                        Total time: 3793.58s
                               ETA: 1007837.9s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.472s, learning 0.181s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0418
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 59.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 8.65s
                        Total time: 3802.23s
                               ETA: 1007440.1s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.422s, learning 0.298s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 2.20
               Mean episode length: 58.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 8.72s
                        Total time: 3810.95s
                               ETA: 1007062.0s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.529s, learning 0.189s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0413
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 58.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 8.72s
                        Total time: 3819.67s
                               ETA: 1006685.3s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.222s, learning 0.292s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0388
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 57.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 8.51s
                        Total time: 3828.18s
                               ETA: 1006256.9s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.099s, learning 0.217s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0341
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 59.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 8.32s
                        Total time: 3836.50s
                               ETA: 1005778.9s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.497s, learning 0.175s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 60.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 8.67s
                        Total time: 3845.17s
                               ETA: 1005396.2s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.281s, learning 0.197s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 58.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.48s
                        Total time: 3853.65s
                               ETA: 1004964.9s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.610s, learning 0.202s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 58.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 8.81s
                        Total time: 3862.46s
                               ETA: 1004622.9s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.550s, learning 0.184s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0398
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 60.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 8.73s
                        Total time: 3871.19s
                               ETA: 1004262.4s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.216s, learning 0.178s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0373
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 60.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.39s
                        Total time: 3879.59s
                               ETA: 1003815.7s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.523s, learning 0.180s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0423
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 60.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 8.70s
                        Total time: 3888.29s
                               ETA: 1003451.0s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.611s, learning 0.289s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.35
               Mean episode length: 62.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.90s
                        Total time: 3897.19s
                               ETA: 1003138.9s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.594s, learning 0.173s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.46
               Mean episode length: 62.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.77s
                        Total time: 3905.96s
                               ETA: 1002794.3s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.695s, learning 0.310s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 60.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 9.01s
                        Total time: 3914.96s
                               ETA: 1002512.4s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.316s, learning 0.174s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 58.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 8.49s
                        Total time: 3923.45s
                               ETA: 1002100.3s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.325s, learning 0.180s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 2.25
               Mean episode length: 57.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.51s
                        Total time: 3931.96s
                               ETA: 1001694.3s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.198s, learning 0.205s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0417
             Mean action noise std: 0.76
                       Mean reward: 2.37
               Mean episode length: 59.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.40s
                        Total time: 3940.36s
                               ETA: 1001264.3s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.401s, learning 0.158s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0398
             Mean action noise std: 0.76
                       Mean reward: 2.39
               Mean episode length: 61.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.56s
                        Total time: 3948.92s
                               ETA: 1000875.8s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.169s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 55.62
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 8.55s
                        Total time: 3957.47s
                               ETA: 1000487.8s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.553s, learning 0.170s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0413
             Mean action noise std: 0.76
                       Mean reward: 2.38
               Mean episode length: 61.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.72s
                        Total time: 3966.20s
                               ETA: 1000144.7s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.466s, learning 0.241s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 2.35
               Mean episode length: 58.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.71s
                        Total time: 3974.91s
                               ETA: 999799.3s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.720s, learning 0.193s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 56.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.91s
                        Total time: 3983.82s
                               ETA: 999507.1s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.717s, learning 0.195s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0427
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 58.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.91s
                        Total time: 3992.73s
                               ETA: 999216.2s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.637s, learning 0.335s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0409
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 61.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.97s
                        Total time: 4001.70s
                               ETA: 998941.5s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.608s, learning 0.171s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0405
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 57.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.78s
                        Total time: 4010.48s
                               ETA: 998620.1s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.497s, learning 0.202s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 58.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.70s
                        Total time: 4019.18s
                               ETA: 998280.5s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.483s, learning 0.158s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0411
             Mean action noise std: 0.76
                       Mean reward: 2.25
               Mean episode length: 57.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.64s
                        Total time: 4027.82s
                               ETA: 997927.9s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.322s, learning 0.177s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 2.20
               Mean episode length: 57.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.50s
                        Total time: 4036.32s
                               ETA: 997542.1s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.675s, learning 0.202s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0410
             Mean action noise std: 0.76
                       Mean reward: 2.33
               Mean episode length: 59.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.88s
                        Total time: 4045.20s
                               ETA: 997251.4s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.388s, learning 0.296s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 2.30
               Mean episode length: 60.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.68s
                        Total time: 4053.88s
                               ETA: 996914.6s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.706s, learning 0.268s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 2.38
               Mean episode length: 59.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.97s
                        Total time: 4062.86s
                               ETA: 996650.6s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.577s, learning 0.162s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 2.32
               Mean episode length: 59.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.74s
                        Total time: 4071.60s
                               ETA: 996330.3s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.628s, learning 0.175s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0388
             Mean action noise std: 0.76
                       Mean reward: 2.29
               Mean episode length: 57.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.80s
                        Total time: 4080.40s
                               ETA: 996027.2s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.458s, learning 0.318s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0402
             Mean action noise std: 0.76
                       Mean reward: 2.35
               Mean episode length: 58.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.78s
                        Total time: 4089.17s
                               ETA: 995718.9s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.648s, learning 0.191s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 2.31
               Mean episode length: 59.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.84s
                        Total time: 4098.01s
                               ETA: 995427.3s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.431s, learning 0.164s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 56.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.59s
                        Total time: 4106.61s
                               ETA: 995078.0s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.399s, learning 0.194s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 2.31
               Mean episode length: 57.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.59s
                        Total time: 4115.20s
                               ETA: 994729.7s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.543s, learning 0.315s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0396
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 57.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.86s
                        Total time: 4124.06s
                               ETA: 994447.3s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.380s, learning 0.292s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 2.22
               Mean episode length: 56.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.67s
                        Total time: 4132.73s
                               ETA: 994121.4s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.809s, learning 0.224s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 56.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 9.03s
                        Total time: 4141.76s
                               ETA: 993883.8s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.696s, learning 0.195s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0418
             Mean action noise std: 0.76
                       Mean reward: 2.28
               Mean episode length: 58.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.89s
                        Total time: 4150.66s
                               ETA: 993613.0s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.410s, learning 0.185s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 2.17
               Mean episode length: 53.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.59s
                        Total time: 4159.25s
                               ETA: 993272.7s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.343s, learning 0.165s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 2.34
               Mean episode length: 58.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.51s
                        Total time: 4167.76s
                               ETA: 992913.4s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.707s, learning 0.199s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 55.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.91s
                        Total time: 4176.66s
                               ETA: 992650.2s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.596s, learning 0.170s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 2.23
               Mean episode length: 55.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.77s
                        Total time: 4185.43s
                               ETA: 992355.2s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.495s, learning 0.181s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0391
             Mean action noise std: 0.76
                       Mean reward: 2.24
               Mean episode length: 55.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.68s
                        Total time: 4194.11s
                               ETA: 992040.4s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.650s, learning 0.183s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 58.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.83s
                        Total time: 4202.94s
                               ETA: 991764.0s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.435s, learning 0.192s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0366
             Mean action noise std: 0.75
                       Mean reward: 2.30
               Mean episode length: 56.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 8.63s
                        Total time: 4211.57s
                               ETA: 991440.5s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.685s, learning 0.232s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0407
             Mean action noise std: 0.75
                       Mean reward: 2.17
               Mean episode length: 55.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.92s
                        Total time: 4220.48s
                               ETA: 991186.5s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.411s, learning 0.160s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 56.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.57s
                        Total time: 4229.05s
                               ETA: 990852.5s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.545s, learning 0.189s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.31
               Mean episode length: 58.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 8.73s
                        Total time: 4237.79s
                               ETA: 990558.4s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.340s, learning 0.174s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 2.22
               Mean episode length: 54.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.51s
                        Total time: 4246.30s
                               ETA: 990213.9s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.504s, learning 0.338s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0363
             Mean action noise std: 0.75
                       Mean reward: 2.16
               Mean episode length: 55.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.84s
                        Total time: 4255.15s
                               ETA: 989947.6s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.574s, learning 0.174s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 57.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.75s
                        Total time: 4263.89s
                               ETA: 989660.7s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.480s, learning 0.356s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0403
             Mean action noise std: 0.75
                       Mean reward: 2.16
               Mean episode length: 53.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.84s
                        Total time: 4272.73s
                               ETA: 989395.1s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.198s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 2.24
               Mean episode length: 56.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.64s
                        Total time: 4281.37s
                               ETA: 989085.2s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.650s, learning 0.180s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0402
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 57.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.83s
                        Total time: 4290.20s
                               ETA: 988820.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.308s, learning 0.333s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0395
             Mean action noise std: 0.75
                       Mean reward: 2.17
               Mean episode length: 54.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.64s
                        Total time: 4298.84s
                               ETA: 988514.2s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.383s, learning 0.190s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 2.24
               Mean episode length: 56.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.57s
                        Total time: 4307.41s
                               ETA: 988193.3s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.333s, learning 0.192s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0414
             Mean action noise std: 0.75
                       Mean reward: 2.22
               Mean episode length: 54.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.52s
                        Total time: 4315.93s
                               ETA: 987862.9s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.592s, learning 0.174s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0425
             Mean action noise std: 0.75
                       Mean reward: 2.17
               Mean episode length: 51.62
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.77s
                        Total time: 4324.70s
                               ETA: 987589.1s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.306s, learning 0.185s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0412
             Mean action noise std: 0.75
                       Mean reward: 2.14
               Mean episode length: 51.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.49s
                        Total time: 4333.19s
                               ETA: 987253.6s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.299s, learning 0.171s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 54.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.47s
                        Total time: 4341.66s
                               ETA: 986915.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.248s, learning 0.175s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0431
             Mean action noise std: 0.75
                       Mean reward: 2.30
               Mean episode length: 55.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.42s
                        Total time: 4350.08s
                               ETA: 986567.3s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.654s, learning 0.193s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0427
             Mean action noise std: 0.75
                       Mean reward: 2.21
               Mean episode length: 52.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.85s
                        Total time: 4358.93s
                               ETA: 986317.2s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.446s, learning 0.220s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0399
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 54.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.67s
                        Total time: 4367.60s
                               ETA: 986027.2s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.587s, learning 0.196s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 55.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.78s
                        Total time: 4376.38s
                               ETA: 985765.0s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.450s, learning 0.185s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0426
             Mean action noise std: 0.75
                       Mean reward: 2.20
               Mean episode length: 53.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.64s
                        Total time: 4385.02s
                               ETA: 985470.5s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.366s, learning 0.229s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.23
               Mean episode length: 52.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.59s
                        Total time: 4393.61s
                               ETA: 985168.2s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.169s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 54.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.53s
                        Total time: 4402.14s
                               ETA: 984852.6s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.324s, learning 0.183s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.12
               Mean episode length: 53.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.51s
                        Total time: 4410.65s
                               ETA: 984533.4s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.022s, learning 0.230s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 2.19
               Mean episode length: 52.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.25s
                        Total time: 4418.90s
                               ETA: 984158.9s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.157s, learning 0.196s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 56.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.35s
                        Total time: 4427.25s
                               ETA: 983808.4s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.592s, learning 0.170s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 56.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.76s
                        Total time: 4436.01s
                               ETA: 983550.3s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.420s, learning 0.295s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0342
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 55.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.72s
                        Total time: 4444.73s
                               ETA: 983282.8s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.604s, learning 0.270s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0362
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 55.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.87s
                        Total time: 4453.60s
                               ETA: 983051.7s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.270s, learning 0.179s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 2.25
               Mean episode length: 53.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 8.45s
                        Total time: 4462.05s
                               ETA: 982727.7s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.166s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0404
             Mean action noise std: 0.75
                       Mean reward: 2.16
               Mean episode length: 54.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.67s
                        Total time: 4470.73s
                               ETA: 982454.3s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.621s, learning 0.160s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 2.20
               Mean episode length: 52.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 8.78s
                        Total time: 4479.51s
                               ETA: 982206.0s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.440s, learning 0.321s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0396
             Mean action noise std: 0.75
                       Mean reward: 2.41
               Mean episode length: 56.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.76s
                        Total time: 4488.27s
                               ETA: 981954.2s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1807 steps/s (collection: 8.888s, learning 0.178s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 2.30
               Mean episode length: 56.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 9.07s
                        Total time: 4497.33s
                               ETA: 981770.0s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.776s, learning 0.164s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0405
             Mean action noise std: 0.75
                       Mean reward: 2.25
               Mean episode length: 55.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.94s
                        Total time: 4506.27s
                               ETA: 981559.3s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.668s, learning 0.170s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0399
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 53.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 8.84s
                        Total time: 4515.11s
                               ETA: 981327.3s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.750s, learning 0.264s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0336
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 54.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 9.01s
                        Total time: 4524.13s
                               ETA: 981134.2s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.442s, learning 0.346s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.25
               Mean episode length: 52.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 8.79s
                        Total time: 4532.91s
                               ETA: 980893.1s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.612s, learning 0.170s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 2.38
               Mean episode length: 56.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.78s
                        Total time: 4541.70s
                               ETA: 980651.6s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.503s, learning 0.260s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0411
             Mean action noise std: 0.75
                       Mean reward: 2.30
               Mean episode length: 53.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 8.76s
                        Total time: 4550.46s
                               ETA: 980407.1s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.819s, learning 0.260s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 2.30
               Mean episode length: 54.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 9.08s
                        Total time: 4559.54s
                               ETA: 980231.7s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.442s, learning 0.260s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0409
             Mean action noise std: 0.75
                       Mean reward: 2.24
               Mean episode length: 52.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.70s
                        Total time: 4568.24s
                               ETA: 979976.1s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.519s, learning 0.271s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 2.25
               Mean episode length: 53.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.79s
                        Total time: 4577.03s
                               ETA: 979740.3s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.800s, learning 0.244s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 2.33
               Mean episode length: 56.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 9.04s
                        Total time: 4586.07s
                               ETA: 979559.8s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.754s, learning 0.191s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0389
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 55.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.94s
                        Total time: 4595.02s
                               ETA: 979358.8s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.667s, learning 0.266s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0378
             Mean action noise std: 0.75
                       Mean reward: 2.33
               Mean episode length: 54.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 8.93s
                        Total time: 4603.95s
                               ETA: 979156.1s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.682s, learning 0.352s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0396
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 53.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 9.03s
                        Total time: 4612.98s
                               ETA: 978975.7s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.428s, learning 0.292s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0402
             Mean action noise std: 0.75
                       Mean reward: 2.38
               Mean episode length: 54.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 8.72s
                        Total time: 4621.71s
                               ETA: 978729.7s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.569s, learning 0.297s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0409
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 52.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 8.87s
                        Total time: 4630.57s
                               ETA: 978515.5s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.838s, learning 0.258s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0361
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 53.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 9.10s
                        Total time: 4639.67s
                               ETA: 978350.5s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.543s, learning 0.185s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0366
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 53.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 8.73s
                        Total time: 4648.39s
                               ETA: 978108.7s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.661s, learning 0.373s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 2.37
               Mean episode length: 56.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 9.03s
                        Total time: 4657.43s
                               ETA: 977932.5s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.518s, learning 0.256s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 2.33
               Mean episode length: 53.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 8.77s
                        Total time: 4666.20s
                               ETA: 977702.1s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.548s, learning 0.260s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 55.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 8.81s
                        Total time: 4675.01s
                               ETA: 977479.9s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.436s, learning 0.198s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 2.40
               Mean episode length: 56.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 8.63s
                        Total time: 4683.65s
                               ETA: 977222.5s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.785s, learning 0.265s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0392
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 55.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 9.05s
                        Total time: 4692.70s
                               ETA: 977052.5s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.416s, learning 0.163s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0373
             Mean action noise std: 0.75
                       Mean reward: 2.41
               Mean episode length: 56.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 8.58s
                        Total time: 4701.27s
                               ETA: 976785.4s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.831s, learning 0.189s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 2.40
               Mean episode length: 54.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 9.02s
                        Total time: 4710.29s
                               ETA: 976610.8s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.159s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 2.34
               Mean episode length: 54.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 8.62s
                        Total time: 4718.91s
                               ETA: 976354.1s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.217s, learning 0.316s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 56.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 8.53s
                        Total time: 4727.45s
                               ETA: 976080.6s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.464s, learning 0.159s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0373
             Mean action noise std: 0.75
                       Mean reward: 2.43
               Mean episode length: 56.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 8.62s
                        Total time: 4736.07s
                               ETA: 975826.7s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1647 steps/s (collection: 9.677s, learning 0.265s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 2.39
               Mean episode length: 54.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 9.94s
                        Total time: 4746.01s
                               ETA: 975844.9s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.726s, learning 0.177s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0387
             Mean action noise std: 0.75
                       Mean reward: 2.19
               Mean episode length: 50.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 16.90s
                        Total time: 4762.92s
                               ETA: 977291.5s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.643s, learning 0.168s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0384
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 52.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 16.81s
                        Total time: 4779.73s
                               ETA: 978713.1s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.647s, learning 0.278s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0393
             Mean action noise std: 0.75
                       Mean reward: 2.33
               Mean episode length: 53.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 16.93s
                        Total time: 4796.65s
                               ETA: 980152.1s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.605s, learning 0.282s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 2.41
               Mean episode length: 54.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 16.89s
                        Total time: 4813.54s
                               ETA: 981577.3s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.302s, learning 0.158s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0412
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 51.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 16.46s
                        Total time: 4830.00s
                               ETA: 982909.6s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.551s, learning 0.161s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 51.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 16.71s
                        Total time: 4846.71s
                               ETA: 984287.8s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.760s, learning 0.171s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0395
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 53.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 16.93s
                        Total time: 4863.64s
                               ETA: 985704.6s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.859s, learning 0.164s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 2.33
               Mean episode length: 51.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 17.02s
                        Total time: 4880.66s
                               ETA: 987134.3s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.587s, learning 0.274s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 52.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 16.86s
                        Total time: 4897.53s
                               ETA: 988525.4s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.638s, learning 0.164s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 50.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 16.80s
                        Total time: 4914.33s
                               ETA: 989898.9s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.114s, learning 0.258s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 2.23
               Mean episode length: 50.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 17.37s
                        Total time: 4931.70s
                               ETA: 991381.4s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.271s, learning 0.172s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 53.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 17.44s
                        Total time: 4949.14s
                               ETA: 992872.1s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.271s, learning 0.169s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 2.25
               Mean episode length: 50.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 17.44s
                        Total time: 4966.58s
                               ETA: 994356.1s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.590s, learning 0.167s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 52.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 16.76s
                        Total time: 4983.34s
                               ETA: 995697.6s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 950 steps/s (collection: 17.080s, learning 0.166s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 51.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 17.25s
                        Total time: 5000.59s
                               ETA: 997131.1s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.762s, learning 0.169s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 2.43
               Mean episode length: 55.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 16.93s
                        Total time: 5017.52s
                               ETA: 998496.1s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.920s, learning 0.199s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 2.19
               Mean episode length: 49.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 17.12s
                        Total time: 5034.64s
                               ETA: 999892.8s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.692s, learning 0.159s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0347
             Mean action noise std: 0.75
                       Mean reward: 2.34
               Mean episode length: 52.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 16.85s
                        Total time: 5051.49s
                               ETA: 1001230.8s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.728s, learning 0.170s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0392
             Mean action noise std: 0.75
                       Mean reward: 2.34
               Mean episode length: 52.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 16.90s
                        Total time: 5068.38s
                               ETA: 1002572.8s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.274s, learning 0.167s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 51.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 17.44s
                        Total time: 5085.82s
                               ETA: 1004016.5s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.713s, learning 0.180s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 2.38
               Mean episode length: 52.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 16.89s
                        Total time: 5102.72s
                               ETA: 1005346.6s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.524s, learning 0.315s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 50.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 16.84s
                        Total time: 5119.56s
                               ETA: 1006660.7s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.915s, learning 0.168s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 2.50
               Mean episode length: 52.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 17.08s
                        Total time: 5136.64s
                               ETA: 1008017.4s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.380s, learning 0.224s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0371
             Mean action noise std: 0.75
                       Mean reward: 2.38
               Mean episode length: 54.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 16.60s
                        Total time: 5153.24s
                               ETA: 1009274.9s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.672s, learning 0.162s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 52.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 16.83s
                        Total time: 5170.08s
                               ETA: 1010572.3s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.120s, learning 0.159s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0367
             Mean action noise std: 0.75
                       Mean reward: 2.26
               Mean episode length: 51.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 17.28s
                        Total time: 5187.36s
                               ETA: 1011951.3s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.698s, learning 0.175s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 2.23
               Mean episode length: 48.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 16.87s
                        Total time: 5204.23s
                               ETA: 1013245.9s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 949 steps/s (collection: 17.092s, learning 0.164s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 53.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 17.26s
                        Total time: 5221.48s
                               ETA: 1014609.9s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.556s, learning 0.177s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 2.37
               Mean episode length: 51.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 16.73s
                        Total time: 5238.22s
                               ETA: 1015866.9s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.643s, learning 0.167s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0361
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 48.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 16.81s
                        Total time: 5255.03s
                               ETA: 1017134.1s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.648s, learning 0.329s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0393
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 50.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 16.98s
                        Total time: 5272.00s
                               ETA: 1018428.5s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 964 steps/s (collection: 16.779s, learning 0.205s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0399
             Mean action noise std: 0.75
                       Mean reward: 2.45
               Mean episode length: 54.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 16.98s
                        Total time: 5288.99s
                               ETA: 1019719.1s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.637s, learning 0.186s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 2.38
               Mean episode length: 53.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 16.82s
                        Total time: 5305.81s
                               ETA: 1020973.6s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.565s, learning 0.166s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0389
             Mean action noise std: 0.75
                       Mean reward: 2.41
               Mean episode length: 52.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 16.73s
                        Total time: 5322.54s
                               ETA: 1022205.7s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.852s, learning 0.166s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0407
             Mean action noise std: 0.75
                       Mean reward: 2.37
               Mean episode length: 50.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 17.02s
                        Total time: 5339.56s
                               ETA: 1023487.8s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.714s, learning 0.205s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 50.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 16.92s
                        Total time: 5356.48s
                               ETA: 1024746.0s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.270s, learning 0.172s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 2.29
               Mean episode length: 51.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 17.44s
                        Total time: 5373.92s
                               ETA: 1026099.4s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.645s, learning 0.203s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0384
             Mean action noise std: 0.75
                       Mean reward: 2.22
               Mean episode length: 51.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 12.85s
                        Total time: 5386.77s
                               ETA: 1026572.0s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.773s, learning 0.253s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 2.36
               Mean episode length: 51.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 9.03s
                        Total time: 5395.80s
                               ETA: 1026315.6s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.627s, learning 0.176s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0390
             Mean action noise std: 0.75
                       Mean reward: 2.34
               Mean episode length: 50.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 8.80s
                        Total time: 5404.60s
                               ETA: 1026017.8s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.476s, learning 0.171s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 49.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.65s
                        Total time: 5413.25s
                               ETA: 1025691.7s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.417s, learning 0.164s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0365
             Mean action noise std: 0.75
                       Mean reward: 2.34
               Mean episode length: 52.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.58s
                        Total time: 5421.83s
                               ETA: 1025354.1s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.674s, learning 0.241s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0362
             Mean action noise std: 0.75
                       Mean reward: 2.37
               Mean episode length: 52.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 8.91s
                        Total time: 5430.74s
                               ETA: 1025080.8s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.535s, learning 0.176s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0378
             Mean action noise std: 0.75
                       Mean reward: 2.44
               Mean episode length: 53.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.71s
                        Total time: 5439.45s
                               ETA: 1024770.2s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.505s, learning 0.240s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0373
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 50.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.75s
                        Total time: 5448.20s
                               ETA: 1024467.2s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.753s, learning 0.160s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 2.37
               Mean episode length: 52.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 8.91s
                        Total time: 5457.11s
                               ETA: 1024196.7s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.146s, learning 0.284s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 49.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.43s
                        Total time: 5465.54s
                               ETA: 1023836.9s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.665s, learning 0.158s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 2.49
               Mean episode length: 52.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.82s
                        Total time: 5474.36s
                               ETA: 1023551.8s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.330s, learning 0.168s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0391
             Mean action noise std: 0.75
                       Mean reward: 2.23
               Mean episode length: 48.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 8.50s
                        Total time: 5482.86s
                               ETA: 1023207.0s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.639s, learning 0.160s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 49.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.80s
                        Total time: 5491.66s
                               ETA: 1022919.7s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.319s, learning 0.162s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0378
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 48.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 8.48s
                        Total time: 5500.14s
                               ETA: 1022574.1s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.472s, learning 0.212s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0368
             Mean action noise std: 0.75
                       Mean reward: 2.28
               Mean episode length: 48.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 8.68s
                        Total time: 5508.83s
                               ETA: 1022267.4s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.512s, learning 0.176s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0398
             Mean action noise std: 0.75
                       Mean reward: 2.32
               Mean episode length: 49.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.69s
                        Total time: 5517.51s
                               ETA: 1021962.7s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.754s, learning 0.179s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 49.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 8.93s
                        Total time: 5526.45s
                               ETA: 1021704.3s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.619s, learning 0.164s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 2.33
               Mean episode length: 49.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.78s
                        Total time: 5535.23s
                               ETA: 1021419.4s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.760s, learning 0.191s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0345
             Mean action noise std: 0.74
                       Mean reward: 2.30
               Mean episode length: 49.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 8.95s
                        Total time: 5544.18s
                               ETA: 1021166.2s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.857s, learning 0.163s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0377
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 52.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 9.02s
                        Total time: 5553.20s
                               ETA: 1020926.6s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.658s, learning 0.185s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 2.29
               Mean episode length: 49.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.84s
                        Total time: 5562.04s
                               ETA: 1020655.4s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.192s, learning 0.247s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.30
               Mean episode length: 48.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.44s
                        Total time: 5570.48s
                               ETA: 1020311.2s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.161s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0354
             Mean action noise std: 0.74
                       Mean reward: 2.33
               Mean episode length: 49.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.34s
                        Total time: 5578.82s
                               ETA: 1019949.4s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.178s, learning 0.160s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 2.32
               Mean episode length: 50.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.34s
                        Total time: 5587.16s
                               ETA: 1019589.3s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.469s, learning 0.157s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0378
             Mean action noise std: 0.74
                       Mean reward: 2.27
               Mean episode length: 49.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.63s
                        Total time: 5595.78s
                               ETA: 1019282.9s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.040s, learning 0.187s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0373
             Mean action noise std: 0.74
                       Mean reward: 2.24
               Mean episode length: 47.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.23s
                        Total time: 5604.01s
                               ETA: 1018905.2s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.732s, learning 0.163s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 2.32
               Mean episode length: 49.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.90s
                        Total time: 5612.90s
                               ETA: 1018650.0s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.504s, learning 0.256s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 2.40
               Mean episode length: 50.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.76s
                        Total time: 5621.67s
                               ETA: 1018371.3s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.610s, learning 0.311s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 2.40
               Mean episode length: 50.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.92s
                        Total time: 5630.59s
                               ETA: 1018122.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.727s, learning 0.262s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 2.46
               Mean episode length: 50.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.99s
                        Total time: 5639.57s
                               ETA: 1017886.9s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.565s, learning 0.239s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 49.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.80s
                        Total time: 5648.38s
                               ETA: 1017618.7s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.640s, learning 0.230s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0384
             Mean action noise std: 0.74
                       Mean reward: 2.27
               Mean episode length: 48.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 8.87s
                        Total time: 5657.25s
                               ETA: 1017363.4s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1772 steps/s (collection: 8.994s, learning 0.251s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0373
             Mean action noise std: 0.74
                       Mean reward: 2.25
               Mean episode length: 49.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 9.24s
                        Total time: 5666.49s
                               ETA: 1017176.3s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.421s, learning 0.218s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 2.28
               Mean episode length: 47.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 8.64s
                        Total time: 5675.13s
                               ETA: 1016881.3s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.591s, learning 0.362s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 2.34
               Mean episode length: 49.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.95s
                        Total time: 5684.08s
                               ETA: 1016643.4s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.596s, learning 0.258s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0387
             Mean action noise std: 0.74
                       Mean reward: 2.34
               Mean episode length: 48.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.85s
                        Total time: 5692.94s
                               ETA: 1016388.8s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.697s, learning 0.217s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0355
             Mean action noise std: 0.74
                       Mean reward: 2.39
               Mean episode length: 49.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.91s
                        Total time: 5701.85s
                               ETA: 1016145.8s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.526s, learning 0.263s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0362
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 49.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 8.79s
                        Total time: 5710.64s
                               ETA: 1015881.3s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.515s, learning 0.180s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0371
             Mean action noise std: 0.74
                       Mean reward: 2.27
               Mean episode length: 48.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 8.70s
                        Total time: 5719.34s
                               ETA: 1015601.0s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.761s, learning 0.260s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 2.25
               Mean episode length: 49.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 9.02s
                        Total time: 5728.36s
                               ETA: 1015379.4s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.935s, learning 0.160s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 49.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 9.10s
                        Total time: 5737.45s
                               ETA: 1015171.9s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.551s, learning 0.414s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0347
             Mean action noise std: 0.74
                       Mean reward: 2.47
               Mean episode length: 50.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.96s
                        Total time: 5746.42s
                               ETA: 1014941.8s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.201s, learning 0.238s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0341
             Mean action noise std: 0.74
                       Mean reward: 2.29
               Mean episode length: 50.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.44s
                        Total time: 5754.86s
                               ETA: 1014619.8s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.550s, learning 0.369s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0371
             Mean action noise std: 0.74
                       Mean reward: 2.34
               Mean episode length: 49.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.92s
                        Total time: 5763.78s
                               ETA: 1014383.6s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.592s, learning 0.260s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.29
               Mean episode length: 48.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.85s
                        Total time: 5772.63s
                               ETA: 1014136.2s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.881s, learning 0.237s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 2.42
               Mean episode length: 50.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 9.12s
                        Total time: 5781.74s
                               ETA: 1013936.4s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.481s, learning 0.259s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0382
             Mean action noise std: 0.74
                       Mean reward: 2.22
               Mean episode length: 48.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.74s
                        Total time: 5790.48s
                               ETA: 1013671.2s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.735s, learning 0.264s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 50.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 9.00s
                        Total time: 5799.48s
                               ETA: 1013452.0s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.549s, learning 0.192s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0364
             Mean action noise std: 0.74
                       Mean reward: 2.46
               Mean episode length: 52.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.74s
                        Total time: 5808.22s
                               ETA: 1013188.5s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.530s, learning 0.234s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 49.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.76s
                        Total time: 5816.99s
                               ETA: 1012930.0s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.542s, learning 0.163s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0368
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 49.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.71s
                        Total time: 5825.69s
                               ETA: 1012662.2s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.374s, learning 0.233s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0382
             Mean action noise std: 0.74
                       Mean reward: 2.46
               Mean episode length: 52.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.61s
                        Total time: 5834.30s
                               ETA: 1012378.2s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.497s, learning 0.331s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 48.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.83s
                        Total time: 5843.13s
                               ETA: 1012133.5s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.062s, learning 0.161s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0355
             Mean action noise std: 0.74
                       Mean reward: 2.30
               Mean episode length: 47.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.22s
                        Total time: 5851.35s
                               ETA: 1011784.9s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.608s, learning 0.166s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.40
               Mean episode length: 49.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.77s
                        Total time: 5860.12s
                               ETA: 1011532.7s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.471s, learning 0.171s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 47.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.64s
                        Total time: 5868.77s
                               ETA: 1011258.4s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.598s, learning 0.297s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0381
             Mean action noise std: 0.74
                       Mean reward: 2.39
               Mean episode length: 49.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.90s
                        Total time: 5877.66s
                               ETA: 1011028.9s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.161s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 51.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.57s
                        Total time: 5886.24s
                               ETA: 1010744.8s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.494s, learning 0.159s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 49.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.65s
                        Total time: 5894.89s
                               ETA: 1010475.4s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.591s, learning 0.158s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0345
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 50.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.75s
                        Total time: 5903.64s
                               ETA: 1010223.1s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.673s, learning 0.263s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 2.33
               Mean episode length: 47.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.94s
                        Total time: 5912.57s
                               ETA: 1010003.6s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.668s, learning 0.242s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 49.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.91s
                        Total time: 5921.48s
                               ETA: 1009780.6s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1821 steps/s (collection: 8.732s, learning 0.262s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 2.42
               Mean episode length: 47.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.99s
                        Total time: 5930.48s
                               ETA: 1009572.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.683s, learning 0.257s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0365
             Mean action noise std: 0.74
                       Mean reward: 2.42
               Mean episode length: 49.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.94s
                        Total time: 5939.42s
                               ETA: 1009355.7s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.594s, learning 0.198s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 49.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.79s
                        Total time: 5948.21s
                               ETA: 1009114.6s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.392s, learning 0.189s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0361
             Mean action noise std: 0.74
                       Mean reward: 2.39
               Mean episode length: 49.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.58s
                        Total time: 5956.79s
                               ETA: 1008838.7s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.458s, learning 0.245s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 2.49
               Mean episode length: 49.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.70s
                        Total time: 5965.49s
                               ETA: 1008584.2s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.669s, learning 0.284s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0360
             Mean action noise std: 0.74
                       Mean reward: 2.38
               Mean episode length: 47.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.95s
                        Total time: 5974.45s
                               ETA: 1008372.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.511s, learning 0.253s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0416
             Mean action noise std: 0.74
                       Mean reward: 2.32
               Mean episode length: 46.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.76s
                        Total time: 5983.21s
                               ETA: 1008130.2s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.513s, learning 0.363s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 49.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.88s
                        Total time: 5992.09s
                               ETA: 1007907.3s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1776 steps/s (collection: 8.820s, learning 0.404s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 2.38
               Mean episode length: 47.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 9.22s
                        Total time: 6001.31s
                               ETA: 1007743.6s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.276s, learning 0.159s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0336
             Mean action noise std: 0.74
                       Mean reward: 2.42
               Mean episode length: 47.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 8.44s
                        Total time: 6009.75s
                               ETA: 1007448.2s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.483s, learning 0.157s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0377
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 46.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.64s
                        Total time: 6018.39s
                               ETA: 1007188.0s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.560s, learning 0.184s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 46.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.74s
                        Total time: 6027.13s
                               ETA: 1006946.0s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.570s, learning 0.301s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 46.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 8.87s
                        Total time: 6036.00s
                               ETA: 1006726.0s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.264s, learning 0.158s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0378
             Mean action noise std: 0.74
                       Mean reward: 2.33
               Mean episode length: 44.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.42s
                        Total time: 6044.42s
                               ETA: 1006431.9s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.349s, learning 0.388s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0381
             Mean action noise std: 0.74
                       Mean reward: 2.46
               Mean episode length: 47.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.74s
                        Total time: 6053.16s
                               ETA: 1006191.0s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.219s, learning 0.164s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 2.27
               Mean episode length: 44.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.38s
                        Total time: 6061.54s
                               ETA: 1005892.3s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.536s, learning 0.335s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 2.30
               Mean episode length: 45.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.87s
                        Total time: 6070.41s
                               ETA: 1005675.3s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.286s, learning 0.186s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 45.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 8.47s
                        Total time: 6078.89s
                               ETA: 1005393.0s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.555s, learning 0.380s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 2.30
               Mean episode length: 45.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 8.93s
                        Total time: 6087.82s
                               ETA: 1005188.0s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.547s, learning 0.218s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.36
               Mean episode length: 45.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 8.77s
                        Total time: 6096.59s
                               ETA: 1004955.8s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.479s, learning 0.170s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 47.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.65s
                        Total time: 6105.23s
                               ETA: 1004705.1s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.408s, learning 0.237s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 2.33
               Mean episode length: 44.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 8.65s
                        Total time: 6113.88s
                               ETA: 1004454.7s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.643s, learning 0.172s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0384
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 45.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.81s
                        Total time: 6122.69s
                               ETA: 1004232.9s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.679s, learning 0.399s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 46.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 9.08s
                        Total time: 6131.77s
                               ETA: 1004054.9s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1802 steps/s (collection: 8.775s, learning 0.317s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0353
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 44.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 9.09s
                        Total time: 6140.86s
                               ETA: 1003879.8s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.254s, learning 0.269s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0411
             Mean action noise std: 0.74
                       Mean reward: 2.28
               Mean episode length: 43.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 8.52s
                        Total time: 6149.39s
                               ETA: 1003612.2s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1765 steps/s (collection: 8.911s, learning 0.372s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0343
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 46.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 9.28s
                        Total time: 6158.67s
                               ETA: 1003469.3s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.174s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0388
             Mean action noise std: 0.74
                       Mean reward: 2.39
               Mean episode length: 46.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 8.61s
                        Total time: 6167.28s
                               ETA: 1003217.2s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.578s, learning 0.202s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0383
             Mean action noise std: 0.74
                       Mean reward: 2.34
               Mean episode length: 45.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 8.78s
                        Total time: 6176.06s
                               ETA: 1002993.8s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.164s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0355
             Mean action noise std: 0.74
                       Mean reward: 2.36
               Mean episode length: 46.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 8.47s
                        Total time: 6184.53s
                               ETA: 1002721.3s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.513s, learning 0.195s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0370
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 46.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 8.71s
                        Total time: 6193.24s
                               ETA: 1002487.7s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.384s, learning 0.166s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0387
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 46.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 8.55s
                        Total time: 6201.79s
                               ETA: 1002229.1s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.683s, learning 0.265s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0369
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 45.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 8.95s
                        Total time: 6210.74s
                               ETA: 1002035.6s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1750 steps/s (collection: 9.025s, learning 0.335s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 2.26
               Mean episode length: 44.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 9.36s
                        Total time: 6220.10s
                               ETA: 1001909.2s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.461s, learning 0.299s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 45.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 8.76s
                        Total time: 6228.86s
                               ETA: 1001686.7s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.694s, learning 0.232s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0370
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 44.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 8.93s
                        Total time: 6237.78s
                               ETA: 1001491.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.302s, learning 0.158s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 46.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 8.46s
                        Total time: 6246.24s
                               ETA: 1001222.3s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1774 steps/s (collection: 8.968s, learning 0.266s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0356
             Mean action noise std: 0.74
                       Mean reward: 2.39
               Mean episode length: 45.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 9.23s
                        Total time: 6255.48s
                               ETA: 1001077.7s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.588s, learning 0.245s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 2.38
               Mean episode length: 44.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 8.83s
                        Total time: 6264.31s
                               ETA: 1000869.5s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.527s, learning 0.163s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0384
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 44.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 8.69s
                        Total time: 6273.00s
                               ETA: 1000639.0s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.407s, learning 0.268s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0361
             Mean action noise std: 0.74
                       Mean reward: 2.34
               Mean episode length: 42.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 8.67s
                        Total time: 6281.67s
                               ETA: 1000406.9s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1749 steps/s (collection: 9.062s, learning 0.304s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0352
             Mean action noise std: 0.74
                       Mean reward: 2.32
               Mean episode length: 44.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 9.37s
                        Total time: 6291.04s
                               ETA: 1000285.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.493s, learning 0.345s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 2.35
               Mean episode length: 44.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 8.84s
                        Total time: 6299.88s
                               ETA: 1000080.5s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.404s, learning 0.244s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0361
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 45.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 8.65s
                        Total time: 6308.53s
                               ETA: 999846.0s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.368s, learning 0.400s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0363
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 44.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 8.77s
                        Total time: 6317.29s
                               ETA: 999631.2s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.706s, learning 0.300s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 2.40
               Mean episode length: 44.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 9.01s
                        Total time: 6326.30s
                               ETA: 999454.7s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.753s, learning 0.303s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0374
             Mean action noise std: 0.74
                       Mean reward: 2.47
               Mean episode length: 44.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 9.06s
                        Total time: 6335.36s
                               ETA: 999286.6s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.497s, learning 0.161s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 2.47
               Mean episode length: 44.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.66s
                        Total time: 6344.01s
                               ETA: 999056.5s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.457s, learning 0.280s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0373
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 44.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.74s
                        Total time: 6352.75s
                               ETA: 998839.4s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.098s, learning 0.267s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0369
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 44.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.36s
                        Total time: 6361.12s
                               ETA: 998564.5s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.543s, learning 0.171s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0353
             Mean action noise std: 0.74
                       Mean reward: 2.42
               Mean episode length: 45.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 8.71s
                        Total time: 6369.83s
                               ETA: 998345.1s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.573s, learning 0.167s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0360
             Mean action noise std: 0.74
                       Mean reward: 2.49
               Mean episode length: 44.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.74s
                        Total time: 6378.57s
                               ETA: 998130.6s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.510s, learning 0.182s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 44.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.69s
                        Total time: 6387.26s
                               ETA: 997909.1s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.868s, learning 0.181s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.37
               Mean episode length: 44.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 9.05s
                        Total time: 6396.31s
                               ETA: 997744.0s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.187s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 2.47
               Mean episode length: 44.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.59s
                        Total time: 6404.90s
                               ETA: 997507.5s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.654s, learning 0.192s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0394
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 44.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 8.85s
                        Total time: 6413.74s
                               ETA: 997311.9s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.748s, learning 0.157s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0323
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 44.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.91s
                        Total time: 6422.65s
                               ETA: 997126.2s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.777s, learning 0.163s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.52
               Mean episode length: 44.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.94s
                        Total time: 6431.59s
                               ETA: 996946.4s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.311s, learning 0.262s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0348
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 43.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 8.57s
                        Total time: 6440.16s
                               ETA: 996710.4s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.683s, learning 0.228s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0321
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 43.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.91s
                        Total time: 6449.07s
                               ETA: 996527.2s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.363s, learning 0.172s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0367
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 43.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.54s
                        Total time: 6457.61s
                               ETA: 996286.7s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.582s, learning 0.309s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0371
             Mean action noise std: 0.74
                       Mean reward: 2.53
               Mean episode length: 44.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.89s
                        Total time: 6466.50s
                               ETA: 996101.5s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.663s, learning 0.202s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0381
             Mean action noise std: 0.74
                       Mean reward: 2.50
               Mean episode length: 43.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 8.87s
                        Total time: 6475.36s
                               ETA: 995913.1s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.561s, learning 0.271s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0346
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 42.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.83s
                        Total time: 6484.20s
                               ETA: 995720.1s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1781 steps/s (collection: 8.913s, learning 0.283s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0364
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 43.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 9.20s
                        Total time: 6493.39s
                               ETA: 995583.5s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.525s, learning 0.204s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 43.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.73s
                        Total time: 6502.12s
                               ETA: 995375.7s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.762s, learning 0.162s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 2.51
               Mean episode length: 42.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.92s
                        Total time: 6511.05s
                               ETA: 995198.4s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.684s, learning 0.172s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0398
             Mean action noise std: 0.74
                       Mean reward: 2.44
               Mean episode length: 42.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.86s
                        Total time: 6519.90s
                               ETA: 995011.1s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.198s, learning 0.233s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 43.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.43s
                        Total time: 6528.33s
                               ETA: 994759.6s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.426s, learning 0.220s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0321
             Mean action noise std: 0.74
                       Mean reward: 2.54
               Mean episode length: 43.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.65s
                        Total time: 6536.98s
                               ETA: 994541.7s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.563s, learning 0.227s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0338
             Mean action noise std: 0.74
                       Mean reward: 2.53
               Mean episode length: 42.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 8.79s
                        Total time: 6545.77s
                               ETA: 994346.3s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.434s, learning 0.290s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0376
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 42.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.72s
                        Total time: 6554.49s
                               ETA: 994141.5s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1756 steps/s (collection: 9.062s, learning 0.268s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0348
             Mean action noise std: 0.74
                       Mean reward: 2.53
               Mean episode length: 43.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 9.33s
                        Total time: 6563.82s
                               ETA: 994029.0s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.446s, learning 0.264s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0316
             Mean action noise std: 0.74
                       Mean reward: 2.52
               Mean episode length: 42.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 8.71s
                        Total time: 6572.53s
                               ETA: 993823.0s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.418s, learning 0.304s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 2.48
               Mean episode length: 42.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 8.72s
                        Total time: 6581.26s
                               ETA: 993619.6s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.747s, learning 0.169s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0384
             Mean action noise std: 0.74
                       Mean reward: 2.56
               Mean episode length: 42.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.92s
                        Total time: 6590.17s
                               ETA: 993446.0s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.632s, learning 0.267s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0358
             Mean action noise std: 0.74
                       Mean reward: 2.54
               Mean episode length: 42.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 8.90s
                        Total time: 6599.07s
                               ETA: 993270.2s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.620s, learning 0.256s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0391
             Mean action noise std: 0.74
                       Mean reward: 2.52
               Mean episode length: 42.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 8.88s
                        Total time: 6607.95s
                               ETA: 993091.5s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.750s, learning 0.272s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0358
             Mean action noise std: 0.74
                       Mean reward: 2.61
               Mean episode length: 43.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 9.02s
                        Total time: 6616.97s
                               ETA: 992935.3s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.483s, learning 0.260s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0373
             Mean action noise std: 0.74
                       Mean reward: 2.53
               Mean episode length: 42.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.74s
                        Total time: 6625.71s
                               ETA: 992737.7s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.761s, learning 0.289s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0370
             Mean action noise std: 0.74
                       Mean reward: 2.43
               Mean episode length: 42.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 9.05s
                        Total time: 6634.76s
                               ETA: 992586.5s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.931s, learning 0.164s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0354
             Mean action noise std: 0.74
                       Mean reward: 2.60
               Mean episode length: 42.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 9.10s
                        Total time: 6643.86s
                               ETA: 992442.5s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1595 steps/s (collection: 10.062s, learning 0.209s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.57
               Mean episode length: 41.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 10.27s
                        Total time: 6654.13s
                               ETA: 992474.3s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 958 steps/s (collection: 16.902s, learning 0.188s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0346
             Mean action noise std: 0.74
                       Mean reward: 2.59
               Mean episode length: 42.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 17.09s
                        Total time: 6671.22s
                               ETA: 993521.5s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 933 steps/s (collection: 17.398s, learning 0.161s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0339
             Mean action noise std: 0.74
                       Mean reward: 2.54
               Mean episode length: 42.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 17.56s
                        Total time: 6688.78s
                               ETA: 994635.2s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 935 steps/s (collection: 17.236s, learning 0.269s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0361
             Mean action noise std: 0.74
                       Mean reward: 2.62
               Mean episode length: 43.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 17.50s
                        Total time: 6706.28s
                               ETA: 995737.6s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 932 steps/s (collection: 17.390s, learning 0.185s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 2.45
               Mean episode length: 43.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 17.57s
                        Total time: 6723.86s
                               ETA: 996847.0s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 955 steps/s (collection: 16.981s, learning 0.166s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0359
             Mean action noise std: 0.74
                       Mean reward: 2.61
               Mean episode length: 43.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 17.15s
                        Total time: 6741.00s
                               ETA: 997889.6s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.796s, learning 0.161s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0373
             Mean action noise std: 0.74
                       Mean reward: 2.63
               Mean episode length: 43.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 16.96s
                        Total time: 6757.96s
                               ETA: 998901.1s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.567s, learning 0.198s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.62
               Mean episode length: 42.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 16.76s
                        Total time: 6774.73s
                               ETA: 999881.1s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 953 steps/s (collection: 16.850s, learning 0.326s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0368
             Mean action noise std: 0.74
                       Mean reward: 2.63
               Mean episode length: 43.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 17.18s
                        Total time: 6791.90s
                               ETA: 1000918.8s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 942 steps/s (collection: 17.228s, learning 0.163s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0388
             Mean action noise std: 0.74
                       Mean reward: 2.58
               Mean episode length: 42.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 17.39s
                        Total time: 6809.29s
                               ETA: 1001985.0s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 950 steps/s (collection: 17.065s, learning 0.163s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0362
             Mean action noise std: 0.74
                       Mean reward: 2.73
               Mean episode length: 43.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 17.23s
                        Total time: 6826.52s
                               ETA: 1003024.0s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.715s, learning 0.205s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0392
             Mean action noise std: 0.74
                       Mean reward: 2.63
               Mean episode length: 42.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 16.92s
                        Total time: 6843.44s
                               ETA: 1004014.9s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 952 steps/s (collection: 17.008s, learning 0.185s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.68
               Mean episode length: 42.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 17.19s
                        Total time: 6860.64s
                               ETA: 1005042.7s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 950 steps/s (collection: 16.964s, learning 0.272s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0359
             Mean action noise std: 0.74
                       Mean reward: 2.63
               Mean episode length: 42.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 17.24s
                        Total time: 6877.87s
                               ETA: 1006073.6s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.754s, learning 0.185s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0379
             Mean action noise std: 0.74
                       Mean reward: 2.55
               Mean episode length: 42.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 16.94s
                        Total time: 6894.81s
                               ETA: 1007058.1s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.096s, learning 0.170s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0351
             Mean action noise std: 0.74
                       Mean reward: 2.70
               Mean episode length: 43.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 17.27s
                        Total time: 6912.08s
                               ETA: 1008087.3s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.935s, learning 0.184s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 2.56
               Mean episode length: 42.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 17.12s
                        Total time: 6929.20s
                               ETA: 1009092.1s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 930 steps/s (collection: 17.409s, learning 0.201s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0365
             Mean action noise std: 0.74
                       Mean reward: 2.66
               Mean episode length: 42.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 17.61s
                        Total time: 6946.81s
                               ETA: 1010165.3s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.739s, learning 0.168s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0363
             Mean action noise std: 0.74
                       Mean reward: 2.64
               Mean episode length: 41.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 16.91s
                        Total time: 6963.71s
                               ETA: 1011133.2s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.907s, learning 0.161s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0324
             Mean action noise std: 0.74
                       Mean reward: 2.69
               Mean episode length: 42.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 17.07s
                        Total time: 6980.78s
                               ETA: 1012121.6s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.709s, learning 0.164s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0347
             Mean action noise std: 0.74
                       Mean reward: 2.70
               Mean episode length: 42.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 16.87s
                        Total time: 6997.65s
                               ETA: 1013078.7s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.505s, learning 0.256s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0358
             Mean action noise std: 0.74
                       Mean reward: 2.59
               Mean episode length: 41.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 16.76s
                        Total time: 7014.41s
                               ETA: 1014016.8s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.872s, learning 0.201s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0365
             Mean action noise std: 0.74
                       Mean reward: 2.61
               Mean episode length: 41.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 17.07s
                        Total time: 7031.49s
                               ETA: 1014997.3s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.568s, learning 0.159s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0322
             Mean action noise std: 0.74
                       Mean reward: 2.77
               Mean episode length: 41.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 16.73s
                        Total time: 7048.21s
                               ETA: 1015924.9s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 948 steps/s (collection: 16.938s, learning 0.342s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0293
             Mean action noise std: 0.74
                       Mean reward: 2.67
               Mean episode length: 41.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 17.28s
                        Total time: 7065.49s
                               ETA: 1016929.5s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.954s, learning 0.174s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0327
             Mean action noise std: 0.74
                       Mean reward: 2.69
               Mean episode length: 42.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 17.13s
                        Total time: 7082.62s
                               ETA: 1017909.3s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.620s, learning 0.215s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0369
             Mean action noise std: 0.74
                       Mean reward: 2.68
               Mean episode length: 41.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 16.84s
                        Total time: 7099.46s
                               ETA: 1018844.1s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.194s, learning 0.172s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0350
             Mean action noise std: 0.74
                       Mean reward: 2.75
               Mean episode length: 41.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 17.37s
                        Total time: 7116.82s
                               ETA: 1019852.2s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.765s, learning 0.186s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 2.68
               Mean episode length: 41.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 16.95s
                        Total time: 7133.78s
                               ETA: 1020798.0s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.766s, learning 0.169s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0360
             Mean action noise std: 0.74
                       Mean reward: 2.70
               Mean episode length: 42.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 16.94s
                        Total time: 7150.71s
                               ETA: 1021738.8s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.156s, learning 0.251s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0395
             Mean action noise std: 0.74
                       Mean reward: 2.71
               Mean episode length: 42.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 17.41s
                        Total time: 7168.12s
                               ETA: 1022744.1s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.545s, learning 0.165s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0349
             Mean action noise std: 0.74
                       Mean reward: 2.80
               Mean episode length: 42.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 16.71s
                        Total time: 7184.83s
                               ETA: 1023647.2s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.684s, learning 0.160s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0349
             Mean action noise std: 0.74
                       Mean reward: 2.93
               Mean episode length: 43.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 16.84s
                        Total time: 7201.67s
                               ETA: 1024566.8s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.609s, learning 0.255s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0359
             Mean action noise std: 0.74
                       Mean reward: 2.70
               Mean episode length: 42.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 16.86s
                        Total time: 7218.54s
                               ETA: 1025486.5s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.709s, learning 0.160s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0369
             Mean action noise std: 0.74
                       Mean reward: 2.76
               Mean episode length: 42.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 16.87s
                        Total time: 7235.41s
                               ETA: 1026404.3s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.958s, learning 0.168s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0387
             Mean action noise std: 0.74
                       Mean reward: 2.75
               Mean episode length: 42.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 17.13s
                        Total time: 7252.53s
                               ETA: 1027355.8s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.158s, learning 0.215s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0330
             Mean action noise std: 0.74
                       Mean reward: 2.73
               Mean episode length: 42.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 16.37s
                        Total time: 7268.91s
                               ETA: 1028198.0s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.826s, learning 0.170s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 2.82
               Mean episode length: 43.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 17.00s
                        Total time: 7285.90s
                               ETA: 1029125.8s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1233 steps/s (collection: 13.030s, learning 0.258s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0385
             Mean action noise std: 0.74
                       Mean reward: 2.85
               Mean episode length: 43.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 13.29s
                        Total time: 7299.19s
                               ETA: 1029527.8s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.849s, learning 0.170s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0378
             Mean action noise std: 0.74
                       Mean reward: 2.76
               Mean episode length: 42.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 9.02s
                        Total time: 7308.21s
                               ETA: 1029327.3s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.468s, learning 0.167s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0327
             Mean action noise std: 0.74
                       Mean reward: 2.82
               Mean episode length: 43.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.64s
                        Total time: 7316.84s
                               ETA: 1029073.5s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.317s, learning 0.181s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0387
             Mean action noise std: 0.74
                       Mean reward: 2.74
               Mean episode length: 42.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.50s
                        Total time: 7325.34s
                               ETA: 1028801.0s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.441s, learning 0.172s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 2.81
               Mean episode length: 43.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.61s
                        Total time: 7333.95s
                               ETA: 1028545.5s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.683s, learning 0.181s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 2.81
               Mean episode length: 43.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 8.86s
                        Total time: 7342.82s
                               ETA: 1028325.8s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.684s, learning 0.190s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0369
             Mean action noise std: 0.74
                       Mean reward: 2.72
               Mean episode length: 43.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.87s
                        Total time: 7351.69s
                               ETA: 1028108.1s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.373s, learning 0.165s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0351
             Mean action noise std: 0.74
                       Mean reward: 2.95
               Mean episode length: 43.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.54s
                        Total time: 7360.23s
                               ETA: 1027844.0s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.162s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0330
             Mean action noise std: 0.74
                       Mean reward: 2.83
               Mean episode length: 43.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 8.74s
                        Total time: 7368.97s
                               ETA: 1027608.6s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.109s, learning 0.168s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 2.77
               Mean episode length: 43.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 8.28s
                        Total time: 7377.24s
                               ETA: 1027309.6s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.559s, learning 0.214s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0327
             Mean action noise std: 0.74
                       Mean reward: 2.80
               Mean episode length: 43.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 8.77s
                        Total time: 7386.02s
                               ETA: 1027080.4s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.293s, learning 0.174s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0357
             Mean action noise std: 0.74
                       Mean reward: 2.80
               Mean episode length: 42.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 8.47s
                        Total time: 7394.48s
                               ETA: 1026809.3s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.243s, learning 0.227s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0288
             Mean action noise std: 0.74
                       Mean reward: 2.73
               Mean episode length: 42.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 8.47s
                        Total time: 7402.95s
                               ETA: 1026539.4s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.082s, learning 0.158s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0362
             Mean action noise std: 0.74
                       Mean reward: 2.76
               Mean episode length: 42.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 8.24s
                        Total time: 7411.19s
                               ETA: 1026238.4s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.497s, learning 0.178s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0340
             Mean action noise std: 0.73
                       Mean reward: 2.83
               Mean episode length: 42.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 8.67s
                        Total time: 7419.87s
                               ETA: 1025998.3s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.562s, learning 0.164s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0337
             Mean action noise std: 0.73
                       Mean reward: 2.78
               Mean episode length: 42.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 8.73s
                        Total time: 7428.59s
                               ETA: 1025766.0s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.686s, learning 0.161s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 2.95
               Mean episode length: 43.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 8.85s
                        Total time: 7437.44s
                               ETA: 1025550.8s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.390s, learning 0.182s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 2.80
               Mean episode length: 42.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 8.57s
                        Total time: 7446.01s
                               ETA: 1025298.4s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.472s, learning 0.164s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0344
             Mean action noise std: 0.73
                       Mean reward: 2.88
               Mean episode length: 42.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 8.64s
                        Total time: 7454.65s
                               ETA: 1025055.5s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.167s, learning 0.236s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0338
             Mean action noise std: 0.73
                       Mean reward: 2.83
               Mean episode length: 42.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 8.40s
                        Total time: 7463.05s
                               ETA: 1024781.2s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.373s, learning 0.173s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0342
             Mean action noise std: 0.73
                       Mean reward: 2.76
               Mean episode length: 42.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 8.55s
                        Total time: 7471.60s
                               ETA: 1024527.5s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.645s, learning 0.246s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0351
             Mean action noise std: 0.73
                       Mean reward: 2.79
               Mean episode length: 42.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 8.89s
                        Total time: 7480.49s
                               ETA: 1024321.4s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.597s, learning 0.203s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 43.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 8.80s
                        Total time: 7489.29s
                               ETA: 1024103.5s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.297s, learning 0.174s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 2.86
               Mean episode length: 42.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 8.47s
                        Total time: 7497.76s
                               ETA: 1023841.4s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.163s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 2.81
               Mean episode length: 42.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 8.57s
                        Total time: 7506.33s
                               ETA: 1023593.3s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.455s, learning 0.390s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 2.71
               Mean episode length: 42.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 8.84s
                        Total time: 7515.17s
                               ETA: 1023383.3s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.638s, learning 0.167s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 2.76
               Mean episode length: 42.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 8.81s
                        Total time: 7523.98s
                               ETA: 1023168.5s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.544s, learning 0.178s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 2.87
               Mean episode length: 42.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 8.72s
                        Total time: 7532.70s
                               ETA: 1022942.9s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.687s, learning 0.212s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 2.88
               Mean episode length: 42.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 8.90s
                        Total time: 7541.60s
                               ETA: 1022742.0s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.168s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 2.94
               Mean episode length: 43.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 8.63s
                        Total time: 7550.23s
                               ETA: 1022505.1s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.569s, learning 0.172s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 42.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 8.74s
                        Total time: 7558.97s
                               ETA: 1022283.9s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.516s, learning 0.315s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 44.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 8.83s
                        Total time: 7567.80s
                               ETA: 1022075.5s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.356s, learning 0.157s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 2.82
               Mean episode length: 43.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 8.51s
                        Total time: 7576.32s
                               ETA: 1021824.7s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.519s, learning 0.158s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 43.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 8.68s
                        Total time: 7584.99s
                               ETA: 1021596.5s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.024s, learning 0.156s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 2.91
               Mean episode length: 43.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 8.18s
                        Total time: 7593.17s
                               ETA: 1021302.3s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.525s, learning 0.192s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 2.81
               Mean episode length: 43.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 8.72s
                        Total time: 7601.89s
                               ETA: 1021080.9s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.398s, learning 0.173s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0345
             Mean action noise std: 0.73
                       Mean reward: 2.95
               Mean episode length: 43.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 8.57s
                        Total time: 7610.46s
                               ETA: 1020840.6s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.794s, learning 0.206s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 2.90
               Mean episode length: 43.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 9.00s
                        Total time: 7619.46s
                               ETA: 1020658.2s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.580s, learning 0.159s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 2.80
               Mean episode length: 43.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 8.74s
                        Total time: 7628.20s
                               ETA: 1020441.5s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.578s, learning 0.291s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 2.99
               Mean episode length: 44.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 8.87s
                        Total time: 7637.07s
                               ETA: 1020242.6s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.599s, learning 0.179s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 43.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 8.78s
                        Total time: 7645.85s
                               ETA: 1020032.1s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.491s, learning 0.209s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 2.90
               Mean episode length: 43.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 8.70s
                        Total time: 7654.55s
                               ETA: 1019811.7s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.185s, learning 0.177s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 44.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 8.36s
                        Total time: 7662.91s
                               ETA: 1019547.0s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.643s, learning 0.192s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 2.88
               Mean episode length: 43.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 8.84s
                        Total time: 7671.74s
                               ETA: 1019345.9s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.651s, learning 0.167s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 2.85
               Mean episode length: 43.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 8.82s
                        Total time: 7680.56s
                               ETA: 1019142.9s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.537s, learning 0.307s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 2.85
               Mean episode length: 43.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 8.84s
                        Total time: 7689.41s
                               ETA: 1018943.9s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.739s, learning 0.164s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 2.95
               Mean episode length: 43.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 8.90s
                        Total time: 7698.31s
                               ETA: 1018753.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.623s, learning 0.173s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 43.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 8.80s
                        Total time: 7707.11s
                               ETA: 1018549.0s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.203s, learning 0.244s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 2.96
               Mean episode length: 42.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 8.45s
                        Total time: 7715.55s
                               ETA: 1018299.1s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.042s, learning 0.196s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 2.90
               Mean episode length: 42.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 8.24s
                        Total time: 7723.79s
                               ETA: 1018022.3s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.296s, learning 0.319s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 2.99
               Mean episode length: 42.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.62s
                        Total time: 7732.41s
                               ETA: 1017796.0s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.560s, learning 0.196s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 2.87
               Mean episode length: 43.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.76s
                        Total time: 7741.16s
                               ETA: 1017588.6s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.492s, learning 0.241s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0360
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 43.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 8.73s
                        Total time: 7749.90s
                               ETA: 1017378.8s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.637s, learning 0.232s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0345
             Mean action noise std: 0.73
                       Mean reward: 2.88
               Mean episode length: 42.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 8.87s
                        Total time: 7758.76s
                               ETA: 1017187.3s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.389s, learning 0.199s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 2.99
               Mean episode length: 42.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.59s
                        Total time: 7767.35s
                               ETA: 1016959.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.708s, learning 0.194s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 2.99
               Mean episode length: 43.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.90s
                        Total time: 7776.25s
                               ETA: 1016773.4s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.672s, learning 0.160s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0353
             Mean action noise std: 0.73
                       Mean reward: 3.08
               Mean episode length: 42.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.83s
                        Total time: 7785.09s
                               ETA: 1016578.6s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.129s, learning 0.222s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 2.92
               Mean episode length: 42.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.35s
                        Total time: 7793.44s
                               ETA: 1016321.5s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.471s, learning 0.192s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 3.00
               Mean episode length: 42.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.66s
                        Total time: 7802.10s
                               ETA: 1016105.8s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.543s, learning 0.243s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 3.03
               Mean episode length: 42.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.79s
                        Total time: 7810.89s
                               ETA: 1015906.5s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.562s, learning 0.172s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 2.97
               Mean episode length: 42.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.73s
                        Total time: 7819.62s
                               ETA: 1015701.1s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.415s, learning 0.282s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 43.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 8.70s
                        Total time: 7828.32s
                               ETA: 1015491.4s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.429s, learning 0.159s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 43.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.59s
                        Total time: 7836.91s
                               ETA: 1015268.1s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.317s, learning 0.387s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 2.92
               Mean episode length: 43.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 8.70s
                        Total time: 7845.61s
                               ETA: 1015060.3s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.132s, learning 0.199s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0353
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 43.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.33s
                        Total time: 7853.94s
                               ETA: 1014804.8s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.480s, learning 0.177s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 2.97
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.66s
                        Total time: 7862.60s
                               ETA: 1014592.1s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.561s, learning 0.313s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 2.96
               Mean episode length: 43.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.87s
                        Total time: 7871.47s
                               ETA: 1014407.9s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.620s, learning 0.174s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 42.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.79s
                        Total time: 7880.27s
                               ETA: 1014213.7s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.756s, learning 0.163s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.08
               Mean episode length: 43.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 8.92s
                        Total time: 7889.19s
                               ETA: 1014036.3s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.646s, learning 0.180s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 43.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.83s
                        Total time: 7898.01s
                               ETA: 1013847.2s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.368s, learning 0.171s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0322
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 42.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.54s
                        Total time: 7906.55s
                               ETA: 1013621.8s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.326s, learning 0.209s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 3.07
               Mean episode length: 43.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.54s
                        Total time: 7915.09s
                               ETA: 1013396.5s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.813s, learning 0.165s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 2.97
               Mean episode length: 42.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 8.98s
                        Total time: 7924.06s
                               ETA: 1013228.3s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.545s, learning 0.175s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.06
               Mean episode length: 42.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.72s
                        Total time: 7932.78s
                               ETA: 1013027.6s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.619s, learning 0.237s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0365
             Mean action noise std: 0.73
                       Mean reward: 2.89
               Mean episode length: 42.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.86s
                        Total time: 7941.64s
                               ETA: 1012844.8s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.698s, learning 0.212s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 3.06
               Mean episode length: 42.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.91s
                        Total time: 7950.55s
                               ETA: 1012669.3s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.191s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 3.06
               Mean episode length: 43.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.79s
                        Total time: 7959.34s
                               ETA: 1012479.2s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.518s, learning 0.245s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0343
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 42.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.76s
                        Total time: 7968.10s
                               ETA: 1012285.9s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.441s, learning 0.195s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 2.92
               Mean episode length: 42.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.64s
                        Total time: 7976.74s
                               ETA: 1012077.0s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.734s, learning 0.337s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 42.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 9.07s
                        Total time: 7985.81s
                               ETA: 1011923.7s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.385s, learning 0.174s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 3.06
               Mean episode length: 42.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.56s
                        Total time: 7994.37s
                               ETA: 1011706.1s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.637s, learning 0.178s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 2.96
               Mean episode length: 42.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.82s
                        Total time: 8003.19s
                               ETA: 1011521.2s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.769s, learning 0.176s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0344
             Mean action noise std: 0.73
                       Mean reward: 2.93
               Mean episode length: 42.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.94s
                        Total time: 8012.13s
                               ETA: 1011353.2s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.327s, learning 0.178s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 2.94
               Mean episode length: 42.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.50s
                        Total time: 8020.64s
                               ETA: 1011130.1s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.458s, learning 0.162s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 3.10
               Mean episode length: 43.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.62s
                        Total time: 8029.26s
                               ETA: 1010922.1s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.338s, learning 0.183s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0351
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 43.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.52s
                        Total time: 8037.78s
                               ETA: 1010702.2s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.221s, learning 0.206s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.00
               Mean episode length: 42.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.43s
                        Total time: 8046.20s
                               ETA: 1010470.9s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.747s, learning 0.162s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 42.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.91s
                        Total time: 8055.11s
                               ETA: 1010300.6s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.389s, learning 0.176s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0366
             Mean action noise std: 0.73
                       Mean reward: 3.15
               Mean episode length: 42.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.56s
                        Total time: 8063.68s
                               ETA: 1010087.6s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.260s, learning 0.157s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 2.98
               Mean episode length: 42.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.42s
                        Total time: 8072.09s
                               ETA: 1009856.7s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.592s, learning 0.164s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 43.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.76s
                        Total time: 8080.85s
                               ETA: 1009668.6s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.553s, learning 0.236s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0346
             Mean action noise std: 0.73
                       Mean reward: 3.06
               Mean episode length: 43.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.79s
                        Total time: 8089.64s
                               ETA: 1009485.2s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.470s, learning 0.159s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 43.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.63s
                        Total time: 8098.27s
                               ETA: 1009282.2s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.223s, learning 0.169s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 3.03
               Mean episode length: 43.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.39s
                        Total time: 8106.66s
                               ETA: 1009050.3s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.594s, learning 0.366s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0345
             Mean action noise std: 0.73
                       Mean reward: 3.04
               Mean episode length: 42.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.96s
                        Total time: 8115.62s
                               ETA: 1008889.5s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.533s, learning 0.176s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.09
               Mean episode length: 42.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.71s
                        Total time: 8124.33s
                               ETA: 1008697.9s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.378s, learning 0.330s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0372
             Mean action noise std: 0.73
                       Mean reward: 3.09
               Mean episode length: 43.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.71s
                        Total time: 8133.04s
                               ETA: 1008506.8s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.594s, learning 0.178s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 44.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.77s
                        Total time: 8141.81s
                               ETA: 1008323.9s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.193s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 3.17
               Mean episode length: 43.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.79s
                        Total time: 8150.60s
                               ETA: 1008144.2s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.897s, learning 0.201s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 3.14
               Mean episode length: 43.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 9.10s
                        Total time: 8159.70s
                               ETA: 1008002.5s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.144s, learning 0.327s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 3.14
               Mean episode length: 44.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.47s
                        Total time: 8168.17s
                               ETA: 1007783.9s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.617s, learning 0.229s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 3.15
               Mean episode length: 44.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.85s
                        Total time: 8177.02s
                               ETA: 1007611.8s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.568s, learning 0.315s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 43.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.88s
                        Total time: 8185.90s
                               ETA: 1007444.8s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.538s, learning 0.175s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0360
             Mean action noise std: 0.73
                       Mean reward: 3.11
               Mean episode length: 43.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.71s
                        Total time: 8194.61s
                               ETA: 1007257.2s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.482s, learning 0.303s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.13
               Mean episode length: 42.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.79s
                        Total time: 8203.40s
                               ETA: 1007079.0s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.559s, learning 0.210s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 3.01
               Mean episode length: 43.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 8.77s
                        Total time: 8212.17s
                               ETA: 1006899.1s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.379s, learning 0.161s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 3.36
               Mean episode length: 43.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 8.54s
                        Total time: 8220.71s
                               ETA: 1006691.6s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.513s, learning 0.189s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 43.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.70s
                        Total time: 8229.41s
                               ETA: 1006504.4s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.162s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0329
             Mean action noise std: 0.73
                       Mean reward: 3.17
               Mean episode length: 43.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.62s
                        Total time: 8238.03s
                               ETA: 1006307.9s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.759s, learning 0.268s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0359
             Mean action noise std: 0.73
                       Mean reward: 3.11
               Mean episode length: 43.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 9.03s
                        Total time: 8247.06s
                               ETA: 1006161.2s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.666s, learning 0.166s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0417
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 43.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.83s
                        Total time: 8255.89s
                               ETA: 1005991.1s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.617s, learning 0.196s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0329
             Mean action noise std: 0.73
                       Mean reward: 3.18
               Mean episode length: 43.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.81s
                        Total time: 8264.70s
                               ETA: 1005819.1s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.475s, learning 0.207s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 44.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.68s
                        Total time: 8273.38s
                               ETA: 1005631.6s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.212s, learning 0.298s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0351
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 44.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.51s
                        Total time: 8281.89s
                               ETA: 1005423.7s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.637s, learning 0.168s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 43.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.81s
                        Total time: 8290.70s
                               ETA: 1005252.1s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.559s, learning 0.174s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.23
               Mean episode length: 43.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.73s
                        Total time: 8299.43s
                               ETA: 1005072.2s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.575s, learning 0.170s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 3.20
               Mean episode length: 44.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.75s
                        Total time: 8308.18s
                               ETA: 1004894.2s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.535s, learning 0.166s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0354
             Mean action noise std: 0.73
                       Mean reward: 3.20
               Mean episode length: 43.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.70s
                        Total time: 8316.88s
                               ETA: 1004711.1s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.672s, learning 0.159s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0348
             Mean action noise std: 0.73
                       Mean reward: 3.10
               Mean episode length: 43.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.83s
                        Total time: 8325.71s
                               ETA: 1004544.3s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.790s, learning 0.172s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 2.94
               Mean episode length: 42.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.96s
                        Total time: 8334.67s
                               ETA: 1004393.6s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.405s, learning 0.169s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 43.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.57s
                        Total time: 8343.24s
                               ETA: 1004196.5s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.649s, learning 0.169s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 44.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.82s
                        Total time: 8352.06s
                               ETA: 1004029.1s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.162s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.14
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.48s
                        Total time: 8360.54s
                               ETA: 1003821.8s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.597s, learning 0.206s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 3.29
               Mean episode length: 42.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.80s
                        Total time: 8369.35s
                               ETA: 1003653.5s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.581s, learning 0.164s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 3.25
               Mean episode length: 43.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 11.75s
                        Total time: 8381.09s
                               ETA: 1003838.0s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.678s, learning 0.283s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.18
               Mean episode length: 43.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 16.96s
                        Total time: 8398.05s
                               ETA: 1004646.0s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.233s, learning 0.163s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0330
             Mean action noise std: 0.73
                       Mean reward: 3.18
               Mean episode length: 43.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 16.40s
                        Total time: 8414.45s
                               ETA: 1005384.5s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.392s, learning 0.168s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 3.16
               Mean episode length: 43.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 16.56s
                        Total time: 8431.01s
                               ETA: 1006140.7s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.424s, learning 0.363s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0346
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 44.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 16.79s
                        Total time: 8447.79s
                               ETA: 1006922.1s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.384s, learning 0.165s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 3.23
               Mean episode length: 43.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 16.55s
                        Total time: 8464.34s
                               ETA: 1007673.4s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.527s, learning 0.160s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 42.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 16.69s
                        Total time: 8481.03s
                               ETA: 1008439.2s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.289s, learning 0.187s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 3.16
               Mean episode length: 43.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 16.48s
                        Total time: 8497.51s
                               ETA: 1009178.1s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 936 steps/s (collection: 17.214s, learning 0.288s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 42.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 17.50s
                        Total time: 8515.01s
                               ETA: 1010036.8s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.604s, learning 0.170s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 3.25
               Mean episode length: 43.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 16.77s
                        Total time: 8531.78s
                               ETA: 1010807.3s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 944 steps/s (collection: 17.181s, learning 0.164s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 3.19
               Mean episode length: 42.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 17.35s
                        Total time: 8549.13s
                               ETA: 1011643.4s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.024s, learning 0.167s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.18
               Mean episode length: 43.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 16.19s
                        Total time: 8565.32s
                               ETA: 1012341.0s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.546s, learning 0.244s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.39
               Mean episode length: 42.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 16.79s
                        Total time: 8582.11s
                               ETA: 1013107.7s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.539s, learning 0.162s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 43.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 16.70s
                        Total time: 8598.81s
                               ETA: 1013862.0s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.533s, learning 0.206s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.03
               Mean episode length: 43.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 16.74s
                        Total time: 8615.55s
                               ETA: 1014619.0s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.465s, learning 0.166s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 44.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 16.63s
                        Total time: 8632.18s
                               ETA: 1015361.4s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.524s, learning 0.348s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 43.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 16.87s
                        Total time: 8649.05s
                               ETA: 1016130.3s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 980 steps/s (collection: 16.529s, learning 0.182s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0349
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 42.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 16.71s
                        Total time: 8665.76s
                               ETA: 1016878.4s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.320s, learning 0.162s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.29
               Mean episode length: 43.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 16.48s
                        Total time: 8682.24s
                               ETA: 1017598.0s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.606s, learning 0.191s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 3.08
               Mean episode length: 43.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 16.80s
                        Total time: 8699.04s
                               ETA: 1018352.7s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.027s, learning 0.185s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 43.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 17.21s
                        Total time: 8716.25s
                               ETA: 1019154.1s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.606s, learning 0.167s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 43.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 16.77s
                        Total time: 8733.03s
                               ETA: 1019902.2s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.676s, learning 0.264s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0326
             Mean action noise std: 0.73
                       Mean reward: 3.35
               Mean episode length: 43.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 16.94s
                        Total time: 8749.97s
                               ETA: 1020668.2s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.705s, learning 0.172s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 43.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 16.88s
                        Total time: 8766.84s
                               ETA: 1021424.8s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.036s, learning 0.187s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0378
             Mean action noise std: 0.73
                       Mean reward: 3.40
               Mean episode length: 43.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 17.22s
                        Total time: 8784.07s
                               ETA: 1022219.9s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.739s, learning 0.168s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.23
               Mean episode length: 43.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 16.91s
                        Total time: 8800.97s
                               ETA: 1022976.4s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.586s, learning 0.296s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 43.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 16.88s
                        Total time: 8817.85s
                               ETA: 1023728.1s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.972s, learning 0.165s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 3.25
               Mean episode length: 43.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 17.14s
                        Total time: 8834.99s
                               ETA: 1024507.7s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.113s, learning 0.167s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 3.33
               Mean episode length: 44.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 17.28s
                        Total time: 8852.27s
                               ETA: 1025302.0s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.765s, learning 0.232s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.23
               Mean episode length: 43.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 17.00s
                        Total time: 8869.27s
                               ETA: 1026061.6s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.721s, learning 0.171s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0352
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 43.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 16.89s
                        Total time: 8886.16s
                               ETA: 1026807.3s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.835s, learning 0.281s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 43.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 17.12s
                        Total time: 8903.28s
                               ETA: 1027576.9s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.389s, learning 0.162s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 44.04
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 16.55s
                        Total time: 8919.83s
                               ETA: 1028279.7s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.612s, learning 0.181s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 43.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 16.79s
                        Total time: 8936.62s
                               ETA: 1029008.6s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.744s, learning 0.384s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 44.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 17.13s
                        Total time: 8953.75s
                               ETA: 1029774.4s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.686s, learning 0.245s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 16.93s
                        Total time: 8970.68s
                               ETA: 1030515.8s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.649s, learning 0.165s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.27
               Mean episode length: 44.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 16.81s
                        Total time: 8987.49s
                               ETA: 1031241.9s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.643s, learning 0.183s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 3.34
               Mean episode length: 43.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 16.83s
                        Total time: 9004.32s
                               ETA: 1031967.6s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1543 steps/s (collection: 10.433s, learning 0.182s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0370
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 44.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 10.61s
                        Total time: 9014.93s
                               ETA: 1031980.7s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1781 steps/s (collection: 8.981s, learning 0.217s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 43.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 9.20s
                        Total time: 9024.13s
                               ETA: 1031831.7s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.610s, learning 0.256s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 43.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 8.87s
                        Total time: 9033.00s
                               ETA: 1031645.2s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.545s, learning 0.169s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 3.17
               Mean episode length: 43.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.71s
                        Total time: 9041.71s
                               ETA: 1031441.6s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.726s, learning 0.298s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 43.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 9.02s
                        Total time: 9050.73s
                               ETA: 1031273.9s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.490s, learning 0.163s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 3.35
               Mean episode length: 43.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.65s
                        Total time: 9059.39s
                               ETA: 1031064.4s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.308s, learning 0.236s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 3.12
               Mean episode length: 44.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 8.54s
                        Total time: 9067.93s
                               ETA: 1030842.9s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1789 steps/s (collection: 8.809s, learning 0.345s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 44.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 9.15s
                        Total time: 9077.08s
                               ETA: 1030691.0s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.219s, learning 0.301s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0360
             Mean action noise std: 0.73
                       Mean reward: 3.38
               Mean episode length: 43.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.52s
                        Total time: 9085.60s
                               ETA: 1030467.7s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.360s, learning 0.165s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 44.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.53s
                        Total time: 9094.13s
                               ETA: 1030245.4s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.641s, learning 0.158s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 3.35
               Mean episode length: 43.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.80s
                        Total time: 9102.93s
                               ETA: 1030054.7s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.369s, learning 0.309s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0373
             Mean action noise std: 0.73
                       Mean reward: 3.33
               Mean episode length: 43.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 8.68s
                        Total time: 9111.61s
                               ETA: 1029850.5s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.512s, learning 0.178s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 43.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 8.69s
                        Total time: 9120.30s
                               ETA: 1029648.3s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.452s, learning 0.186s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0318
             Mean action noise std: 0.73
                       Mean reward: 3.31
               Mean episode length: 43.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.64s
                        Total time: 9128.93s
                               ETA: 1029440.5s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.515s, learning 0.191s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 3.33
               Mean episode length: 43.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.71s
                        Total time: 9137.64s
                               ETA: 1029241.0s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.700s, learning 0.176s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 44.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.88s
                        Total time: 9146.52s
                               ETA: 1029060.9s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.185s, learning 0.281s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.36
               Mean episode length: 43.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 8.47s
                        Total time: 9154.98s
                               ETA: 1028835.3s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.389s, learning 0.206s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.48
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 8.59s
                        Total time: 9163.58s
                               ETA: 1028624.5s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.220s, learning 0.166s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.50
               Mean episode length: 43.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.39s
                        Total time: 9171.96s
                               ETA: 1028390.8s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.746s, learning 0.176s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 3.29
               Mean episode length: 43.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 8.92s
                        Total time: 9180.88s
                               ETA: 1028217.5s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.717s, learning 0.214s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.42
               Mean episode length: 44.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.93s
                        Total time: 9189.82s
                               ETA: 1028045.8s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.243s, learning 0.167s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.39
               Mean episode length: 44.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.41s
                        Total time: 9198.23s
                               ETA: 1027816.1s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.591s, learning 0.239s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 3.48
               Mean episode length: 44.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.83s
                        Total time: 9207.06s
                               ETA: 1027633.8s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.383s, learning 0.170s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 3.27
               Mean episode length: 43.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.55s
                        Total time: 9215.61s
                               ETA: 1027421.0s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.163s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 3.45
               Mean episode length: 43.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 8.41s
                        Total time: 9224.02s
                               ETA: 1027193.3s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.176s, learning 0.267s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 3.31
               Mean episode length: 43.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 8.44s
                        Total time: 9232.47s
                               ETA: 1026969.3s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.489s, learning 0.193s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0340
             Mean action noise std: 0.73
                       Mean reward: 3.37
               Mean episode length: 44.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 8.68s
                        Total time: 9241.15s
                               ETA: 1026772.3s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.009s, learning 0.267s)
               Value function loss: 47.3534
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 43.72
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 8.28s
                        Total time: 9249.42s
                               ETA: 1026530.6s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.529s, learning 0.190s)
               Value function loss: 0.1015
                    Surrogate loss: -0.0495
             Mean action noise std: 0.73
                       Mean reward: 8.58
               Mean episode length: 43.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 8.72s
                        Total time: 9258.14s
                               ETA: 1026338.5s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.175s, learning 0.277s)
               Value function loss: 0.0544
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 3.20
               Mean episode length: 42.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 8.45s
                        Total time: 9266.59s
                               ETA: 1026117.3s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.543s, learning 0.228s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0426
             Mean action noise std: 0.73
                       Mean reward: 3.31
               Mean episode length: 42.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 8.77s
                        Total time: 9275.36s
                               ETA: 1025931.8s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.653s, learning 0.253s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0444
             Mean action noise std: 0.73
                       Mean reward: 3.37
               Mean episode length: 43.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 8.91s
                        Total time: 9284.27s
                               ETA: 1025761.7s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.586s, learning 0.406s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 3.27
               Mean episode length: 43.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 8.99s
                        Total time: 9293.26s
                               ETA: 1025601.3s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.356s, learning 0.376s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 3.72
               Mean episode length: 44.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 8.73s
                        Total time: 9301.99s
                               ETA: 1025412.8s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.675s, learning 0.269s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 3.56
               Mean episode length: 44.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 8.94s
                        Total time: 9310.94s
                               ETA: 1025248.0s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.358s, learning 0.406s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 3.46
               Mean episode length: 44.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 8.76s
                        Total time: 9319.70s
                               ETA: 1025063.6s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.316s, learning 0.252s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 3.29
               Mean episode length: 43.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 8.57s
                        Total time: 9328.27s
                               ETA: 1024858.1s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.160s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.31
               Mean episode length: 43.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 8.55s
                        Total time: 9336.82s
                               ETA: 1024651.3s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.660s, learning 0.205s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0307
             Mean action noise std: 0.73
                       Mean reward: 3.27
               Mean episode length: 44.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 8.87s
                        Total time: 9345.68s
                               ETA: 1024479.4s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.841s, learning 0.170s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 3.24
               Mean episode length: 43.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 9.01s
                        Total time: 9354.70s
                               ETA: 1024323.7s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.781s, learning 0.157s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 43.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 8.94s
                        Total time: 9363.63s
                               ETA: 1024160.4s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.163s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0431
             Mean action noise std: 0.73
                       Mean reward: 3.36
               Mean episode length: 43.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 8.39s
                        Total time: 9372.02s
                               ETA: 1023937.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.792s, learning 0.170s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 3.39
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 8.96s
                        Total time: 9380.99s
                               ETA: 1023777.6s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.552s, learning 0.189s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 44.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 8.74s
                        Total time: 9389.73s
                               ETA: 1023593.9s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.782s, learning 0.173s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 3.46
               Mean episode length: 43.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 8.96s
                        Total time: 9398.68s
                               ETA: 1023433.9s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.171s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 43.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 8.72s
                        Total time: 9407.40s
                               ETA: 1023248.5s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.447s, learning 0.165s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 3.41
               Mean episode length: 44.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 8.61s
                        Total time: 9416.01s
                               ETA: 1023051.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.658s, learning 0.223s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 3.40
               Mean episode length: 44.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 8.88s
                        Total time: 9424.89s
                               ETA: 1022884.9s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.668s, learning 0.182s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 3.34
               Mean episode length: 43.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 8.85s
                        Total time: 9433.74s
                               ETA: 1022714.9s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.798s, learning 0.165s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.36
               Mean episode length: 43.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 8.96s
                        Total time: 9442.71s
                               ETA: 1022557.5s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.164s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 3.51
               Mean episode length: 44.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 8.64s
                        Total time: 9451.35s
                               ETA: 1022365.3s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.924s, learning 0.224s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 3.41
               Mean episode length: 43.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 9.15s
                        Total time: 9460.49s
                               ETA: 1022228.6s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.498s, learning 0.192s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0411
             Mean action noise std: 0.73
                       Mean reward: 3.50
               Mean episode length: 44.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 8.69s
                        Total time: 9469.18s
                               ETA: 1022042.7s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.678s, learning 0.204s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0318
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 43.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 8.88s
                        Total time: 9478.07s
                               ETA: 1021877.9s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.751s, learning 0.229s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 3.34
               Mean episode length: 44.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 8.98s
                        Total time: 9487.05s
                               ETA: 1021724.0s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.584s, learning 0.161s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0391
             Mean action noise std: 0.73
                       Mean reward: 3.53
               Mean episode length: 43.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 8.75s
                        Total time: 9495.79s
                               ETA: 1021545.1s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.818s, learning 0.188s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 43.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 9.01s
                        Total time: 9504.80s
                               ETA: 1021394.7s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.677s, learning 0.255s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 3.40
               Mean episode length: 43.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 8.93s
                        Total time: 9513.73s
                               ETA: 1021236.7s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.160s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 3.34
               Mean episode length: 43.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 8.48s
                        Total time: 9522.21s
                               ETA: 1021030.1s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.395s, learning 0.308s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 3.53
               Mean episode length: 44.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 8.70s
                        Total time: 9530.91s
                               ETA: 1020848.2s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.634s, learning 0.159s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 3.26
               Mean episode length: 42.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 8.79s
                        Total time: 9539.70s
                               ETA: 1020676.3s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.797s, learning 0.159s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 44.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 8.96s
                        Total time: 9548.66s
                               ETA: 1020522.1s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.258s, learning 0.219s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 3.66
               Mean episode length: 44.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 8.48s
                        Total time: 9557.14s
                               ETA: 1020317.0s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.730s, learning 0.221s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 3.74
               Mean episode length: 44.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 8.95s
                        Total time: 9566.09s
                               ETA: 1020163.0s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.636s, learning 0.202s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0365
             Mean action noise std: 0.73
                       Mean reward: 3.44
               Mean episode length: 42.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 8.84s
                        Total time: 9574.93s
                               ETA: 1019997.2s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.607s, learning 0.161s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 3.35
               Mean episode length: 44.04
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 8.77s
                        Total time: 9583.69s
                               ETA: 1019824.4s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.644s, learning 0.157s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0346
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 43.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 8.80s
                        Total time: 9592.49s
                               ETA: 1019655.4s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.420s, learning 0.160s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 3.40
               Mean episode length: 43.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 8.58s
                        Total time: 9601.07s
                               ETA: 1019463.3s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.605s, learning 0.257s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 43.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 8.86s
                        Total time: 9609.94s
                               ETA: 1019301.4s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.643s, learning 0.298s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 43.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 8.94s
                        Total time: 9618.88s
                               ETA: 1019148.3s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.648s, learning 0.172s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 3.50
               Mean episode length: 44.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 8.82s
                        Total time: 9627.70s
                               ETA: 1018982.7s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.797s, learning 0.235s)
               Value function loss: 29.2952
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 3.54
               Mean episode length: 44.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 9.03s
                        Total time: 9636.73s
                               ETA: 1018839.9s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.624s, learning 0.298s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0450
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 43.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 8.92s
                        Total time: 9645.65s
                               ETA: 1018685.7s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.409s, learning 0.162s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0371
             Mean action noise std: 0.73
                       Mean reward: 3.68
               Mean episode length: 44.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 8.57s
                        Total time: 9654.22s
                               ETA: 1018494.8s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.568s, learning 0.203s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.49
               Mean episode length: 44.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 8.77s
                        Total time: 9662.99s
                               ETA: 1018325.2s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.737s, learning 0.175s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.37
               Mean episode length: 44.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 8.91s
                        Total time: 9671.90s
                               ETA: 1018171.0s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.664s, learning 0.163s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 3.47
               Mean episode length: 44.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 8.83s
                        Total time: 9680.73s
                               ETA: 1018008.1s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.905s, learning 0.182s)
               Value function loss: 17.4313
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 3.42
               Mean episode length: 44.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 9.09s
                        Total time: 9689.82s
                               ETA: 1017872.8s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.375s, learning 0.352s)
               Value function loss: 45.5226
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 3.62
               Mean episode length: 44.98
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 8.73s
                        Total time: 9698.55s
                               ETA: 1017700.1s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.885s, learning 0.167s)
               Value function loss: 0.2548
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 44.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 9.05s
                        Total time: 9707.60s
                               ETA: 1017561.7s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.651s, learning 0.207s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0468
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 44.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 8.86s
                        Total time: 9716.46s
                               ETA: 1017403.3s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.506s, learning 0.180s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 3.30
               Mean episode length: 44.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 8.69s
                        Total time: 9725.14s
                               ETA: 1017227.2s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.688s, learning 0.234s)
               Value function loss: 0.0475
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 3.32
               Mean episode length: 43.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 8.92s
                        Total time: 9734.06s
                               ETA: 1017076.1s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1776 steps/s (collection: 9.054s, learning 0.169s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0434
             Mean action noise std: 0.73
                       Mean reward: 3.51
               Mean episode length: 44.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 9.22s
                        Total time: 9743.29s
                               ETA: 1016956.8s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.257s, learning 0.158s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0438
             Mean action noise std: 0.73
                       Mean reward: 3.56
               Mean episode length: 44.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 8.41s
                        Total time: 9751.70s
                               ETA: 1016753.4s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.710s, learning 0.178s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0436
             Mean action noise std: 0.73
                       Mean reward: 3.41
               Mean episode length: 44.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 8.89s
                        Total time: 9760.59s
                               ETA: 1016599.7s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1797 steps/s (collection: 8.800s, learning 0.317s)
               Value function loss: 0.0446
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 3.56
               Mean episode length: 44.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 9.12s
                        Total time: 9769.71s
                               ETA: 1016470.2s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.271s, learning 0.268s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0433
             Mean action noise std: 0.73
                       Mean reward: 3.47
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 8.54s
                        Total time: 9778.25s
                               ETA: 1016280.8s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.736s, learning 0.159s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0430
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 45.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 8.90s
                        Total time: 9787.14s
                               ETA: 1016128.8s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.722s, learning 0.182s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0433
             Mean action noise std: 0.73
                       Mean reward: 3.31
               Mean episode length: 44.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 8.90s
                        Total time: 9796.04s
                               ETA: 1015978.0s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.637s, learning 0.215s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0429
             Mean action noise std: 0.73
                       Mean reward: 3.50
               Mean episode length: 45.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 8.85s
                        Total time: 9804.90s
                               ETA: 1015822.1s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.327s, learning 0.170s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0420
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 44.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 8.50s
                        Total time: 9813.39s
                               ETA: 1015629.8s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.566s, learning 0.234s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0445
             Mean action noise std: 0.73
                       Mean reward: 3.50
               Mean episode length: 44.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 8.80s
                        Total time: 9822.19s
                               ETA: 1015469.2s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.666s, learning 0.201s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 3.42
               Mean episode length: 45.04
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 8.87s
                        Total time: 9831.06s
                               ETA: 1015315.8s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.353s, learning 0.180s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0438
             Mean action noise std: 0.73
                       Mean reward: 3.61
               Mean episode length: 44.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 8.53s
                        Total time: 9839.59s
                               ETA: 1015128.3s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.289s, learning 0.240s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 45.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 8.53s
                        Total time: 9848.12s
                               ETA: 1014940.7s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.370s, learning 0.158s)
               Value function loss: 17.3138
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: 3.42
               Mean episode length: 43.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 8.53s
                        Total time: 9856.65s
                               ETA: 1014753.4s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.443s, learning 0.167s)
               Value function loss: 0.1387
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 3.60
               Mean episode length: 44.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 8.61s
                        Total time: 9865.26s
                               ETA: 1014574.9s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.374s, learning 0.296s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0458
             Mean action noise std: 0.73
                       Mean reward: 3.81
               Mean episode length: 44.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 8.67s
                        Total time: 9873.93s
                               ETA: 1014402.9s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.427s, learning 0.161s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0455
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 44.04
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 8.59s
                        Total time: 9882.52s
                               ETA: 1014222.9s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.557s, learning 0.178s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 3.40
               Mean episode length: 44.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 8.74s
                        Total time: 9891.25s
                               ETA: 1014058.3s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.541s, learning 0.164s)
               Value function loss: 53.4942
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 3.72
               Mean episode length: 45.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 8.70s
                        Total time: 9899.96s
                               ETA: 1013890.8s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.422s, learning 0.177s)
               Value function loss: 0.1851
                    Surrogate loss: -0.0488
             Mean action noise std: 0.73
                       Mean reward: 3.75
               Mean episode length: 44.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 8.60s
                        Total time: 9908.56s
                               ETA: 1013712.9s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1761 steps/s (collection: 9.122s, learning 0.180s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0466
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 44.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 9.30s
                        Total time: 9917.86s
                               ETA: 1013607.3s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.251s, learning 0.179s)
               Value function loss: 9.3293
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 6.07
               Mean episode length: 44.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 8.43s
                        Total time: 9926.29s
                               ETA: 1013412.7s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.762s, learning 0.168s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0463
             Mean action noise std: 0.73
                       Mean reward: 3.47
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.93s
                        Total time: 9935.22s
                               ETA: 1013269.6s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.170s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0452
             Mean action noise std: 0.73
                       Mean reward: 3.73
               Mean episode length: 45.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.63s
                        Total time: 9943.85s
                               ETA: 1013096.5s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.704s, learning 0.162s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0473
             Mean action noise std: 0.73
                       Mean reward: 3.63
               Mean episode length: 44.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.87s
                        Total time: 9952.72s
                               ETA: 1012947.3s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.661s, learning 0.218s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0455
             Mean action noise std: 0.73
                       Mean reward: 3.75
               Mean episode length: 45.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.88s
                        Total time: 9961.60s
                               ETA: 1012799.9s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.220s, learning 0.166s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 3.64
               Mean episode length: 44.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 8.39s
                        Total time: 9969.98s
                               ETA: 1012602.5s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.647s, learning 0.166s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 3.66
               Mean episode length: 45.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.81s
                        Total time: 9978.80s
                               ETA: 1012449.0s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.313s, learning 0.206s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 3.63
               Mean episode length: 45.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.52s
                        Total time: 9987.32s
                               ETA: 1012266.0s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.416s, learning 0.180s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0447
             Mean action noise std: 0.73
                       Mean reward: 3.41
               Mean episode length: 44.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.60s
                        Total time: 9995.91s
                               ETA: 1012091.1s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.685s, learning 0.182s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 3.49
               Mean episode length: 45.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.87s
                        Total time: 10004.78s
                               ETA: 1011944.0s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.648s, learning 0.176s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0457
             Mean action noise std: 0.73
                       Mean reward: 3.69
               Mean episode length: 44.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.82s
                        Total time: 10013.60s
                               ETA: 1011792.8s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.409s, learning 0.211s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0452
             Mean action noise std: 0.73
                       Mean reward: 3.54
               Mean episode length: 44.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.62s
                        Total time: 10022.22s
                               ETA: 1011621.2s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.164s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 3.49
               Mean episode length: 44.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.31s
                        Total time: 10030.53s
                               ETA: 1011418.5s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.534s, learning 0.206s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0438
             Mean action noise std: 0.73
                       Mean reward: 3.51
               Mean episode length: 44.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.74s
                        Total time: 10039.27s
                               ETA: 1011259.8s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.362s, learning 0.179s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0429
             Mean action noise std: 0.73
                       Mean reward: 3.49
               Mean episode length: 44.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.54s
                        Total time: 10047.81s
                               ETA: 1011081.4s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.352s, learning 0.265s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 3.62
               Mean episode length: 44.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.62s
                        Total time: 10056.43s
                               ETA: 1010911.0s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.472s, learning 0.177s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 45.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.65s
                        Total time: 10065.08s
                               ETA: 1010744.1s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.751s, learning 0.176s)
               Value function loss: 11.9310
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 6.23
               Mean episode length: 44.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 8.93s
                        Total time: 10074.00s
                               ETA: 1010605.3s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.457s, learning 0.166s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0456
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 44.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.62s
                        Total time: 10082.63s
                               ETA: 1010436.3s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.405s, learning 0.173s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 3.59
               Mean episode length: 44.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.58s
                        Total time: 10091.20s
                               ETA: 1010263.2s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.620s, learning 0.200s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0462
             Mean action noise std: 0.73
                       Mean reward: 3.48
               Mean episode length: 43.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.82s
                        Total time: 10100.02s
                               ETA: 1010114.6s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.551s, learning 0.162s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0434
             Mean action noise std: 0.73
                       Mean reward: 3.58
               Mean episode length: 43.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.71s
                        Total time: 10108.74s
                               ETA: 1009955.7s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1260 steps/s (collection: 12.705s, learning 0.291s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 3.47
               Mean episode length: 43.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 13.00s
                        Total time: 10121.73s
                               ETA: 1010224.5s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.480s, learning 0.240s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 3.47
               Mean episode length: 43.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 16.72s
                        Total time: 10138.45s
                               ETA: 1010864.0s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.252s, learning 0.166s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 3.74
               Mean episode length: 44.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 16.42s
                        Total time: 10154.87s
                               ETA: 1011472.3s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.095s, learning 0.172s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0444
             Mean action noise std: 0.73
                       Mean reward: 3.94
               Mean episode length: 45.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 16.27s
                        Total time: 10171.14s
                               ETA: 1012064.1s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.340s, learning 0.174s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 3.52
               Mean episode length: 43.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 16.51s
                        Total time: 10187.65s
                               ETA: 1012679.3s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.886s, learning 0.172s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0423
             Mean action noise std: 0.73
                       Mean reward: 3.62
               Mean episode length: 44.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 17.06s
                        Total time: 10204.71s
                               ETA: 1013347.2s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.460s, learning 0.351s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 3.56
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 16.81s
                        Total time: 10221.52s
                               ETA: 1013989.3s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.328s, learning 0.175s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 44.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 16.50s
                        Total time: 10238.02s
                               ETA: 1014599.5s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.251s, learning 0.180s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 3.65
               Mean episode length: 44.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 16.43s
                        Total time: 10254.46s
                               ETA: 1015201.4s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.661s, learning 0.175s)
               Value function loss: 0.0410
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 3.70
               Mean episode length: 45.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 16.84s
                        Total time: 10271.29s
                               ETA: 1015842.0s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1002 steps/s (collection: 15.978s, learning 0.367s)
               Value function loss: 9.7993
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 6.39
               Mean episode length: 45.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 16.35s
                        Total time: 10287.64s
                               ETA: 1016432.9s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.454s, learning 0.168s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0437
             Mean action noise std: 0.73
                       Mean reward: 3.72
               Mean episode length: 45.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 16.62s
                        Total time: 10304.26s
                               ETA: 1017049.9s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.640s, learning 0.171s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0446
             Mean action noise std: 0.73
                       Mean reward: 3.68
               Mean episode length: 45.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 16.81s
                        Total time: 10321.07s
                               ETA: 1017684.2s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.705s, learning 0.158s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 3.51
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 16.86s
                        Total time: 10337.93s
                               ETA: 1018322.4s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.549s, learning 0.180s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0429
             Mean action noise std: 0.73
                       Mean reward: 3.76
               Mean episode length: 45.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 16.73s
                        Total time: 10354.66s
                               ETA: 1018946.1s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.708s, learning 0.171s)
               Value function loss: 0.0452
                    Surrogate loss: -0.0433
             Mean action noise std: 0.73
                       Mean reward: 3.61
               Mean episode length: 44.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 16.88s
                        Total time: 10371.54s
                               ETA: 1019583.3s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.790s, learning 0.158s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0426
             Mean action noise std: 0.73
                       Mean reward: 3.86
               Mean episode length: 44.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 16.95s
                        Total time: 10388.49s
                               ETA: 1020225.9s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.669s, learning 0.168s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0447
             Mean action noise std: 0.73
                       Mean reward: 3.46
               Mean episode length: 44.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 16.84s
                        Total time: 10405.33s
                               ETA: 1020856.4s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.438s, learning 0.180s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 3.73
               Mean episode length: 44.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 16.62s
                        Total time: 10421.94s
                               ETA: 1021464.1s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.274s, learning 0.189s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 3.58
               Mean episode length: 44.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 16.46s
                        Total time: 10438.41s
                               ETA: 1022055.4s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.410s, learning 0.167s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0428
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 44.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 16.58s
                        Total time: 10454.98s
                               ETA: 1022656.6s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.712s, learning 0.248s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0427
             Mean action noise std: 0.73
                       Mean reward: 3.69
               Mean episode length: 44.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 16.96s
                        Total time: 10471.94s
                               ETA: 1023294.0s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.246s, learning 0.163s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 3.63
               Mean episode length: 44.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 16.41s
                        Total time: 10488.35s
                               ETA: 1023876.4s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.232s, learning 0.162s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 3.63
               Mean episode length: 44.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 17.39s
                        Total time: 10505.75s
                               ETA: 1024553.6s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.724s, learning 0.172s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 3.80
               Mean episode length: 44.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 16.90s
                        Total time: 10522.64s
                               ETA: 1025180.9s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.797s, learning 0.275s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 3.65
               Mean episode length: 44.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 17.07s
                        Total time: 10539.71s
                               ETA: 1025824.1s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.927s, learning 0.183s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 3.68
               Mean episode length: 45.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 17.11s
                        Total time: 10556.82s
                               ETA: 1026469.7s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.413s, learning 0.193s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0437
             Mean action noise std: 0.73
                       Mean reward: 3.86
               Mean episode length: 44.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 16.61s
                        Total time: 10573.43s
                               ETA: 1027065.1s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.183s, learning 0.274s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 3.62
               Mean episode length: 43.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 16.46s
                        Total time: 10589.89s
                               ETA: 1027644.8s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.451s, learning 0.166s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 3.78
               Mean episode length: 44.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 16.62s
                        Total time: 10606.50s
                               ETA: 1028238.8s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.175s, learning 0.307s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 3.60
               Mean episode length: 44.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 16.48s
                        Total time: 10622.99s
                               ETA: 1028818.6s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.423s, learning 0.298s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 3.68
               Mean episode length: 44.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 16.72s
                        Total time: 10639.71s
                               ETA: 1029420.3s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.403s, learning 0.174s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 3.66
               Mean episode length: 44.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 16.58s
                        Total time: 10656.28s
                               ETA: 1030006.9s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.704s, learning 0.194s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0427
             Mean action noise std: 0.73
                       Mean reward: 3.76
               Mean episode length: 44.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 16.90s
                        Total time: 10673.18s
                               ETA: 1030623.4s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.631s, learning 0.180s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.77
               Mean episode length: 45.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 16.81s
                        Total time: 10689.99s
                               ETA: 1031230.2s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.515s, learning 0.188s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 3.72
               Mean episode length: 44.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 16.70s
                        Total time: 10706.70s
                               ETA: 1031825.4s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.590s, learning 0.180s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 3.75
               Mean episode length: 44.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 16.77s
                        Total time: 10723.47s
                               ETA: 1032425.8s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.413s, learning 0.176s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 3.63
               Mean episode length: 44.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 16.59s
                        Total time: 10740.06s
                               ETA: 1033007.6s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.482s, learning 0.179s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 3.88
               Mean episode length: 45.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 9.66s
                        Total time: 10749.72s
                               ETA: 1032922.6s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.298s, learning 0.292s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0431
             Mean action noise std: 0.73
                       Mean reward: 3.81
               Mean episode length: 44.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 8.59s
                        Total time: 10758.31s
                               ETA: 1032734.8s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.322s, learning 0.173s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 3.49
               Mean episode length: 44.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.50s
                        Total time: 10766.80s
                               ETA: 1032538.4s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.668s, learning 0.182s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0423
             Mean action noise std: 0.73
                       Mean reward: 3.81
               Mean episode length: 45.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.85s
                        Total time: 10775.65s
                               ETA: 1032376.3s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.353s, learning 0.163s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0447
             Mean action noise std: 0.73
                       Mean reward: 3.59
               Mean episode length: 44.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.52s
                        Total time: 10784.17s
                               ETA: 1032182.6s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.703s, learning 0.171s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 3.56
               Mean episode length: 44.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.87s
                        Total time: 10793.04s
                               ETA: 1032023.5s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.551s, learning 0.173s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 3.62
               Mean episode length: 45.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.72s
                        Total time: 10801.77s
                               ETA: 1031850.2s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.540s, learning 0.173s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 3.58
               Mean episode length: 44.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 8.71s
                        Total time: 10810.48s
                               ETA: 1031676.2s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.508s, learning 0.222s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 3.67
               Mean episode length: 44.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 8.73s
                        Total time: 10819.21s
                               ETA: 1031504.3s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.952s, learning 0.159s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0418
             Mean action noise std: 0.73
                       Mean reward: 3.81
               Mean episode length: 45.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 9.11s
                        Total time: 10828.32s
                               ETA: 1031368.8s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.329s, learning 0.175s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0431
             Mean action noise std: 0.73
                       Mean reward: 3.61
               Mean episode length: 44.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 8.50s
                        Total time: 10836.82s
                               ETA: 1031175.9s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.654s, learning 0.233s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 3.48
               Mean episode length: 43.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 8.89s
                        Total time: 10845.71s
                               ETA: 1031019.9s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.331s, learning 0.158s)
               Value function loss: 17.4783
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 43.99
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 8.49s
                        Total time: 10854.20s
                               ETA: 1030826.3s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.055s, learning 0.398s)
               Value function loss: 45.9512
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 45.25
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 9.45s
                        Total time: 10863.65s
                               ETA: 1030724.4s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.573s, learning 0.183s)
               Value function loss: 0.2255
                    Surrogate loss: -0.0463
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 44.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 8.76s
                        Total time: 10872.41s
                               ETA: 1030556.7s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.493s, learning 0.181s)
               Value function loss: 0.0875
                    Surrogate loss: -0.0458
             Mean action noise std: 0.73
                       Mean reward: 3.64
               Mean episode length: 43.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 8.67s
                        Total time: 10881.09s
                               ETA: 1030381.5s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.405s, learning 0.171s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0417
             Mean action noise std: 0.73
                       Mean reward: 3.71
               Mean episode length: 44.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 8.58s
                        Total time: 10889.66s
                               ETA: 1030197.3s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.904s, learning 0.182s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 44.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 9.09s
                        Total time: 10898.75s
                               ETA: 1030061.7s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.467s, learning 0.265s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 45.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 8.73s
                        Total time: 10907.48s
                               ETA: 1029892.8s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.631s, learning 0.171s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 44.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 8.80s
                        Total time: 10916.28s
                               ETA: 1029730.9s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.661s, learning 0.170s)
               Value function loss: 0.0516
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 44.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 8.83s
                        Total time: 10925.11s
                               ETA: 1029572.1s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.184s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 44.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 8.59s
                        Total time: 10933.70s
                               ETA: 1029390.8s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.721s, learning 0.228s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 44.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 8.95s
                        Total time: 10942.65s
                               ETA: 1029243.7s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.639s, learning 0.163s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 44.03
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.80s
                        Total time: 10951.45s
                               ETA: 1029082.9s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.606s, learning 0.254s)
               Value function loss: 0.0501
                    Surrogate loss: -0.0442
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.86s
                        Total time: 10960.31s
                               ETA: 1028928.0s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.301s, learning 0.160s)
               Value function loss: 0.0487
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 44.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.46s
                        Total time: 10968.77s
                               ETA: 1028735.9s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.543s, learning 0.184s)
               Value function loss: 0.0511
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 44.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.73s
                        Total time: 10977.50s
                               ETA: 1028569.1s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.649s, learning 0.173s)
               Value function loss: 0.0487
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 43.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.82s
                        Total time: 10986.32s
                               ETA: 1028411.4s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.163s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 44.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.54s
                        Total time: 10994.87s
                               ETA: 1028228.0s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.401s, learning 0.339s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 44.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.74s
                        Total time: 11003.61s
                               ETA: 1028063.3s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.635s, learning 0.167s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 44.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.80s
                        Total time: 11012.41s
                               ETA: 1027904.7s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.483s, learning 0.162s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 44.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.64s
                        Total time: 11021.06s
                               ETA: 1027731.6s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.649s, learning 0.168s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 43.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.82s
                        Total time: 11029.87s
                               ETA: 1027574.9s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.551s, learning 0.256s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0413
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.81s
                        Total time: 11038.68s
                               ETA: 1027417.7s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.716s, learning 0.164s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 43.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.88s
                        Total time: 11047.56s
                               ETA: 1027267.4s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.452s, learning 0.204s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 43.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 8.66s
                        Total time: 11056.22s
                               ETA: 1027096.5s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.314s, learning 0.170s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 44.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 8.48s
                        Total time: 11064.70s
                               ETA: 1026909.9s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.744s, learning 0.166s)
               Value function loss: 0.0487
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 8.91s
                        Total time: 11073.61s
                               ETA: 1026763.3s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.337s, learning 0.184s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.52s
                        Total time: 11082.13s
                               ETA: 1026580.9s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.493s, learning 0.304s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 44.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.80s
                        Total time: 11090.93s
                               ETA: 1026424.3s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.715s, learning 0.194s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0414
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 44.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.91s
                        Total time: 11099.84s
                               ETA: 1026278.4s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.693s, learning 0.158s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.85s
                        Total time: 11108.69s
                               ETA: 1026127.4s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.622s, learning 0.165s)
               Value function loss: 0.0491
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 43.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.79s
                        Total time: 11117.47s
                               ETA: 1025970.7s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.370s, learning 0.203s)
               Value function loss: 32.6278
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 43.90
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.57s
                        Total time: 11126.05s
                               ETA: 1025794.7s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.463s, learning 0.165s)
               Value function loss: 0.1868
                    Surrogate loss: -0.0434
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 44.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.63s
                        Total time: 11134.68s
                               ETA: 1025623.9s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.750s, learning 0.269s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 3.93
               Mean episode length: 43.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 9.02s
                        Total time: 11143.69s
                               ETA: 1025489.4s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.179s, learning 0.322s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 44.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.50s
                        Total time: 11152.20s
                               ETA: 1025307.6s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.832s, learning 0.235s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.45
               Mean episode length: 43.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 9.07s
                        Total time: 11161.26s
                               ETA: 1025178.1s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.507s, learning 0.235s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0434
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 43.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.74s
                        Total time: 11170.01s
                               ETA: 1025019.0s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.629s, learning 0.160s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 44.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.79s
                        Total time: 11178.79s
                               ETA: 1024864.4s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.548s, learning 0.214s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 43.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.76s
                        Total time: 11187.56s
                               ETA: 1024707.7s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.518s, learning 0.173s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.69s
                        Total time: 11196.25s
                               ETA: 1024544.7s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.387s, learning 0.237s)
               Value function loss: 0.0518
                    Surrogate loss: -0.0402
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 44.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.62s
                        Total time: 11204.87s
                               ETA: 1024375.9s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.769s, learning 0.207s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 44.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.98s
                        Total time: 11213.85s
                               ETA: 1024239.5s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.463s, learning 0.190s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.65s
                        Total time: 11222.50s
                               ETA: 1024074.0s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.595s, learning 0.281s)
               Value function loss: 0.0492
                    Surrogate loss: -0.0439
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.88s
                        Total time: 11231.38s
                               ETA: 1023929.0s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.292s, learning 0.217s)
               Value function loss: 17.6752
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.51s
                        Total time: 11239.89s
                               ETA: 1023750.8s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.529s, learning 0.183s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 44.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.71s
                        Total time: 11248.60s
                               ETA: 1023591.5s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.392s, learning 0.289s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0404
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 44.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.68s
                        Total time: 11257.28s
                               ETA: 1023429.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.449s, learning 0.169s)
               Value function loss: 0.0481
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 44.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.62s
                        Total time: 11265.90s
                               ETA: 1023262.2s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.864s, learning 0.186s)
               Value function loss: 0.0475
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 9.05s
                        Total time: 11274.95s
                               ETA: 1023134.2s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.542s, learning 0.178s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 44.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.72s
                        Total time: 11283.67s
                               ETA: 1022976.6s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.556s, learning 0.175s)
               Value function loss: 0.0491
                    Surrogate loss: -0.0414
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 44.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.73s
                        Total time: 11292.40s
                               ETA: 1022820.3s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.695s, learning 0.240s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 44.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.94s
                        Total time: 11301.33s
                               ETA: 1022682.7s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.698s, learning 0.174s)
               Value function loss: 0.0475
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 4.20
               Mean episode length: 45.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.87s
                        Total time: 11310.20s
                               ETA: 1022539.7s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.616s, learning 0.229s)
               Value function loss: 15.2884
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 8.84s
                        Total time: 11319.05s
                               ETA: 1022394.4s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.823s, learning 0.286s)
               Value function loss: 0.1239
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 9.11s
                        Total time: 11328.16s
                               ETA: 1022273.2s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.639s, learning 0.158s)
               Value function loss: 0.0935
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 43.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.80s
                        Total time: 11336.95s
                               ETA: 1022124.0s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.585s, learning 0.179s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 3.89
               Mean episode length: 44.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 8.76s
                        Total time: 11345.72s
                               ETA: 1021972.3s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.479s, learning 0.360s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 43.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.84s
                        Total time: 11354.56s
                               ETA: 1021827.5s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.558s, learning 0.161s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 3.88
               Mean episode length: 45.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.72s
                        Total time: 11363.28s
                               ETA: 1021672.2s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.161s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 4.12
               Mean episode length: 45.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 8.72s
                        Total time: 11371.99s
                               ETA: 1021517.0s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.763s, learning 0.194s)
               Value function loss: 0.0517
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 44.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.96s
                        Total time: 11380.95s
                               ETA: 1021383.5s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.716s, learning 0.163s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.88s
                        Total time: 11389.83s
                               ETA: 1021243.2s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.640s, learning 0.203s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0449
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 45.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 8.84s
                        Total time: 11398.67s
                               ETA: 1021100.0s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.464s, learning 0.182s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.65s
                        Total time: 11407.32s
                               ETA: 1020939.4s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.912s, learning 0.175s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 43.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 9.09s
                        Total time: 11416.40s
                               ETA: 1020818.5s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.649s, learning 0.184s)
               Value function loss: 0.0556
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 44.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 8.83s
                        Total time: 11425.24s
                               ETA: 1020675.1s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.162s)
               Value function loss: 15.2270
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.88
               Mean episode length: 45.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.36s
                        Total time: 11433.60s
                               ETA: 1020489.7s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.100s, learning 0.217s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0492
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 44.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 8.32s
                        Total time: 11441.91s
                               ETA: 1020301.0s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.355s, learning 0.254s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 44.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 8.61s
                        Total time: 11450.52s
                               ETA: 1020138.4s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.609s, learning 0.156s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 44.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.77s
                        Total time: 11459.29s
                               ETA: 1019990.1s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.358s, learning 0.173s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0453
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 8.53s
                        Total time: 11467.82s
                               ETA: 1019821.2s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.748s, learning 0.236s)
               Value function loss: 131.8566
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 11.11
               Mean episode length: 44.30
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.98s
                        Total time: 11476.80s
                               ETA: 1019692.9s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.435s, learning 0.158s)
               Value function loss: 0.1953
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 44.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.59s
                        Total time: 11485.40s
                               ETA: 1019529.9s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.626s, learning 0.229s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 45.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.86s
                        Total time: 11494.25s
                               ETA: 1019390.6s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.209s, learning 0.173s)
               Value function loss: 0.0951
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 45.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.38s
                        Total time: 11502.63s
                               ETA: 1019209.7s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.106s, learning 0.248s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.35s
                        Total time: 11510.99s
                               ETA: 1019026.4s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.522s, learning 0.209s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 3.94
               Mean episode length: 44.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.73s
                        Total time: 11519.72s
                               ETA: 1018876.9s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1786 steps/s (collection: 8.890s, learning 0.282s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 9.17s
                        Total time: 11528.89s
                               ETA: 1018766.5s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.353s, learning 0.171s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 43.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.52s
                        Total time: 11537.41s
                               ETA: 1018599.1s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.651s, learning 0.200s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 43.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.85s
                        Total time: 11546.27s
                               ETA: 1018461.0s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.158s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 43.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.68s
                        Total time: 11554.95s
                               ETA: 1018308.1s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.784s, learning 0.168s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 44.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.95s
                        Total time: 11563.90s
                               ETA: 1018179.3s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.223s, learning 0.272s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0457
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.50s
                        Total time: 11572.40s
                               ETA: 1018010.4s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.165s)
               Value function loss: 0.0572
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 8.37s
                        Total time: 11580.77s
                               ETA: 1017831.2s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.661s, learning 0.185s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0485
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 44.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.85s
                        Total time: 11589.61s
                               ETA: 1017693.7s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.487s, learning 0.159s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0439
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 8.65s
                        Total time: 11598.26s
                               ETA: 1017539.0s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.778s, learning 0.278s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 9.06s
                        Total time: 11607.32s
                               ETA: 1017420.4s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.687s, learning 0.165s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.85s
                        Total time: 11616.17s
                               ETA: 1017284.2s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.872s, learning 0.160s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 9.03s
                        Total time: 11625.20s
                               ETA: 1017163.9s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.566s, learning 0.182s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 43.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 8.75s
                        Total time: 11633.95s
                               ETA: 1017019.0s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.650s, learning 0.229s)
               Value function loss: 0.0519
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 44.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.88s
                        Total time: 11642.83s
                               ETA: 1016885.8s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.173s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.65s
                        Total time: 11651.48s
                               ETA: 1016733.0s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.601s, learning 0.292s)
               Value function loss: 0.0515
                    Surrogate loss: -0.0449
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 44.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.89s
                        Total time: 11660.37s
                               ETA: 1016601.5s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.161s)
               Value function loss: 15.1114
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.69s
                        Total time: 11669.06s
                               ETA: 1016452.6s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.606s, learning 0.168s)
               Value function loss: 0.1595
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.77s
                        Total time: 11677.84s
                               ETA: 1016311.2s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.163s)
               Value function loss: 9.4676
                    Surrogate loss: 0.0045
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.49s
                        Total time: 11686.33s
                               ETA: 1016145.1s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.541s, learning 0.288s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0463
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.83s
                        Total time: 11695.16s
                               ETA: 1016008.9s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.301s, learning 0.195s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 44.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 8.50s
                        Total time: 11703.65s
                               ETA: 1015844.1s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.400s, learning 0.180s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.11
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.58s
                        Total time: 11712.23s
                               ETA: 1015686.8s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.400s, learning 0.267s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.67s
                        Total time: 11720.90s
                               ETA: 1015537.3s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.171s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 45.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.41s
                        Total time: 11729.31s
                               ETA: 1015365.6s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.292s, learning 0.179s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0447
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 8.47s
                        Total time: 11737.78s
                               ETA: 1015199.6s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.643s, learning 0.266s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0457
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 44.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 8.91s
                        Total time: 11746.69s
                               ETA: 1015071.8s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.667s, learning 0.163s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0462
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.83s
                        Total time: 11755.52s
                               ETA: 1014937.4s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.588s, learning 0.172s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.90
               Mean episode length: 44.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.76s
                        Total time: 11764.28s
                               ETA: 1014797.1s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.142s, learning 0.203s)
               Value function loss: 47.0348
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 44.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.34s
                        Total time: 11772.62s
                               ETA: 1014621.3s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.414s, learning 0.276s)
               Value function loss: 0.0984
                    Surrogate loss: -0.0489
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.69s
                        Total time: 11781.31s
                               ETA: 1014475.5s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.309s, learning 0.165s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 44.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.47s
                        Total time: 11789.78s
                               ETA: 1014311.4s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.457s, learning 0.205s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 43.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.66s
                        Total time: 11798.45s
                               ETA: 1014163.7s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.463s, learning 0.157s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 42.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.62s
                        Total time: 11807.07s
                               ETA: 1014012.6s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.365s, learning 0.198s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.56s
                        Total time: 11815.63s
                               ETA: 1013857.0s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.764s, learning 0.292s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 3.56
               Mean episode length: 43.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 9.06s
                        Total time: 11824.69s
                               ETA: 1013743.8s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.150s, learning 0.320s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 43.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.47s
                        Total time: 11833.16s
                               ETA: 1013580.6s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.932s, learning 0.159s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 43.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 9.09s
                        Total time: 11842.25s
                               ETA: 1013470.9s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.489s, learning 0.211s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 44.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.70s
                        Total time: 11850.95s
                               ETA: 1013327.8s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.519s, learning 0.175s)
               Value function loss: 16.4410
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 43.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.69s
                        Total time: 11859.64s
                               ETA: 1013184.5s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.678s, learning 0.329s)
               Value function loss: 0.1355
                    Surrogate loss: -0.0496
             Mean action noise std: 0.72
                       Mean reward: 3.93
               Mean episode length: 44.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 9.01s
                        Total time: 11868.65s
                               ETA: 1013068.1s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.432s, learning 0.171s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 43.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.60s
                        Total time: 11877.25s
                               ETA: 1012917.4s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.361s, learning 0.177s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 4.01
               Mean episode length: 43.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 8.54s
                        Total time: 11885.79s
                               ETA: 1012761.4s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.478s, learning 0.201s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 44.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 8.68s
                        Total time: 11894.47s
                               ETA: 1012617.8s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1100 steps/s (collection: 14.732s, learning 0.161s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0458
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 45.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 14.89s
                        Total time: 11909.36s
                               ETA: 1013002.9s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.580s, learning 0.167s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0454
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 44.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 16.75s
                        Total time: 11926.11s
                               ETA: 1013544.9s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.639s, learning 0.160s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 44.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 16.80s
                        Total time: 11942.91s
                               ETA: 1014090.4s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.773s, learning 0.203s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 44.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 16.98s
                        Total time: 11959.88s
                               ETA: 1014649.8s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.250s, learning 0.163s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 16.41s
                        Total time: 11976.30s
                               ETA: 1015160.6s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.558s, learning 0.170s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 43.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 16.73s
                        Total time: 11993.02s
                               ETA: 1015697.2s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.832s, learning 0.254s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 43.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 17.09s
                        Total time: 12010.11s
                               ETA: 1016263.1s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.595s, learning 0.165s)
               Value function loss: 17.3159
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 4.05
               Mean episode length: 45.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 16.76s
                        Total time: 12026.87s
                               ETA: 1016800.4s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.080s, learning 0.253s)
               Value function loss: 14.8170
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 43.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 17.33s
                        Total time: 12044.20s
                               ETA: 1017385.2s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.031s, learning 0.162s)
               Value function loss: 0.6055
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 45.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 17.19s
                        Total time: 12061.40s
                               ETA: 1017957.1s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.702s, learning 0.169s)
               Value function loss: 0.1703
                    Surrogate loss: -0.0454
             Mean action noise std: 0.72
                       Mean reward: 6.30
               Mean episode length: 43.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 16.87s
                        Total time: 12078.27s
                               ETA: 1018500.9s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.652s, learning 0.230s)
               Value function loss: 0.1056
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 44.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 16.88s
                        Total time: 12095.15s
                               ETA: 1019044.7s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.062s, learning 0.184s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0469
             Mean action noise std: 0.72
                       Mean reward: 4.18
               Mean episode length: 45.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 17.25s
                        Total time: 12112.40s
                               ETA: 1019618.1s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.734s, learning 0.163s)
               Value function loss: 15.2364
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 16.90s
                        Total time: 12129.29s
                               ETA: 1020161.2s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.822s, learning 0.262s)
               Value function loss: 29.3673
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 44.79
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 17.08s
                        Total time: 12146.38s
                               ETA: 1020719.0s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.918s, learning 0.178s)
               Value function loss: 15.0617
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 44.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 17.10s
                        Total time: 12163.47s
                               ETA: 1021276.9s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.815s, learning 0.174s)
               Value function loss: 0.3241
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 43.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 16.99s
                        Total time: 12180.46s
                               ETA: 1021824.8s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.889s, learning 0.162s)
               Value function loss: 0.1864
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 4.06
               Mean episode length: 44.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 17.05s
                        Total time: 12197.51s
                               ETA: 1022376.9s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.387s, learning 0.243s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 44.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 16.63s
                        Total time: 12214.14s
                               ETA: 1022892.9s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 947 steps/s (collection: 16.890s, learning 0.395s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0473
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 45.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 17.29s
                        Total time: 12231.43s
                               ETA: 1023462.8s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.782s, learning 0.167s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0475
             Mean action noise std: 0.72
                       Mean reward: 3.89
               Mean episode length: 45.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 16.95s
                        Total time: 12248.37s
                               ETA: 1024003.5s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.724s, learning 0.238s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 45.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 16.96s
                        Total time: 12265.34s
                               ETA: 1024544.4s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.563s, learning 0.279s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0472
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 44.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 16.84s
                        Total time: 12282.18s
                               ETA: 1025074.3s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.584s, learning 0.166s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 44.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 16.75s
                        Total time: 12298.93s
                               ETA: 1025595.7s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.464s, learning 0.183s)
               Value function loss: 15.2992
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 16.65s
                        Total time: 12315.58s
                               ETA: 1026107.6s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.375s, learning 0.163s)
               Value function loss: 0.0980
                    Surrogate loss: -0.0495
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 16.54s
                        Total time: 12332.11s
                               ETA: 1026609.4s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.007s, learning 0.180s)
               Value function loss: 11.9023
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 44.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 17.19s
                        Total time: 12349.30s
                               ETA: 1027164.5s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.869s, learning 0.206s)
               Value function loss: 0.0935
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 4.02
               Mean episode length: 43.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 17.08s
                        Total time: 12366.38s
                               ETA: 1027709.2s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.025s, learning 0.225s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0462
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 44.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 17.25s
                        Total time: 12383.62s
                               ETA: 1028267.5s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.160s, learning 0.189s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0447
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 44.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 17.35s
                        Total time: 12400.97s
                               ETA: 1028833.1s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.502s, learning 0.248s)
               Value function loss: 12.1514
                    Surrogate loss: 0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 16.75s
                        Total time: 12417.72s
                               ETA: 1029348.1s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.941s, learning 0.188s)
               Value function loss: 11.7519
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 6.30
               Mean episode length: 44.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 17.13s
                        Total time: 12434.85s
                               ETA: 1029893.5s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.903s, learning 0.167s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0500
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 43.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 17.07s
                        Total time: 12451.92s
                               ETA: 1030433.2s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 934 steps/s (collection: 17.346s, learning 0.188s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 3.90
               Mean episode length: 44.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 17.53s
                        Total time: 12469.46s
                               ETA: 1031010.2s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.431s, learning 0.169s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0482
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 44.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 16.60s
                        Total time: 12486.06s
                               ETA: 1031509.1s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.701s, learning 0.162s)
               Value function loss: 3.7771
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 44.67
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 16.86s
                        Total time: 12502.92s
                               ETA: 1032028.8s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.547s, learning 0.167s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 44.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 16.71s
                        Total time: 12519.63s
                               ETA: 1032535.5s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.736s, learning 0.283s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0435
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 43.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 16.02s
                        Total time: 12535.65s
                               ETA: 1032983.9s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.175s)
               Value function loss: 47.1723
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 43.57
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 8.67s
                        Total time: 12544.33s
                               ETA: 1032826.7s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.361s, learning 0.258s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0498
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 44.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 8.62s
                        Total time: 12552.95s
                               ETA: 1032665.3s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.178s, learning 0.375s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0459
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 9.55s
                        Total time: 12562.50s
                               ETA: 1032580.9s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.587s, learning 0.301s)
               Value function loss: 0.0672
                    Surrogate loss: -0.0481
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 44.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.89s
                        Total time: 12571.39s
                               ETA: 1032442.1s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.551s, learning 0.268s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0476
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.82s
                        Total time: 12580.21s
                               ETA: 1032297.8s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.882s, learning 0.249s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 44.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 9.13s
                        Total time: 12589.34s
                               ETA: 1032179.3s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.777s, learning 0.202s)
               Value function loss: 9.7088
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.84
               Mean episode length: 44.50
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.98s
                        Total time: 12598.32s
                               ETA: 1032048.6s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.518s, learning 0.225s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0483
             Mean action noise std: 0.72
                       Mean reward: 3.84
               Mean episode length: 43.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.74s
                        Total time: 12607.06s
                               ETA: 1031898.7s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.774s, learning 0.209s)
               Value function loss: 0.0673
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 43.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.98s
                        Total time: 12616.04s
                               ETA: 1031768.7s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.641s, learning 0.271s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 44.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.91s
                        Total time: 12624.95s
                               ETA: 1031633.1s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.787s, learning 0.221s)
               Value function loss: 16.4597
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 43.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 9.01s
                        Total time: 12633.96s
                               ETA: 1031505.5s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.574s, learning 0.166s)
               Value function loss: 59.2186
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 44.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.74s
                        Total time: 12642.70s
                               ETA: 1031356.2s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.426s, learning 0.173s)
               Value function loss: 0.3301
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 42.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.60s
                        Total time: 12651.30s
                               ETA: 1031195.8s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.502s, learning 0.165s)
               Value function loss: 16.1456
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: 6.35
               Mean episode length: 42.90
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.67s
                        Total time: 12659.97s
                               ETA: 1031041.1s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.567s, learning 0.164s)
               Value function loss: 0.2764
                    Surrogate loss: -0.0498
             Mean action noise std: 0.72
                       Mean reward: 3.89
               Mean episode length: 43.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.73s
                        Total time: 12668.70s
                               ETA: 1030891.8s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.927s, learning 0.224s)
               Value function loss: 81.3870
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 43.01
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 9.15s
                        Total time: 12677.85s
                               ETA: 1030776.9s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.864s, learning 0.179s)
               Value function loss: 0.4210
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 43.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 9.04s
                        Total time: 12686.89s
                               ETA: 1030653.5s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.503s, learning 0.165s)
               Value function loss: 101.0769
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 43.09
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 8.67s
                        Total time: 12695.56s
                               ETA: 1030499.7s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.646s, learning 0.161s)
               Value function loss: 10.1963
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 43.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.81s
                        Total time: 12704.37s
                               ETA: 1030357.5s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.804s, learning 0.161s)
               Value function loss: 34.5154
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.03
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.96s
                        Total time: 12713.33s
                               ETA: 1030228.3s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.575s, learning 0.210s)
               Value function loss: 0.6159
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 3.84
               Mean episode length: 43.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.78s
                        Total time: 12722.12s
                               ETA: 1030084.7s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.728s, learning 0.167s)
               Value function loss: 62.6153
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.88
               Mean episode length: 44.35
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.89s
                        Total time: 12731.01s
                               ETA: 1029950.2s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.502s, learning 0.182s)
               Value function loss: 14.5236
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 43.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.68s
                        Total time: 12739.69s
                               ETA: 1029798.9s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.471s, learning 0.166s)
               Value function loss: 0.7312
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.64s
                        Total time: 12748.33s
                               ETA: 1029644.0s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.684s, learning 0.161s)
               Value function loss: 0.1773
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 43.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.85s
                        Total time: 12757.18s
                               ETA: 1029506.2s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.683s, learning 0.181s)
               Value function loss: 0.1229
                    Surrogate loss: -0.0403
             Mean action noise std: 0.72
                       Mean reward: 4.03
               Mean episode length: 44.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.86s
                        Total time: 12766.04s
                               ETA: 1029370.1s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.713s, learning 0.340s)
               Value function loss: 3.9063
                    Surrogate loss: 0.0019
             Mean action noise std: 0.72
                       Mean reward: 6.59
               Mean episode length: 44.62
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 9.05s
                        Total time: 12775.09s
                               ETA: 1029249.4s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.785s, learning 0.163s)
               Value function loss: 0.0987
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 44.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 8.95s
                        Total time: 12784.04s
                               ETA: 1029120.5s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.946s, learning 0.203s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0458
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 43.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 9.15s
                        Total time: 12793.19s
                               ETA: 1029007.9s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.840s, learning 0.221s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 9.06s
                        Total time: 12802.25s
                               ETA: 1028888.4s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.344s, learning 0.183s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 43.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.53s
                        Total time: 12810.78s
                               ETA: 1028726.2s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.731s, learning 0.280s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0469
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 9.01s
                        Total time: 12819.79s
                               ETA: 1028603.1s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.415s, learning 0.183s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 43.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.60s
                        Total time: 12828.39s
                               ETA: 1028447.1s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.506s, learning 0.336s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0500
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 44.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.84s
                        Total time: 12837.23s
                               ETA: 1028310.9s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.676s, learning 0.163s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0514
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 44.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.84s
                        Total time: 12846.07s
                               ETA: 1028174.7s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.557s, learning 0.194s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0487
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 43.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.75s
                        Total time: 12854.82s
                               ETA: 1028031.6s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.101s, learning 0.337s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 43.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.44s
                        Total time: 12863.26s
                               ETA: 1027863.7s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.684s, learning 0.178s)
               Value function loss: 33.9747
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 44.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.86s
                        Total time: 12872.12s
                               ETA: 1027729.9s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.267s, learning 0.232s)
               Value function loss: 0.1512
                    Surrogate loss: -0.0479
             Mean action noise std: 0.72
                       Mean reward: 3.93
               Mean episode length: 44.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 9.50s
                        Total time: 12881.62s
                               ETA: 1027647.2s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.752s, learning 0.163s)
               Value function loss: 13.8604
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 44.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.91s
                        Total time: 12890.53s
                               ETA: 1027518.0s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.461s, learning 0.162s)
               Value function loss: 9.6828
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 6.26
               Mean episode length: 43.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.62s
                        Total time: 12899.16s
                               ETA: 1027365.8s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.501s, learning 0.160s)
               Value function loss: 0.1607
                    Surrogate loss: -0.0519
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.66s
                        Total time: 12907.82s
                               ETA: 1027216.8s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.380s, learning 0.196s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 44.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.58s
                        Total time: 12916.39s
                               ETA: 1027061.3s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.835s, learning 0.200s)
               Value function loss: 7.1020
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 44.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 9.03s
                        Total time: 12925.43s
                               ETA: 1026942.4s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.479s, learning 0.182s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0533
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 44.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.66s
                        Total time: 12934.09s
                               ETA: 1026794.1s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.605s, learning 0.159s)
               Value function loss: 3.9096
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.76s
                        Total time: 12942.85s
                               ETA: 1026654.2s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.595s, learning 0.188s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0481
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 43.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.78s
                        Total time: 12951.64s
                               ETA: 1026515.9s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.741s, learning 0.231s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 44.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.97s
                        Total time: 12960.61s
                               ETA: 1026392.8s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.619s, learning 0.189s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 44.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.81s
                        Total time: 12969.42s
                               ETA: 1026256.9s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.577s, learning 0.167s)
               Value function loss: 0.0586
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 44.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.74s
                        Total time: 12978.16s
                               ETA: 1026116.2s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.700s, learning 0.162s)
               Value function loss: 16.6475
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 44.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.86s
                        Total time: 12987.02s
                               ETA: 1025985.1s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.483s, learning 0.233s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0524
             Mean action noise std: 0.72
                       Mean reward: 3.45
               Mean episode length: 44.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.72s
                        Total time: 12995.74s
                               ETA: 1025842.6s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.159s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 43.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.61s
                        Total time: 13004.35s
                               ETA: 1025692.1s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.170s)
               Value function loss: 29.3714
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 44.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.53s
                        Total time: 13012.88s
                               ETA: 1025535.3s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.164s)
               Value function loss: 0.1462
                    Surrogate loss: -0.0512
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 44.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.61s
                        Total time: 13021.49s
                               ETA: 1025384.9s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.413s, learning 0.159s)
               Value function loss: 27.1233
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 44.56
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.57s
                        Total time: 13030.06s
                               ETA: 1025232.0s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.220s, learning 0.159s)
               Value function loss: 0.2302
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 43.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.38s
                        Total time: 13038.44s
                               ETA: 1025064.2s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.573s, learning 0.179s)
               Value function loss: 3.9443
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 44.74
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.75s
                        Total time: 13047.19s
                               ETA: 1024925.8s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.565s, learning 0.190s)
               Value function loss: 0.1321
                    Surrogate loss: -0.0508
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 44.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.76s
                        Total time: 13055.95s
                               ETA: 1024787.9s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.736s, learning 0.208s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0502
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 44.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.94s
                        Total time: 13064.89s
                               ETA: 1024665.1s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.610s, learning 0.185s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0489
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 44.03
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.80s
                        Total time: 13073.68s
                               ETA: 1024530.7s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.162s)
               Value function loss: 53.8700
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 44.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.70s
                        Total time: 13082.38s
                               ETA: 1024389.0s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.816s, learning 0.174s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0501
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 43.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.99s
                        Total time: 13091.37s
                               ETA: 1024270.3s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.372s, learning 0.205s)
               Value function loss: 19.0726
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: 8.97
               Mean episode length: 44.74
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.58s
                        Total time: 13099.95s
                               ETA: 1024119.5s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.649s, learning 0.173s)
               Value function loss: 13.7556
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 44.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.82s
                        Total time: 13108.77s
                               ETA: 1023988.0s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.524s, learning 0.167s)
               Value function loss: 13.5847
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 44.73
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.69s
                        Total time: 13117.46s
                               ETA: 1023846.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.788s, learning 0.171s)
               Value function loss: 0.2822
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 44.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.96s
                        Total time: 13126.42s
                               ETA: 1023726.2s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.270s, learning 0.191s)
               Value function loss: 0.1447
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 43.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.46s
                        Total time: 13134.88s
                               ETA: 1023567.3s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.314s, learning 0.220s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0476
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 43.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.53s
                        Total time: 13143.42s
                               ETA: 1023414.2s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.139s, learning 0.157s)
               Value function loss: 15.1402
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 6.21
               Mean episode length: 43.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 8.30s
                        Total time: 13151.72s
                               ETA: 1023242.8s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.335s, learning 0.202s)
               Value function loss: 0.1225
                    Surrogate loss: -0.0523
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 43.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.54s
                        Total time: 13160.25s
                               ETA: 1023090.5s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.166s)
               Value function loss: 46.7371
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 44.29
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.39s
                        Total time: 13168.65s
                               ETA: 1022927.3s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.497s, learning 0.159s)
               Value function loss: 13.8579
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 44.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.66s
                        Total time: 13177.30s
                               ETA: 1022784.5s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.161s)
               Value function loss: 0.1976
                    Surrogate loss: -0.0500
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 44.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.53s
                        Total time: 13185.83s
                               ETA: 1022632.3s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.406s, learning 0.230s)
               Value function loss: 19.4425
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 44.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.64s
                        Total time: 13194.47s
                               ETA: 1022488.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.630s, learning 0.227s)
               Value function loss: 12.0040
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 44.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.86s
                        Total time: 13203.33s
                               ETA: 1022362.0s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.676s, learning 0.159s)
               Value function loss: 13.9756
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.88
               Mean episode length: 44.33
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.84s
                        Total time: 13212.16s
                               ETA: 1022234.0s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.169s)
               Value function loss: 17.1069
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.57s
                        Total time: 13220.73s
                               ETA: 1022085.9s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.452s, learning 0.183s)
               Value function loss: 0.5480
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 3.45
               Mean episode length: 43.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.64s
                        Total time: 13229.37s
                               ETA: 1021942.9s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.312s, learning 0.187s)
               Value function loss: 0.3421
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 43.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.50s
                        Total time: 13237.87s
                               ETA: 1021789.5s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.648s, learning 0.210s)
               Value function loss: 0.1848
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 6.35
               Mean episode length: 44.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.86s
                        Total time: 13246.73s
                               ETA: 1021664.2s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.481s, learning 0.176s)
               Value function loss: 0.1468
                    Surrogate loss: -0.0426
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 43.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.66s
                        Total time: 13255.38s
                               ETA: 1021523.4s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.457s, learning 0.178s)
               Value function loss: 7.1503
                    Surrogate loss: 0.0046
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 43.87
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.63s
                        Total time: 13264.02s
                               ETA: 1021381.1s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.580s, learning 0.212s)
               Value function loss: 34.0407
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 6.20
               Mean episode length: 44.07
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.79s
                        Total time: 13272.81s
                               ETA: 1021251.2s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.637s, learning 0.208s)
               Value function loss: 58.8637
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 8.70
               Mean episode length: 44.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.85s
                        Total time: 13281.65s
                               ETA: 1021125.5s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.112s, learning 0.160s)
               Value function loss: 0.5750
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 43.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.27s
                        Total time: 13289.93s
                               ETA: 1020956.0s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.366s, learning 0.179s)
               Value function loss: 0.2610
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 43.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.55s
                        Total time: 13298.47s
                               ETA: 1020807.7s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.734s, learning 0.211s)
               Value function loss: 0.1579
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 45.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.95s
                        Total time: 13307.42s
                               ETA: 1020690.3s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.710s, learning 0.159s)
               Value function loss: 16.5821
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 44.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.87s
                        Total time: 13316.29s
                               ETA: 1020567.2s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.726s, learning 0.210s)
               Value function loss: 54.0061
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 6.44
               Mean episode length: 44.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.94s
                        Total time: 13325.22s
                               ETA: 1020449.4s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.738s, learning 0.163s)
               Value function loss: 17.7034
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.01
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.90s
                        Total time: 13334.12s
                               ETA: 1020329.2s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.450s, learning 0.171s)
               Value function loss: 23.2210
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 43.17
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.62s
                        Total time: 13342.74s
                               ETA: 1020187.7s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.204s, learning 0.162s)
               Value function loss: 0.9477
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 8.75
               Mean episode length: 43.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.37s
                        Total time: 13351.11s
                               ETA: 1020026.9s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.718s, learning 0.163s)
               Value function loss: 10.0202
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 43.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.88s
                        Total time: 13359.99s
                               ETA: 1019905.7s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.164s)
               Value function loss: 0.3316
                    Surrogate loss: -0.0275
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 43.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.61s
                        Total time: 13368.60s
                               ETA: 1019763.8s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.408s, learning 0.163s)
               Value function loss: 0.1818
                    Surrogate loss: -0.0462
             Mean action noise std: 0.72
                       Mean reward: 3.84
               Mean episode length: 44.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 8.57s
                        Total time: 13377.17s
                               ETA: 1019619.3s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.510s, learning 0.224s)
               Value function loss: 0.1195
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 3.95
               Mean episode length: 43.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.73s
                        Total time: 13385.90s
                               ETA: 1019487.4s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.582s, learning 0.166s)
               Value function loss: 0.1106
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 44.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.75s
                        Total time: 13394.65s
                               ETA: 1019356.8s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.419s, learning 0.164s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 43.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.58s
                        Total time: 13403.24s
                               ETA: 1019213.9s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.764s, learning 0.321s)
               Value function loss: 0.0923
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 44.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 9.08s
                        Total time: 13412.32s
                               ETA: 1019109.2s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.514s, learning 0.166s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0473
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 43.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 8.68s
                        Total time: 13421.00s
                               ETA: 1018974.0s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.773s, learning 0.171s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0498
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 43.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.94s
                        Total time: 13429.94s
                               ETA: 1018858.9s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.442s, learning 0.186s)
               Value function loss: 15.3078
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.96
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.63s
                        Total time: 13438.57s
                               ETA: 1018720.1s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.692s, learning 0.201s)
               Value function loss: 3.9428
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 43.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 8.89s
                        Total time: 13447.46s
                               ETA: 1018601.6s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.232s, learning 0.175s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 43.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.41s
                        Total time: 13455.87s
                               ETA: 1018446.4s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.159s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.89
               Mean episode length: 44.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 8.41s
                        Total time: 13464.28s
                               ETA: 1018291.8s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.371s, learning 0.162s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 43.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.53s
                        Total time: 13472.82s
                               ETA: 1018146.6s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.377s, learning 0.265s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 43.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.64s
                        Total time: 13481.46s
                               ETA: 1018009.9s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.447s, learning 0.159s)
               Value function loss: 32.8090
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 43.72
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.61s
                        Total time: 13490.06s
                               ETA: 1017870.7s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.170s)
               Value function loss: 64.0493
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 43.39
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 8.59s
                        Total time: 13498.66s
                               ETA: 1017730.5s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.556s, learning 0.223s)
               Value function loss: 4.0914
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 43.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.78s
                        Total time: 13507.43s
                               ETA: 1017604.7s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.629s, learning 0.174s)
               Value function loss: 0.3298
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 43.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.80s
                        Total time: 13516.24s
                               ETA: 1017480.9s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.596s, learning 0.188s)
               Value function loss: 13.7337
                    Surrogate loss: 0.0033
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 43.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.78s
                        Total time: 13525.02s
                               ETA: 1017355.9s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.463s, learning 0.173s)
               Value function loss: 0.2265
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 43.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.64s
                        Total time: 13533.66s
                               ETA: 1017219.8s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.664s, learning 0.372s)
               Value function loss: 0.1429
                    Surrogate loss: -0.0478
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 43.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 9.04s
                        Total time: 13542.69s
                               ETA: 1017114.0s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.708s, learning 0.159s)
               Value function loss: 17.6259
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 6.24
               Mean episode length: 43.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.87s
                        Total time: 13551.56s
                               ETA: 1016995.7s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.562s, learning 0.160s)
               Value function loss: 0.1364
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 43.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.72s
                        Total time: 13560.28s
                               ETA: 1016866.7s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.464s, learning 0.184s)
               Value function loss: 18.9635
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 8.70
               Mean episode length: 43.70
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 8.65s
                        Total time: 13568.93s
                               ETA: 1016732.2s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.495s, learning 0.164s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0505
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 43.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 8.66s
                        Total time: 13577.59s
                               ETA: 1016598.9s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.275s, learning 0.157s)
               Value function loss: 17.6709
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.48
               Mean episode length: 42.34
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 8.43s
                        Total time: 13586.02s
                               ETA: 1016448.7s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.335s, learning 0.166s)
               Value function loss: 0.1695
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 43.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 8.50s
                        Total time: 13594.52s
                               ETA: 1016303.8s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.342s, learning 0.205s)
               Value function loss: 0.1186
                    Surrogate loss: -0.0483
             Mean action noise std: 0.72
                       Mean reward: 6.39
               Mean episode length: 45.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 8.55s
                        Total time: 13603.07s
                               ETA: 1016162.7s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.161s)
               Value function loss: 7.1703
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 6.11
               Mean episode length: 43.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 8.56s
                        Total time: 13611.63s
                               ETA: 1016023.1s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1108 steps/s (collection: 14.616s, learning 0.159s)
               Value function loss: 17.6589
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 43.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 14.78s
                        Total time: 13626.41s
                               ETA: 1016346.9s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.528s, learning 0.199s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0495
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 44.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 16.73s
                        Total time: 13643.14s
                               ETA: 1016815.6s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.779s, learning 0.252s)
               Value function loss: 93.3277
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 8.74
               Mean episode length: 44.44
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 17.03s
                        Total time: 13660.17s
                               ETA: 1017306.2s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.078s, learning 0.172s)
               Value function loss: 0.2089
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 43.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 17.25s
                        Total time: 13677.42s
                               ETA: 1017812.4s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.409s, learning 0.158s)
               Value function loss: 0.1526
                    Surrogate loss: -0.0436
             Mean action noise std: 0.72
                       Mean reward: 3.56
               Mean episode length: 43.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 16.57s
                        Total time: 13693.99s
                               ETA: 1018267.0s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.721s, learning 0.169s)
               Value function loss: 16.4714
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 44.22
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 16.89s
                        Total time: 13710.88s
                               ETA: 1018744.9s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.722s, learning 0.163s)
               Value function loss: 0.1479
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 16.88s
                        Total time: 13727.76s
                               ETA: 1019221.6s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.649s, learning 0.288s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 43.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 16.94s
                        Total time: 13744.70s
                               ETA: 1019701.5s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.621s, learning 0.199s)
               Value function loss: 7.1120
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 6.11
               Mean episode length: 44.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 16.82s
                        Total time: 13761.52s
                               ETA: 1020171.9s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.878s, learning 0.207s)
               Value function loss: 73.2012
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 44.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 17.08s
                        Total time: 13778.60s
                               ETA: 1020661.3s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.646s, learning 0.204s)
               Value function loss: 7.6885
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 44.91
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 16.85s
                        Total time: 13795.45s
                               ETA: 1021132.4s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.660s, learning 0.166s)
               Value function loss: 7.2313
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 11.21
               Mean episode length: 44.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 16.83s
                        Total time: 13812.28s
                               ETA: 1021601.1s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.844s, learning 0.199s)
               Value function loss: 38.9084
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 44.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 17.04s
                        Total time: 13829.32s
                               ETA: 1022085.1s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.993s, learning 0.173s)
               Value function loss: 14.7677
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 6.08
               Mean episode length: 45.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 17.17s
                        Total time: 13846.49s
                               ETA: 1022577.5s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.970s, learning 0.177s)
               Value function loss: 16.3814
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 43.88
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 17.15s
                        Total time: 13863.63s
                               ETA: 1023067.6s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.570s, learning 0.157s)
               Value function loss: 53.9321
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 8.65
               Mean episode length: 44.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 16.73s
                        Total time: 13880.36s
                               ETA: 1023526.1s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.698s, learning 0.184s)
               Value function loss: 0.5710
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 16.88s
                        Total time: 13897.24s
                               ETA: 1023995.2s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.320s, learning 0.164s)
               Value function loss: 0.2375
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 45.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 16.48s
                        Total time: 13913.73s
                               ETA: 1024434.3s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.709s, learning 0.234s)
               Value function loss: 15.9331
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 44.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 16.94s
                        Total time: 13930.67s
                               ETA: 1024906.5s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.726s, learning 0.163s)
               Value function loss: 0.1727
                    Surrogate loss: -0.0485
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 45.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 16.89s
                        Total time: 13947.56s
                               ETA: 1025374.1s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.510s, learning 0.191s)
               Value function loss: 33.5259
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 44.18
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 16.70s
                        Total time: 13964.26s
                               ETA: 1025827.0s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.927s, learning 0.189s)
               Value function loss: 64.8343
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 8.62
               Mean episode length: 44.90
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 17.12s
                        Total time: 13981.37s
                               ETA: 1026309.7s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.350s, learning 0.203s)
               Value function loss: 15.6425
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 44.23
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 16.55s
                        Total time: 13997.92s
                               ETA: 1026750.4s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.025s, learning 0.277s)
               Value function loss: 0.3640
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 6.23
               Mean episode length: 44.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 17.30s
                        Total time: 14015.23s
                               ETA: 1027245.4s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.403s, learning 0.207s)
               Value function loss: 0.2404
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 16.61s
                        Total time: 14031.84s
                               ETA: 1027688.8s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.602s, learning 0.175s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0477
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 45.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 16.78s
                        Total time: 14048.61s
                               ETA: 1028143.8s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.310s, learning 0.163s)
               Value function loss: 0.1167
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 44.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 16.47s
                        Total time: 14065.09s
                               ETA: 1028576.0s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.749s, learning 0.266s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 45.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 17.02s
                        Total time: 14082.10s
                               ETA: 1029047.0s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.089s, learning 0.164s)
               Value function loss: 0.0973
                    Surrogate loss: -0.0514
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 44.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 17.25s
                        Total time: 14099.35s
                               ETA: 1029534.7s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.696s, learning 0.165s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0478
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 44.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 16.86s
                        Total time: 14116.22s
                               ETA: 1029993.0s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.947s, learning 0.186s)
               Value function loss: 12.0247
                    Surrogate loss: 0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 44.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 17.13s
                        Total time: 14133.35s
                               ETA: 1030470.5s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.712s, learning 0.168s)
               Value function loss: 0.0893
                    Surrogate loss: -0.0477
             Mean action noise std: 0.72
                       Mean reward: 3.33
               Mean episode length: 44.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 16.88s
                        Total time: 14150.23s
                               ETA: 1030928.8s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.314s, learning 0.274s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 3.22
               Mean episode length: 43.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 16.59s
                        Total time: 14166.82s
                               ETA: 1031365.1s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.561s, learning 0.158s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0453
             Mean action noise std: 0.72
                       Mean reward: 3.31
               Mean episode length: 44.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 16.72s
                        Total time: 14183.53s
                               ETA: 1031810.3s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.096s, learning 0.168s)
               Value function loss: 0.0676
                    Surrogate loss: -0.0502
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 44.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 17.26s
                        Total time: 14200.80s
                               ETA: 1032294.5s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.794s, learning 0.251s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0509
             Mean action noise std: 0.72
                       Mean reward: 3.27
               Mean episode length: 44.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 17.05s
                        Total time: 14217.84s
                               ETA: 1032762.0s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.398s, learning 0.161s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0469
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 16.56s
                        Total time: 14234.40s
                               ETA: 1033193.5s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.432s, learning 0.169s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 44.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 16.60s
                        Total time: 14251.00s
                               ETA: 1033627.5s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.398s, learning 0.156s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0509
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 45.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.55s
                        Total time: 14259.56s
                               ETA: 1033477.5s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.525s, learning 0.164s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0479
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 45.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.69s
                        Total time: 14268.25s
                               ETA: 1033337.6s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.786s, learning 0.183s)
               Value function loss: 9.7098
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 43.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.97s
                        Total time: 14277.22s
                               ETA: 1033218.0s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.391s, learning 0.293s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0507
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 44.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.68s
                        Total time: 14285.90s
                               ETA: 1033078.0s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.777s, learning 0.167s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0472
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 45.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.94s
                        Total time: 14294.85s
                               ETA: 1032957.0s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.783s, learning 0.165s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 44.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.95s
                        Total time: 14303.79s
                               ETA: 1032836.4s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.175s)
               Value function loss: 17.9400
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 44.93
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.65s
                        Total time: 14312.45s
                               ETA: 1032694.9s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.516s, learning 0.163s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 44.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.68s
                        Total time: 14321.13s
                               ETA: 1032555.3s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.239s, learning 0.174s)
               Value function loss: 18.0153
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 8.70
               Mean episode length: 45.26
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.41s
                        Total time: 14329.54s
                               ETA: 1032396.7s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.857s, learning 0.166s)
               Value function loss: 150.0901
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 44.67
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 9.02s
                        Total time: 14338.56s
                               ETA: 1032282.3s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.685s, learning 0.163s)
               Value function loss: 0.2276
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 44.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.85s
                        Total time: 14347.41s
                               ETA: 1032155.4s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.501s, learning 0.287s)
               Value function loss: 0.1589
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 45.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.79s
                        Total time: 14356.20s
                               ETA: 1032024.3s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.665s, learning 0.187s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0470
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 45.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.85s
                        Total time: 14365.05s
                               ETA: 1031898.1s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.559s, learning 0.182s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0439
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 44.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 8.74s
                        Total time: 14373.79s
                               ETA: 1031764.1s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.465s, learning 0.187s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0421
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 46.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.65s
                        Total time: 14382.44s
                               ETA: 1031623.9s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.730s, learning 0.220s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 44.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.95s
                        Total time: 14391.39s
                               ETA: 1031505.2s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.547s, learning 0.162s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0453
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 44.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.71s
                        Total time: 14400.10s
                               ETA: 1031369.5s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.712s, learning 0.190s)
               Value function loss: 7.1467
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 45.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.90s
                        Total time: 14409.01s
                               ETA: 1031247.7s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.497s, learning 0.309s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0508
             Mean action noise std: 0.72
                       Mean reward: 3.10
               Mean episode length: 44.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.81s
                        Total time: 14417.81s
                               ETA: 1031119.2s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.162s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0463
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 44.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.44s
                        Total time: 14426.25s
                               ETA: 1030964.7s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.597s, learning 0.198s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 3.25
               Mean episode length: 45.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.80s
                        Total time: 14435.05s
                               ETA: 1030835.8s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.683s, learning 0.166s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0494
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 45.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.85s
                        Total time: 14443.89s
                               ETA: 1030710.9s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.924s, learning 0.179s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 45.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 9.10s
                        Total time: 14453.00s
                               ETA: 1030604.3s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.662s, learning 0.167s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0483
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 45.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.83s
                        Total time: 14461.83s
                               ETA: 1030478.3s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.402s, learning 0.286s)
               Value function loss: 7.2445
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 44.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.69s
                        Total time: 14470.51s
                               ETA: 1030342.4s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.694s, learning 0.163s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0515
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 45.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.86s
                        Total time: 14479.37s
                               ETA: 1030218.8s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.597s, learning 0.281s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 45.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.88s
                        Total time: 14488.25s
                               ETA: 1030096.8s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.535s, learning 0.178s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0471
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 45.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.71s
                        Total time: 14496.96s
                               ETA: 1029963.2s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.561s, learning 0.186s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 44.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.75s
                        Total time: 14505.71s
                               ETA: 1029832.3s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.419s, learning 0.170s)
               Value function loss: 17.4736
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 44.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.59s
                        Total time: 14514.30s
                               ETA: 1029690.3s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.167s)
               Value function loss: 0.0970
                    Surrogate loss: -0.0516
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 46.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.35s
                        Total time: 14522.65s
                               ETA: 1029531.5s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.683s, learning 0.335s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0457
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 44.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 9.02s
                        Total time: 14531.67s
                               ETA: 1029420.3s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.168s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0482
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 45.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.62s
                        Total time: 14540.29s
                               ETA: 1029281.0s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.660s, learning 0.171s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0502
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 44.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.83s
                        Total time: 14549.12s
                               ETA: 1029156.9s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.623s, learning 0.193s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0472
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 44.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.82s
                        Total time: 14557.93s
                               ETA: 1029031.9s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.449s, learning 0.158s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0483
             Mean action noise std: 0.72
                       Mean reward: 3.48
               Mean episode length: 44.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.61s
                        Total time: 14566.54s
                               ETA: 1028892.3s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.630s, learning 0.193s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0462
             Mean action noise std: 0.72
                       Mean reward: 3.31
               Mean episode length: 45.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.82s
                        Total time: 14575.36s
                               ETA: 1028768.2s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.298s, learning 0.172s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 45.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.47s
                        Total time: 14583.83s
                               ETA: 1028619.2s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.630s, learning 0.310s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0500
             Mean action noise std: 0.72
                       Mean reward: 3.09
               Mean episode length: 44.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.94s
                        Total time: 14592.77s
                               ETA: 1028503.6s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.575s, learning 0.187s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 45.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.76s
                        Total time: 14601.53s
                               ETA: 1028375.6s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.697s, learning 0.183s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 45.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.88s
                        Total time: 14610.41s
                               ETA: 1028256.2s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.479s, learning 0.173s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 44.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.65s
                        Total time: 14619.07s
                               ETA: 1028120.8s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.591s, learning 0.185s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0454
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 44.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.78s
                        Total time: 14627.84s
                               ETA: 1027994.4s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.748s, learning 0.165s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0469
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 45.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.91s
                        Total time: 14636.76s
                               ETA: 1027877.6s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.515s, learning 0.172s)
               Value function loss: 3.9155
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 44.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.69s
                        Total time: 14645.44s
                               ETA: 1027745.3s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.721s, learning 0.174s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0489
             Mean action noise std: 0.72
                       Mean reward: 6.18
               Mean episode length: 45.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.90s
                        Total time: 14654.34s
                               ETA: 1027627.6s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.622s, learning 0.156s)
               Value function loss: 59.7553
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 44.91
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 8.78s
                        Total time: 14663.12s
                               ETA: 1027502.0s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.165s)
               Value function loss: 0.1395
                    Surrogate loss: -0.0468
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 45.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 8.49s
                        Total time: 14671.60s
                               ETA: 1027356.1s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.413s, learning 0.269s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 45.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.68s
                        Total time: 14680.29s
                               ETA: 1027224.1s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.639s, learning 0.162s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 8.88
               Mean episode length: 45.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 8.80s
                        Total time: 14689.09s
                               ETA: 1027100.5s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.342s, learning 0.165s)
               Value function loss: 17.4045
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 6.30
               Mean episode length: 46.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.51s
                        Total time: 14697.59s
                               ETA: 1026956.6s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.454s, learning 0.294s)
               Value function loss: 0.1427
                    Surrogate loss: -0.0465
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 45.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.75s
                        Total time: 14706.34s
                               ETA: 1026829.7s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.829s, learning 0.164s)
               Value function loss: 0.1070
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 46.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.99s
                        Total time: 14715.33s
                               ETA: 1026720.1s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.775s, learning 0.169s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 45.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 8.94s
                        Total time: 14724.28s
                               ETA: 1026607.1s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.171s)
               Value function loss: 9.6207
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 6.00
               Mean episode length: 45.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.42s
                        Total time: 14732.70s
                               ETA: 1026458.0s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.312s, learning 0.164s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0505
             Mean action noise std: 0.72
                       Mean reward: 3.56
               Mean episode length: 45.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.48s
                        Total time: 14741.18s
                               ETA: 1026312.8s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.243s, learning 0.171s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 45.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.41s
                        Total time: 14749.59s
                               ETA: 1026163.5s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.674s, learning 0.174s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 44.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.85s
                        Total time: 14758.44s
                               ETA: 1026044.6s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.631s, learning 0.305s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0433
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 46.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 8.94s
                        Total time: 14767.38s
                               ETA: 1025931.9s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.609s, learning 0.159s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 45.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.77s
                        Total time: 14776.14s
                               ETA: 1025807.8s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.595s, learning 0.247s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0472
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 45.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.84s
                        Total time: 14784.99s
                               ETA: 1025688.9s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.440s, learning 0.184s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 45.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.62s
                        Total time: 14793.61s
                               ETA: 1025555.1s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.764s, learning 0.200s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 46.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.96s
                        Total time: 14802.57s
                               ETA: 1025445.0s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.735s, learning 0.188s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 45.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.92s
                        Total time: 14811.50s
                               ETA: 1025332.1s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.575s, learning 0.166s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 45.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.74s
                        Total time: 14820.24s
                               ETA: 1025206.9s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.180s, learning 0.230s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0465
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 46.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.41s
                        Total time: 14828.65s
                               ETA: 1025058.9s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.502s, learning 0.191s)
               Value function loss: 15.3133
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 46.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.69s
                        Total time: 14837.34s
                               ETA: 1024930.7s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.039s, learning 0.166s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 45.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 9.21s
                        Total time: 14846.55s
                               ETA: 1024838.0s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.639s, learning 0.159s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0485
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 45.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.80s
                        Total time: 14855.35s
                               ETA: 1024717.4s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.611s, learning 0.174s)
               Value function loss: 0.0940
                    Surrogate loss: -0.0477
             Mean action noise std: 0.72
                       Mean reward: 6.14
               Mean episode length: 46.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 8.78s
                        Total time: 14864.13s
                               ETA: 1024596.0s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.391s, learning 0.172s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 46.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.56s
                        Total time: 14872.69s
                               ETA: 1024459.4s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.677s, learning 0.177s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0478
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 45.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.85s
                        Total time: 14881.55s
                               ETA: 1024343.0s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.580s, learning 0.162s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0453
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 46.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.74s
                        Total time: 14890.29s
                               ETA: 1024219.2s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.165s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 45.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.39s
                        Total time: 14898.68s
                               ETA: 1024071.4s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.308s, learning 0.264s)
               Value function loss: 42.9949
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 8.78
               Mean episode length: 46.88
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.57s
                        Total time: 14907.25s
                               ETA: 1023936.2s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.162s)
               Value function loss: 0.1648
                    Surrogate loss: -0.0435
             Mean action noise std: 0.72
                       Mean reward: 3.56
               Mean episode length: 45.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.56s
                        Total time: 14915.81s
                               ETA: 1023800.0s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.365s, learning 0.283s)
               Value function loss: 0.1352
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 46.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.65s
                        Total time: 14924.46s
                               ETA: 1023670.3s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.603s, learning 0.226s)
               Value function loss: 29.6010
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 47.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.83s
                        Total time: 14933.29s
                               ETA: 1023553.2s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.638s, learning 0.194s)
               Value function loss: 4.0318
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 3.99
               Mean episode length: 48.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.83s
                        Total time: 14942.12s
                               ETA: 1023436.5s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.285s, learning 0.174s)
               Value function loss: 0.1691
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 47.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 8.46s
                        Total time: 14950.58s
                               ETA: 1023294.4s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.758s, learning 0.199s)
               Value function loss: 0.1270
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.21
               Mean episode length: 44.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.96s
                        Total time: 14959.53s
                               ETA: 1023186.5s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.651s, learning 0.172s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 3.79
               Mean episode length: 46.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.82s
                        Total time: 14968.36s
                               ETA: 1023069.6s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.575s, learning 0.167s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0482
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 47.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.74s
                        Total time: 14977.10s
                               ETA: 1022947.3s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.159s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0457
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 46.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 8.69s
                        Total time: 14985.79s
                               ETA: 1022821.5s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.393s, learning 0.183s)
               Value function loss: 17.8627
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 46.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.58s
                        Total time: 14994.36s
                               ETA: 1022688.2s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.845s, learning 0.166s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0493
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 45.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 9.01s
                        Total time: 15003.37s
                               ETA: 1022584.8s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.507s, learning 0.191s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 46.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.70s
                        Total time: 15012.07s
                               ETA: 1022460.1s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.759s, learning 0.190s)
               Value function loss: 83.3234
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 4.10
               Mean episode length: 47.44
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.95s
                        Total time: 15021.02s
                               ETA: 1022352.7s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.580s, learning 0.192s)
               Value function loss: 0.2754
                    Surrogate loss: -0.0426
             Mean action noise std: 0.72
                       Mean reward: 8.71
               Mean episode length: 45.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.77s
                        Total time: 15029.79s
                               ETA: 1022233.4s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.494s, learning 0.188s)
               Value function loss: 0.1534
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 47.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.68s
                        Total time: 15038.47s
                               ETA: 1022108.1s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.683s, learning 0.208s)
               Value function loss: 15.3336
                    Surrogate loss: 0.0024
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 46.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.89s
                        Total time: 15047.37s
                               ETA: 1021997.1s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.434s, learning 0.190s)
               Value function loss: 0.1494
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 5.98
               Mean episode length: 46.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.62s
                        Total time: 15055.99s
                               ETA: 1021868.2s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.643s, learning 0.196s)
               Value function loss: 4.0563
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 45.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.84s
                        Total time: 15064.83s
                               ETA: 1021754.0s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.612s, learning 0.185s)
               Value function loss: 301.6699
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 47.20
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.80s
                        Total time: 15073.62s
                               ETA: 1021637.2s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.571s, learning 0.166s)
               Value function loss: 93.6082
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 46.44
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.74s
                        Total time: 15082.36s
                               ETA: 1021516.5s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.500s, learning 0.195s)
               Value function loss: 0.4160
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 45.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.69s
                        Total time: 15091.06s
                               ETA: 1021393.0s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.616s, learning 0.177s)
               Value function loss: 0.2269
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 47.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.79s
                        Total time: 15099.85s
                               ETA: 1021276.3s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.873s, learning 0.189s)
               Value function loss: 0.1424
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 45.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 9.06s
                        Total time: 15108.91s
                               ETA: 1021178.0s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.347s, learning 0.183s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0410
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 47.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.53s
                        Total time: 15117.44s
                               ETA: 1021043.9s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.312s, learning 0.181s)
               Value function loss: 17.7565
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 45.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.49s
                        Total time: 15125.94s
                               ETA: 1020907.4s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.450s, learning 0.325s)
               Value function loss: 27.7386
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 47.10
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.77s
                        Total time: 15134.71s
                               ETA: 1020790.1s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.369s, learning 0.183s)
               Value function loss: 0.2203
                    Surrogate loss: -0.0414
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 47.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.55s
                        Total time: 15143.26s
                               ETA: 1020658.0s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.160s)
               Value function loss: 47.1077
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 8.68
               Mean episode length: 47.33
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.67s
                        Total time: 15151.93s
                               ETA: 1020533.8s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.755s, learning 0.223s)
               Value function loss: 0.1898
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 47.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 8.98s
                        Total time: 15160.91s
                               ETA: 1020430.6s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.767s, learning 0.166s)
               Value function loss: 0.1575
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 6.49
               Mean episode length: 47.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.93s
                        Total time: 15169.84s
                               ETA: 1020324.5s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.702s, learning 0.169s)
               Value function loss: 0.1259
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 47.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.87s
                        Total time: 15178.71s
                               ETA: 1020214.4s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.447s, learning 0.188s)
               Value function loss: 3.9098
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 46.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.64s
                        Total time: 15187.35s
                               ETA: 1020088.7s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.454s, learning 0.158s)
               Value function loss: 12.1448
                    Surrogate loss: 0.0023
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 47.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.61s
                        Total time: 15195.96s
                               ETA: 1019961.5s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.846s, learning 0.161s)
               Value function loss: 193.3069
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 48.71
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 9.01s
                        Total time: 15204.97s
                               ETA: 1019861.0s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.493s, learning 0.172s)
               Value function loss: 0.8125
                    Surrogate loss: -0.0262
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 46.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.67s
                        Total time: 15213.63s
                               ETA: 1019737.7s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.588s, learning 0.295s)
               Value function loss: 0.5175
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 48.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.88s
                        Total time: 15222.52s
                               ETA: 1019629.1s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.401s, learning 0.163s)
               Value function loss: 17.6660
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: 3.63
               Mean episode length: 48.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.56s
                        Total time: 15231.08s
                               ETA: 1019499.4s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.556s, learning 0.203s)
               Value function loss: 3.9405
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 8.64
               Mean episode length: 47.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 8.76s
                        Total time: 15239.84s
                               ETA: 1019382.8s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.495s, learning 0.166s)
               Value function loss: 0.2814
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 47.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 8.66s
                        Total time: 15248.50s
                               ETA: 1019259.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.311s, learning 0.160s)
               Value function loss: 0.1688
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 47.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 8.47s
                        Total time: 15256.97s
                               ETA: 1019124.3s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.544s, learning 0.173s)
               Value function loss: 0.1304
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 48.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 8.72s
                        Total time: 15265.69s
                               ETA: 1019005.3s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.496s, learning 0.159s)
               Value function loss: 0.1062
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 47.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 8.65s
                        Total time: 15274.34s
                               ETA: 1018882.4s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.566s, learning 0.218s)
               Value function loss: 17.5502
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 47.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 8.78s
                        Total time: 15283.13s
                               ETA: 1018768.3s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.465s, learning 0.201s)
               Value function loss: 12.1290
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 47.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 8.67s
                        Total time: 15291.79s
                               ETA: 1018646.3s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.566s, learning 0.187s)
               Value function loss: 16.3239
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 6.35
               Mean episode length: 48.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 8.75s
                        Total time: 15300.54s
                               ETA: 1018530.4s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.562s, learning 0.340s)
               Value function loss: 0.3495
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.15
               Mean episode length: 46.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 8.90s
                        Total time: 15309.45s
                               ETA: 1018424.5s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.250s, learning 0.158s)
               Value function loss: 0.2032
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 3.54
               Mean episode length: 47.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 8.41s
                        Total time: 15317.86s
                               ETA: 1018286.0s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.456s, learning 0.189s)
               Value function loss: 0.1471
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 48.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 8.64s
                        Total time: 15326.50s
                               ETA: 1018163.3s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.755s, learning 0.198s)
               Value function loss: 25.6490
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.67
               Mean episode length: 46.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 8.95s
                        Total time: 15335.45s
                               ETA: 1018061.2s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.481s, learning 0.170s)
               Value function loss: 59.4652
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 46.34
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 8.65s
                        Total time: 15344.10s
                               ETA: 1017939.2s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.498s, learning 0.181s)
               Value function loss: 0.3392
                    Surrogate loss: -0.0410
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 47.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 8.68s
                        Total time: 15352.78s
                               ETA: 1017819.2s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.382s, learning 0.339s)
               Value function loss: 150.0439
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 47.83
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 8.72s
                        Total time: 15361.50s
                               ETA: 1017702.2s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.558s, learning 0.160s)
               Value function loss: 198.7971
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 48.23
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 8.72s
                        Total time: 15370.22s
                               ETA: 1017585.1s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.400s, learning 0.273s)
               Value function loss: 93.0001
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 3.86
               Mean episode length: 46.99
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 8.67s
                        Total time: 15378.89s
                               ETA: 1017465.2s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.314s, learning 0.158s)
               Value function loss: 1.1660
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 3.30
               Mean episode length: 46.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 8.47s
                        Total time: 15387.37s
                               ETA: 1017332.1s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.501s, learning 0.338s)
               Value function loss: 44.1684
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 47.37
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 8.84s
                        Total time: 15396.20s
                               ETA: 1017223.4s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.656s, learning 0.190s)
               Value function loss: 0.8007
                    Surrogate loss: -0.0336
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 47.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 8.85s
                        Total time: 15405.05s
                               ETA: 1017115.4s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1083 steps/s (collection: 14.961s, learning 0.165s)
               Value function loss: 46.0438
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 46.88
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 15.13s
                        Total time: 15420.18s
                               ETA: 1017421.8s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.627s, learning 0.234s)
               Value function loss: 0.4512
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 47.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 16.86s
                        Total time: 15437.04s
                               ETA: 1017842.2s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.208s, learning 0.172s)
               Value function loss: 16.4603
                    Surrogate loss: 0.0019
             Mean action noise std: 0.72
                       Mean reward: 3.43
               Mean episode length: 46.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 16.38s
                        Total time: 15453.42s
                               ETA: 1018230.3s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.697s, learning 0.286s)
               Value function loss: 0.5759
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 5.79
               Mean episode length: 46.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 16.98s
                        Total time: 15470.40s
                               ETA: 1018657.6s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.153s, learning 0.253s)
               Value function loss: 0.2937
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 47.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 16.41s
                        Total time: 15486.81s
                               ETA: 1019046.3s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.849s, learning 0.210s)
               Value function loss: 9.9548
                    Surrogate loss: 0.0073
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 46.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 17.06s
                        Total time: 15503.87s
                               ETA: 1019477.5s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.890s, learning 0.183s)
               Value function loss: 9.9705
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.23
               Mean episode length: 46.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 17.07s
                        Total time: 15520.94s
                               ETA: 1019908.9s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.417s, learning 0.164s)
               Value function loss: 0.1733
                    Surrogate loss: -0.0470
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 48.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 16.58s
                        Total time: 15537.52s
                               ETA: 1020307.5s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.520s, learning 0.175s)
               Value function loss: 0.1291
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 46.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 16.70s
                        Total time: 15554.21s
                               ETA: 1020712.9s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.982s, learning 0.161s)
               Value function loss: 16.9894
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.11
               Mean episode length: 47.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 16.14s
                        Total time: 15570.36s
                               ETA: 1021081.7s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.039s, learning 0.176s)
               Value function loss: 0.1302
                    Surrogate loss: -0.0496
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 47.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 16.22s
                        Total time: 15586.57s
                               ETA: 1021454.6s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.728s, learning 0.182s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0476
             Mean action noise std: 0.72
                       Mean reward: 3.11
               Mean episode length: 47.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 16.91s
                        Total time: 15603.48s
                               ETA: 1021872.5s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.873s, learning 0.184s)
               Value function loss: 16.5797
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 3.13
               Mean episode length: 46.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 17.06s
                        Total time: 15620.54s
                               ETA: 1022299.4s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.380s, learning 0.204s)
               Value function loss: 0.1441
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 5.77
               Mean episode length: 46.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 16.58s
                        Total time: 15637.12s
                               ETA: 1022694.9s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.617s, learning 0.169s)
               Value function loss: 14.0348
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.24
               Mean episode length: 46.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 16.79s
                        Total time: 15653.91s
                               ETA: 1023103.0s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.018s, learning 0.161s)
               Value function loss: 4.0324
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 3.15
               Mean episode length: 46.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 17.18s
                        Total time: 15671.09s
                               ETA: 1023536.2s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.386s, learning 0.157s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 46.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 16.54s
                        Total time: 15687.63s
                               ETA: 1023927.3s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.529s, learning 0.162s)
               Value function loss: 0.1052
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 3.22
               Mean episode length: 46.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 16.69s
                        Total time: 15704.32s
                               ETA: 1024327.5s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.400s, learning 0.276s)
               Value function loss: 7.0756
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 5.60
               Mean episode length: 45.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 16.68s
                        Total time: 15721.00s
                               ETA: 1024726.1s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.788s, learning 0.178s)
               Value function loss: 3.9577
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 5.89
               Mean episode length: 47.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 16.97s
                        Total time: 15737.96s
                               ETA: 1025143.1s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.818s, learning 0.211s)
               Value function loss: 13.9177
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.17
               Mean episode length: 47.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 17.03s
                        Total time: 15754.99s
                               ETA: 1025563.7s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.004s, learning 0.179s)
               Value function loss: 0.1599
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 46.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 17.18s
                        Total time: 15772.18s
                               ETA: 1025993.7s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.581s, learning 0.175s)
               Value function loss: 61.6899
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.29
               Mean episode length: 47.01
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 16.76s
                        Total time: 15788.93s
                               ETA: 1026395.3s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.690s, learning 0.160s)
               Value function loss: 0.2765
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 8.58
               Mean episode length: 47.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 16.85s
                        Total time: 15805.78s
                               ETA: 1026802.4s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.535s, learning 0.183s)
               Value function loss: 35.0893
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 47.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 16.72s
                        Total time: 15822.50s
                               ETA: 1027200.5s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.746s, learning 0.180s)
               Value function loss: 4.2604
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 8.49
               Mean episode length: 48.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 16.93s
                        Total time: 15839.43s
                               ETA: 1027611.5s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.508s, learning 0.253s)
               Value function loss: 0.1984
                    Surrogate loss: -0.0429
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 46.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 16.76s
                        Total time: 15856.19s
                               ETA: 1028011.2s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.713s, learning 0.175s)
               Value function loss: 0.1259
                    Surrogate loss: -0.0459
             Mean action noise std: 0.72
                       Mean reward: 8.34
               Mean episode length: 46.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 16.89s
                        Total time: 15873.08s
                               ETA: 1028418.7s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.496s, learning 0.186s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0449
             Mean action noise std: 0.72
                       Mean reward: 3.27
               Mean episode length: 46.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 16.68s
                        Total time: 15889.76s
                               ETA: 1028812.2s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.418s, learning 0.173s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0505
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 46.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 16.59s
                        Total time: 15906.35s
                               ETA: 1029199.2s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.295s, learning 0.180s)
               Value function loss: 15.4610
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.07
               Mean episode length: 46.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 16.47s
                        Total time: 15922.82s
                               ETA: 1029578.3s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.655s, learning 0.257s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 3.29
               Mean episode length: 47.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 16.91s
                        Total time: 15939.74s
                               ETA: 1029985.1s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.165s, learning 0.164s)
               Value function loss: 0.1215
                    Surrogate loss: -0.0474
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 47.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 17.33s
                        Total time: 15957.06s
                               ETA: 1030418.3s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1005 steps/s (collection: 15.968s, learning 0.330s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0471
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 47.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 16.30s
                        Total time: 15973.36s
                               ETA: 1030784.3s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 954 steps/s (collection: 17.004s, learning 0.169s)
               Value function loss: 3.8551
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 46.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 17.17s
                        Total time: 15990.54s
                               ETA: 1031206.3s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.838s, learning 0.279s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0506
             Mean action noise std: 0.72
                       Mean reward: 3.15
               Mean episode length: 45.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 17.12s
                        Total time: 16007.65s
                               ETA: 1031624.0s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.535s, learning 0.164s)
               Value function loss: 0.0875
                    Surrogate loss: -0.0475
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 46.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 16.70s
                        Total time: 16024.35s
                               ETA: 1032014.3s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.458s, learning 0.167s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0446
             Mean action noise std: 0.72
                       Mean reward: 3.14
               Mean episode length: 46.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 15.63s
                        Total time: 16039.98s
                               ETA: 1032335.0s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.164s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 47.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 8.37s
                        Total time: 16048.34s
                               ETA: 1032188.3s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.210s, learning 0.267s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 46.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 8.48s
                        Total time: 16056.82s
                               ETA: 1032048.9s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.619s, learning 0.470s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0435
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 46.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 9.09s
                        Total time: 16065.91s
                               ETA: 1031949.1s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.524s, learning 0.245s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0449
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 46.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.77s
                        Total time: 16074.68s
                               ETA: 1031828.8s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.639s, learning 0.205s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0436
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 47.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.84s
                        Total time: 16083.52s
                               ETA: 1031713.4s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.660s, learning 0.199s)
               Value function loss: 132.4979
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 47.85
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.86s
                        Total time: 16092.38s
                               ETA: 1031599.2s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.820s, learning 0.395s)
               Value function loss: 93.3254
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.22
               Mean episode length: 46.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 9.22s
                        Total time: 16101.60s
                               ETA: 1031507.9s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.334s, learning 0.163s)
               Value function loss: 0.5485
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 48.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 8.50s
                        Total time: 16110.09s
                               ETA: 1031370.7s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.344s, learning 0.255s)
               Value function loss: 0.2428
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 48.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 8.60s
                        Total time: 16118.69s
                               ETA: 1031240.3s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.417s, learning 0.205s)
               Value function loss: 0.1576
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 46.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.62s
                        Total time: 16127.31s
                               ETA: 1031111.4s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.364s, learning 0.266s)
               Value function loss: 17.7086
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 47.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.63s
                        Total time: 16135.94s
                               ETA: 1030983.2s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.331s, learning 0.292s)
               Value function loss: 0.1416
                    Surrogate loss: -0.0475
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 47.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.62s
                        Total time: 16144.57s
                               ETA: 1030854.7s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1758 steps/s (collection: 8.971s, learning 0.346s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 46.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 9.32s
                        Total time: 16153.89s
                               ETA: 1030770.7s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.168s)
               Value function loss: 0.1084
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 46.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.55s
                        Total time: 16162.44s
                               ETA: 1030638.2s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.399s, learning 0.418s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 46.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.82s
                        Total time: 16171.26s
                               ETA: 1030522.5s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.726s, learning 0.304s)
               Value function loss: 63.3068
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 46.83
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 9.03s
                        Total time: 16180.29s
                               ETA: 1030420.5s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.389s, learning 0.265s)
               Value function loss: 0.1748
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 3.90
               Mean episode length: 48.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.65s
                        Total time: 16188.94s
                               ETA: 1030294.8s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.507s, learning 0.339s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 46.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 8.85s
                        Total time: 16197.79s
                               ETA: 1030181.3s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.784s, learning 0.197s)
               Value function loss: 0.1390
                    Surrogate loss: -0.0341
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 47.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 8.98s
                        Total time: 16206.77s
                               ETA: 1030076.6s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.332s, learning 0.224s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 46.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.56s
                        Total time: 16215.32s
                               ETA: 1029945.1s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1783 steps/s (collection: 8.971s, learning 0.216s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 46.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 9.19s
                        Total time: 16224.51s
                               ETA: 1029853.8s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.511s, learning 0.170s)
               Value function loss: 0.1148
                    Surrogate loss: -0.0465
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 46.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.68s
                        Total time: 16233.19s
                               ETA: 1029730.4s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.568s, learning 0.157s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 3.66
               Mean episode length: 47.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.73s
                        Total time: 16241.92s
                               ETA: 1029610.0s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.720s, learning 0.286s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 4.04
               Mean episode length: 48.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 9.01s
                        Total time: 16250.92s
                               ETA: 1029507.6s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.135s, learning 0.346s)
               Value function loss: 69.0989
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 4.04
               Mean episode length: 48.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.48s
                        Total time: 16259.40s
                               ETA: 1029371.9s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.248s, learning 0.161s)
               Value function loss: 58.6973
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3.97
               Mean episode length: 47.50
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.41s
                        Total time: 16267.81s
                               ETA: 1029231.9s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.257s, learning 0.263s)
               Value function loss: 1.0453
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 47.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.52s
                        Total time: 16276.33s
                               ETA: 1029099.1s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.573s, learning 0.167s)
               Value function loss: 0.5858
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 4.04
               Mean episode length: 48.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.74s
                        Total time: 16285.07s
                               ETA: 1028980.4s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.554s, learning 0.160s)
               Value function loss: 19.0361
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 11.61
               Mean episode length: 46.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.71s
                        Total time: 16293.79s
                               ETA: 1028860.2s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.562s, learning 0.172s)
               Value function loss: 26.6360
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 4.11
               Mean episode length: 47.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.73s
                        Total time: 16302.52s
                               ETA: 1028741.3s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.473s, learning 0.190s)
               Value function loss: 21.4332
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 48.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.66s
                        Total time: 16311.18s
                               ETA: 1028618.1s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.666s, learning 0.188s)
               Value function loss: 44.4989
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 9.04
               Mean episode length: 47.01
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.85s
                        Total time: 16320.04s
                               ETA: 1028507.1s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.659s, learning 0.170s)
               Value function loss: 40.1061
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 9.70
               Mean episode length: 49.87
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 8.83s
                        Total time: 16328.87s
                               ETA: 1028394.7s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.405s, learning 0.193s)
               Value function loss: 108.0142
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 48.11
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.60s
                        Total time: 16337.46s
                               ETA: 1028267.8s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.799s, learning 0.188s)
               Value function loss: 0.7584
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 9.30
               Mean episode length: 48.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.99s
                        Total time: 16346.45s
                               ETA: 1028165.6s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.542s, learning 0.169s)
               Value function loss: 0.4665
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 9.48
               Mean episode length: 49.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.71s
                        Total time: 16355.16s
                               ETA: 1028046.1s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.646s, learning 0.163s)
               Value function loss: 54.3554
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 9.53
               Mean episode length: 49.69
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.81s
                        Total time: 16363.97s
                               ETA: 1027933.0s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.384s, learning 0.262s)
               Value function loss: 77.2057
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.97
               Mean episode length: 49.45
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.65s
                        Total time: 16372.62s
                               ETA: 1027809.7s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.379s, learning 0.162s)
               Value function loss: 16.8905
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 48.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.54s
                        Total time: 16381.16s
                               ETA: 1027680.0s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.674s, learning 0.174s)
               Value function loss: 161.0592
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 48.70
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.85s
                        Total time: 16390.00s
                               ETA: 1027569.7s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.607s, learning 0.174s)
               Value function loss: 46.8775
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 9.13
               Mean episode length: 48.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.78s
                        Total time: 16398.78s
                               ETA: 1027455.4s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.638s, learning 0.189s)
               Value function loss: 68.0078
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 6.77
               Mean episode length: 48.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.83s
                        Total time: 16407.61s
                               ETA: 1027344.0s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.666s, learning 0.306s)
               Value function loss: 1.8179
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 4.04
               Mean episode length: 49.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.97s
                        Total time: 16416.58s
                               ETA: 1027241.9s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.353s, learning 0.231s)
               Value function loss: 54.5534
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: 4.18
               Mean episode length: 49.70
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.58s
                        Total time: 16425.17s
                               ETA: 1027115.6s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.159s)
               Value function loss: 49.6480
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 4.17
               Mean episode length: 49.39
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.70s
                        Total time: 16433.87s
                               ETA: 1026996.9s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.359s, learning 0.161s)
               Value function loss: 18.5638
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: 6.48
               Mean episode length: 48.16
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.52s
                        Total time: 16442.39s
                               ETA: 1026866.9s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.623s, learning 0.161s)
               Value function loss: 0.4230
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 9.24
               Mean episode length: 49.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.78s
                        Total time: 16451.17s
                               ETA: 1026753.6s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.168s)
               Value function loss: 10.0163
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 9.17
               Mean episode length: 49.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.72s
                        Total time: 16459.89s
                               ETA: 1026636.3s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.372s, learning 0.181s)
               Value function loss: 0.2685
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 6.59
               Mean episode length: 48.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.55s
                        Total time: 16468.45s
                               ETA: 1026508.9s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.837s, learning 0.199s)
               Value function loss: 0.2446
                    Surrogate loss: -0.0276
             Mean action noise std: 0.72
                       Mean reward: 4.28
               Mean episode length: 49.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 9.04s
                        Total time: 16477.48s
                               ETA: 1026411.6s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.516s, learning 0.190s)
               Value function loss: 0.1925
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 4.07
               Mean episode length: 49.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.71s
                        Total time: 16486.19s
                               ETA: 1026293.9s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.253s, learning 0.203s)
               Value function loss: 15.2322
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: 3.95
               Mean episode length: 48.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.46s
                        Total time: 16494.65s
                               ETA: 1026160.9s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.258s, learning 0.181s)
               Value function loss: 16.6033
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 4.06
               Mean episode length: 49.99
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.44s
                        Total time: 16503.08s
                               ETA: 1026026.9s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.204s, learning 0.280s)
               Value function loss: 76.0605
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 4.05
               Mean episode length: 49.89
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.48s
                        Total time: 16511.57s
                               ETA: 1025895.8s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.828s, learning 0.184s)
               Value function loss: 0.6013
                    Surrogate loss: -0.0250
             Mean action noise std: 0.72
                       Mean reward: 3.97
               Mean episode length: 49.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 9.01s
                        Total time: 16520.58s
                               ETA: 1025797.7s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.417s, learning 0.192s)
               Value function loss: 0.4637
                    Surrogate loss: -0.0265
             Mean action noise std: 0.72
                       Mean reward: 9.23
               Mean episode length: 49.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.61s
                        Total time: 16529.19s
                               ETA: 1025674.7s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.415s, learning 0.162s)
               Value function loss: 9.6050
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 4.22
               Mean episode length: 49.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.58s
                        Total time: 16537.77s
                               ETA: 1025549.9s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.324s, learning 0.284s)
               Value function loss: 17.7959
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 48.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.61s
                        Total time: 16546.37s
                               ETA: 1025427.1s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.446s, learning 0.168s)
               Value function loss: 18.0785
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.97
               Mean episode length: 48.27
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.61s
                        Total time: 16554.99s
                               ETA: 1025304.9s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.544s, learning 0.304s)
               Value function loss: 0.3135
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 3.48
               Mean episode length: 48.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.85s
                        Total time: 16563.84s
                               ETA: 1025197.3s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.593s, learning 0.159s)
               Value function loss: 18.2791
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 10.82
               Mean episode length: 47.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.75s
                        Total time: 16572.59s
                               ETA: 1025083.8s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.436s, learning 0.195s)
               Value function loss: 0.2254
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 3.45
               Mean episode length: 48.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.63s
                        Total time: 16581.22s
                               ETA: 1024963.1s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.582s, learning 0.187s)
               Value function loss: 29.9471
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 48.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.77s
                        Total time: 16589.99s
                               ETA: 1024851.0s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.587s, learning 0.184s)
               Value function loss: 0.2332
                    Surrogate loss: -0.0408
             Mean action noise std: 0.72
                       Mean reward: 3.90
               Mean episode length: 48.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.77s
                        Total time: 16598.76s
                               ETA: 1024739.1s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.593s, learning 0.287s)
               Value function loss: 0.1782
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 48.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.88s
                        Total time: 16607.64s
                               ETA: 1024634.0s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.162s)
               Value function loss: 0.1474
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 48.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.62s
                        Total time: 16616.26s
                               ETA: 1024513.2s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.691s, learning 0.271s)
               Value function loss: 0.1422
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 3.93
               Mean episode length: 50.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 8.96s
                        Total time: 16625.22s
                               ETA: 1024413.5s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.401s, learning 0.163s)
               Value function loss: 18.0485
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 48.67
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.56s
                        Total time: 16633.79s
                               ETA: 1024289.4s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.300s, learning 0.164s)
               Value function loss: 9.9558
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.22
               Mean episode length: 49.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.46s
                        Total time: 16642.25s
                               ETA: 1024159.3s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.336s, learning 0.163s)
               Value function loss: 0.1517
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 49.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.50s
                        Total time: 16650.75s
                               ETA: 1024031.5s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1752 steps/s (collection: 8.949s, learning 0.402s)
               Value function loss: 0.1452
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 8.10
               Mean episode length: 48.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 9.35s
                        Total time: 16660.10s
                               ETA: 1023956.2s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.319s, learning 0.171s)
               Value function loss: 17.8337
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 6.01
               Mean episode length: 48.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.49s
                        Total time: 16668.59s
                               ETA: 1023828.1s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.499s, learning 0.166s)
               Value function loss: 0.2070
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 48.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.66s
                        Total time: 16677.26s
                               ETA: 1023710.9s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.296s, learning 0.187s)
               Value function loss: 0.1799
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 48.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.48s
                        Total time: 16685.74s
                               ETA: 1023582.6s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.833s, learning 0.184s)
               Value function loss: 4.0216
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 6.01
               Mean episode length: 49.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 9.02s
                        Total time: 16694.75s
                               ETA: 1023487.2s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.192s, learning 0.208s)
               Value function loss: 0.1391
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 48.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.40s
                        Total time: 16703.15s
                               ETA: 1023354.2s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.654s, learning 0.165s)
               Value function loss: 11.8848
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 6.02
               Mean episode length: 48.32
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.82s
                        Total time: 16711.97s
                               ETA: 1023247.0s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.711s, learning 0.190s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 48.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.90s
                        Total time: 16720.87s
                               ETA: 1023144.9s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.574s, learning 0.160s)
               Value function loss: 7.2961
                    Surrogate loss: 0.0048
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 49.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.73s
                        Total time: 16729.61s
                               ETA: 1023032.7s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.680s, learning 0.227s)
               Value function loss: 0.1353
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 3.27
               Mean episode length: 48.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.91s
                        Total time: 16738.52s
                               ETA: 1022931.3s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.655s, learning 0.183s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 48.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.84s
                        Total time: 16747.35s
                               ETA: 1022825.7s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.499s, learning 0.172s)
               Value function loss: 0.1161
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 6.21
               Mean episode length: 49.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.67s
                        Total time: 16756.03s
                               ETA: 1022710.0s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.418s, learning 0.281s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 2.96
               Mean episode length: 48.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.70s
                        Total time: 16764.72s
                               ETA: 1022596.2s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.290s, learning 0.161s)
               Value function loss: 3.9666
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 3.21
               Mean episode length: 48.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.45s
                        Total time: 16773.18s
                               ETA: 1022467.4s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.800s, learning 0.259s)
               Value function loss: 16.2802
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 2.86
               Mean episode length: 47.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 9.06s
                        Total time: 16782.24s
                               ETA: 1022375.9s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.386s, learning 0.162s)
               Value function loss: 15.3006
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 5.92
               Mean episode length: 48.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.55s
                        Total time: 16790.78s
                               ETA: 1022253.2s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.476s, learning 0.164s)
               Value function loss: 0.3871
                    Surrogate loss: -0.0434
             Mean action noise std: 0.72
                       Mean reward: 2.95
               Mean episode length: 48.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.64s
                        Total time: 16799.42s
                               ETA: 1022136.3s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.744s, learning 0.161s)
               Value function loss: 0.2617
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 48.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.90s
                        Total time: 16808.33s
                               ETA: 1022035.6s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.633s, learning 0.163s)
               Value function loss: 16.7739
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 48.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.80s
                        Total time: 16817.12s
                               ETA: 1021928.5s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.386s, learning 0.333s)
               Value function loss: 0.2055
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 3.33
               Mean episode length: 48.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.72s
                        Total time: 16825.84s
                               ETA: 1021816.8s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.161s)
               Value function loss: 131.3875
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 49.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.71s
                        Total time: 16834.55s
                               ETA: 1021704.7s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.650s, learning 0.175s)
               Value function loss: 0.3651
                    Surrogate loss: -0.0280
             Mean action noise std: 0.72
                       Mean reward: 5.93
               Mean episode length: 47.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.83s
                        Total time: 16843.38s
                               ETA: 1021599.7s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.161s)
               Value function loss: 0.2990
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 49.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.67s
                        Total time: 16852.05s
                               ETA: 1021485.6s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.386s, learning 0.181s)
               Value function loss: 13.6547
                    Surrogate loss: 0.0056
             Mean action noise std: 0.72
                       Mean reward: 2.98
               Mean episode length: 47.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.57s
                        Total time: 16860.62s
                               ETA: 1021365.2s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.394s, learning 0.164s)
               Value function loss: 0.2842
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 48.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.56s
                        Total time: 16869.18s
                               ETA: 1021244.4s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.379s, learning 0.212s)
               Value function loss: 0.2840
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 48.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 8.59s
                        Total time: 16877.77s
                               ETA: 1021125.8s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.173s)
               Value function loss: 0.2255
                    Surrogate loss: -0.0271
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 47.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 8.54s
                        Total time: 16886.31s
                               ETA: 1021004.1s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.881s, learning 0.165s)
               Value function loss: 0.1883
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 3.43
               Mean episode length: 49.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 9.05s
                        Total time: 16895.36s
                               ETA: 1020913.2s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.724s, learning 0.180s)
               Value function loss: 59.2045
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 48.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.90s
                        Total time: 16904.26s
                               ETA: 1020813.9s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.219s, learning 0.183s)
               Value function loss: 16.1336
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 48.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 8.40s
                        Total time: 16912.66s
                               ETA: 1020684.3s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.697s, learning 0.198s)
               Value function loss: 0.5621
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 3.45
               Mean episode length: 49.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 8.89s
                        Total time: 16921.56s
                               ETA: 1020584.6s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.578s, learning 0.160s)
               Value function loss: 227.3827
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 48.38
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 8.74s
                        Total time: 16930.29s
                               ETA: 1020475.6s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.538s, learning 0.178s)
               Value function loss: 4.3388
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 3.84
               Mean episode length: 48.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 8.72s
                        Total time: 16939.01s
                               ETA: 1020365.4s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.318s, learning 0.355s)
               Value function loss: 0.4622
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 3.29
               Mean episode length: 48.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 8.67s
                        Total time: 16947.68s
                               ETA: 1020252.7s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.552s, learning 0.181s)
               Value function loss: 0.3170
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 48.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 8.73s
                        Total time: 16956.42s
                               ETA: 1020143.7s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.590s, learning 0.160s)
               Value function loss: 0.2420
                    Surrogate loss: -0.0310
             Mean action noise std: 0.72
                       Mean reward: 3.33
               Mean episode length: 48.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 8.75s
                        Total time: 16965.17s
                               ETA: 1020035.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.077s, learning 0.343s)
               Value function loss: 57.4091
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.60
               Mean episode length: 49.10
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 9.42s
                        Total time: 16974.59s
                               ETA: 1019968.4s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.445s, learning 0.183s)
               Value function loss: 0.2085
                    Surrogate loss: -0.0435
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 47.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 8.63s
                        Total time: 16983.21s
                               ETA: 1019853.4s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.228s, learning 0.167s)
               Value function loss: 0.1715
                    Surrogate loss: -0.0309
             Mean action noise std: 0.72
                       Mean reward: 3.97
               Mean episode length: 49.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 8.40s
                        Total time: 16991.61s
                               ETA: 1019724.7s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.659s, learning 0.192s)
               Value function loss: 0.1658
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 3.48
               Mean episode length: 47.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 8.85s
                        Total time: 17000.46s
                               ETA: 1019623.4s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.522s, learning 0.315s)
               Value function loss: 12.2527
                    Surrogate loss: 0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 47.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 8.84s
                        Total time: 17009.30s
                               ETA: 1019521.4s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.433s, learning 0.400s)
               Value function loss: 0.1530
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 48.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 8.83s
                        Total time: 17018.13s
                               ETA: 1019419.3s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.393s, learning 0.168s)
               Value function loss: 0.1424
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 49.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 8.56s
                        Total time: 17026.69s
                               ETA: 1019301.0s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.315s, learning 0.256s)
               Value function loss: 0.1171
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 48.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 8.57s
                        Total time: 17035.26s
                               ETA: 1019183.4s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.754s, learning 0.157s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 48.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 8.91s
                        Total time: 17044.18s
                               ETA: 1019086.3s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.604s, learning 0.280s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 48.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 8.88s
                        Total time: 17053.06s
                               ETA: 1018987.7s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.642s, learning 0.222s)
               Value function loss: 16.5649
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 47.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 8.86s
                        Total time: 17061.92s
                               ETA: 1018887.9s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.475s, learning 0.179s)
               Value function loss: 3.9805
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 2.93
               Mean episode length: 45.93
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 8.65s
                        Total time: 17070.58s
                               ETA: 1018775.8s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.513s, learning 0.325s)
               Value function loss: 26.2998
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 47.08
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 8.84s
                        Total time: 17079.42s
                               ETA: 1018674.7s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.797s, learning 0.207s)
               Value function loss: 0.3157
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 6.05
               Mean episode length: 47.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 9.00s
                        Total time: 17088.42s
                               ETA: 1018583.7s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.521s, learning 0.185s)
               Value function loss: 0.2400
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 47.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 8.71s
                        Total time: 17097.13s
                               ETA: 1018475.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.805s, learning 0.211s)
               Value function loss: 0.1678
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 6.18
               Mean episode length: 49.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 9.02s
                        Total time: 17106.14s
                               ETA: 1018384.9s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.553s, learning 0.161s)
               Value function loss: 144.8295
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 5.91
               Mean episode length: 47.48
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 8.71s
                        Total time: 17114.86s
                               ETA: 1018277.0s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.159s)
               Value function loss: 15.4998
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 47.89
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 8.58s
                        Total time: 17123.43s
                               ETA: 1018160.9s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.364s, learning 0.168s)
               Value function loss: 4.0660
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 3.00
               Mean episode length: 46.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 8.53s
                        Total time: 17131.96s
                               ETA: 1018042.3s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.641s, learning 0.298s)
               Value function loss: 0.2778
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 3.51
               Mean episode length: 47.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 8.94s
                        Total time: 17140.90s
                               ETA: 1017948.1s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.699s, learning 0.197s)
               Value function loss: 188.4359
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 46.70
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 8.90s
                        Total time: 17149.80s
                               ETA: 1017851.4s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.329s, learning 0.182s)
               Value function loss: 0.7388
                    Surrogate loss: -0.0289
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 47.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 8.51s
                        Total time: 17158.31s
                               ETA: 1017731.9s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.699s, learning 0.157s)
               Value function loss: 15.7008
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 6.01
               Mean episode length: 49.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.86s
                        Total time: 17167.16s
                               ETA: 1017633.1s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.632s, learning 0.168s)
               Value function loss: 4.0706
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 2.84
               Mean episode length: 46.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.80s
                        Total time: 17175.96s
                               ETA: 1017531.0s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.349s, learning 0.158s)
               Value function loss: 0.6105
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 3.69
               Mean episode length: 49.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 8.51s
                        Total time: 17184.47s
                               ETA: 1017411.7s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.314s, learning 0.161s)
               Value function loss: 18.5437
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.11
               Mean episode length: 47.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 8.48s
                        Total time: 17192.95s
                               ETA: 1017290.7s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.276s, learning 0.175s)
               Value function loss: 0.2457
                    Surrogate loss: -0.0453
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 49.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 8.45s
                        Total time: 17201.40s
                               ETA: 1017168.3s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1167 steps/s (collection: 13.873s, learning 0.159s)
               Value function loss: 16.8805
                    Surrogate loss: 0.0038
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 46.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 14.03s
                        Total time: 17215.43s
                               ETA: 1017376.0s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.016s, learning 0.167s)
               Value function loss: 0.1859
                    Surrogate loss: -0.0488
             Mean action noise std: 0.72
                       Mean reward: 3.02
               Mean episode length: 46.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 17.18s
                        Total time: 17232.61s
                               ETA: 1017769.4s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.353s, learning 0.167s)
               Value function loss: 16.5504
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 5.82
               Mean episode length: 47.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 16.52s
                        Total time: 17249.13s
                               ETA: 1018123.2s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.029s, learning 0.174s)
               Value function loss: 62.9491
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 48.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 17.20s
                        Total time: 17266.33s
                               ETA: 1018516.9s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.680s, learning 0.185s)
               Value function loss: 0.5077
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 3.10
               Mean episode length: 46.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 16.86s
                        Total time: 17283.20s
                               ETA: 1018890.2s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.048s, learning 0.159s)
               Value function loss: 302.2888
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 47.08
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 17.21s
                        Total time: 17300.41s
                               ETA: 1019283.1s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.539s, learning 0.200s)
               Value function loss: 0.6118
                    Surrogate loss: -0.0288
             Mean action noise std: 0.72
                       Mean reward: 3.07
               Mean episode length: 47.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 16.74s
                        Total time: 17317.15s
                               ETA: 1019648.0s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.255s, learning 0.164s)
               Value function loss: 0.4228
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 48.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 16.42s
                        Total time: 17333.56s
                               ETA: 1019993.7s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.244s, learning 0.170s)
               Value function loss: 67.8069
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 47.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 16.41s
                        Total time: 17349.98s
                               ETA: 1020338.6s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.564s, learning 0.284s)
               Value function loss: 46.2658
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 47.41
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 16.85s
                        Total time: 17366.83s
                               ETA: 1020708.5s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.209s, learning 0.158s)
               Value function loss: 92.9531
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 48.86
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 16.37s
                        Total time: 17383.19s
                               ETA: 1021049.8s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 936 steps/s (collection: 17.227s, learning 0.278s)
               Value function loss: 0.5607
                    Surrogate loss: -0.0344
             Mean action noise std: 0.72
                       Mean reward: 3.10
               Mean episode length: 47.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 17.50s
                        Total time: 17400.70s
                               ETA: 1021457.4s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.707s, learning 0.176s)
               Value function loss: 0.3842
                    Surrogate loss: -0.0315
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 48.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 16.88s
                        Total time: 17417.58s
                               ETA: 1021828.0s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.538s, learning 0.257s)
               Value function loss: 0.2747
                    Surrogate loss: -0.0284
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 47.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 16.79s
                        Total time: 17434.38s
                               ETA: 1022193.0s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.826s, learning 0.189s)
               Value function loss: 0.2386
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 3.02
               Mean episode length: 46.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 17.01s
                        Total time: 17451.39s
                               ETA: 1022570.4s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.429s, learning 0.283s)
               Value function loss: 0.1905
                    Surrogate loss: -0.0413
             Mean action noise std: 0.72
                       Mean reward: 3.02
               Mean episode length: 47.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 16.71s
                        Total time: 17468.10s
                               ETA: 1022929.6s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.644s, learning 0.164s)
               Value function loss: 0.1473
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 2.88
               Mean episode length: 47.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 16.81s
                        Total time: 17484.91s
                               ETA: 1023294.0s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.275s, learning 0.170s)
               Value function loss: 0.1391
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 3.15
               Mean episode length: 48.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 16.44s
                        Total time: 17501.35s
                               ETA: 1023636.6s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.391s, learning 0.157s)
               Value function loss: 14.2987
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 48.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 16.55s
                        Total time: 17517.90s
                               ETA: 1023985.0s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.563s, learning 0.166s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 47.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 16.73s
                        Total time: 17534.63s
                               ETA: 1024343.5s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.907s, learning 0.168s)
               Value function loss: 0.1365
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 2.98
               Mean episode length: 46.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 17.07s
                        Total time: 17551.71s
                               ETA: 1024721.7s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.745s, learning 0.300s)
               Value function loss: 0.1015
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 3.01
               Mean episode length: 46.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 17.05s
                        Total time: 17568.75s
                               ETA: 1025097.7s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.214s, learning 0.167s)
               Value function loss: 46.7571
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 5.79
               Mean episode length: 47.70
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 16.38s
                        Total time: 17585.14s
                               ETA: 1025434.5s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 950 steps/s (collection: 16.937s, learning 0.308s)
               Value function loss: 41.4527
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 3.08
               Mean episode length: 47.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 17.25s
                        Total time: 17602.38s
                               ETA: 1025821.3s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.728s, learning 0.172s)
               Value function loss: 21.3101
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 46.62
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 16.90s
                        Total time: 17619.28s
                               ETA: 1026187.4s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.820s, learning 0.157s)
               Value function loss: 0.7380
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 3.08
               Mean episode length: 46.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 16.98s
                        Total time: 17636.26s
                               ETA: 1026557.6s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.238s, learning 0.169s)
               Value function loss: 0.3391
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 47.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 16.41s
                        Total time: 17652.67s
                               ETA: 1026894.2s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 942 steps/s (collection: 17.054s, learning 0.324s)
               Value function loss: 0.2462
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 5.50
               Mean episode length: 46.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 17.38s
                        Total time: 17670.04s
                               ETA: 1027286.8s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.583s, learning 0.164s)
               Value function loss: 0.1911
                    Surrogate loss: -0.0462
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 47.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 16.75s
                        Total time: 17686.79s
                               ETA: 1027642.2s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.512s, learning 0.278s)
               Value function loss: 0.1399
                    Surrogate loss: -0.0471
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 46.03
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 16.79s
                        Total time: 17703.58s
                               ETA: 1027999.7s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.664s, learning 0.178s)
               Value function loss: 15.4999
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.21
               Mean episode length: 45.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 16.84s
                        Total time: 17720.42s
                               ETA: 1028359.8s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.286s, learning 0.363s)
               Value function loss: 39.2733
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 45.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 16.65s
                        Total time: 17737.07s
                               ETA: 1028708.2s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.901s, learning 0.178s)
               Value function loss: 0.1910
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 46.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 17.08s
                        Total time: 17754.15s
                               ETA: 1029081.1s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.838s, learning 0.179s)
               Value function loss: 11.9294
                    Surrogate loss: 0.0040
             Mean action noise std: 0.72
                       Mean reward: 3.40
               Mean episode length: 45.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 17.02s
                        Total time: 17771.17s
                               ETA: 1029450.0s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.505s, learning 0.273s)
               Value function loss: 0.2290
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 47.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 16.78s
                        Total time: 17787.94s
                               ETA: 1029804.6s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.588s, learning 0.180s)
               Value function loss: 17.4428
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 47.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 16.77s
                        Total time: 17804.71s
                               ETA: 1030158.2s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.820s, learning 0.159s)
               Value function loss: 4.0409
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 4.36
               Mean episode length: 49.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 16.98s
                        Total time: 17821.69s
                               ETA: 1030523.6s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.356s, learning 0.350s)
               Value function loss: 0.2410
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 46.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 16.71s
                        Total time: 17838.40s
                               ETA: 1030872.6s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.686s, learning 0.169s)
               Value function loss: 17.8453
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 46.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 8.86s
                        Total time: 17847.25s
                               ETA: 1030767.9s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.769s, learning 0.160s)
               Value function loss: 59.0219
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 6.58
               Mean episode length: 49.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 8.93s
                        Total time: 17856.18s
                               ETA: 1030667.5s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.167s)
               Value function loss: 0.3602
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 3.43
               Mean episode length: 47.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 8.40s
                        Total time: 17864.58s
                               ETA: 1030536.6s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.653s, learning 0.169s)
               Value function loss: 10.2285
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 4.39
               Mean episode length: 48.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 8.82s
                        Total time: 17873.40s
                               ETA: 1030430.3s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.220s, learning 0.268s)
               Value function loss: 0.2410
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 47.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 8.49s
                        Total time: 17881.89s
                               ETA: 1030304.9s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.207s, learning 0.159s)
               Value function loss: 17.4552
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 48.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 8.37s
                        Total time: 17890.25s
                               ETA: 1030172.6s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.604s, learning 0.166s)
               Value function loss: 4.1018
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3.83
               Mean episode length: 48.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 8.77s
                        Total time: 17899.02s
                               ETA: 1030063.7s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.624s, learning 0.191s)
               Value function loss: 98.8262
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.42
               Mean episode length: 46.70
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 8.82s
                        Total time: 17907.84s
                               ETA: 1029957.5s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.636s, learning 0.173s)
               Value function loss: 0.4820
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 6.05
               Mean episode length: 46.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 8.81s
                        Total time: 17916.65s
                               ETA: 1029851.0s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.531s, learning 0.160s)
               Value function loss: 0.3612
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 47.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 8.69s
                        Total time: 17925.34s
                               ETA: 1029738.0s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.152s, learning 0.326s)
               Value function loss: 0.2949
                    Surrogate loss: -0.0328
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 48.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 8.48s
                        Total time: 17933.82s
                               ETA: 1029612.8s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.591s, learning 0.166s)
               Value function loss: 16.5686
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.64
               Mean episode length: 48.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 8.76s
                        Total time: 17942.58s
                               ETA: 1029503.7s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.432s, learning 0.179s)
               Value function loss: 46.2043
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 4.25
               Mean episode length: 48.76
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 8.61s
                        Total time: 17951.19s
                               ETA: 1029386.4s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.337s, learning 0.206s)
               Value function loss: 80.2183
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 4.08
               Mean episode length: 47.93
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 8.54s
                        Total time: 17959.73s
                               ETA: 1029265.3s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.346s, learning 0.247s)
               Value function loss: 18.1471
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 6.71
               Mean episode length: 48.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 8.59s
                        Total time: 17968.32s
                               ETA: 1029147.2s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.162s)
               Value function loss: 1.1067
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 4.15
               Mean episode length: 48.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 8.63s
                        Total time: 17976.95s
                               ETA: 1029031.2s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.462s, learning 0.168s)
               Value function loss: 0.6399
                    Surrogate loss: -0.0415
             Mean action noise std: 0.72
                       Mean reward: 5.90
               Mean episode length: 47.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 8.63s
                        Total time: 17985.58s
                               ETA: 1028915.5s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.113s, learning 0.186s)
               Value function loss: 18.1660
                    Surrogate loss: 0.0058
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 48.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 8.30s
                        Total time: 17993.88s
                               ETA: 1028780.9s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.764s, learning 0.261s)
               Value function loss: 4.1627
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 48.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 9.03s
                        Total time: 18002.90s
                               ETA: 1028688.0s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.161s)
               Value function loss: 0.3578
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 49.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 8.67s
                        Total time: 18011.57s
                               ETA: 1028574.8s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.797s, learning 0.167s)
               Value function loss: 0.2830
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 4.02
               Mean episode length: 48.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 8.96s
                        Total time: 18020.54s
                               ETA: 1028478.6s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.493s, learning 0.277s)
               Value function loss: 64.5430
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 4.18
               Mean episode length: 49.55
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 8.77s
                        Total time: 18029.31s
                               ETA: 1028371.5s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.162s)
               Value function loss: 4.1351
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 47.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 8.43s
                        Total time: 18037.73s
                               ETA: 1028245.0s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.166s)
               Value function loss: 0.2105
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 48.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 8.63s
                        Total time: 18046.36s
                               ETA: 1028130.1s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.364s, learning 0.270s)
               Value function loss: 0.1795
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 3.85
               Mean episode length: 49.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 8.63s
                        Total time: 18055.00s
                               ETA: 1028015.6s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.460s, learning 0.266s)
               Value function loss: 0.1695
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 47.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 8.73s
                        Total time: 18063.73s
                               ETA: 1027906.5s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.601s, learning 0.224s)
               Value function loss: 0.1473
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 4.01
               Mean episode length: 48.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 8.83s
                        Total time: 18072.55s
                               ETA: 1027803.1s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.357s, learning 0.184s)
               Value function loss: 0.1434
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 48.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 8.54s
                        Total time: 18081.09s
                               ETA: 1027683.6s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.810s, learning 0.181s)
               Value function loss: 17.6879
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.56
               Mean episode length: 48.03
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 8.99s
                        Total time: 18090.08s
                               ETA: 1027589.8s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.738s, learning 0.160s)
               Value function loss: 78.0690
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 4.11
               Mean episode length: 49.86
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 8.90s
                        Total time: 18098.98s
                               ETA: 1027490.8s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.316s, learning 0.165s)
               Value function loss: 17.5069
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 47.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 8.48s
                        Total time: 18107.46s
                               ETA: 1027368.3s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.730s, learning 0.159s)
               Value function loss: 18.2159
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 13.94
               Mean episode length: 48.58
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 8.89s
                        Total time: 18116.35s
                               ETA: 1027269.1s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.528s, learning 0.174s)
               Value function loss: 58.3224
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 48.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.70s
                        Total time: 18125.05s
                               ETA: 1027159.5s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.536s, learning 0.340s)
               Value function loss: 0.7095
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 49.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 8.88s
                        Total time: 18133.93s
                               ETA: 1027059.7s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.234s, learning 0.156s)
               Value function loss: 0.4821
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 4.26
               Mean episode length: 49.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 8.39s
                        Total time: 18142.32s
                               ETA: 1026932.6s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.278s, learning 0.421s)
               Value function loss: 70.6378
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 47.89
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 8.70s
                        Total time: 18151.02s
                               ETA: 1026823.0s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.404s, learning 0.217s)
               Value function loss: 39.1023
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 48.06
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.62s
                        Total time: 18159.64s
                               ETA: 1026709.2s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.801s, learning 0.293s)
               Value function loss: 0.6460
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 4.06
               Mean episode length: 48.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 9.09s
                        Total time: 18168.73s
                               ETA: 1026622.1s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.488s, learning 0.252s)
               Value function loss: 0.3932
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 4.09
               Mean episode length: 48.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.74s
                        Total time: 18177.47s
                               ETA: 1026515.2s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.149s, learning 0.283s)
               Value function loss: 9.8375
                    Surrogate loss: 0.0064
             Mean action noise std: 0.72
                       Mean reward: 4.34
               Mean episode length: 50.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 8.43s
                        Total time: 18185.90s
                               ETA: 1026391.1s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.353s, learning 0.161s)
               Value function loss: 134.9027
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 4.08
               Mean episode length: 48.67
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.51s
                        Total time: 18194.42s
                               ETA: 1026271.7s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.410s, learning 0.256s)
               Value function loss: 0.3968
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 4.03
               Mean episode length: 48.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.67s
                        Total time: 18203.08s
                               ETA: 1026161.0s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.049s, learning 0.168s)
               Value function loss: 0.2962
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 4.33
               Mean episode length: 49.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.22s
                        Total time: 18211.30s
                               ETA: 1026025.1s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.161s)
               Value function loss: 17.0455
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 4.23
               Mean episode length: 49.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.47s
                        Total time: 18219.77s
                               ETA: 1025903.6s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.415s, learning 0.174s)
               Value function loss: 0.2686
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 48.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.59s
                        Total time: 18228.36s
                               ETA: 1025788.9s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.706s, learning 0.175s)
               Value function loss: 0.2565
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 4.22
               Mean episode length: 48.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.88s
                        Total time: 18237.24s
                               ETA: 1025690.8s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.260s, learning 0.160s)
               Value function loss: 12.1154
                    Surrogate loss: 0.0075
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 47.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.42s
                        Total time: 18245.66s
                               ETA: 1025566.9s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.298s, learning 0.198s)
               Value function loss: 17.6667
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.87
               Mean episode length: 47.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.50s
                        Total time: 18254.16s
                               ETA: 1025447.4s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.295s, learning 0.176s)
               Value function loss: 7.7572
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 4.51
               Mean episode length: 49.14
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.47s
                        Total time: 18262.63s
                               ETA: 1025326.6s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.743s, learning 0.194s)
               Value function loss: 0.2714
                    Surrogate loss: -0.0436
             Mean action noise std: 0.72
                       Mean reward: 4.27
               Mean episode length: 48.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.94s
                        Total time: 18271.57s
                               ETA: 1025232.1s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.735s, learning 0.204s)
               Value function loss: 76.7112
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 4.40
               Mean episode length: 48.57
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.94s
                        Total time: 18280.51s
                               ETA: 1025137.8s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.561s, learning 0.166s)
               Value function loss: 0.3665
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 9.42
               Mean episode length: 49.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.73s
                        Total time: 18289.23s
                               ETA: 1025031.7s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.553s, learning 0.174s)
               Value function loss: 94.0056
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 4.30
               Mean episode length: 49.08
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.73s
                        Total time: 18297.96s
                               ETA: 1024925.6s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.559s, learning 0.163s)
               Value function loss: 14.0397
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 4.27
               Mean episode length: 48.96
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.72s
                        Total time: 18306.68s
                               ETA: 1024819.5s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.156s, learning 0.164s)
               Value function loss: 59.3068
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 4.30
               Mean episode length: 48.44
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.32s
                        Total time: 18315.00s
                               ETA: 1024690.9s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.515s, learning 0.172s)
               Value function loss: 18.0989
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 4.47
               Mean episode length: 48.98
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.69s
                        Total time: 18323.69s
                               ETA: 1024583.0s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.700s, learning 0.206s)
               Value function loss: 213.8096
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 4.46
               Mean episode length: 49.76
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.91s
                        Total time: 18332.59s
                               ETA: 1024487.5s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.545s, learning 0.168s)
               Value function loss: 136.2685
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 4.54
               Mean episode length: 50.44
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.71s
                        Total time: 18341.31s
                               ETA: 1024381.3s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.482s, learning 0.193s)
               Value function loss: 17.3781
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 7.13
               Mean episode length: 49.33
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.67s
                        Total time: 18349.98s
                               ETA: 1024273.0s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.667s, learning 0.161s)
               Value function loss: 23.4688
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 4.50
               Mean episode length: 49.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.83s
                        Total time: 18358.81s
                               ETA: 1024173.4s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.213s, learning 0.157s)
               Value function loss: 4.2501
                    Surrogate loss: -0.0253
             Mean action noise std: 0.72
                       Mean reward: 4.62
               Mean episode length: 49.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.37s
                        Total time: 18367.18s
                               ETA: 1024048.4s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.439s, learning 0.192s)
               Value function loss: 1.2410
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 4.28
               Mean episode length: 48.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.63s
                        Total time: 18375.81s
                               ETA: 1023938.0s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.291s, learning 0.159s)
               Value function loss: 4.3787
                    Surrogate loss: 0.0035
             Mean action noise std: 0.72
                       Mean reward: 4.00
               Mean episode length: 48.12
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.45s
                        Total time: 18384.26s
                               ETA: 1023817.7s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.454s, learning 0.195s)
               Value function loss: 69.6089
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 4.02
               Mean episode length: 48.38
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 8.65s
                        Total time: 18392.91s
                               ETA: 1023708.6s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.444s, learning 0.185s)
               Value function loss: 0.3906
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 4.61
               Mean episode length: 50.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.63s
                        Total time: 18401.54s
                               ETA: 1023598.5s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.929s, learning 0.191s)
               Value function loss: 47.3853
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 6.66
               Mean episode length: 48.62
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 9.12s
                        Total time: 18410.66s
                               ETA: 1023515.8s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.676s, learning 0.165s)
               Value function loss: 0.3995
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 10.31
               Mean episode length: 51.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.84s
                        Total time: 18419.50s
                               ETA: 1023417.7s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.967s, learning 0.162s)
               Value function loss: 54.8177
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: 4.51
               Mean episode length: 50.59
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 9.13s
                        Total time: 18428.63s
                               ETA: 1023335.7s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.428s, learning 0.168s)
               Value function loss: 7.4311
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 4.49
               Mean episode length: 48.79
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 8.60s
                        Total time: 18437.22s
                               ETA: 1023224.1s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.576s, learning 0.168s)
               Value function loss: 0.3439
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 4.70
               Mean episode length: 49.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.74s
                        Total time: 18445.97s
                               ETA: 1023120.9s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.818s, learning 0.187s)
               Value function loss: 90.6096
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 4.57
               Mean episode length: 50.22
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 9.00s
                        Total time: 18454.97s
                               ETA: 1023032.3s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.181s, learning 0.165s)
               Value function loss: 16.0953
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 4.50
               Mean episode length: 50.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.35s
                        Total time: 18463.32s
                               ETA: 1022907.3s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.757s, learning 0.210s)
               Value function loss: 125.7384
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 4.10
               Mean episode length: 49.11
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.97s
                        Total time: 18472.28s
                               ETA: 1022816.8s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.402s, learning 0.180s)
               Value function loss: 238.7101
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 12.41
               Mean episode length: 51.45
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.58s
                        Total time: 18480.87s
                               ETA: 1022705.1s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.649s, learning 0.160s)
               Value function loss: 152.4930
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: 4.76
               Mean episode length: 50.33
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.81s
                        Total time: 18489.67s
                               ETA: 1022606.0s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.164s)
               Value function loss: 98.7777
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 4.79
               Mean episode length: 50.65
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.49s
                        Total time: 18498.16s
                               ETA: 1022489.4s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.396s, learning 0.160s)
               Value function loss: 73.8278
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 4.38
               Mean episode length: 49.40
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 8.56s
                        Total time: 18506.72s
                               ETA: 1022376.6s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.553s, learning 0.181s)
               Value function loss: 3.5024
                    Surrogate loss: -0.0274
             Mean action noise std: 0.72
                       Mean reward: 4.18
               Mean episode length: 48.58
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 8.73s
                        Total time: 18515.45s
                               ETA: 1022273.7s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.053s, learning 0.239s)
               Value function loss: 1.1185
                    Surrogate loss: -0.0351
             Mean action noise std: 0.72
                       Mean reward: 4.50
               Mean episode length: 49.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 9.29s
                        Total time: 18524.75s
                               ETA: 1022201.7s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.719s, learning 0.278s)
               Value function loss: 107.9902
                    Surrogate loss: 0.0070
             Mean action noise std: 0.72
                       Mean reward: 14.83
               Mean episode length: 51.51
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 9.00s
                        Total time: 18533.74s
                               ETA: 1022113.6s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.456s, learning 0.160s)
               Value function loss: 20.6848
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 4.77
               Mean episode length: 50.89
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 8.62s
                        Total time: 18542.36s
                               ETA: 1022004.5s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1774 steps/s (collection: 8.998s, learning 0.237s)
               Value function loss: 1.3790
                    Surrogate loss: -0.0257
             Mean action noise std: 0.72
                       Mean reward: 4.83
               Mean episode length: 51.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 9.24s
                        Total time: 18551.59s
                               ETA: 1021929.6s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.617s, learning 0.172s)
               Value function loss: 0.5726
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 4.42
               Mean episode length: 50.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 8.79s
                        Total time: 18560.38s
                               ETA: 1021830.3s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.402s, learning 0.158s)
               Value function loss: 0.3761
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 7.03
               Mean episode length: 49.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 8.56s
                        Total time: 18568.94s
                               ETA: 1021718.4s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.538s, learning 0.160s)
               Value function loss: 4.2014
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 4.74
               Mean episode length: 50.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 8.70s
                        Total time: 18577.64s
                               ETA: 1021614.2s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.161s)
               Value function loss: 95.6620
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 4.42
               Mean episode length: 49.61
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 8.48s
                        Total time: 18586.12s
                               ETA: 1021498.1s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.565s, learning 0.345s)
               Value function loss: 86.2114
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 4.42
               Mean episode length: 49.14
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 8.91s
                        Total time: 18595.03s
                               ETA: 1021405.9s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.398s, learning 0.162s)
               Value function loss: 1.5861
                    Surrogate loss: -0.0344
             Mean action noise std: 0.72
                       Mean reward: 5.28
               Mean episode length: 52.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 8.56s
                        Total time: 18603.59s
                               ETA: 1021294.5s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.578s, learning 0.257s)
               Value function loss: 12.5160
                    Surrogate loss: 0.0038
             Mean action noise std: 0.72
                       Mean reward: 14.43
               Mean episode length: 49.81
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 8.83s
                        Total time: 18612.42s
                               ETA: 1021198.2s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.632s, learning 0.349s)
               Value function loss: 0.5733
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 9.23
               Mean episode length: 49.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 8.98s
                        Total time: 18621.41s
                               ETA: 1021110.1s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.733s, learning 0.264s)
               Value function loss: 16.5768
                    Surrogate loss: 0.0096
             Mean action noise std: 0.72
                       Mean reward: 4.57
               Mean episode length: 49.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 9.00s
                        Total time: 18630.40s
                               ETA: 1021023.0s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.515s, learning 0.254s)
               Value function loss: 21.7050
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 4.66
               Mean episode length: 50.22
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 8.77s
                        Total time: 18639.17s
                               ETA: 1020923.5s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.728s, learning 0.226s)
               Value function loss: 18.6910
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 7.07
               Mean episode length: 50.56
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 8.95s
                        Total time: 18648.13s
                               ETA: 1020834.2s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.262s, learning 0.238s)
               Value function loss: 7.5678
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 4.16
               Mean episode length: 49.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 8.50s
                        Total time: 18656.63s
                               ETA: 1020720.1s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.567s, learning 0.238s)
               Value function loss: 0.4361
                    Surrogate loss: -0.0374
             Mean action noise std: 0.72
                       Mean reward: 4.98
               Mean episode length: 51.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 8.81s
                        Total time: 18665.43s
                               ETA: 1020622.9s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1786 steps/s (collection: 8.902s, learning 0.268s)
               Value function loss: 0.3371
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 6.88
               Mean episode length: 50.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 9.17s
                        Total time: 18674.60s
                               ETA: 1020545.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.446s, learning 0.419s)
               Value function loss: 15.6824
                    Surrogate loss: 0.0040
             Mean action noise std: 0.72
                       Mean reward: 6.10
               Mean episode length: 54.24
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 8.86s
                        Total time: 18683.47s
                               ETA: 1020451.8s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.729s, learning 0.204s)
               Value function loss: 276.6057
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 4.68
               Mean episode length: 51.53
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 8.93s
                        Total time: 18692.40s
                               ETA: 1020361.8s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.252s, learning 0.359s)
               Value function loss: 71.8636
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 49.16
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 8.61s
                        Total time: 18701.01s
                               ETA: 1020254.4s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1746 steps/s (collection: 8.967s, learning 0.413s)
               Value function loss: 0.9110
                    Surrogate loss: -0.0239
             Mean action noise std: 0.72
                       Mean reward: 7.10
               Mean episode length: 50.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 9.38s
                        Total time: 18710.39s
                               ETA: 1020189.0s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.473s, learning 0.274s)
               Value function loss: 48.8081
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 9.23
               Mean episode length: 48.76
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 8.75s
                        Total time: 18719.14s
                               ETA: 1020089.1s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.599s, learning 0.261s)
               Value function loss: 0.7845
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 6.58
               Mean episode length: 48.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.86s
                        Total time: 18728.00s
                               ETA: 1019995.5s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.531s, learning 0.387s)
               Value function loss: 0.5319
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 9.63
               Mean episode length: 49.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.92s
                        Total time: 18736.91s
                               ETA: 1019905.1s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.701s, learning 0.318s)
               Value function loss: 17.8224
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 4.76
               Mean episode length: 50.96
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 9.02s
                        Total time: 18745.93s
                               ETA: 1019820.3s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.845s, learning 0.270s)
               Value function loss: 87.1032
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 5.34
               Mean episode length: 53.14
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 9.11s
                        Total time: 18755.05s
                               ETA: 1019740.9s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.456s, learning 0.231s)
               Value function loss: 35.3115
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 4.82
               Mean episode length: 49.97
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 8.69s
                        Total time: 18763.74s
                               ETA: 1019638.2s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.574s, learning 0.397s)
               Value function loss: 22.0330
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 4.38
               Mean episode length: 50.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 8.97s
                        Total time: 18772.71s
                               ETA: 1019551.1s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1750 steps/s (collection: 8.959s, learning 0.399s)
               Value function loss: 1.1771
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 4.71
               Mean episode length: 51.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 9.36s
                        Total time: 18782.06s
                               ETA: 1019485.1s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.533s, learning 0.159s)
               Value function loss: 0.7494
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 4.81
               Mean episode length: 50.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.69s
                        Total time: 18790.76s
                               ETA: 1019383.0s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.163s)
               Value function loss: 0.4977
                    Surrogate loss: -0.0250
             Mean action noise std: 0.72
                       Mean reward: 4.54
               Mean episode length: 50.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.61s
                        Total time: 18799.37s
                               ETA: 1019276.6s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.320s, learning 0.182s)
               Value function loss: 14.1773
                    Surrogate loss: 0.0019
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 50.17
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 8.50s
                        Total time: 18807.87s
                               ETA: 1019164.5s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.805s, learning 0.208s)
               Value function loss: 0.4321
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 4.62
               Mean episode length: 50.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 9.01s
                        Total time: 18816.88s
                               ETA: 1019080.1s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.515s, learning 0.162s)
               Value function loss: 0.3402
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 5.42
               Mean episode length: 52.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 8.68s
                        Total time: 18825.56s
                               ETA: 1018977.5s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.164s)
               Value function loss: 0.2903
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 7.54
               Mean episode length: 51.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.58s
                        Total time: 18834.14s
                               ETA: 1018869.7s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.746s, learning 0.156s)
               Value function loss: 57.0809
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 4.32
               Mean episode length: 51.36
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 8.90s
                        Total time: 18843.04s
                               ETA: 1018779.6s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.598s, learning 0.288s)
               Value function loss: 0.2790
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 4.98
               Mean episode length: 51.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.89s
                        Total time: 18851.92s
                               ETA: 1018688.7s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.735s, learning 0.236s)
               Value function loss: 17.7381
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 48.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 8.97s
                        Total time: 18860.90s
                               ETA: 1018602.5s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.384s, learning 0.171s)
               Value function loss: 0.3362
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 9.84
               Mean episode length: 50.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.56s
                        Total time: 18869.45s
                               ETA: 1018493.9s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.482s, learning 0.267s)
               Value function loss: 15.3652
                    Surrogate loss: 0.0025
             Mean action noise std: 0.72
                       Mean reward: 4.08
               Mean episode length: 48.74
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.75s
                        Total time: 18878.20s
                               ETA: 1018395.9s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.359s, learning 0.225s)
               Value function loss: 165.7623
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 4.17
               Mean episode length: 50.24
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.58s
                        Total time: 18886.78s
                               ETA: 1018289.1s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.610s, learning 0.170s)
               Value function loss: 0.4898
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 6.96
               Mean episode length: 50.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.78s
                        Total time: 18895.56s
                               ETA: 1018193.0s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.488s, learning 0.155s)
               Value function loss: 4.2370
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: 4.32
               Mean episode length: 49.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.64s
                        Total time: 18904.21s
                               ETA: 1018089.6s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.758s, learning 0.301s)
               Value function loss: 127.4804
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 4.43
               Mean episode length: 51.71
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 9.06s
                        Total time: 18913.27s
                               ETA: 1018008.6s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.586s, learning 0.218s)
               Value function loss: 46.8933
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 9.65
               Mean episode length: 51.25
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.80s
                        Total time: 18922.07s
                               ETA: 1017914.0s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.464s, learning 0.264s)
               Value function loss: 0.7118
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 11.96
               Mean episode length: 50.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.73s
                        Total time: 18930.80s
                               ETA: 1017815.5s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.436s, learning 0.194s)
               Value function loss: 10.0584
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: 4.44
               Mean episode length: 49.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.63s
                        Total time: 18939.43s
                               ETA: 1017711.8s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.585s, learning 0.220s)
               Value function loss: 0.4359
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 4.31
               Mean episode length: 49.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.80s
                        Total time: 18948.23s
                               ETA: 1017617.5s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.619s, learning 0.165s)
               Value function loss: 0.3527
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 4.55
               Mean episode length: 50.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.78s
                        Total time: 18957.02s
                               ETA: 1017522.3s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.541s, learning 0.268s)
               Value function loss: 0.3277
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 4.23
               Mean episode length: 50.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.81s
                        Total time: 18965.83s
                               ETA: 1017428.4s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.585s, learning 0.331s)
               Value function loss: 29.4051
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 4.36
               Mean episode length: 50.96
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.92s
                        Total time: 18974.74s
                               ETA: 1017340.4s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.164s)
               Value function loss: 0.3352
                    Surrogate loss: -0.0379
             Mean action noise std: 0.72
                       Mean reward: 9.87
               Mean episode length: 51.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.53s
                        Total time: 18983.27s
                               ETA: 1017231.7s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.479s, learning 0.346s)
               Value function loss: 13.9074
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 4.45
               Mean episode length: 51.26
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.83s
                        Total time: 18992.09s
                               ETA: 1017139.0s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.171s)
               Value function loss: 149.7509
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 4.60
               Mean episode length: 52.37
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.53s
                        Total time: 19000.62s
                               ETA: 1017030.6s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.478s, learning 0.162s)
               Value function loss: 14.2725
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 4.61
               Mean episode length: 51.09
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.64s
                        Total time: 19009.26s
                               ETA: 1016928.2s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.185s)
               Value function loss: 14.7350
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 6.77
               Mean episode length: 49.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.80s
                        Total time: 19018.06s
                               ETA: 1016834.5s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.463s, learning 0.180s)
               Value function loss: 17.5591
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 4.72
               Mean episode length: 52.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.64s
                        Total time: 19026.71s
                               ETA: 1016732.5s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.167s)
               Value function loss: 0.4716
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 4.95
               Mean episode length: 51.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.64s
                        Total time: 19035.34s
                               ETA: 1016630.2s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.769s, learning 0.163s)
               Value function loss: 39.3615
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 7.21
               Mean episode length: 51.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.93s
                        Total time: 19044.27s
                               ETA: 1016543.8s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.251s, learning 0.364s)
               Value function loss: 26.3306
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 6.42
               Mean episode length: 49.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.61s
                        Total time: 19052.89s
                               ETA: 1016440.5s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.159s)
               Value function loss: 0.7096
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 4.12
               Mean episode length: 49.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.74s
                        Total time: 19061.62s
                               ETA: 1016343.8s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.323s, learning 0.167s)
               Value function loss: 0.5695
                    Surrogate loss: -0.0270
             Mean action noise std: 0.72
                       Mean reward: 4.43
               Mean episode length: 50.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.49s
                        Total time: 19070.11s
                               ETA: 1016234.2s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.903s, learning 0.189s)
               Value function loss: 0.4589
                    Surrogate loss: -0.0381
             Mean action noise std: 0.72
                       Mean reward: 4.32
               Mean episode length: 49.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 9.09s
                        Total time: 19079.21s
                               ETA: 1016156.7s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.413s, learning 0.221s)
               Value function loss: 4.2176
                    Surrogate loss: 0.0053
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 50.01
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.63s
                        Total time: 19087.84s
                               ETA: 1016054.8s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.689s, learning 0.165s)
               Value function loss: 0.3588
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 4.32
               Mean episode length: 50.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.85s
                        Total time: 19096.69s
                               ETA: 1015964.8s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.836s, learning 0.167s)
               Value function loss: 131.9172
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 4.42
               Mean episode length: 49.79
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 12.00s
                        Total time: 19108.70s
                               ETA: 1016042.4s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.788s, learning 0.232s)
               Value function loss: 0.3703
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 6.89
               Mean episode length: 49.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 17.02s
                        Total time: 19125.72s
                               ETA: 1016386.4s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.665s, learning 0.173s)
               Value function loss: 0.2810
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 4.03
               Mean episode length: 49.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 16.84s
                        Total time: 19142.56s
                               ETA: 1016720.4s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.682s, learning 0.162s)
               Value function loss: 0.2701
                    Surrogate loss: -0.0302
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 49.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 16.84s
                        Total time: 19159.40s
                               ETA: 1017054.3s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.847s, learning 0.190s)
               Value function loss: 57.3810
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 10.13
               Mean episode length: 51.87
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 17.04s
                        Total time: 19176.44s
                               ETA: 1017398.1s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.713s, learning 0.164s)
               Value function loss: 29.3461
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 7.64
               Mean episode length: 53.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 16.88s
                        Total time: 19193.31s
                               ETA: 1017733.0s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.588s, learning 0.165s)
               Value function loss: 7.2659
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 4.56
               Mean episode length: 50.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 16.75s
                        Total time: 19210.07s
                               ETA: 1018060.9s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.871s, learning 0.188s)
               Value function loss: 48.5532
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 4.28
               Mean episode length: 50.51
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 17.06s
                        Total time: 19227.13s
                               ETA: 1018404.7s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.750s, learning 0.164s)
               Value function loss: 131.0927
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 4.45
               Mean episode length: 51.18
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 16.91s
                        Total time: 19244.04s
                               ETA: 1018740.4s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.818s, learning 0.259s)
               Value function loss: 18.1005
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 4.21
               Mean episode length: 50.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 17.08s
                        Total time: 19261.12s
                               ETA: 1019084.4s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.441s, learning 0.227s)
               Value function loss: 149.5461
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 4.88
               Mean episode length: 51.66
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 16.67s
                        Total time: 19277.78s
                               ETA: 1019406.3s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.557s, learning 0.209s)
               Value function loss: 2.2068
                    Surrogate loss: -0.0298
             Mean action noise std: 0.72
                       Mean reward: 4.66
               Mean episode length: 50.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 16.77s
                        Total time: 19294.55s
                               ETA: 1019733.1s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.322s, learning 0.160s)
               Value function loss: 48.3325
                    Surrogate loss: 0.0042
             Mean action noise std: 0.72
                       Mean reward: 4.45
               Mean episode length: 49.81
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 16.48s
                        Total time: 19311.03s
                               ETA: 1020044.5s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.048s, learning 0.231s)
               Value function loss: 10.5620
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 4.43
               Mean episode length: 51.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 17.28s
                        Total time: 19328.31s
                               ETA: 1020397.6s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.274s, learning 0.174s)
               Value function loss: 0.8580
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 4.18
               Mean episode length: 49.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 16.45s
                        Total time: 19344.76s
                               ETA: 1020706.5s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.776s, learning 0.161s)
               Value function loss: 105.8802
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 4.68
               Mean episode length: 50.64
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 16.94s
                        Total time: 19361.70s
                               ETA: 1021040.8s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.637s, learning 0.160s)
               Value function loss: 0.8640
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 4.37
               Mean episode length: 50.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 16.80s
                        Total time: 19378.49s
                               ETA: 1021367.3s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.479s, learning 0.298s)
               Value function loss: 0.6440
                    Surrogate loss: 0.0079
             Mean action noise std: 0.72
                       Mean reward: 6.89
               Mean episode length: 51.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 16.78s
                        Total time: 19395.27s
                               ETA: 1021692.4s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.430s, learning 0.256s)
               Value function loss: 178.0963
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 4.79
               Mean episode length: 52.01
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 16.69s
                        Total time: 19411.96s
                               ETA: 1022012.4s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.037s, learning 0.161s)
               Value function loss: 18.3446
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 4.68
               Mean episode length: 52.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 17.20s
                        Total time: 19429.15s
                               ETA: 1022358.9s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.504s, learning 0.183s)
               Value function loss: 39.7498
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 17.91
               Mean episode length: 52.18
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 16.69s
                        Total time: 19445.84s
                               ETA: 1022678.2s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.692s, learning 0.207s)
               Value function loss: 0.4966
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 4.36
               Mean episode length: 50.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 16.90s
                        Total time: 19462.74s
                               ETA: 1023008.3s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.886s, learning 0.191s)
               Value function loss: 63.4402
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 4.71
               Mean episode length: 51.30
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 17.08s
                        Total time: 19479.82s
                               ETA: 1023347.4s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.580s, learning 0.231s)
               Value function loss: 501.9103
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 4.59
               Mean episode length: 51.20
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 16.81s
                        Total time: 19496.63s
                               ETA: 1023672.1s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.382s, learning 0.307s)
               Value function loss: 51.1511
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 37.37
               Mean episode length: 51.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 16.69s
                        Total time: 19513.32s
                               ETA: 1023990.0s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.857s, learning 0.159s)
               Value function loss: 19.0548
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 4.69
               Mean episode length: 51.32
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 17.02s
                        Total time: 19530.33s
                               ETA: 1024324.7s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.819s, learning 0.380s)
               Value function loss: 2.1212
                    Surrogate loss: -0.0271
             Mean action noise std: 0.71
                       Mean reward: 4.58
               Mean episode length: 50.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 17.20s
                        Total time: 19547.53s
                               ETA: 1024668.7s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.814s, learning 0.170s)
               Value function loss: 166.5473
                    Surrogate loss: 0.0038
             Mean action noise std: 0.71
                       Mean reward: 5.43
               Mean episode length: 52.25
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 16.98s
                        Total time: 19564.52s
                               ETA: 1025001.0s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.852s, learning 0.164s)
               Value function loss: 5.1269
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 50.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 17.02s
                        Total time: 19581.53s
                               ETA: 1025334.5s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.569s, learning 0.169s)
               Value function loss: 10.2656
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 7.06
               Mean episode length: 50.90
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 16.74s
                        Total time: 19598.27s
                               ETA: 1025653.2s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.667s, learning 0.254s)
               Value function loss: 4.7880
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 5.27
               Mean episode length: 52.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 16.92s
                        Total time: 19615.19s
                               ETA: 1025981.1s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.758s, learning 0.170s)
               Value function loss: 17.2106
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 50.38
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 16.93s
                        Total time: 19632.12s
                               ETA: 1026309.0s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.694s, learning 0.246s)
               Value function loss: 500.5264
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 7.30
               Mean episode length: 51.02
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 16.94s
                        Total time: 19649.06s
                               ETA: 1026637.2s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.446s, learning 0.235s)
               Value function loss: 80.8340
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.65
               Mean episode length: 51.11
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 16.68s
                        Total time: 19665.74s
                               ETA: 1026951.4s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.527s, learning 0.161s)
               Value function loss: 116.7769
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 12.04
               Mean episode length: 51.78
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 16.69s
                        Total time: 19682.43s
                               ETA: 1027265.7s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.551s, learning 0.161s)
               Value function loss: 30.2688
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 5.35
               Mean episode length: 52.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 16.71s
                        Total time: 19699.14s
                               ETA: 1027580.9s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.558s, learning 0.171s)
               Value function loss: 19.2912
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 9.14
               Mean episode length: 50.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 16.73s
                        Total time: 19715.87s
                               ETA: 1027896.5s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.632s, learning 0.160s)
               Value function loss: 36.3013
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 49.41
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 16.79s
                        Total time: 19732.66s
                               ETA: 1028215.2s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1561 steps/s (collection: 10.331s, learning 0.164s)
               Value function loss: 94.7147
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 4.82
               Mean episode length: 51.19
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 10.49s
                        Total time: 19743.16s
                               ETA: 1028205.5s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.429s, learning 0.287s)
               Value function loss: 17.5092
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 4.67
               Mean episode length: 51.83
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 8.72s
                        Total time: 19751.87s
                               ETA: 1028103.3s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.436s, learning 0.167s)
               Value function loss: 12.5126
                    Surrogate loss: 0.0094
             Mean action noise std: 0.71
                       Mean reward: 7.16
               Mean episode length: 51.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 8.60s
                        Total time: 19760.48s
                               ETA: 1027995.3s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.551s, learning 0.169s)
               Value function loss: 1.2504
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 4.58
               Mean episode length: 50.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.72s
                        Total time: 19769.20s
                               ETA: 1027893.4s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.268s, learning 0.202s)
               Value function loss: 15.8495
                    Surrogate loss: 0.0017
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 51.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.47s
                        Total time: 19777.67s
                               ETA: 1027778.7s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.553s, learning 0.182s)
               Value function loss: 83.8120
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 50.83
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 8.73s
                        Total time: 19786.40s
                               ETA: 1027677.8s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.706s, learning 0.179s)
               Value function loss: 169.9192
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 49.78
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.89s
                        Total time: 19795.29s
                               ETA: 1027584.8s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.770s, learning 0.168s)
               Value function loss: 81.1182
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.91
               Mean episode length: 52.19
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 8.94s
                        Total time: 19804.22s
                               ETA: 1027494.7s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.453s, learning 0.159s)
               Value function loss: 289.8311
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 50.30
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 8.61s
                        Total time: 19812.84s
                               ETA: 1027387.7s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.882s, learning 0.175s)
               Value function loss: 118.5373
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 52.30
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 8.06s
                        Total time: 19820.89s
                               ETA: 1027252.1s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.147s, learning 0.215s)
               Value function loss: 4.2962
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 50.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 8.36s
                        Total time: 19829.26s
                               ETA: 1027132.4s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.365s, learning 0.162s)
               Value function loss: 147.8595
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.63
               Mean episode length: 51.66
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 8.53s
                        Total time: 19837.78s
                               ETA: 1027021.4s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.541s, learning 0.241s)
               Value function loss: 501.9306
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 7.25
               Mean episode length: 51.49
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 8.78s
                        Total time: 19846.56s
                               ETA: 1026923.6s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.544s, learning 0.165s)
               Value function loss: 7.2226
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 50.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 8.71s
                        Total time: 19855.27s
                               ETA: 1026822.2s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.531s, learning 0.166s)
               Value function loss: 21.4065
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.74
               Mean episode length: 53.84
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 8.70s
                        Total time: 19863.97s
                               ETA: 1026720.3s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.580s, learning 0.196s)
               Value function loss: 48.9338
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 27.54
               Mean episode length: 53.05
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 8.78s
                        Total time: 19872.75s
                               ETA: 1026622.5s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.490s, learning 0.169s)
               Value function loss: 54.2590
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.29
               Mean episode length: 50.89
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 8.66s
                        Total time: 19881.40s
                               ETA: 1026518.8s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.723s, learning 0.179s)
               Value function loss: 19.0828
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 4.85
               Mean episode length: 51.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 8.90s
                        Total time: 19890.31s
                               ETA: 1026427.7s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.099s, learning 0.168s)
               Value function loss: 18.2745
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 50.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 9.27s
                        Total time: 19899.57s
                               ETA: 1026355.6s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.520s, learning 0.161s)
               Value function loss: 39.4671
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 50.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 8.68s
                        Total time: 19908.26s
                               ETA: 1026253.3s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.430s, learning 0.248s)
               Value function loss: 0.6716
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 50.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 8.68s
                        Total time: 19916.93s
                               ETA: 1026151.0s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.670s, learning 0.230s)
               Value function loss: 0.5386
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 49.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 8.90s
                        Total time: 19925.83s
                               ETA: 1026060.2s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.672s, learning 0.197s)
               Value function loss: 0.4162
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 50.89
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 8.87s
                        Total time: 19934.70s
                               ETA: 1025967.8s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.785s, learning 0.168s)
               Value function loss: 0.3483
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 50.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 8.95s
                        Total time: 19943.65s
                               ETA: 1025879.9s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.162s)
               Value function loss: 29.8522
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.64
               Mean episode length: 50.71
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 8.55s
                        Total time: 19952.21s
                               ETA: 1025771.5s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.324s, learning 0.198s)
               Value function loss: 0.3334
                    Surrogate loss: -0.0432
             Mean action noise std: 0.71
                       Mean reward: 4.99
               Mean episode length: 51.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 8.52s
                        Total time: 19960.73s
                               ETA: 1025661.6s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.346s, learning 0.277s)
               Value function loss: 143.7039
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 51.41
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 8.62s
                        Total time: 19969.35s
                               ETA: 1025557.0s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.676s, learning 0.164s)
               Value function loss: 63.5563
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 50.31
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 8.84s
                        Total time: 19978.19s
                               ETA: 1025463.6s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.269s, learning 0.169s)
               Value function loss: 29.4836
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 5.28
               Mean episode length: 53.10
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 8.44s
                        Total time: 19986.63s
                               ETA: 1025349.7s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.511s, learning 0.178s)
               Value function loss: 0.5199
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 51.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 8.69s
                        Total time: 19995.32s
                               ETA: 1025248.8s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.558s, learning 0.184s)
               Value function loss: 0.4482
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 51.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.74s
                        Total time: 20004.06s
                               ETA: 1025150.7s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.702s, learning 0.267s)
               Value function loss: 27.6453
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 49.80
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 8.97s
                        Total time: 20013.03s
                               ETA: 1025064.3s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.546s, learning 0.209s)
               Value function loss: 0.3893
                    Surrogate loss: -0.0420
             Mean action noise std: 0.71
                       Mean reward: 4.80
               Mean episode length: 51.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 8.75s
                        Total time: 20021.79s
                               ETA: 1024967.0s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.288s, learning 0.158s)
               Value function loss: 132.2873
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.54
               Mean episode length: 50.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 8.45s
                        Total time: 20030.23s
                               ETA: 1024854.1s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.619s, learning 0.260s)
               Value function loss: 0.4074
                    Surrogate loss: -0.0413
             Mean action noise std: 0.71
                       Mean reward: 11.63
               Mean episode length: 49.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 8.88s
                        Total time: 20039.11s
                               ETA: 1024763.3s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.522s, learning 0.171s)
               Value function loss: 0.3236
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 50.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.69s
                        Total time: 20047.80s
                               ETA: 1024663.1s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.817s, learning 0.170s)
               Value function loss: 13.5939
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 51.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 8.99s
                        Total time: 20056.79s
                               ETA: 1024578.1s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.419s, learning 0.170s)
               Value function loss: 0.3275
                    Surrogate loss: -0.0412
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 50.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 8.59s
                        Total time: 20065.38s
                               ETA: 1024472.9s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.748s, learning 0.163s)
               Value function loss: 0.2706
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 4.76
               Mean episode length: 51.32
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.91s
                        Total time: 20074.29s
                               ETA: 1024384.1s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.159s)
               Value function loss: 0.2503
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 50.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.56s
                        Total time: 20082.85s
                               ETA: 1024277.3s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.827s, learning 0.164s)
               Value function loss: 64.8468
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 9.13
               Mean episode length: 49.86
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 8.99s
                        Total time: 20091.84s
                               ETA: 1024192.8s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.518s, learning 0.198s)
               Value function loss: 106.5488
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 50.29
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.72s
                        Total time: 20100.55s
                               ETA: 1024094.5s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.566s, learning 0.166s)
               Value function loss: 15.5484
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 9.19
               Mean episode length: 49.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 8.73s
                        Total time: 20109.29s
                               ETA: 1023997.0s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.762s, learning 0.165s)
               Value function loss: 0.5288
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 11.86
               Mean episode length: 51.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 8.93s
                        Total time: 20118.21s
                               ETA: 1023909.5s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.421s, learning 0.178s)
               Value function loss: 0.4250
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 50.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 8.60s
                        Total time: 20126.81s
                               ETA: 1023805.4s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.393s, learning 0.175s)
               Value function loss: 0.4055
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 7.52
               Mean episode length: 51.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 8.57s
                        Total time: 20135.38s
                               ETA: 1023699.9s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.486s, learning 0.163s)
               Value function loss: 64.4625
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.65s
                        Total time: 20144.03s
                               ETA: 1023598.5s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.545s, learning 0.157s)
               Value function loss: 15.4161
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 4.45
               Mean episode length: 50.15
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.70s
                        Total time: 20152.73s
                               ETA: 1023499.9s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.403s, learning 0.259s)
               Value function loss: 0.4184
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 4.76
               Mean episode length: 51.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.66s
                        Total time: 20161.39s
                               ETA: 1023399.4s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.467s, learning 0.160s)
               Value function loss: 0.3404
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 4.51
               Mean episode length: 50.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 8.63s
                        Total time: 20170.02s
                               ETA: 1023297.2s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.586s, learning 0.308s)
               Value function loss: 167.2621
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 48.40
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 8.89s
                        Total time: 20178.91s
                               ETA: 1023208.6s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.206s, learning 0.163s)
               Value function loss: 17.2353
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 10.93
               Mean episode length: 53.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 8.37s
                        Total time: 20187.28s
                               ETA: 1023093.6s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.106s, learning 0.204s)
               Value function loss: 46.7521
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 4.65
               Mean episode length: 50.59
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 9.31s
                        Total time: 20196.59s
                               ETA: 1023026.3s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.477s, learning 0.199s)
               Value function loss: 187.4176
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 14.35
               Mean episode length: 49.53
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 8.68s
                        Total time: 20205.27s
                               ETA: 1022927.0s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.597s, learning 0.178s)
               Value function loss: 2.5636
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 4.37
               Mean episode length: 48.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 8.77s
                        Total time: 20214.04s
                               ETA: 1022832.7s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.413s, learning 0.188s)
               Value function loss: 32.9704
                    Surrogate loss: 0.0038
             Mean action noise std: 0.71
                       Mean reward: 12.29
               Mean episode length: 50.68
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 8.60s
                        Total time: 20222.64s
                               ETA: 1022729.7s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.662s, learning 0.184s)
               Value function loss: 76.9028
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 11.99
               Mean episode length: 48.40
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 8.85s
                        Total time: 20231.49s
                               ETA: 1022639.3s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.996s, learning 0.164s)
               Value function loss: 377.0521
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 4.58
               Mean episode length: 50.37
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 8.16s
                        Total time: 20239.65s
                               ETA: 1022514.2s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.169s)
               Value function loss: 1.7573
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 4.90
               Mean episode length: 49.31
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 8.45s
                        Total time: 20248.10s
                               ETA: 1022404.2s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.065s, learning 0.352s)
               Value function loss: 0.7575
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 48.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 8.42s
                        Total time: 20256.52s
                               ETA: 1022292.4s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.582s, learning 0.158s)
               Value function loss: 168.8719
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.65
               Mean episode length: 48.88
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 8.74s
                        Total time: 20265.26s
                               ETA: 1022196.9s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.615s, learning 0.183s)
               Value function loss: 521.7075
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 5.19
               Mean episode length: 51.41
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 8.80s
                        Total time: 20274.06s
                               ETA: 1022104.5s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.431s, learning 0.190s)
               Value function loss: 147.4850
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 9.43
               Mean episode length: 49.39
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 8.62s
                        Total time: 20282.68s
                               ETA: 1022003.3s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.549s, learning 0.176s)
               Value function loss: 36.9391
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 4.63
               Mean episode length: 49.85
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 8.73s
                        Total time: 20291.41s
                               ETA: 1021907.4s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.400s, learning 0.306s)
               Value function loss: 40.7565
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 9.64
               Mean episode length: 50.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 8.71s
                        Total time: 20300.11s
                               ETA: 1021810.5s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.524s, learning 0.177s)
               Value function loss: 507.0497
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.16
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 8.70s
                        Total time: 20308.81s
                               ETA: 1021713.6s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.842s, learning 0.162s)
               Value function loss: 85.9535
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 6.80
               Mean episode length: 48.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 9.00s
                        Total time: 20317.82s
                               ETA: 1021632.0s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.517s, learning 0.167s)
               Value function loss: 445.3297
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 46.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 8.68s
                        Total time: 20326.50s
                               ETA: 1021534.4s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.532s, learning 0.227s)
               Value function loss: 405.2388
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 39.45
               Mean episode length: 49.85
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 8.76s
                        Total time: 20335.26s
                               ETA: 1021440.6s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.348s, learning 0.171s)
               Value function loss: 15.1550
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 4.69
               Mean episode length: 50.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 8.52s
                        Total time: 20343.78s
                               ETA: 1021334.8s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.172s)
               Value function loss: 28.6210
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 49.23
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 8.58s
                        Total time: 20352.36s
                               ETA: 1021232.4s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.600s, learning 0.212s)
               Value function loss: 641.1718
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 49.39
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 8.81s
                        Total time: 20361.18s
                               ETA: 1021141.6s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.616s, learning 0.204s)
               Value function loss: 379.8782
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 12.20
               Mean episode length: 49.88
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 8.82s
                        Total time: 20370.00s
                               ETA: 1021051.2s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.633s, learning 0.159s)
               Value function loss: 210.4919
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 48.58
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 8.79s
                        Total time: 20378.79s
                               ETA: 1020959.6s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.161s)
               Value function loss: 43.2087
                    Surrogate loss: 0.0236
             Mean action noise std: 0.71
                       Mean reward: 4.57
               Mean episode length: 49.27
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 8.53s
                        Total time: 20387.32s
                               ETA: 1020854.9s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.165s)
               Value function loss: 119.3588
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 47.92
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 8.48s
                        Total time: 20395.80s
                               ETA: 1020747.7s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.804s, learning 0.279s)
               Value function loss: 212.8747
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 9.37
               Mean episode length: 48.54
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 9.08s
                        Total time: 20404.88s
                               ETA: 1020670.9s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.635s, learning 0.181s)
               Value function loss: 102.5432
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 48.50
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 8.82s
                        Total time: 20413.70s
                               ETA: 1020580.7s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.786s, learning 0.170s)
               Value function loss: 25.0486
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 4.71
               Mean episode length: 50.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 8.96s
                        Total time: 20422.65s
                               ETA: 1020497.7s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.772s, learning 0.201s)
               Value function loss: 132.8151
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 8.97s
                        Total time: 20431.63s
                               ETA: 1020415.6s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.401s, learning 0.168s)
               Value function loss: 67.6297
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 48.99
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 8.57s
                        Total time: 20440.20s
                               ETA: 1020313.4s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.163s)
               Value function loss: 2.1253
                    Surrogate loss: -0.0259
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 48.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 8.55s
                        Total time: 20448.74s
                               ETA: 1020210.2s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.457s, learning 0.232s)
               Value function loss: 54.7070
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 48.91
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 8.69s
                        Total time: 20457.43s
                               ETA: 1020114.2s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.690s, learning 0.301s)
               Value function loss: 1.5270
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 49.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 8.99s
                        Total time: 20466.42s
                               ETA: 1020033.2s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.089s, learning 0.169s)
               Value function loss: 18.3221
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.50
               Mean episode length: 49.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 8.26s
                        Total time: 20474.68s
                               ETA: 1019915.9s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.387s, learning 0.164s)
               Value function loss: 64.1653
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 48.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 8.55s
                        Total time: 20483.23s
                               ETA: 1019813.2s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.564s, learning 0.259s)
               Value function loss: 100.6899
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 49.74
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 8.82s
                        Total time: 20492.06s
                               ETA: 1019724.2s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.682s, learning 0.170s)
               Value function loss: 1.2175
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 49.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 8.85s
                        Total time: 20500.91s
                               ETA: 1019636.7s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.400s, learning 0.273s)
               Value function loss: 52.8870
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 51.23
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.67s
                        Total time: 20509.58s
                               ETA: 1019540.4s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.409s, learning 0.163s)
               Value function loss: 13.1297
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.57s
                        Total time: 20518.15s
                               ETA: 1019439.2s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.271s, learning 0.166s)
               Value function loss: 0.9363
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 49.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.44s
                        Total time: 20526.59s
                               ETA: 1019331.4s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.306s, learning 0.228s)
               Value function loss: 0.7251
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 49.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 8.53s
                        Total time: 20535.12s
                               ETA: 1019228.4s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.526s, learning 0.229s)
               Value function loss: 0.5401
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 49.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.75s
                        Total time: 20543.88s
                               ETA: 1019136.5s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.639s, learning 0.207s)
               Value function loss: 0.4767
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.85s
                        Total time: 20552.73s
                               ETA: 1019049.3s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.167s)
               Value function loss: 65.1471
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 49.63
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.93s
                        Total time: 20561.66s
                               ETA: 1018966.4s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.314s, learning 0.249s)
               Value function loss: 29.1991
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.63
               Mean episode length: 49.85
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.56s
                        Total time: 20570.22s
                               ETA: 1018865.3s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.383s, learning 0.168s)
               Value function loss: 0.4261
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 50.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.55s
                        Total time: 20578.77s
                               ETA: 1018763.7s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.385s, learning 0.176s)
               Value function loss: 301.7999
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 8.45
               Mean episode length: 47.96
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.56s
                        Total time: 20587.34s
                               ETA: 1018662.6s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.384s, learning 0.159s)
               Value function loss: 374.1025
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 34.61
               Mean episode length: 48.05
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 8.54s
                        Total time: 20595.88s
                               ETA: 1018560.8s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.168s)
               Value function loss: 1.3307
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.56s
                        Total time: 20604.43s
                               ETA: 1018459.6s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.520s, learning 0.166s)
               Value function loss: 0.8518
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.69s
                        Total time: 20613.12s
                               ETA: 1018365.0s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.697s, learning 0.181s)
               Value function loss: 53.9349
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 7.25
               Mean episode length: 50.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.88s
                        Total time: 20622.00s
                               ETA: 1018280.0s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.509s, learning 0.276s)
               Value function loss: 164.3608
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.67
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.79s
                        Total time: 20630.78s
                               ETA: 1018190.5s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.644s, learning 0.172s)
               Value function loss: 211.6575
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.60
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.82s
                        Total time: 20639.60s
                               ETA: 1018102.6s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.426s, learning 0.169s)
               Value function loss: 255.1025
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 17.15
               Mean episode length: 49.85
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.59s
                        Total time: 20648.20s
                               ETA: 1018003.8s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.530s, learning 0.317s)
               Value function loss: 104.8490
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 7.05
               Mean episode length: 49.78
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.85s
                        Total time: 20657.04s
                               ETA: 1017917.6s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.839s, learning 0.278s)
               Value function loss: 17.8074
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 11.99
               Mean episode length: 50.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 9.12s
                        Total time: 20666.16s
                               ETA: 1017844.7s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.683s, learning 0.159s)
               Value function loss: 467.7375
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.52
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.84s
                        Total time: 20675.00s
                               ETA: 1017758.3s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.899s, learning 0.168s)
               Value function loss: 112.2713
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 48.40
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 9.07s
                        Total time: 20684.07s
                               ETA: 1017683.1s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.460s, learning 0.164s)
               Value function loss: 171.5189
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.31
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.62s
                        Total time: 20692.69s
                               ETA: 1017586.2s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.672s, learning 0.163s)
               Value function loss: 129.1342
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 34.27
               Mean episode length: 48.94
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.84s
                        Total time: 20701.53s
                               ETA: 1017499.7s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.160s)
               Value function loss: 88.2398
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 4.27
               Mean episode length: 49.57
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.71s
                        Total time: 20710.23s
                               ETA: 1017407.0s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.778s, learning 0.183s)
               Value function loss: 58.1912
                    Surrogate loss: 0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.72
               Mean episode length: 49.53
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.96s
                        Total time: 20719.19s
                               ETA: 1017326.9s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.377s, learning 0.160s)
               Value function loss: 42.0506
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.54s
                        Total time: 20727.73s
                               ETA: 1017226.0s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.444s, learning 0.193s)
               Value function loss: 149.3516
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.80
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.64s
                        Total time: 20736.37s
                               ETA: 1017130.2s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.578s, learning 0.158s)
               Value function loss: 568.4612
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 6.64
               Mean episode length: 48.81
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.74s
                        Total time: 20745.10s
                               ETA: 1017039.3s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.442s, learning 0.335s)
               Value function loss: 182.4819
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 8.67
               Mean episode length: 47.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 8.78s
                        Total time: 20753.88s
                               ETA: 1016950.5s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1748 steps/s (collection: 8.950s, learning 0.419s)
               Value function loss: 247.6312
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.94
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 9.37s
                        Total time: 20763.25s
                               ETA: 1016890.8s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.426s, learning 0.169s)
               Value function loss: 184.4209
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 6.08
               Mean episode length: 46.94
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.60s
                        Total time: 20771.84s
                               ETA: 1016793.2s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.484s, learning 0.253s)
               Value function loss: 23.5173
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 49.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.74s
                        Total time: 20780.58s
                               ETA: 1016702.6s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.161s)
               Value function loss: 188.7206
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 6.46
               Mean episode length: 48.88
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.52s
                        Total time: 20789.10s
                               ETA: 1016601.6s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.640s, learning 0.167s)
               Value function loss: 93.9009
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 6.63
               Mean episode length: 48.59
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.81s
                        Total time: 20797.91s
                               ETA: 1016514.7s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.162s)
               Value function loss: 21.4835
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 14.07
               Mean episode length: 49.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.71s
                        Total time: 20806.62s
                               ETA: 1016423.0s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.517s, learning 0.210s)
               Value function loss: 75.4624
                    Surrogate loss: 0.0048
             Mean action noise std: 0.71
                       Mean reward: 18.88
               Mean episode length: 48.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.73s
                        Total time: 20815.34s
                               ETA: 1016332.3s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.739s, learning 0.182s)
               Value function loss: 284.8812
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 47.80
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 8.92s
                        Total time: 20824.27s
                               ETA: 1016251.2s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.384s, learning 0.315s)
               Value function loss: 163.8501
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 13.45
               Mean episode length: 47.31
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.70s
                        Total time: 20832.96s
                               ETA: 1016159.2s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.441s, learning 0.262s)
               Value function loss: 281.7441
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 47.65
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.70s
                        Total time: 20841.67s
                               ETA: 1016067.6s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.570s, learning 0.168s)
               Value function loss: 28.3809
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 16.51
               Mean episode length: 48.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.74s
                        Total time: 20850.41s
                               ETA: 1015977.8s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.258s, learning 0.339s)
               Value function loss: 132.8861
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 13.84
               Mean episode length: 48.69
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.60s
                        Total time: 20859.00s
                               ETA: 1015881.2s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.600s, learning 0.168s)
               Value function loss: 40.6780
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 48.26
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.77s
                        Total time: 20867.77s
                               ETA: 1015793.0s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.425s, learning 0.219s)
               Value function loss: 182.1424
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 22.31
               Mean episode length: 50.55
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.64s
                        Total time: 20876.42s
                               ETA: 1015698.8s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.360s, learning 0.192s)
               Value function loss: 295.4326
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 47.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.55s
                        Total time: 20884.97s
                               ETA: 1015600.3s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.600s, learning 0.238s)
               Value function loss: 185.6508
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 8.81
               Mean episode length: 48.47
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.84s
                        Total time: 20893.81s
                               ETA: 1015515.7s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.507s, learning 0.160s)
               Value function loss: 32.9068
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 4.32
               Mean episode length: 48.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.67s
                        Total time: 20902.47s
                               ETA: 1015422.9s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.502s, learning 0.309s)
               Value function loss: 7.5482
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 9.21
               Mean episode length: 49.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.81s
                        Total time: 20911.28s
                               ETA: 1015337.1s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.693s, learning 0.168s)
               Value function loss: 279.7324
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 11.19
               Mean episode length: 47.93
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.86s
                        Total time: 20920.14s
                               ETA: 1015253.8s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.438s, learning 0.192s)
               Value function loss: 24.2411
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 48.87
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.63s
                        Total time: 20928.77s
                               ETA: 1015159.5s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.765s, learning 0.162s)
               Value function loss: 112.8762
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.20
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.93s
                        Total time: 20937.70s
                               ETA: 1015079.6s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.661s, learning 0.159s)
               Value function loss: 30.6305
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 6.06
               Mean episode length: 48.63
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.82s
                        Total time: 20946.52s
                               ETA: 1014994.6s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.458s, learning 0.169s)
               Value function loss: 7.2797
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.63s
                        Total time: 20955.15s
                               ETA: 1014900.4s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.479s, learning 0.207s)
               Value function loss: 1.4341
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.69s
                        Total time: 20963.83s
                               ETA: 1014809.0s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.059s, learning 0.159s)
               Value function loss: 18.0735
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 47.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 11.22s
                        Total time: 20975.05s
                               ETA: 1014840.3s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.655s, learning 0.172s)
               Value function loss: 64.5737
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 48.70
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 16.83s
                        Total time: 20991.88s
                               ETA: 1015142.8s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.558s, learning 0.291s)
               Value function loss: 1.0108
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 48.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 16.85s
                        Total time: 21008.73s
                               ETA: 1015446.0s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 942 steps/s (collection: 17.217s, learning 0.160s)
               Value function loss: 71.5411
                    Surrogate loss: 0.0054
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 49.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 17.38s
                        Total time: 21026.10s
                               ETA: 1015774.4s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.834s, learning 0.262s)
               Value function loss: 190.7838
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 50.22
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 17.10s
                        Total time: 21043.20s
                               ETA: 1016088.9s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.634s, learning 0.156s)
               Value function loss: 14.5191
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 16.79s
                        Total time: 21059.99s
                               ETA: 1016388.3s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.834s, learning 0.187s)
               Value function loss: 32.5976
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 49.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 17.02s
                        Total time: 21077.01s
                               ETA: 1016698.6s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.700s, learning 0.163s)
               Value function loss: 182.2780
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 49.25
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 16.86s
                        Total time: 21093.87s
                               ETA: 1017000.8s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.829s, learning 0.204s)
               Value function loss: 62.6348
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 5.96
               Mean episode length: 47.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 17.03s
                        Total time: 21110.91s
                               ETA: 1017311.0s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.385s, learning 0.320s)
               Value function loss: 36.5417
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 6.28
               Mean episode length: 50.16
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 16.70s
                        Total time: 21127.61s
                               ETA: 1017605.1s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.580s, learning 0.180s)
               Value function loss: 3.2849
                    Surrogate loss: -0.0240
             Mean action noise std: 0.71
                       Mean reward: 5.91
               Mean episode length: 48.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 16.76s
                        Total time: 21144.37s
                               ETA: 1017901.5s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.423s, learning 0.191s)
               Value function loss: 13.7822
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 48.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 16.61s
                        Total time: 21160.99s
                               ETA: 1018190.5s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.755s, learning 0.156s)
               Value function loss: 5.7552
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 3.00
               Mean episode length: 47.11
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 16.91s
                        Total time: 21177.90s
                               ETA: 1018493.6s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.635s, learning 0.163s)
               Value function loss: 54.6489
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 8.72
               Mean episode length: 49.73
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 16.80s
                        Total time: 21194.70s
                               ETA: 1018790.9s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.763s, learning 0.195s)
               Value function loss: 120.0899
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 11.24
               Mean episode length: 48.98
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 16.96s
                        Total time: 21211.65s
                               ETA: 1019095.6s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.006s, learning 0.193s)
               Value function loss: 0.8068
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 48.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 17.20s
                        Total time: 21228.85s
                               ETA: 1019411.5s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.234s, learning 0.187s)
               Value function loss: 247.8333
                    Surrogate loss: 0.0028
             Mean action noise std: 0.71
                       Mean reward: 2.94
               Mean episode length: 48.38
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 17.42s
                        Total time: 21246.27s
                               ETA: 1019737.8s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.693s, learning 0.164s)
               Value function loss: 359.2945
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 13.65
               Mean episode length: 49.30
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 16.86s
                        Total time: 21263.13s
                               ETA: 1020036.7s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.511s, learning 0.297s)
               Value function loss: 66.0209
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 3.12
               Mean episode length: 48.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 16.81s
                        Total time: 21279.94s
                               ETA: 1020332.9s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.326s, learning 0.159s)
               Value function loss: 64.6380
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 26.37
               Mean episode length: 50.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 16.48s
                        Total time: 21296.42s
                               ETA: 1020613.3s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.822s, learning 0.348s)
               Value function loss: 41.7879
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 49.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 17.17s
                        Total time: 21313.59s
                               ETA: 1020926.3s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.407s, learning 0.183s)
               Value function loss: 17.0951
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 13.27
               Mean episode length: 47.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 16.59s
                        Total time: 21330.18s
                               ETA: 1021211.1s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.094s, learning 0.299s)
               Value function loss: 1.0912
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 8.69
               Mean episode length: 48.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 17.39s
                        Total time: 21347.58s
                               ETA: 1021534.2s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.900s, learning 0.167s)
               Value function loss: 160.0429
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 49.28
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 17.07s
                        Total time: 21364.64s
                               ETA: 1021841.3s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.671s, learning 0.184s)
               Value function loss: 121.3317
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 8.65
               Mean episode length: 49.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 16.86s
                        Total time: 21381.50s
                               ETA: 1022137.9s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.976s, learning 0.167s)
               Value function loss: 8.7320
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 2.78
               Mean episode length: 46.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 17.14s
                        Total time: 21398.64s
                               ETA: 1022447.9s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.578s, learning 0.187s)
               Value function loss: 65.0385
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.17
               Mean episode length: 48.66
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 16.77s
                        Total time: 21415.41s
                               ETA: 1022739.6s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.629s, learning 0.215s)
               Value function loss: 325.7333
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 2.93
               Mean episode length: 47.74
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 16.84s
                        Total time: 21432.25s
                               ETA: 1023034.8s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.689s, learning 0.176s)
               Value function loss: 21.0809
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 49.65
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 16.86s
                        Total time: 21449.11s
                               ETA: 1023330.7s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.754s, learning 0.158s)
               Value function loss: 51.4909
                    Surrogate loss: 0.0048
             Mean action noise std: 0.71
                       Mean reward: 16.29
               Mean episode length: 48.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 16.91s
                        Total time: 21466.03s
                               ETA: 1023628.5s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.691s, learning 0.169s)
               Value function loss: 1.9773
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 16.86s
                        Total time: 21482.89s
                               ETA: 1023923.5s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.602s, learning 0.258s)
               Value function loss: 152.3550
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 49.50
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 16.86s
                        Total time: 21499.75s
                               ETA: 1024218.2s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.576s, learning 0.287s)
               Value function loss: 218.1163
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 2.72
               Mean episode length: 47.21
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 16.86s
                        Total time: 21516.61s
                               ETA: 1024512.8s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.082s, learning 0.316s)
               Value function loss: 363.1884
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.35
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 17.40s
                        Total time: 21534.01s
                               ETA: 1024832.4s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 949 steps/s (collection: 16.997s, learning 0.265s)
               Value function loss: 221.2322
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 49.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 17.26s
                        Total time: 21551.27s
                               ETA: 1025145.4s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.095s, learning 0.158s)
               Value function loss: 25.6651
                    Surrogate loss: -0.0095
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 50.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 17.25s
                        Total time: 21568.52s
                               ETA: 1025457.6s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 948 steps/s (collection: 16.880s, learning 0.389s)
               Value function loss: 20.1081
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 8.68
               Mean episode length: 48.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 17.27s
                        Total time: 21585.79s
                               ETA: 1025770.2s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.988s, learning 0.213s)
               Value function loss: 1.9818
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 48.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 17.20s
                        Total time: 21602.99s
                               ETA: 1026079.3s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.726s, learning 0.161s)
               Value function loss: 58.3999
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 48.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 11.89s
                        Total time: 21614.88s
                               ETA: 1026135.7s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.952s, learning 0.185s)
               Value function loss: 453.6825
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 48.71
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 9.14s
                        Total time: 21624.02s
                               ETA: 1026061.6s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.727s, learning 0.159s)
               Value function loss: 18.4053
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 50.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 8.89s
                        Total time: 21632.90s
                               ETA: 1025975.7s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.189s, learning 0.180s)
               Value function loss: 5.1817
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 47.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.37s
                        Total time: 21641.27s
                               ETA: 1025865.3s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.622s, learning 0.168s)
               Value function loss: 93.3527
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 49.25
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.79s
                        Total time: 21650.06s
                               ETA: 1025775.0s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.471s, learning 0.259s)
               Value function loss: 267.5148
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 18.69
               Mean episode length: 48.96
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.73s
                        Total time: 21658.79s
                               ETA: 1025681.9s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.888s, learning 0.160s)
               Value function loss: 345.9218
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 50.14
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 9.05s
                        Total time: 21667.84s
                               ETA: 1025604.0s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 97.4544
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 3.10
               Mean episode length: 46.92
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.47s
                        Total time: 21676.31s
                               ETA: 1025498.8s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1775 steps/s (collection: 8.846s, learning 0.384s)
               Value function loss: 22.0779
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 13.47
               Mean episode length: 48.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 9.23s
                        Total time: 21685.54s
                               ETA: 1025429.7s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.249s, learning 0.164s)
               Value function loss: 263.0616
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 48.52
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.41s
                        Total time: 21693.95s
                               ETA: 1025321.9s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.674s, learning 0.190s)
               Value function loss: 73.3671
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 48.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.86s
                        Total time: 21702.82s
                               ETA: 1025235.6s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.164s)
               Value function loss: 141.9612
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 48.17
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.69s
                        Total time: 21711.51s
                               ETA: 1025141.3s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.401s, learning 0.198s)
               Value function loss: 53.5006
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 11.05
               Mean episode length: 48.29
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.60s
                        Total time: 21720.11s
                               ETA: 1025042.6s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.343s, learning 0.167s)
               Value function loss: 331.6598
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 15.81
               Mean episode length: 47.87
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.51s
                        Total time: 21728.62s
                               ETA: 1024939.8s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.165s)
               Value function loss: 161.8145
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 15.57
               Mean episode length: 48.83
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.47s
                        Total time: 21737.09s
                               ETA: 1024835.2s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.604s, learning 0.159s)
               Value function loss: 170.1484
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 6.08
               Mean episode length: 48.86
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.76s
                        Total time: 21745.85s
                               ETA: 1024744.5s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.544s, learning 0.177s)
               Value function loss: 25.1370
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 10.43
               Mean episode length: 47.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 8.72s
                        Total time: 21754.57s
                               ETA: 1024652.0s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.433s, learning 0.279s)
               Value function loss: 22.6560
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 48.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.71s
                        Total time: 21763.29s
                               ETA: 1024559.0s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.533s, learning 0.162s)
               Value function loss: 110.0836
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 49.12
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.70s
                        Total time: 21771.98s
                               ETA: 1024465.4s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.440s, learning 0.301s)
               Value function loss: 185.6880
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 47.54
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.74s
                        Total time: 21780.72s
                               ETA: 1024373.9s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.758s, learning 0.167s)
               Value function loss: 273.9742
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 48.27
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.92s
                        Total time: 21789.65s
                               ETA: 1024291.3s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.391s, learning 0.156s)
               Value function loss: 65.6603
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 47.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.55s
                        Total time: 21798.19s
                               ETA: 1024190.9s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.296s, learning 0.176s)
               Value function loss: 208.9134
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.24
               Mean episode length: 48.98
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 8.47s
                        Total time: 21806.67s
                               ETA: 1024087.1s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.162s)
               Value function loss: 418.1573
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 8.42
               Mean episode length: 48.51
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.55s
                        Total time: 21815.22s
                               ETA: 1023987.1s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.444s, learning 0.189s)
               Value function loss: 9.3263
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 2.86
               Mean episode length: 46.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 8.63s
                        Total time: 21823.85s
                               ETA: 1023891.1s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.188s, learning 0.175s)
               Value function loss: 35.7703
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 49.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 8.36s
                        Total time: 21832.21s
                               ETA: 1023782.4s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.331s, learning 0.260s)
               Value function loss: 12.3937
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 11.03
               Mean episode length: 48.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 8.59s
                        Total time: 21840.81s
                               ETA: 1023684.6s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.643s, learning 0.174s)
               Value function loss: 2.5692
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 6.60
               Mean episode length: 49.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 8.82s
                        Total time: 21849.62s
                               ETA: 1023597.3s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.509s, learning 0.162s)
               Value function loss: 85.9733
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 3.12
               Mean episode length: 48.03
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.67s
                        Total time: 21858.29s
                               ETA: 1023503.4s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.164s)
               Value function loss: 429.3001
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 16.24
               Mean episode length: 49.69
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 8.71s
                        Total time: 21867.00s
                               ETA: 1023411.3s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.436s, learning 0.324s)
               Value function loss: 502.1773
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.75
               Mean episode length: 46.66
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.76s
                        Total time: 21875.76s
                               ETA: 1023321.7s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.567s, learning 0.163s)
               Value function loss: 394.2270
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 8.16
               Mean episode length: 48.51
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 8.73s
                        Total time: 21884.49s
                               ETA: 1023230.8s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.315s, learning 0.204s)
               Value function loss: 64.2832
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 8.52
               Mean episode length: 48.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.52s
                        Total time: 21893.01s
                               ETA: 1023130.0s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1766 steps/s (collection: 8.897s, learning 0.378s)
               Value function loss: 10.8326
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 11.00
               Mean episode length: 47.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 9.28s
                        Total time: 21902.29s
                               ETA: 1023064.7s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.714s, learning 0.165s)
               Value function loss: 6.1278
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 49.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 8.88s
                        Total time: 21911.17s
                               ETA: 1022980.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.384s, learning 0.175s)
               Value function loss: 66.2814
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 46.78
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 8.56s
                        Total time: 21919.73s
                               ETA: 1022882.3s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.544s, learning 0.161s)
               Value function loss: 47.5793
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 6.34
               Mean episode length: 48.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 8.71s
                        Total time: 21928.43s
                               ETA: 1022790.6s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.363s, learning 0.359s)
               Value function loss: 30.4299
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 5.94
               Mean episode length: 47.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 8.72s
                        Total time: 21937.15s
                               ETA: 1022699.7s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.161s)
               Value function loss: 393.4605
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 13.64
               Mean episode length: 48.47
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 8.61s
                        Total time: 21945.77s
                               ETA: 1022603.8s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.623s, learning 0.175s)
               Value function loss: 270.8905
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 49.07
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 8.80s
                        Total time: 21954.57s
                               ETA: 1022516.6s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.714s, learning 0.171s)
               Value function loss: 107.3114
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 48.54
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 8.89s
                        Total time: 21963.45s
                               ETA: 1022433.6s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.444s, learning 0.182s)
               Value function loss: 138.4411
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 49.33
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 8.63s
                        Total time: 21972.08s
                               ETA: 1022338.6s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.814s, learning 0.170s)
               Value function loss: 12.0334
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 11.79
               Mean episode length: 48.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 8.98s
                        Total time: 21981.06s
                               ETA: 1022260.3s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.350s, learning 0.242s)
               Value function loss: 77.7517
                    Surrogate loss: 0.0035
             Mean action noise std: 0.71
                       Mean reward: 12.54
               Mean episode length: 51.14
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 8.59s
                        Total time: 21989.65s
                               ETA: 1022163.8s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.517s, learning 0.174s)
               Value function loss: 5.5157
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 8.69s
                        Total time: 21998.34s
                               ETA: 1022072.1s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.654s, learning 0.178s)
               Value function loss: 325.1140
                    Surrogate loss: 0.0066
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 47.33
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 8.83s
                        Total time: 22007.18s
                               ETA: 1021986.9s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.837s, learning 0.172s)
               Value function loss: 111.0788
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.32
               Mean episode length: 48.47
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 9.01s
                        Total time: 22016.18s
                               ETA: 1021910.1s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.607s, learning 0.175s)
               Value function loss: 183.7671
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 8.94
               Mean episode length: 49.37
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 8.78s
                        Total time: 22024.97s
                               ETA: 1021822.8s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.164s)
               Value function loss: 76.0655
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 20.98
               Mean episode length: 47.68
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 8.61s
                        Total time: 22033.58s
                               ETA: 1021727.5s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.594s, learning 0.315s)
               Value function loss: 80.3366
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 8.59
               Mean episode length: 47.56
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 8.91s
                        Total time: 22042.48s
                               ETA: 1021646.2s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.772s, learning 0.175s)
               Value function loss: 263.3203
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 49.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 8.95s
                        Total time: 22051.43s
                               ETA: 1021566.8s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.402s, learning 0.162s)
               Value function loss: 364.3817
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.97
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 8.56s
                        Total time: 22060.00s
                               ETA: 1021469.6s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.168s)
               Value function loss: 286.2287
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.61
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 8.45s
                        Total time: 22068.45s
                               ETA: 1021367.4s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.638s, learning 0.327s)
               Value function loss: 12.4013
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 48.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 8.97s
                        Total time: 22077.41s
                               ETA: 1021289.0s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.702s, learning 0.164s)
               Value function loss: 166.5235
                    Surrogate loss: 0.0026
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.91
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 8.87s
                        Total time: 22086.28s
                               ETA: 1021206.1s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.685s, learning 0.182s)
               Value function loss: 85.6697
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 48.37
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 8.87s
                        Total time: 22095.15s
                               ETA: 1021123.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.938s, learning 0.158s)
               Value function loss: 20.0165
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 6.30
               Mean episode length: 48.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 9.10s
                        Total time: 22104.24s
                               ETA: 1021051.1s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.821s, learning 0.209s)
               Value function loss: 56.1759
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 6.13
               Mean episode length: 47.86
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 9.03s
                        Total time: 22113.27s
                               ETA: 1020976.0s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.161s)
               Value function loss: 426.2887
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 14.03
               Mean episode length: 48.76
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 8.55s
                        Total time: 22121.82s
                               ETA: 1020878.9s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.468s, learning 0.198s)
               Value function loss: 5.1491
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 8.67s
                        Total time: 22130.49s
                               ETA: 1020787.1s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.520s, learning 0.255s)
               Value function loss: 39.6099
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 48.86
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 8.77s
                        Total time: 22139.26s
                               ETA: 1020700.4s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.651s, learning 0.167s)
               Value function loss: 217.8318
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 48.74
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 8.82s
                        Total time: 22148.08s
                               ETA: 1020615.7s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.632s, learning 0.342s)
               Value function loss: 53.5495
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 6.36
               Mean episode length: 48.78
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 8.97s
                        Total time: 22157.06s
                               ETA: 1020538.3s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.160s)
               Value function loss: 32.7131
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 6.67
               Mean episode length: 49.41
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 8.62s
                        Total time: 22165.67s
                               ETA: 1020444.7s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.490s, learning 0.173s)
               Value function loss: 69.6991
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 49.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 8.66s
                        Total time: 22174.34s
                               ETA: 1020353.2s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.809s, learning 0.165s)
               Value function loss: 387.6436
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 49.30
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 8.97s
                        Total time: 22183.31s
                               ETA: 1020276.0s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.456s, learning 0.160s)
               Value function loss: 63.9841
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 49.02
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 8.62s
                        Total time: 22191.93s
                               ETA: 1020182.4s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.328s, learning 0.160s)
               Value function loss: 247.0358
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 24.03
               Mean episode length: 49.86
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 8.49s
                        Total time: 22200.42s
                               ETA: 1020083.0s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.548s, learning 0.323s)
               Value function loss: 22.4582
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 6.06
               Mean episode length: 48.46
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 8.87s
                        Total time: 22209.29s
                               ETA: 1020001.4s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.795s, learning 0.165s)
               Value function loss: 69.0149
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 48.83
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 8.96s
                        Total time: 22218.25s
                               ETA: 1019923.8s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.489s, learning 0.164s)
               Value function loss: 38.6894
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 8.65s
                        Total time: 22226.90s
                               ETA: 1019832.3s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.465s, learning 0.327s)
               Value function loss: 72.3400
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 13.73
               Mean episode length: 48.52
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 8.79s
                        Total time: 22235.69s
                               ETA: 1019747.2s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.676s, learning 0.183s)
               Value function loss: 532.2774
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 49.41
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 8.86s
                        Total time: 22244.55s
                               ETA: 1019665.2s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.424s, learning 0.167s)
               Value function loss: 215.5351
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 18.63
               Mean episode length: 48.92
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 8.59s
                        Total time: 22253.14s
                               ETA: 1019571.0s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.673s, learning 0.209s)
               Value function loss: 198.9899
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.87
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 8.88s
                        Total time: 22262.02s
                               ETA: 1019490.3s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.165s)
               Value function loss: 179.5832
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 6.57
               Mean episode length: 49.08
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 8.64s
                        Total time: 22270.66s
                               ETA: 1019398.4s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.595s, learning 0.161s)
               Value function loss: 82.7050
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 49.25
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 8.76s
                        Total time: 22279.42s
                               ETA: 1019312.0s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.430s, learning 0.205s)
               Value function loss: 181.4098
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 6.88
               Mean episode length: 50.92
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 8.63s
                        Total time: 22288.05s
                               ETA: 1019220.2s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.626s, learning 0.156s)
               Value function loss: 5.1054
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 13.60
               Mean episode length: 48.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 8.78s
                        Total time: 22296.84s
                               ETA: 1019135.1s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.475s, learning 0.220s)
               Value function loss: 15.7849
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 48.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 8.69s
                        Total time: 22305.53s
                               ETA: 1019046.2s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.813s, learning 0.210s)
               Value function loss: 115.5248
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 47.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 9.02s
                        Total time: 22314.55s
                               ETA: 1018972.2s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.701s, learning 0.260s)
               Value function loss: 2.4008
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 8.96s
                        Total time: 22323.51s
                               ETA: 1018895.6s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.765s, learning 0.214s)
               Value function loss: 1.9871
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.98s
                        Total time: 22332.49s
                               ETA: 1018819.8s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.646s, learning 0.229s)
               Value function loss: 1.5966
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 48.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 8.88s
                        Total time: 22341.37s
                               ETA: 1018739.3s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.795s, learning 0.269s)
               Value function loss: 54.8319
                    Surrogate loss: 0.0048
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 47.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 9.06s
                        Total time: 22350.43s
                               ETA: 1018667.5s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.694s, learning 0.257s)
               Value function loss: 271.4602
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 13.19
               Mean episode length: 48.28
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.95s
                        Total time: 22359.38s
                               ETA: 1018590.6s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1784 steps/s (collection: 8.931s, learning 0.249s)
               Value function loss: 1.2868
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 49.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 9.18s
                        Total time: 22368.56s
                               ETA: 1018524.2s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.614s, learning 0.162s)
               Value function loss: 1.6060
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 48.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.78s
                        Total time: 22377.34s
                               ETA: 1018439.4s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.395s, learning 0.189s)
               Value function loss: 435.9093
                    Surrogate loss: 0.0124
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 48.01
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.58s
                        Total time: 22385.92s
                               ETA: 1018346.1s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.580s, learning 0.165s)
               Value function loss: 141.4644
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 13.27
               Mean episode length: 48.46
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.75s
                        Total time: 22394.67s
                               ETA: 1018260.1s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.628s, learning 0.353s)
               Value function loss: 314.6142
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 48.10
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 8.98s
                        Total time: 22403.65s
                               ETA: 1018184.9s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.744s, learning 0.161s)
               Value function loss: 166.4652
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 21.37
               Mean episode length: 49.70
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 8.90s
                        Total time: 22412.55s
                               ETA: 1018106.3s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.412s, learning 0.268s)
               Value function loss: 11.4429
                    Surrogate loss: -0.0205
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.68s
                        Total time: 22421.23s
                               ETA: 1018017.6s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.565s, learning 0.270s)
               Value function loss: 19.2042
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 11.06
               Mean episode length: 49.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.83s
                        Total time: 22430.07s
                               ETA: 1017935.9s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.433s, learning 0.367s)
               Value function loss: 99.9638
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 13.05
               Mean episode length: 48.04
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.80s
                        Total time: 22438.87s
                               ETA: 1017852.8s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.695s, learning 0.257s)
               Value function loss: 57.2019
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 8.45
               Mean episode length: 49.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.95s
                        Total time: 22447.82s
                               ETA: 1017776.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.387s, learning 0.214s)
               Value function loss: 18.4443
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: 3.02
               Mean episode length: 48.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.60s
                        Total time: 22456.42s
                               ETA: 1017684.6s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1781 steps/s (collection: 8.910s, learning 0.284s)
               Value function loss: 322.2017
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 3.27
               Mean episode length: 49.35
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 9.19s
                        Total time: 22465.61s
                               ETA: 1017619.5s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1776 steps/s (collection: 8.987s, learning 0.234s)
               Value function loss: 138.7094
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 5.42
               Mean episode length: 47.87
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 9.22s
                        Total time: 22474.84s
                               ETA: 1017555.7s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.438s, learning 0.250s)
               Value function loss: 3.2858
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 2.59
               Mean episode length: 47.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.69s
                        Total time: 22483.52s
                               ETA: 1017467.9s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.631s, learning 0.179s)
               Value function loss: 65.2194
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.85
               Mean episode length: 47.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.81s
                        Total time: 22492.33s
                               ETA: 1017385.6s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.244s, learning 0.361s)
               Value function loss: 2.8106
                    Surrogate loss: -0.0245
             Mean action noise std: 0.71
                       Mean reward: 3.28
               Mean episode length: 49.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.61s
                        Total time: 22500.94s
                               ETA: 1017294.1s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.422s, learning 0.219s)
               Value function loss: 18.4989
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 49.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.64s
                        Total time: 22509.58s
                               ETA: 1017204.3s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.609s, learning 0.418s)
               Value function loss: 1.8448
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 48.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 9.03s
                        Total time: 22518.61s
                               ETA: 1017132.0s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.531s, learning 0.170s)
               Value function loss: 18.1845
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.04
               Mean episode length: 48.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 8.70s
                        Total time: 22527.31s
                               ETA: 1017045.1s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.837s, learning 0.261s)
               Value function loss: 1.4293
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 49.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 9.10s
                        Total time: 22536.41s
                               ETA: 1016976.2s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.626s, learning 0.187s)
               Value function loss: 109.0572
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 49.90
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.81s
                        Total time: 22545.22s
                               ETA: 1016894.4s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.677s, learning 0.209s)
               Value function loss: 18.1107
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 13.59
               Mean episode length: 50.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.89s
                        Total time: 22554.11s
                               ETA: 1016816.0s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.529s, learning 0.265s)
               Value function loss: 1.0689
                    Surrogate loss: -0.0278
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 51.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.79s
                        Total time: 22562.90s
                               ETA: 1016733.5s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1747 steps/s (collection: 8.956s, learning 0.420s)
               Value function loss: 141.3463
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 48.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 9.38s
                        Total time: 22572.28s
                               ETA: 1016677.3s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.820s, learning 0.338s)
               Value function loss: 302.2376
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 5.62
               Mean episode length: 48.06
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 9.16s
                        Total time: 22581.43s
                               ETA: 1016611.4s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.443s, learning 0.192s)
               Value function loss: 9.6103
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.64s
                        Total time: 22590.07s
                               ETA: 1016521.9s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1781 steps/s (collection: 8.843s, learning 0.353s)
               Value function loss: 429.9977
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 51.67
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 9.20s
                        Total time: 22599.27s
                               ETA: 1016457.8s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.848s, learning 0.363s)
               Value function loss: 20.7776
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 49.06
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 9.21s
                        Total time: 22608.48s
                               ETA: 1016394.4s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.640s, learning 0.160s)
               Value function loss: 31.8960
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 16.94
               Mean episode length: 50.84
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.80s
                        Total time: 22617.28s
                               ETA: 1016312.6s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.427s, learning 0.168s)
               Value function loss: 146.3206
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 11.78
               Mean episode length: 49.46
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.59s
                        Total time: 22625.87s
                               ETA: 1016221.6s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.600s, learning 0.184s)
               Value function loss: 330.9617
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.45
               Mean episode length: 50.14
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.78s
                        Total time: 22634.66s
                               ETA: 1016139.2s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.505s, learning 0.163s)
               Value function loss: 6.3712
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 49.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.67s
                        Total time: 22643.32s
                               ETA: 1016051.6s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.572s, learning 0.189s)
               Value function loss: 1.9103
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 6.24
               Mean episode length: 48.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.76s
                        Total time: 22652.08s
                               ETA: 1015968.3s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.605s, learning 0.202s)
               Value function loss: 55.6172
                    Surrogate loss: 0.0082
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 50.55
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.81s
                        Total time: 22660.89s
                               ETA: 1015887.2s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.769s, learning 0.165s)
               Value function loss: 35.5861
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 6.12
               Mean episode length: 48.95
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.93s
                        Total time: 22669.83s
                               ETA: 1015811.8s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.422s, learning 0.195s)
               Value function loss: 5.0153
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 50.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.62s
                        Total time: 22678.44s
                               ETA: 1015722.2s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.513s, learning 0.167s)
               Value function loss: 77.5562
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 11.26
               Mean episode length: 49.43
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.68s
                        Total time: 22687.12s
                               ETA: 1015635.5s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.627s, learning 0.191s)
               Value function loss: 223.4653
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 50.65
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.82s
                        Total time: 22695.94s
                               ETA: 1015555.1s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.658s, learning 0.186s)
               Value function loss: 0.9030
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.84s
                        Total time: 22704.78s
                               ETA: 1015475.9s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.596s, learning 0.188s)
               Value function loss: 0.7307
                    Surrogate loss: -0.0229
             Mean action noise std: 0.71
                       Mean reward: 18.31
               Mean episode length: 51.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.78s
                        Total time: 22713.57s
                               ETA: 1015394.1s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.713s, learning 0.188s)
               Value function loss: 65.3495
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 49.82
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.90s
                        Total time: 22722.47s
                               ETA: 1015317.6s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.686s, learning 0.181s)
               Value function loss: 0.7575
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 8.14
               Mean episode length: 49.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.87s
                        Total time: 22731.34s
                               ETA: 1015239.6s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.719s, learning 0.186s)
               Value function loss: 13.9344
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.92
               Mean episode length: 48.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.91s
                        Total time: 22740.24s
                               ETA: 1015163.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.498s, learning 0.160s)
               Value function loss: 0.7832
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 5.61
               Mean episode length: 48.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.66s
                        Total time: 22748.90s
                               ETA: 1015076.2s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.913s, learning 0.191s)
               Value function loss: 0.6615
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 50.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 9.10s
                        Total time: 22758.00s
                               ETA: 1015009.1s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.778s, learning 0.160s)
               Value function loss: 0.4260
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 2.78
               Mean episode length: 48.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.94s
                        Total time: 22766.94s
                               ETA: 1014934.5s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.475s, learning 0.182s)
               Value function loss: 39.6476
                    Surrogate loss: 0.0053
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 49.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.66s
                        Total time: 22775.60s
                               ETA: 1014847.5s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.623s, learning 0.221s)
               Value function loss: 0.4021
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 52.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.84s
                        Total time: 22784.44s
                               ETA: 1014768.9s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.965s, learning 0.171s)
               Value function loss: 10.3058
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.01
               Mean episode length: 49.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 9.14s
                        Total time: 22793.58s
                               ETA: 1014703.3s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.677s, learning 0.160s)
               Value function loss: 10.2477
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 2.81
               Mean episode length: 49.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.84s
                        Total time: 22802.42s
                               ETA: 1014624.5s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.550s, learning 0.210s)
               Value function loss: 0.3620
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 51.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.76s
                        Total time: 22811.18s
                               ETA: 1014542.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.171s)
               Value function loss: 16.7960
                    Surrogate loss: 0.0040
             Mean action noise std: 0.71
                       Mean reward: 6.03
               Mean episode length: 50.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.86s
                        Total time: 22820.04s
                               ETA: 1014464.8s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.352s, learning 0.170s)
               Value function loss: 105.6107
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 49.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.52s
                        Total time: 22828.56s
                               ETA: 1014372.2s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.734s, learning 0.160s)
               Value function loss: 29.9243
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 50.70
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 8.89s
                        Total time: 22837.45s
                               ETA: 1014296.2s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.801s, learning 0.161s)
               Value function loss: 0.4206
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 49.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 8.96s
                        Total time: 22846.42s
                               ETA: 1014223.3s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.730s, learning 0.185s)
               Value function loss: 551.5135
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 48.75
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.92s
                        Total time: 22855.33s
                               ETA: 1014148.3s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1791 steps/s (collection: 8.964s, learning 0.179s)
               Value function loss: 262.6355
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 16.48
               Mean episode length: 50.50
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 9.14s
                        Total time: 22864.48s
                               ETA: 1014083.5s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.541s, learning 0.186s)
               Value function loss: 213.6963
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 49.40
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.73s
                        Total time: 22873.20s
                               ETA: 1014000.4s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.426s, learning 0.252s)
               Value function loss: 18.0055
                    Surrogate loss: 0.0079
             Mean action noise std: 0.71
                       Mean reward: 11.04
               Mean episode length: 50.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 16.68s
                        Total time: 22889.88s
                               ETA: 1014269.6s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.089s, learning 0.237s)
               Value function loss: 139.5992
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 48.49
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 17.33s
                        Total time: 22907.21s
                               ETA: 1014567.2s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.305s, learning 0.166s)
               Value function loss: 15.7233
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 49.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 16.47s
                        Total time: 22923.68s
                               ETA: 1014826.7s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.734s, learning 0.173s)
               Value function loss: 115.9568
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 6.58
               Mean episode length: 50.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 16.91s
                        Total time: 22940.58s
                               ETA: 1015105.2s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.642s, learning 0.166s)
               Value function loss: 16.1914
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 11.08
               Mean episode length: 49.77
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 16.81s
                        Total time: 22957.39s
                               ETA: 1015379.2s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.623s, learning 0.222s)
               Value function loss: 3.6510
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 49.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 16.85s
                        Total time: 22974.24s
                               ETA: 1015654.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.734s, learning 0.164s)
               Value function loss: 1.8587
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 50.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 16.90s
                        Total time: 22991.14s
                               ETA: 1015931.8s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.939s, learning 0.161s)
               Value function loss: 90.6420
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 17.10s
                        Total time: 23008.24s
                               ETA: 1016217.8s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.061s, learning 0.195s)
               Value function loss: 18.4193
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 17.26s
                        Total time: 23025.49s
                               ETA: 1016510.4s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.282s, learning 0.198s)
               Value function loss: 19.5131
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 49.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 16.48s
                        Total time: 23041.97s
                               ETA: 1016768.5s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.234s, learning 0.182s)
               Value function loss: 93.1161
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 5.71
               Mean episode length: 48.46
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 17.42s
                        Total time: 23059.39s
                               ETA: 1017067.7s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.151s, learning 0.161s)
               Value function loss: 8.9249
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 3.05
               Mean episode length: 48.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 17.31s
                        Total time: 23076.70s
                               ETA: 1017361.9s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.745s, learning 0.175s)
               Value function loss: 18.5232
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 49.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 16.92s
                        Total time: 23093.62s
                               ETA: 1017638.7s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.709s, learning 0.187s)
               Value function loss: 68.9174
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 49.61
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 16.90s
                        Total time: 23110.51s
                               ETA: 1017914.0s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.648s, learning 0.221s)
               Value function loss: 10.2254
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 8.92
               Mean episode length: 50.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 16.87s
                        Total time: 23127.38s
                               ETA: 1018188.0s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.881s, learning 0.163s)
               Value function loss: 172.7348
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 8.82
               Mean episode length: 50.19
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 17.04s
                        Total time: 23144.43s
                               ETA: 1018469.4s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.720s, learning 0.166s)
               Value function loss: 19.7121
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 48.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 16.89s
                        Total time: 23161.31s
                               ETA: 1018743.5s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.603s, learning 0.180s)
               Value function loss: 199.5843
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 49.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 16.78s
                        Total time: 23178.10s
                               ETA: 1019012.9s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.756s, learning 0.168s)
               Value function loss: 593.6335
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 48.33
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 16.92s
                        Total time: 23195.02s
                               ETA: 1019288.2s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.681s, learning 0.164s)
               Value function loss: 808.1801
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 16.07
               Mean episode length: 48.59
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 16.85s
                        Total time: 23211.86s
                               ETA: 1019559.8s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.658s, learning 0.178s)
               Value function loss: 606.1525
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 49.81
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 16.84s
                        Total time: 23228.70s
                               ETA: 1019830.7s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.925s, learning 0.176s)
               Value function loss: 286.9873
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 49.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 17.10s
                        Total time: 23245.80s
                               ETA: 1020113.0s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.712s, learning 0.157s)
               Value function loss: 51.1551
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 13.94
               Mean episode length: 50.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 16.87s
                        Total time: 23262.67s
                               ETA: 1020384.8s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.679s, learning 0.267s)
               Value function loss: 343.3641
                    Surrogate loss: 0.0052
             Mean action noise std: 0.71
                       Mean reward: 11.35
               Mean episode length: 49.27
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 16.95s
                        Total time: 23279.62s
                               ETA: 1020659.8s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 926 steps/s (collection: 17.400s, learning 0.284s)
               Value function loss: 116.5625
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 16.54
               Mean episode length: 51.29
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 17.68s
                        Total time: 23297.30s
                               ETA: 1020966.9s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.997s, learning 0.186s)
               Value function loss: 83.0649
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 48.41
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 17.18s
                        Total time: 23314.48s
                               ETA: 1021251.7s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.091s, learning 0.158s)
               Value function loss: 4.7650
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 49.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 17.25s
                        Total time: 23331.73s
                               ETA: 1021539.2s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.555s, learning 0.182s)
               Value function loss: 3.4007
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 50.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 16.74s
                        Total time: 23348.47s
                               ETA: 1021803.9s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.593s, learning 0.165s)
               Value function loss: 2.4777
                    Surrogate loss: -0.0212
             Mean action noise std: 0.71
                       Mean reward: 11.47
               Mean episode length: 50.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 16.76s
                        Total time: 23365.23s
                               ETA: 1022069.3s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.593s, learning 0.170s)
               Value function loss: 1.6430
                    Surrogate loss: -0.0255
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 48.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 16.76s
                        Total time: 23381.99s
                               ETA: 1022334.7s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.670s, learning 0.161s)
               Value function loss: 93.0252
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 50.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 16.83s
                        Total time: 23398.82s
                               ETA: 1022602.8s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.734s, learning 0.183s)
               Value function loss: 299.6960
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.96
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 16.92s
                        Total time: 23415.74s
                               ETA: 1022874.5s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.496s, learning 0.186s)
               Value function loss: 1.3872
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 50.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 16.68s
                        Total time: 23432.42s
                               ETA: 1023135.5s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 933 steps/s (collection: 17.388s, learning 0.157s)
               Value function loss: 1.1120
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 48.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 17.54s
                        Total time: 23449.97s
                               ETA: 1023434.0s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 937 steps/s (collection: 17.154s, learning 0.329s)
               Value function loss: 18.1638
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 9.03
               Mean episode length: 48.95
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 17.48s
                        Total time: 23467.45s
                               ETA: 1023729.5s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.137s, learning 0.276s)
               Value function loss: 47.4260
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 48.33
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 17.41s
                        Total time: 23484.86s
                               ETA: 1024021.7s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.713s, learning 0.185s)
               Value function loss: 17.7232
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 49.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 16.90s
                        Total time: 23501.76s
                               ETA: 1024291.1s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1082 steps/s (collection: 14.850s, learning 0.279s)
               Value function loss: 9.8194
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 49.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 15.13s
                        Total time: 23516.89s
                               ETA: 1024483.2s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.393s, learning 0.185s)
               Value function loss: 48.6392
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 47.90
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.58s
                        Total time: 23525.46s
                               ETA: 1024389.9s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.469s, learning 0.159s)
               Value function loss: 237.2870
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 49.38
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.63s
                        Total time: 23534.09s
                               ETA: 1024298.9s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.373s, learning 0.175s)
               Value function loss: 0.9052
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.55s
                        Total time: 23542.64s
                               ETA: 1024204.4s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.535s, learning 0.200s)
               Value function loss: 64.6747
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 48.86
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.74s
                        Total time: 23551.38s
                               ETA: 1024118.2s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.719s, learning 0.162s)
               Value function loss: 191.7820
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 48.58
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.88s
                        Total time: 23560.26s
                               ETA: 1024038.3s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.644s, learning 0.233s)
               Value function loss: 59.1212
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 4.27
               Mean episode length: 50.66
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.88s
                        Total time: 23569.13s
                               ETA: 1023958.4s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.610s, learning 0.158s)
               Value function loss: 1.6584
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.77s
                        Total time: 23577.90s
                               ETA: 1023873.8s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.376s, learning 0.179s)
               Value function loss: 17.9637
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 48.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.56s
                        Total time: 23586.46s
                               ETA: 1023780.0s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.227s, learning 0.236s)
               Value function loss: 108.5072
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 49.48
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.46s
                        Total time: 23594.92s
                               ETA: 1023682.3s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.264s, learning 0.182s)
               Value function loss: 0.9099
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 51.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.45s
                        Total time: 23603.37s
                               ETA: 1023584.0s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.702s, learning 0.186s)
               Value function loss: 511.7251
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 24.19
               Mean episode length: 49.65
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.89s
                        Total time: 23612.25s
                               ETA: 1023504.8s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.415s, learning 0.170s)
               Value function loss: 0.8599
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 48.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.59s
                        Total time: 23620.84s
                               ETA: 1023412.6s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.800s, learning 0.299s)
               Value function loss: 16.7609
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 9.10s
                        Total time: 23629.94s
                               ETA: 1023342.7s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.418s, learning 0.172s)
               Value function loss: 593.1492
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 47.91
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.59s
                        Total time: 23638.53s
                               ETA: 1023250.9s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.471s, learning 0.184s)
               Value function loss: 153.6923
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.41
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.65s
                        Total time: 23647.18s
                               ETA: 1023161.9s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.166s)
               Value function loss: 149.8994
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 48.40
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.63s
                        Total time: 23655.82s
                               ETA: 1023072.1s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.579s, learning 0.184s)
               Value function loss: 279.0766
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 11.39
               Mean episode length: 48.70
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.76s
                        Total time: 23664.58s
                               ETA: 1022988.0s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.548s, learning 0.160s)
               Value function loss: 55.4451
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 6.32
               Mean episode length: 48.39
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.71s
                        Total time: 23673.29s
                               ETA: 1022901.6s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.739s, learning 0.160s)
               Value function loss: 47.5683
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 9.11
               Mean episode length: 47.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.90s
                        Total time: 23682.19s
                               ETA: 1022823.5s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.478s, learning 0.174s)
               Value function loss: 83.5446
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 48.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.65s
                        Total time: 23690.84s
                               ETA: 1022734.7s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.164s)
               Value function loss: 47.6721
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 8.71s
                        Total time: 23699.55s
                               ETA: 1022648.7s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.579s, learning 0.169s)
               Value function loss: 31.5779
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.45
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.75s
                        Total time: 23708.30s
                               ETA: 1022564.3s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.457s, learning 0.173s)
               Value function loss: 2.6883
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.63s
                        Total time: 23716.93s
                               ETA: 1022474.8s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.385s, learning 0.255s)
               Value function loss: 132.6494
                    Surrogate loss: 0.0068
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.81
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 8.64s
                        Total time: 23725.57s
                               ETA: 1022385.9s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.147s, learning 0.196s)
               Value function loss: 210.4988
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 18.79
               Mean episode length: 48.54
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.34s
                        Total time: 23733.91s
                               ETA: 1022284.1s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.752s, learning 0.183s)
               Value function loss: 23.7108
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 8.94s
                        Total time: 23742.85s
                               ETA: 1022208.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.454s, learning 0.157s)
               Value function loss: 2.4393
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 49.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 8.61s
                        Total time: 23751.46s
                               ETA: 1022118.0s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.649s, learning 0.248s)
               Value function loss: 72.9190
                    Surrogate loss: 0.0074
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 49.65
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 8.90s
                        Total time: 23760.36s
                               ETA: 1022040.4s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.161s)
               Value function loss: 470.3925
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 6.13
               Mean episode length: 48.08
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 8.60s
                        Total time: 23768.96s
                               ETA: 1021950.1s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.164s, learning 0.165s)
               Value function loss: 313.9872
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 49.35
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.33s
                        Total time: 23777.29s
                               ETA: 1021848.2s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.504s, learning 0.180s)
               Value function loss: 435.3473
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 6.10
               Mean episode length: 48.91
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 8.68s
                        Total time: 23785.97s
                               ETA: 1021761.6s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.659s, learning 0.221s)
               Value function loss: 500.3537
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 49.82
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 8.88s
                        Total time: 23794.85s
                               ETA: 1021683.6s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.424s, learning 0.165s)
               Value function loss: 109.1418
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 5.74
               Mean episode length: 47.72
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 8.59s
                        Total time: 23803.44s
                               ETA: 1021593.0s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.446s, learning 0.173s)
               Value function loss: 5.1930
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 48.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 8.62s
                        Total time: 23812.06s
                               ETA: 1021503.9s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.484s, learning 0.279s)
               Value function loss: 559.0075
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.12
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 8.76s
                        Total time: 23820.82s
                               ETA: 1021420.9s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.357s, learning 0.170s)
               Value function loss: 467.4741
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 49.29
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 8.53s
                        Total time: 23829.35s
                               ETA: 1021328.0s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.618s, learning 0.284s)
               Value function loss: 131.9548
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 47.82
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 8.90s
                        Total time: 23838.25s
                               ETA: 1021251.1s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.498s, learning 0.168s)
               Value function loss: 26.3499
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 11.42
               Mean episode length: 48.18
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 8.67s
                        Total time: 23846.92s
                               ETA: 1021164.3s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.575s, learning 0.172s)
               Value function loss: 5.3968
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 2.92
               Mean episode length: 46.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 8.75s
                        Total time: 23855.66s
                               ETA: 1021080.9s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.741s, learning 0.165s)
               Value function loss: 3.1853
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 3.11
               Mean episode length: 47.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 8.91s
                        Total time: 23864.57s
                               ETA: 1021004.4s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.606s, learning 0.183s)
               Value function loss: 77.5167
                    Surrogate loss: 0.0017
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 49.54
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 8.79s
                        Total time: 23873.36s
                               ETA: 1020923.0s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.473s, learning 0.322s)
               Value function loss: 198.2909
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 13.95
               Mean episode length: 49.81
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 8.80s
                        Total time: 23882.15s
                               ETA: 1020841.9s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.943s, learning 0.195s)
               Value function loss: 2.3978
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 48.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 9.14s
                        Total time: 23891.29s
                               ETA: 1020775.5s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.777s, learning 0.210s)
               Value function loss: 1.3625
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 8.99s
                        Total time: 23900.28s
                               ETA: 1020702.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.488s, learning 0.253s)
               Value function loss: 20.0739
                    Surrogate loss: 0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 8.74s
                        Total time: 23909.02s
                               ETA: 1020619.5s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.939s, learning 0.166s)
               Value function loss: 91.7668
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 8.43
               Mean episode length: 49.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 9.10s
                        Total time: 23918.12s
                               ETA: 1020551.9s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.742s, learning 0.159s)
               Value function loss: 84.3100
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 48.19
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 8.90s
                        Total time: 23927.02s
                               ETA: 1020475.6s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.452s, learning 0.438s)
               Value function loss: 29.3170
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 49.96
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 8.89s
                        Total time: 23935.92s
                               ETA: 1020398.9s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.459s, learning 0.165s)
               Value function loss: 149.2922
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.27
               Mean episode length: 47.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 8.62s
                        Total time: 23944.54s
                               ETA: 1020311.0s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.507s, learning 0.195s)
               Value function loss: 125.0955
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 8.70s
                        Total time: 23953.24s
                               ETA: 1020226.4s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.418s, learning 0.205s)
               Value function loss: 265.0622
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 47.09
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 8.62s
                        Total time: 23961.86s
                               ETA: 1020138.5s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.651s, learning 0.317s)
               Value function loss: 979.9088
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 48.54
                  Mean reward/step: 0.38
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 8.97s
                        Total time: 23970.83s
                               ETA: 1020065.4s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.163s)
               Value function loss: 598.5209
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 48.15
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 8.49s
                        Total time: 23979.32s
                               ETA: 1019971.8s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.604s, learning 0.160s)
               Value function loss: 73.9174
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 46.21
               Mean episode length: 48.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 8.76s
                        Total time: 23988.08s
                               ETA: 1019890.2s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.588s, learning 0.160s)
               Value function loss: 97.1812
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 14.11
               Mean episode length: 48.02
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 8.75s
                        Total time: 23996.83s
                               ETA: 1019807.9s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.298s, learning 0.197s)
               Value function loss: 242.7621
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 46.29
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 8.49s
                        Total time: 24005.32s
                               ETA: 1019714.9s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.690s, learning 0.227s)
               Value function loss: 527.3421
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 3.04
               Mean episode length: 47.08
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 8.92s
                        Total time: 24014.24s
                               ETA: 1019639.9s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.602s, learning 0.172s)
               Value function loss: 439.3767
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 8.77s
                        Total time: 24023.02s
                               ETA: 1019559.0s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.564s, learning 0.168s)
               Value function loss: 84.2649
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 10.79
               Mean episode length: 47.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 8.73s
                        Total time: 24031.75s
                               ETA: 1019476.3s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.711s, learning 0.170s)
               Value function loss: 66.1667
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 49.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 8.88s
                        Total time: 24040.63s
                               ETA: 1019400.0s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.557s, learning 0.288s)
               Value function loss: 8.3421
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 48.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 8.84s
                        Total time: 24049.48s
                               ETA: 1019322.1s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.468s, learning 0.170s)
               Value function loss: 280.4457
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 47.19
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 8.64s
                        Total time: 24058.11s
                               ETA: 1019235.6s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.766s, learning 0.205s)
               Value function loss: 23.0174
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 3.13
               Mean episode length: 48.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 8.97s
                        Total time: 24067.08s
                               ETA: 1019163.3s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.860s, learning 0.205s)
               Value function loss: 136.0938
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 10.72
               Mean episode length: 47.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 9.06s
                        Total time: 24076.15s
                               ETA: 1019095.0s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.621s, learning 0.301s)
               Value function loss: 3.3805
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 3.11
               Mean episode length: 47.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 8.92s
                        Total time: 24085.07s
                               ETA: 1019020.7s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.474s, learning 0.171s)
               Value function loss: 2.5028
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 3.04
               Mean episode length: 46.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 8.64s
                        Total time: 24093.72s
                               ETA: 1018934.7s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.375s, learning 0.174s)
               Value function loss: 470.9607
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 2.89
               Mean episode length: 47.77
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 8.55s
                        Total time: 24102.27s
                               ETA: 1018844.8s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.787s, learning 0.165s)
               Value function loss: 733.8729
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 46.67
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 8.95s
                        Total time: 24111.22s
                               ETA: 1018771.9s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.323s, learning 0.291s)
               Value function loss: 522.5389
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 5.67
               Mean episode length: 47.47
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.61s
                        Total time: 24119.83s
                               ETA: 1018684.8s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.632s, learning 0.161s)
               Value function loss: 53.0892
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2.87
               Mean episode length: 46.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.79s
                        Total time: 24128.62s
                               ETA: 1018605.4s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.624s, learning 0.169s)
               Value function loss: 37.8442
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 27.86
               Mean episode length: 47.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 8.79s
                        Total time: 24137.42s
                               ETA: 1018526.0s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.167s, learning 0.161s)
               Value function loss: 4.4793
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 48.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.33s
                        Total time: 24145.74s
                               ETA: 1018427.0s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.530s, learning 0.175s)
               Value function loss: 2.0070
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 47.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 8.71s
                        Total time: 24154.45s
                               ETA: 1018344.0s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.578s, learning 0.177s)
               Value function loss: 17.6942
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 48.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.75s
                        Total time: 24163.20s
                               ETA: 1018263.2s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.620s, learning 0.205s)
               Value function loss: 63.6625
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 5.75
               Mean episode length: 47.33
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 8.82s
                        Total time: 24172.03s
                               ETA: 1018185.4s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.712s, learning 0.310s)
               Value function loss: 97.1739
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.70
               Mean episode length: 47.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 9.02s
                        Total time: 24181.05s
                               ETA: 1018116.0s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.741s, learning 0.156s)
               Value function loss: 47.1155
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.31
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 8.90s
                        Total time: 24189.95s
                               ETA: 1018041.3s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.692s, learning 0.277s)
               Value function loss: 19.8000
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 48.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 8.97s
                        Total time: 24198.92s
                               ETA: 1017969.8s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.797s, learning 0.166s)
               Value function loss: 62.8266
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 8.42
               Mean episode length: 48.86
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 8.96s
                        Total time: 24207.88s
                               ETA: 1017898.1s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.585s, learning 0.273s)
               Value function loss: 1.3492
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 2.92
               Mean episode length: 46.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.86s
                        Total time: 24216.74s
                               ETA: 1017821.9s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.631s, learning 0.179s)
               Value function loss: 66.8176
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.77
               Mean episode length: 46.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.81s
                        Total time: 24225.55s
                               ETA: 1017743.9s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.505s, learning 0.203s)
               Value function loss: 18.8702
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 46.35
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.82
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.71s
                        Total time: 24234.26s
                               ETA: 1017661.6s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.542s, learning 0.159s)
               Value function loss: 310.0385
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 47.83
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.70s
                        Total time: 24242.96s
                               ETA: 1017579.1s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.546s, learning 0.273s)
               Value function loss: 367.1358
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 13.25
               Mean episode length: 47.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.82s
                        Total time: 24251.77s
                               ETA: 1017501.5s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.568s, learning 0.202s)
               Value function loss: 261.8353
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 47.30
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.77s
                        Total time: 24260.55s
                               ETA: 1017422.0s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.452s, learning 0.268s)
               Value function loss: 123.5157
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 51.14
               Mean episode length: 48.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.72s
                        Total time: 24269.27s
                               ETA: 1017340.5s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.533s, learning 0.160s)
               Value function loss: 7.8521
                    Surrogate loss: 0.0042
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 47.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.69s
                        Total time: 24277.96s
                               ETA: 1017257.9s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.230s, learning 0.267s)
               Value function loss: 3.5432
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2.77
               Mean episode length: 47.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.50s
                        Total time: 24286.46s
                               ETA: 1017167.2s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.449s, learning 0.158s)
               Value function loss: 154.4464
                    Surrogate loss: 0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 47.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.61s
                        Total time: 24295.06s
                               ETA: 1017081.1s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.163s)
               Value function loss: 18.7428
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.86
               Mean episode length: 47.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.59s
                        Total time: 24303.65s
                               ETA: 1016994.3s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.523s, learning 0.297s)
               Value function loss: 357.5778
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 8.15
               Mean episode length: 47.02
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.82s
                        Total time: 24312.47s
                               ETA: 1016917.3s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.569s, learning 0.225s)
               Value function loss: 6.2976
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 2.90
               Mean episode length: 47.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.79s
                        Total time: 24321.27s
                               ETA: 1016839.2s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.526s, learning 0.162s)
               Value function loss: 294.0811
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 15.63
               Mean episode length: 48.05
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.69s
                        Total time: 24329.95s
                               ETA: 1016756.8s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.918s, learning 0.160s)
               Value function loss: 6.1047
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 5.53
               Mean episode length: 46.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 9.08s
                        Total time: 24339.03s
                               ETA: 1016690.7s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.244s, learning 0.229s)
               Value function loss: 422.3437
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 33.01
               Mean episode length: 47.67
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.47s
                        Total time: 24347.50s
                               ETA: 1016599.4s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.160s)
               Value function loss: 4.3603
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 2.92
               Mean episode length: 48.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.74s
                        Total time: 24356.24s
                               ETA: 1016519.1s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.786s, learning 0.166s)
               Value function loss: 2.9761
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 2.97
               Mean episode length: 48.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.95s
                        Total time: 24365.19s
                               ETA: 1016447.9s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.671s, learning 0.254s)
               Value function loss: 613.4945
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 2.63
               Mean episode length: 45.90
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.93s
                        Total time: 24374.12s
                               ETA: 1016375.7s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.565s, learning 0.268s)
               Value function loss: 488.5136
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.97
               Mean episode length: 46.84
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.83s
                        Total time: 24382.95s
                               ETA: 1016299.7s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.164s)
               Value function loss: 38.7272
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 2.74
               Mean episode length: 47.84
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 8.64s
                        Total time: 24391.59s
                               ETA: 1016215.5s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.915s, learning 0.199s)
               Value function loss: 9.8708
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 5.45
               Mean episode length: 47.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 9.11s
                        Total time: 24400.70s
                               ETA: 1016151.3s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.817s, learning 0.161s)
               Value function loss: 8.0999
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 46.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 8.98s
                        Total time: 24409.68s
                               ETA: 1016081.5s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.700s, learning 0.240s)
               Value function loss: 163.4864
                    Surrogate loss: 0.0061
             Mean action noise std: 0.71
                       Mean reward: 2.95
               Mean episode length: 46.56
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.94s
                        Total time: 24418.62s
                               ETA: 1016010.1s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.162s)
               Value function loss: 69.7604
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 7.94
               Mean episode length: 46.45
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.48s
                        Total time: 24427.09s
                               ETA: 1015919.5s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.781s, learning 0.170s)
               Value function loss: 6.3199
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 46.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 8.95s
                        Total time: 24436.05s
                               ETA: 1015848.7s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.169s)
               Value function loss: 194.8311
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.81
               Mean episode length: 46.14
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.72s
                        Total time: 24444.76s
                               ETA: 1015768.3s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.712s, learning 0.173s)
               Value function loss: 521.3805
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 13.10
               Mean episode length: 46.02
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.88s
                        Total time: 24453.65s
                               ETA: 1015694.9s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.667s, learning 0.260s)
               Value function loss: 21.1667
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 17.83
               Mean episode length: 46.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.93s
                        Total time: 24462.58s
                               ETA: 1015623.3s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.440s, learning 0.159s)
               Value function loss: 112.7915
                    Surrogate loss: 0.0070
             Mean action noise std: 0.71
                       Mean reward: 3.14
               Mean episode length: 46.85
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.60s
                        Total time: 24471.17s
                               ETA: 1015538.1s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.417s, learning 0.160s)
               Value function loss: 49.6801
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 8.38
               Mean episode length: 47.42
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.58s
                        Total time: 24479.75s
                               ETA: 1015452.1s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.541s, learning 0.265s)
               Value function loss: 68.2265
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 46.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.81s
                        Total time: 24488.56s
                               ETA: 1015375.6s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.597s, learning 0.305s)
               Value function loss: 58.0964
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 46.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.90s
                        Total time: 24497.46s
                               ETA: 1015303.1s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.567s, learning 0.169s)
               Value function loss: 250.5841
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 8.02
               Mean episode length: 46.55
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 8.74s
                        Total time: 24506.19s
                               ETA: 1015223.9s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.336s, learning 0.202s)
               Value function loss: 394.8093
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 47.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 8.54s
                        Total time: 24514.73s
                               ETA: 1015136.5s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.591s, learning 0.158s)
               Value function loss: 9.8603
                    Surrogate loss: -0.0216
             Mean action noise std: 0.71
                       Mean reward: 33.15
               Mean episode length: 46.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 8.75s
                        Total time: 24523.48s
                               ETA: 1015057.9s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.578s, learning 0.156s)
               Value function loss: 87.3255
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 5.72
               Mean episode length: 46.35
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 8.73s
                        Total time: 24532.21s
                               ETA: 1014978.8s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.676s, learning 0.161s)
               Value function loss: 488.2890
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.64
               Mean episode length: 45.40
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 8.84s
                        Total time: 24541.05s
                               ETA: 1014904.0s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.525s, learning 0.172s)
               Value function loss: 105.8657
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 46.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 8.70s
                        Total time: 24549.75s
                               ETA: 1014823.4s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.479s, learning 0.290s)
               Value function loss: 19.2466
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 6.00
               Mean episode length: 46.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 8.77s
                        Total time: 24558.52s
                               ETA: 1014745.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.686s, learning 0.184s)
               Value function loss: 4.1181
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 38.28
               Mean episode length: 46.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 8.87s
                        Total time: 24567.39s
                               ETA: 1014672.6s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.377s, learning 0.166s)
               Value function loss: 2.3844
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 3.27
               Mean episode length: 46.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 8.54s
                        Total time: 24575.93s
                               ETA: 1014585.8s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.549s, learning 0.214s)
               Value function loss: 90.5441
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 8.01
               Mean episode length: 46.09
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 8.76s
                        Total time: 24584.69s
                               ETA: 1014508.3s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.463s, learning 0.202s)
               Value function loss: 65.8853
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 3.34
               Mean episode length: 47.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 8.66s
                        Total time: 24593.36s
                               ETA: 1014426.7s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.337s, learning 0.157s)
               Value function loss: 96.5370
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 15.52
               Mean episode length: 45.80
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 8.49s
                        Total time: 24601.85s
                               ETA: 1014338.1s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.462s, learning 0.295s)
               Value function loss: 51.4648
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 10.60
               Mean episode length: 45.43
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 8.76s
                        Total time: 24610.61s
                               ETA: 1014260.4s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.728s, learning 0.167s)
               Value function loss: 25.3137
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 5.51
               Mean episode length: 46.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 8.89s
                        Total time: 24619.50s
                               ETA: 1014188.5s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.674s, learning 0.181s)
               Value function loss: 1.5940
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 2.96
               Mean episode length: 46.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 8.86s
                        Total time: 24628.36s
                               ETA: 1014115.0s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.467s, learning 0.355s)
               Value function loss: 35.1656
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 45.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 8.82s
                        Total time: 24637.18s
                               ETA: 1014040.2s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.583s, learning 0.164s)
               Value function loss: 78.4845
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.05
               Mean episode length: 45.21
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 8.75s
                        Total time: 24645.93s
                               ETA: 1013962.3s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.600s, learning 0.250s)
               Value function loss: 368.6861
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 46.58
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 8.85s
                        Total time: 24654.78s
                               ETA: 1013888.8s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.542s, learning 0.164s)
               Value function loss: 481.7252
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 8.55
               Mean episode length: 47.51
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 8.71s
                        Total time: 24663.48s
                               ETA: 1013809.3s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.482s, learning 0.158s)
               Value function loss: 576.5907
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 8.31
               Mean episode length: 45.72
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 8.64s
                        Total time: 24672.12s
                               ETA: 1013727.2s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.564s, learning 0.162s)
               Value function loss: 19.4226
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 3.22
               Mean episode length: 46.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 8.73s
                        Total time: 24680.85s
                               ETA: 1013648.8s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1768 steps/s (collection: 8.910s, learning 0.356s)
               Value function loss: 47.5510
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 26.27
               Mean episode length: 46.78
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 9.27s
                        Total time: 24690.11s
                               ETA: 1013592.6s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.007s, learning 0.214s)
               Value function loss: 242.6877
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 6.03
               Mean episode length: 47.76
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 12.22s
                        Total time: 24702.34s
                               ETA: 1013657.6s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.109s, learning 0.165s)
               Value function loss: 98.0764
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 47.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 17.27s
                        Total time: 24719.61s
                               ETA: 1013929.9s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 928 steps/s (collection: 17.384s, learning 0.270s)
               Value function loss: 367.1323
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 46.15
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 17.65s
                        Total time: 24737.26s
                               ETA: 1014217.4s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.795s, learning 0.264s)
               Value function loss: 383.7862
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 44.84
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 17.06s
                        Total time: 24754.32s
                               ETA: 1014480.4s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.557s, learning 0.285s)
               Value function loss: 388.7254
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 46.96
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 16.84s
                        Total time: 24771.17s
                               ETA: 1014734.2s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.775s, learning 0.175s)
               Value function loss: 383.3647
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 46.78
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 16.95s
                        Total time: 24788.12s
                               ETA: 1014992.2s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.082s, learning 0.328s)
               Value function loss: 168.9102
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 21.22
               Mean episode length: 47.29
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 17.41s
                        Total time: 24805.53s
                               ETA: 1015268.8s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.858s, learning 0.158s)
               Value function loss: 260.9432
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 38.88
               Mean episode length: 48.34
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 17.02s
                        Total time: 24822.54s
                               ETA: 1015529.1s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.894s, learning 0.158s)
               Value function loss: 281.3985
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 11.02
               Mean episode length: 47.50
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 17.05s
                        Total time: 24839.59s
                               ETA: 1015790.6s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.414s, learning 0.160s)
               Value function loss: 156.7441
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 13.39
               Mean episode length: 45.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 16.57s
                        Total time: 24856.17s
                               ETA: 1016032.3s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.996s, learning 0.162s)
               Value function loss: 468.3106
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 46.21
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 17.16s
                        Total time: 24873.33s
                               ETA: 1016297.7s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.558s, learning 0.263s)
               Value function loss: 543.7939
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 53.55
               Mean episode length: 46.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 16.82s
                        Total time: 24890.15s
                               ETA: 1016549.0s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.900s, learning 0.235s)
               Value function loss: 156.6792
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 11.03
               Mean episode length: 46.06
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 17.14s
                        Total time: 24907.28s
                               ETA: 1016812.9s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.642s, learning 0.363s)
               Value function loss: 142.8426
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 16.54
               Mean episode length: 46.80
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 17.01s
                        Total time: 24924.29s
                               ETA: 1017071.4s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.855s, learning 0.309s)
               Value function loss: 21.6167
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 17.16s
                        Total time: 24941.45s
                               ETA: 1017336.0s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.561s, learning 0.245s)
               Value function loss: 242.1482
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 45.44
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 16.81s
                        Total time: 24958.26s
                               ETA: 1017585.9s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 939 steps/s (collection: 17.288s, learning 0.159s)
               Value function loss: 76.7886
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 2.99
               Mean episode length: 45.59
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 17.45s
                        Total time: 24975.70s
                               ETA: 1017861.6s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.418s, learning 0.286s)
               Value function loss: 202.4642
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.89
               Mean episode length: 45.61
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 16.70s
                        Total time: 24992.41s
                               ETA: 1018106.8s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.601s, learning 0.239s)
               Value function loss: 253.9222
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 5.75
               Mean episode length: 46.59
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 16.84s
                        Total time: 25009.25s
                               ETA: 1018357.3s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.212s, learning 0.196s)
               Value function loss: 256.3627
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 47.29
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 17.41s
                        Total time: 25026.65s
                               ETA: 1018630.8s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.043s, learning 0.160s)
               Value function loss: 172.1494
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 45.75
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 17.20s
                        Total time: 25043.86s
                               ETA: 1018895.6s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.061s, learning 0.183s)
               Value function loss: 152.8258
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.34
               Mean episode length: 45.96
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 17.24s
                        Total time: 25061.10s
                               ETA: 1019161.9s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.331s, learning 0.171s)
               Value function loss: 17.6855
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 23.53
               Mean episode length: 46.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 16.50s
                        Total time: 25077.60s
                               ETA: 1019397.8s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.541s, learning 0.165s)
               Value function loss: 17.6716
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 3.10
               Mean episode length: 46.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 16.71s
                        Total time: 25094.31s
                               ETA: 1019641.8s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.493s, learning 0.180s)
               Value function loss: 23.9000
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 2.92
               Mean episode length: 45.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 16.67s
                        Total time: 25110.98s
                               ETA: 1019884.2s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.540s, learning 0.158s)
               Value function loss: 4.8722
                    Surrogate loss: -0.0175
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 46.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 16.70s
                        Total time: 25127.68s
                               ETA: 1020127.4s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.941s, learning 0.252s)
               Value function loss: 192.2964
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 15.91
               Mean episode length: 46.08
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 17.19s
                        Total time: 25144.87s
                               ETA: 1020390.5s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.941s, learning 0.167s)
               Value function loss: 3.2647
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 46.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 17.11s
                        Total time: 25161.98s
                               ETA: 1020649.9s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 948 steps/s (collection: 16.971s, learning 0.295s)
               Value function loss: 208.4069
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.28
               Mean episode length: 46.27
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 17.27s
                        Total time: 25179.25s
                               ETA: 1020915.5s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.903s, learning 0.259s)
               Value function loss: 6.2181
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 46.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 17.16s
                        Total time: 25196.41s
                               ETA: 1021176.6s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.013s, learning 0.277s)
               Value function loss: 167.0852
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 2.95
               Mean episode length: 45.83
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 17.29s
                        Total time: 25213.70s
                               ETA: 1021442.7s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.725s, learning 0.163s)
               Value function loss: 140.3847
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 12.77
               Mean episode length: 44.90
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 16.89s
                        Total time: 25230.59s
                               ETA: 1021692.3s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.844s, learning 0.171s)
               Value function loss: 2.1187
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 6.11
               Mean episode length: 48.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 17.02s
                        Total time: 25247.60s
                               ETA: 1021946.8s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.909s, learning 0.162s)
               Value function loss: 285.1837
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 46.32
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 17.07s
                        Total time: 25264.68s
                               ETA: 1022203.3s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.620s, learning 0.186s)
               Value function loss: 2.0465
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 47.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 16.81s
                        Total time: 25281.48s
                               ETA: 1022448.9s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.914s, learning 0.164s)
               Value function loss: 89.1172
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 16.30
               Mean episode length: 46.74
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 17.08s
                        Total time: 25298.56s
                               ETA: 1022705.3s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 939 steps/s (collection: 17.253s, learning 0.181s)
               Value function loss: 1.4640
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 46.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 17.43s
                        Total time: 25315.99s
                               ETA: 1022975.8s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.638s, learning 0.174s)
               Value function loss: 799.8462
                    Surrogate loss: 0.0027
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 46.75
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 16.81s
                        Total time: 25332.81s
                               ETA: 1023221.0s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1524 steps/s (collection: 10.555s, learning 0.193s)
               Value function loss: 400.3922
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.22
               Mean episode length: 47.19
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 10.75s
                        Total time: 25343.55s
                               ETA: 1023221.1s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.169s)
               Value function loss: 29.3728
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.88
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.63s
                        Total time: 25352.18s
                               ETA: 1023135.7s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.581s, learning 0.174s)
               Value function loss: 258.4034
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 38.49
               Mean episode length: 46.24
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.75s
                        Total time: 25360.94s
                               ETA: 1023055.4s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.599s, learning 0.186s)
               Value function loss: 42.5501
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 3.34
               Mean episode length: 47.23
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.78s
                        Total time: 25369.72s
                               ETA: 1022976.4s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.627s, learning 0.171s)
               Value function loss: 15.8419
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 6.33
               Mean episode length: 48.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.80s
                        Total time: 25378.52s
                               ETA: 1022898.0s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.434s, learning 0.200s)
               Value function loss: 247.6528
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.08
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.63s
                        Total time: 25387.16s
                               ETA: 1022813.0s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.333s, learning 0.163s)
               Value function loss: 5.9603
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 16.11
               Mean episode length: 46.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 8.50s
                        Total time: 25395.65s
                               ETA: 1022722.6s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.642s, learning 0.273s)
               Value function loss: 10.6718
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 47.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.91s
                        Total time: 25404.57s
                               ETA: 1022649.0s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.696s, learning 0.184s)
               Value function loss: 632.6781
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 6.09
               Mean episode length: 47.27
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.88s
                        Total time: 25413.45s
                               ETA: 1022574.2s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.343s, learning 0.193s)
               Value function loss: 231.5396
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 11.12
               Mean episode length: 47.64
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 8.54s
                        Total time: 25421.98s
                               ETA: 1022485.5s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.389s, learning 0.159s)
               Value function loss: 426.7229
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 6.20
               Mean episode length: 47.29
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.55s
                        Total time: 25430.53s
                               ETA: 1022397.4s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.549s, learning 0.160s)
               Value function loss: 118.1807
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 46.69
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.71s
                        Total time: 25439.24s
                               ETA: 1022315.8s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.524s, learning 0.159s)
               Value function loss: 88.3621
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 48.45
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.68s
                        Total time: 25447.92s
                               ETA: 1022233.2s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.559s, learning 0.156s)
               Value function loss: 186.1806
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 46.52
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.72s
                        Total time: 25456.64s
                               ETA: 1022152.0s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.433s, learning 0.198s)
               Value function loss: 40.8144
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 47.67
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 8.63s
                        Total time: 25465.27s
                               ETA: 1022067.5s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.579s, learning 0.236s)
               Value function loss: 497.4840
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 47.04
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.81s
                        Total time: 25474.08s
                               ETA: 1021990.4s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.683s, learning 0.160s)
               Value function loss: 358.2409
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 46.35
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.84s
                        Total time: 25482.92s
                               ETA: 1021914.5s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.653s, learning 0.171s)
               Value function loss: 11.9610
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.82s
                        Total time: 25491.75s
                               ETA: 1021837.9s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.315s, learning 0.341s)
               Value function loss: 208.4715
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.44
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 8.66s
                        Total time: 25500.40s
                               ETA: 1021754.6s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.351s, learning 0.166s)
               Value function loss: 193.2378
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 10.79
               Mean episode length: 46.60
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.52s
                        Total time: 25508.92s
                               ETA: 1021665.8s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.309s, learning 0.176s)
               Value function loss: 86.7366
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 13.77
               Mean episode length: 47.04
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.48s
                        Total time: 25517.41s
                               ETA: 1021575.8s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.931s, learning 0.161s)
               Value function loss: 136.0286
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 6.38
               Mean episode length: 48.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 9.09s
                        Total time: 25526.50s
                               ETA: 1021510.2s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.694s, learning 0.252s)
               Value function loss: 182.5314
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 3.01
               Mean episode length: 46.62
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.95s
                        Total time: 25535.45s
                               ETA: 1021438.8s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.455s, learning 0.252s)
               Value function loss: 222.8755
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 6.90
               Mean episode length: 49.74
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.71s
                        Total time: 25544.15s
                               ETA: 1021357.8s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.579s, learning 0.160s)
               Value function loss: 385.9002
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 2.93
               Mean episode length: 46.20
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.74s
                        Total time: 25552.89s
                               ETA: 1021278.2s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.682s, learning 0.172s)
               Value function loss: 375.3201
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 18.52
               Mean episode length: 47.54
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.85s
                        Total time: 25561.75s
                               ETA: 1021203.2s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.762s, learning 0.187s)
               Value function loss: 151.1104
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 8.31
               Mean episode length: 46.35
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.95s
                        Total time: 25570.69s
                               ETA: 1021132.1s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.503s, learning 0.189s)
               Value function loss: 313.1564
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 15.87
               Mean episode length: 47.52
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.69s
                        Total time: 25579.39s
                               ETA: 1021050.8s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.826s, learning 0.168s)
               Value function loss: 395.1937
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.24
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.99s
                        Total time: 25588.38s
                               ETA: 1020981.6s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.654s, learning 0.163s)
               Value function loss: 136.7795
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 8.33
               Mean episode length: 47.64
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.82s
                        Total time: 25597.20s
                               ETA: 1020905.4s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.473s, learning 0.209s)
               Value function loss: 302.5045
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 6.02
               Mean episode length: 47.77
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.68s
                        Total time: 25605.88s
                               ETA: 1020823.8s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.308s, learning 0.225s)
               Value function loss: 537.3310
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 49.48
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.53s
                        Total time: 25614.41s
                               ETA: 1020736.4s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.382s, learning 0.160s)
               Value function loss: 239.2940
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 21.40
               Mean episode length: 48.26
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.54s
                        Total time: 25622.95s
                               ETA: 1020649.3s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.568s, learning 0.189s)
               Value function loss: 437.8082
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 16.09
               Mean episode length: 47.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 8.76s
                        Total time: 25631.71s
                               ETA: 1020570.9s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.576s, learning 0.203s)
               Value function loss: 32.6966
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 49.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.78s
                        Total time: 25640.49s
                               ETA: 1020493.5s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.422s, learning 0.160s)
               Value function loss: 172.6316
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 47.42
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.58s
                        Total time: 25649.07s
                               ETA: 1020408.3s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.436s, learning 0.186s)
               Value function loss: 251.9481
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 47.65
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 8.62s
                        Total time: 25657.69s
                               ETA: 1020324.7s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.558s, learning 0.158s)
               Value function loss: 235.4589
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.90
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0221
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.72s
                        Total time: 25666.41s
                               ETA: 1020244.9s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.525s, learning 0.171s)
               Value function loss: 584.2094
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 47.98
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 8.70s
                        Total time: 25675.10s
                               ETA: 1020164.4s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.468s, learning 0.158s)
               Value function loss: 614.6426
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.01
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0221
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.63s
                        Total time: 25683.73s
                               ETA: 1020081.2s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.281s, learning 0.265s)
               Value function loss: 221.3047
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 6.34
               Mean episode length: 48.28
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.55s
                        Total time: 25692.27s
                               ETA: 1019994.8s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.505s, learning 0.171s)
               Value function loss: 675.4328
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 66.52
               Mean episode length: 47.98
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 8.68s
                        Total time: 25700.95s
                               ETA: 1019913.7s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.354s, learning 0.163s)
               Value function loss: 807.2283
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 11.24
               Mean episode length: 47.97
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 8.52s
                        Total time: 25709.47s
                               ETA: 1019826.3s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.283s, learning 0.291s)
               Value function loss: 1218.0923
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 29.07
               Mean episode length: 49.02
                  Mean reward/step: 0.42
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.57s
                        Total time: 25718.04s
                               ETA: 1019741.3s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.695s, learning 0.157s)
               Value function loss: 400.9108
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 6.11
               Mean episode length: 47.97
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0293
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 8.85s
                        Total time: 25726.89s
                               ETA: 1019667.3s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.677s, learning 0.165s)
               Value function loss: 326.7769
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 48.43
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.84s
                        Total time: 25735.74s
                               ETA: 1019593.0s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.468s, learning 0.173s)
               Value function loss: 733.6479
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 45.97
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0322
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.64s
                        Total time: 25744.38s
                               ETA: 1019510.8s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.683s, learning 0.178s)
               Value function loss: 150.8414
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 16.28
               Mean episode length: 48.30
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0308
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 8.86s
                        Total time: 25753.24s
                               ETA: 1019437.3s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.242s, learning 0.181s)
               Value function loss: 352.1860
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 26.20
               Mean episode length: 48.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 8.42s
                        Total time: 25761.66s
                               ETA: 1019346.6s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.423s, learning 0.194s)
               Value function loss: 168.7818
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 16.85
               Mean episode length: 48.66
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.62s
                        Total time: 25770.28s
                               ETA: 1019263.6s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.742s, learning 0.180s)
               Value function loss: 98.6835
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.66
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 8.92s
                        Total time: 25779.20s
                               ETA: 1019192.7s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.704s, learning 0.159s)
               Value function loss: 218.8036
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.01
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 8.86s
                        Total time: 25788.06s
                               ETA: 1019119.6s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.466s, learning 0.212s)
               Value function loss: 148.1922
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 11.22
               Mean episode length: 48.66
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.68s
                        Total time: 25796.74s
                               ETA: 1019039.2s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.488s, learning 0.162s)
               Value function loss: 90.8572
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 21.37
               Mean episode length: 48.39
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0286
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 8.65s
                        Total time: 25805.39s
                               ETA: 1018957.7s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.697s, learning 0.203s)
               Value function loss: 382.9883
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.11
               Mean episode length: 47.53
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 8.90s
                        Total time: 25814.29s
                               ETA: 1018886.1s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.534s, learning 0.165s)
               Value function loss: 106.2145
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 20.91
               Mean episode length: 47.24
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 8.70s
                        Total time: 25822.99s
                               ETA: 1018806.8s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.717s, learning 0.161s)
               Value function loss: 9.7171
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 8.88s
                        Total time: 25831.87s
                               ETA: 1018734.5s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.313s, learning 0.303s)
               Value function loss: 16.6683
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 13.87
               Mean episode length: 48.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 8.62s
                        Total time: 25840.48s
                               ETA: 1018651.9s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.654s, learning 0.221s)
               Value function loss: 25.8773
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 48.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 8.87s
                        Total time: 25849.36s
                               ETA: 1018579.6s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.457s, learning 0.186s)
               Value function loss: 931.1721
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 46.76
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 8.64s
                        Total time: 25858.00s
                               ETA: 1018498.2s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.574s, learning 0.295s)
               Value function loss: 79.4491
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 8.62
               Mean episode length: 48.01
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 8.87s
                        Total time: 25866.87s
                               ETA: 1018425.8s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.606s, learning 0.161s)
               Value function loss: 31.6417
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 47.85
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 8.77s
                        Total time: 25875.64s
                               ETA: 1018349.4s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.170s)
               Value function loss: 478.8762
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 21.27
               Mean episode length: 48.61
                  Mean reward/step: 0.32
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 8.68s
                        Total time: 25884.32s
                               ETA: 1018269.7s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.753s, learning 0.188s)
               Value function loss: 360.1761
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 48.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 8.94s
                        Total time: 25893.26s
                               ETA: 1018200.3s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.581s, learning 0.264s)
               Value function loss: 158.7081
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.34
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 8.85s
                        Total time: 25902.11s
                               ETA: 1018127.1s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.585s, learning 0.242s)
               Value function loss: 116.9819
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 9.07
               Mean episode length: 50.17
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0250
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 8.83s
                        Total time: 25910.93s
                               ETA: 1018053.3s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.499s, learning 0.199s)
               Value function loss: 55.9523
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 47.95
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 8.70s
                        Total time: 25919.63s
                               ETA: 1017974.4s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.762s, learning 0.164s)
               Value function loss: 322.5871
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 49.34
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 8.93s
                        Total time: 25928.56s
                               ETA: 1017904.6s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.551s, learning 0.188s)
               Value function loss: 21.4035
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 6.02
               Mean episode length: 47.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 8.74s
                        Total time: 25937.30s
                               ETA: 1017827.5s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.993s, learning 0.169s)
               Value function loss: 82.6628
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 6.29
               Mean episode length: 47.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 9.16s
                        Total time: 25946.46s
                               ETA: 1017767.0s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.658s, learning 0.211s)
               Value function loss: 91.4897
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 8.33
               Mean episode length: 48.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 8.87s
                        Total time: 25955.33s
                               ETA: 1017695.1s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.764s, learning 0.158s)
               Value function loss: 6.4680
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 48.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 8.92s
                        Total time: 25964.25s
                               ETA: 1017625.3s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.576s, learning 0.155s)
               Value function loss: 258.8128
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.78
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 8.73s
                        Total time: 25972.98s
                               ETA: 1017548.1s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.237s, learning 0.162s)
               Value function loss: 334.9497
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.67
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 8.40s
                        Total time: 25981.38s
                               ETA: 1017457.9s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.657s, learning 0.163s)
               Value function loss: 296.4586
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 8.51
               Mean episode length: 48.15
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 8.82s
                        Total time: 25990.20s
                               ETA: 1017384.3s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.567s, learning 0.160s)
               Value function loss: 10.2834
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 3.28
               Mean episode length: 47.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 8.73s
                        Total time: 25998.93s
                               ETA: 1017307.1s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.439s, learning 0.158s)
               Value function loss: 593.7350
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 47.69
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 8.60s
                        Total time: 26007.52s
                               ETA: 1017224.8s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.160s)
               Value function loss: 302.3473
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 48.50
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 8.67s
                        Total time: 26016.19s
                               ETA: 1017145.4s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.465s, learning 0.162s)
               Value function loss: 122.1934
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 33.82
               Mean episode length: 48.31
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 8.63s
                        Total time: 26024.82s
                               ETA: 1017064.5s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.526s, learning 0.157s)
               Value function loss: 43.0961
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 8.56
               Mean episode length: 47.51
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.68s
                        Total time: 26033.50s
                               ETA: 1016985.8s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.636s, learning 0.186s)
               Value function loss: 32.5148
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 5.96
               Mean episode length: 47.35
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.82s
                        Total time: 26042.32s
                               ETA: 1016912.6s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.601s, learning 0.349s)
               Value function loss: 152.1146
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.49
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.95s
                        Total time: 26051.27s
                               ETA: 1016844.4s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1763 steps/s (collection: 8.983s, learning 0.309s)
               Value function loss: 195.3007
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 11.48
               Mean episode length: 48.24
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 9.29s
                        Total time: 26060.56s
                               ETA: 1016789.6s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.970s, learning 0.246s)
               Value function loss: 518.0995
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 8.78
               Mean episode length: 47.90
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 9.22s
                        Total time: 26069.78s
                               ETA: 1016731.9s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.767s, learning 0.161s)
               Value function loss: 132.2854
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 8.45
               Mean episode length: 47.61
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.93s
                        Total time: 26078.71s
                               ETA: 1016663.0s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.669s, learning 0.159s)
               Value function loss: 40.9527
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.33
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.83s
                        Total time: 26087.54s
                               ETA: 1016590.2s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.162s)
               Value function loss: 22.3319
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 8.69s
                        Total time: 26096.22s
                               ETA: 1016512.0s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.713s, learning 0.171s)
               Value function loss: 103.7120
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 3.53
               Mean episode length: 48.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 8.88s
                        Total time: 26105.11s
                               ETA: 1016441.5s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.362s, learning 0.161s)
               Value function loss: 395.4793
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 47.49
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.52s
                        Total time: 26113.63s
                               ETA: 1016357.1s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1782 steps/s (collection: 9.029s, learning 0.165s)
               Value function loss: 746.5554
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 13.68
               Mean episode length: 48.43
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 9.19s
                        Total time: 26122.82s
                               ETA: 1016298.7s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.430s, learning 0.167s)
               Value function loss: 401.3842
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 49.22
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 8.60s
                        Total time: 26131.42s
                               ETA: 1016217.3s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.689s, learning 0.206s)
               Value function loss: 391.4147
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 18.61
               Mean episode length: 48.15
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.90s
                        Total time: 26140.32s
                               ETA: 1016147.5s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.570s, learning 0.158s)
               Value function loss: 222.3199
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 28.68
               Mean episode length: 48.15
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.73s
                        Total time: 26149.04s
                               ETA: 1016071.2s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.162s)
               Value function loss: 28.4491
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 8.67s
                        Total time: 26157.71s
                               ETA: 1015992.6s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.599s, learning 0.186s)
               Value function loss: 113.1145
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 46.54
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.78s
                        Total time: 26166.49s
                               ETA: 1015918.6s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.828s, learning 0.200s)
               Value function loss: 23.0631
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 6.13
               Mean episode length: 48.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 9.03s
                        Total time: 26175.52s
                               ETA: 1015854.1s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.064s, learning 0.162s)
               Value function loss: 4.6496
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 9.23s
                        Total time: 26184.75s
                               ETA: 1015797.4s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.567s, learning 0.191s)
               Value function loss: 3.3150
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.76s
                        Total time: 26193.51s
                               ETA: 1015722.5s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.600s, learning 0.187s)
               Value function loss: 90.2664
                    Surrogate loss: 0.0066
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 48.09
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.79s
                        Total time: 26202.29s
                               ETA: 1015648.8s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.888s, learning 0.166s)
               Value function loss: 318.6511
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 47.18
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 9.05s
                        Total time: 26211.35s
                               ETA: 1015585.5s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.501s, learning 0.168s)
               Value function loss: 549.7002
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 47.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.67s
                        Total time: 26220.02s
                               ETA: 1015507.4s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.600s, learning 0.189s)
               Value function loss: 254.1390
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 53.45
               Mean episode length: 48.43
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.79s
                        Total time: 26228.81s
                               ETA: 1015433.9s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.592s, learning 0.159s)
               Value function loss: 147.2183
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 47.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.75s
                        Total time: 26237.56s
                               ETA: 1015359.1s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.846s, learning 0.164s)
               Value function loss: 148.4849
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 3.01
               Mean episode length: 46.68
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 9.01s
                        Total time: 26246.57s
                               ETA: 1015294.3s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.272s, learning 0.159s)
               Value function loss: 37.3140
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 49.22
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.43s
                        Total time: 26255.00s
                               ETA: 1015207.1s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.374s, learning 0.164s)
               Value function loss: 17.2262
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.54s
                        Total time: 26263.53s
                               ETA: 1015124.2s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.490s, learning 0.167s)
               Value function loss: 40.9718
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 46.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.66s
                        Total time: 26272.19s
                               ETA: 1015045.9s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.680s, learning 0.341s)
               Value function loss: 71.0857
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.22
               Mean episode length: 47.82
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 9.02s
                        Total time: 26281.21s
                               ETA: 1014981.7s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.515s, learning 0.187s)
               Value function loss: 6.5655
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 6.30
               Mean episode length: 48.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.70s
                        Total time: 26289.91s
                               ETA: 1014905.2s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.167s)
               Value function loss: 303.9738
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.97
               Mean episode length: 46.63
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.46s
                        Total time: 26298.37s
                               ETA: 1014819.4s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.582s, learning 0.189s)
               Value function loss: 5.8374
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 48.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.77s
                        Total time: 26307.14s
                               ETA: 1014745.7s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.399s, learning 0.174s)
               Value function loss: 2.0925
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.57s
                        Total time: 26315.71s
                               ETA: 1014664.4s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.776s, learning 0.183s)
               Value function loss: 131.9505
                    Surrogate loss: 0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.28
               Mean episode length: 47.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.96s
                        Total time: 26324.67s
                               ETA: 1014598.1s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.497s, learning 0.159s)
               Value function loss: 247.1262
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 48.05
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.66s
                        Total time: 26333.33s
                               ETA: 1014520.1s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.642s, learning 0.157s)
               Value function loss: 282.8753
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 47.91
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.80s
                        Total time: 26342.13s
                               ETA: 1014447.8s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.605s, learning 0.207s)
               Value function loss: 155.9638
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 13.83
               Mean episode length: 49.11
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.81s
                        Total time: 26350.94s
                               ETA: 1014375.9s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.920s, learning 0.189s)
               Value function loss: 62.9349
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 13.36
               Mean episode length: 47.19
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 9.11s
                        Total time: 26360.05s
                               ETA: 1014315.6s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.427s, learning 0.175s)
               Value function loss: 102.8500
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 49.25
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.60s
                        Total time: 26368.65s
                               ETA: 1014235.7s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.769s, learning 0.162s)
               Value function loss: 92.6513
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 49.77
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.93s
                        Total time: 26377.58s
                               ETA: 1014168.6s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.772s, learning 0.254s)
               Value function loss: 543.7681
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.63
                  Mean reward/step: 0.26
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 9.03s
                        Total time: 26386.61s
                               ETA: 1014105.2s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.846s, learning 0.200s)
               Value function loss: 63.6097
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 11.54
               Mean episode length: 48.19
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 9.05s
                        Total time: 26395.65s
                               ETA: 1014042.6s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.402s, learning 0.159s)
               Value function loss: 58.8429
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 49.35
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.56s
                        Total time: 26404.22s
                               ETA: 1013961.4s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.719s, learning 0.170s)
               Value function loss: 12.1414
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 14.24
               Mean episode length: 50.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.89s
                        Total time: 26413.10s
                               ETA: 1013892.9s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.839s, learning 0.156s)
               Value function loss: 3.1185
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 8.79
               Mean episode length: 48.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.99s
                        Total time: 26422.10s
                               ETA: 1013828.4s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1786 steps/s (collection: 8.950s, learning 0.219s)
               Value function loss: 15.4730
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 48.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 9.17s
                        Total time: 26431.27s
                               ETA: 1013770.7s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.873s, learning 0.160s)
               Value function loss: 1.8871
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 49.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 9.03s
                        Total time: 26440.30s
                               ETA: 1013707.8s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.569s, learning 0.179s)
               Value function loss: 1.4333
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 50.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.75s
                        Total time: 26449.05s
                               ETA: 1013634.1s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.934s, learning 0.165s)
               Value function loss: 187.3967
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.46
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 9.10s
                        Total time: 26458.15s
                               ETA: 1013573.8s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.534s, learning 0.170s)
               Value function loss: 420.1775
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.55
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.70s
                        Total time: 26466.85s
                               ETA: 1013498.4s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.527s, learning 0.162s)
               Value function loss: 982.8346
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 49.41
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.69s
                        Total time: 26475.54s
                               ETA: 1013422.6s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.170s)
               Value function loss: 27.5567
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 31.42
               Mean episode length: 49.23
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.44s
                        Total time: 26483.98s
                               ETA: 1013337.1s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.509s, learning 0.250s)
               Value function loss: 21.5112
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 33.59
               Mean episode length: 47.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.76s
                        Total time: 26492.74s
                               ETA: 1013264.0s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.484s, learning 0.164s)
               Value function loss: 450.2814
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 18.96
               Mean episode length: 48.97
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.65s
                        Total time: 26501.38s
                               ETA: 1013186.7s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.649s, learning 0.171s)
               Value function loss: 307.1475
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 49.47
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.82s
                        Total time: 26510.20s
                               ETA: 1013116.0s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.171s, learning 0.362s)
               Value function loss: 7.9134
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 48.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.53s
                        Total time: 26518.74s
                               ETA: 1013034.5s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.097s, learning 0.184s)
               Value function loss: 68.9186
                    Surrogate loss: 0.0035
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 9.28s
                        Total time: 26528.02s
                               ETA: 1012981.5s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.595s, learning 0.177s)
               Value function loss: 21.1353
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.25
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.77s
                        Total time: 26536.79s
                               ETA: 1012909.2s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1071 steps/s (collection: 15.036s, learning 0.256s)
               Value function loss: 3.4825
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 8.75
               Mean episode length: 47.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 15.29s
                        Total time: 26552.08s
                               ETA: 1013085.7s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.732s, learning 0.163s)
               Value function loss: 1.9617
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 49.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 16.90s
                        Total time: 26568.98s
                               ETA: 1013323.1s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.664s, learning 0.247s)
               Value function loss: 53.8579
                    Surrogate loss: 0.0035
             Mean action noise std: 0.71
                       Mean reward: 9.15
               Mean episode length: 49.84
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 16.91s
                        Total time: 26585.89s
                               ETA: 1013561.0s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.809s, learning 0.221s)
               Value function loss: 224.4001
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 13.98
               Mean episode length: 48.75
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 17.03s
                        Total time: 26602.92s
                               ETA: 1013803.2s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 931 steps/s (collection: 17.239s, learning 0.351s)
               Value function loss: 17.7207
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 17.59s
                        Total time: 26620.51s
                               ETA: 1014066.6s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.834s, learning 0.176s)
               Value function loss: 2.4542
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 50.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 17.01s
                        Total time: 26637.52s
                               ETA: 1014307.6s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.740s, learning 0.174s)
               Value function loss: 56.0044
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 16.91s
                        Total time: 26654.43s
                               ETA: 1014544.8s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.141s, learning 0.160s)
               Value function loss: 17.8450
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 3.53
               Mean episode length: 48.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 16.30s
                        Total time: 26670.73s
                               ETA: 1014758.4s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.628s, learning 0.163s)
               Value function loss: 492.4986
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 49.16
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 16.79s
                        Total time: 26687.53s
                               ETA: 1014990.6s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.613s, learning 0.170s)
               Value function loss: 5.4111
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 48.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 16.78s
                        Total time: 26704.31s
                               ETA: 1015222.2s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.405s, learning 0.166s)
               Value function loss: 362.6164
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.43
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 16.57s
                        Total time: 26720.88s
                               ETA: 1015445.6s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.530s, learning 0.167s)
               Value function loss: 149.7432
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 47.90
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 16.70s
                        Total time: 26737.58s
                               ETA: 1015673.6s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.748s, learning 0.356s)
               Value function loss: 3.7746
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 48.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 17.10s
                        Total time: 26754.68s
                               ETA: 1015916.8s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.496s, learning 0.163s)
               Value function loss: 14.6701
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 49.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 16.66s
                        Total time: 26771.34s
                               ETA: 1016142.9s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.459s, learning 0.254s)
               Value function loss: 194.4425
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 24.51
               Mean episode length: 50.93
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 16.71s
                        Total time: 26788.05s
                               ETA: 1016370.9s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.255s, learning 0.168s)
               Value function loss: 47.6363
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 8.71
               Mean episode length: 48.05
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 16.42s
                        Total time: 26804.48s
                               ETA: 1016587.7s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.979s, learning 0.186s)
               Value function loss: 2.0934
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 17.17s
                        Total time: 26821.64s
                               ETA: 1016832.5s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.756s, learning 0.297s)
               Value function loss: 1.3369
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 17.05s
                        Total time: 26838.70s
                               ETA: 1017072.8s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.729s, learning 0.174s)
               Value function loss: 0.8573
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 49.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 16.90s
                        Total time: 26855.60s
                               ETA: 1017307.2s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.517s, learning 0.163s)
               Value function loss: 0.6627
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 16.68s
                        Total time: 26872.28s
                               ETA: 1017533.0s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 954 steps/s (collection: 17.005s, learning 0.159s)
               Value function loss: 16.9005
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 49.89
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 17.16s
                        Total time: 26889.44s
                               ETA: 1017776.9s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.346s, learning 0.166s)
               Value function loss: 0.5440
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 49.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 16.51s
                        Total time: 26905.96s
                               ETA: 1017996.0s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.456s, learning 0.176s)
               Value function loss: 0.5265
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 49.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 16.63s
                        Total time: 26922.59s
                               ETA: 1018219.4s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.684s, learning 0.178s)
               Value function loss: 0.4250
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 49.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 16.86s
                        Total time: 26939.45s
                               ETA: 1018451.3s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.676s, learning 0.158s)
               Value function loss: 47.8344
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 6.31
               Mean episode length: 48.88
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 16.83s
                        Total time: 26956.28s
                               ETA: 1018682.0s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.160s, learning 0.164s)
               Value function loss: 0.3788
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 49.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 17.32s
                        Total time: 26973.61s
                               ETA: 1018930.9s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.754s, learning 0.158s)
               Value function loss: 10.4238
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 50.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 16.91s
                        Total time: 26990.52s
                               ETA: 1019164.1s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.833s, learning 0.164s)
               Value function loss: 14.8165
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 49.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 17.00s
                        Total time: 27007.52s
                               ETA: 1019400.4s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.380s, learning 0.311s)
               Value function loss: 14.3698
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 13.72
               Mean episode length: 48.14
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 16.69s
                        Total time: 27024.21s
                               ETA: 1019624.8s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.272s, learning 0.157s)
               Value function loss: 0.3397
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 48.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 16.43s
                        Total time: 27040.64s
                               ETA: 1019839.3s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.420s, learning 0.163s)
               Value function loss: 133.2093
                    Surrogate loss: 0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.54
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 16.58s
                        Total time: 27057.22s
                               ETA: 1020059.3s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.182s, learning 0.172s)
               Value function loss: 488.4046
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.18
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 17.35s
                        Total time: 27074.57s
                               ETA: 1020308.2s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.061s, learning 0.371s)
               Value function loss: 6.0318
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 16.43s
                        Total time: 27091.01s
                               ETA: 1020522.2s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.864s, learning 0.251s)
               Value function loss: 1.3434
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 50.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 17.11s
                        Total time: 27108.12s
                               ETA: 1020761.7s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.550s, learning 0.168s)
               Value function loss: 5.0748
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 49.93
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 16.72s
                        Total time: 27124.84s
                               ETA: 1020986.0s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 936 steps/s (collection: 17.180s, learning 0.318s)
               Value function loss: 301.4130
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 17.50s
                        Total time: 27142.34s
                               ETA: 1021239.6s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.629s, learning 0.176s)
               Value function loss: 244.8999
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 33.86
               Mean episode length: 49.97
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 16.80s
                        Total time: 27159.14s
                               ETA: 1021466.8s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.509s, learning 0.162s)
               Value function loss: 1.9367
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 49.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 15.67s
                        Total time: 27174.81s
                               ETA: 1021651.3s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.511s, learning 0.329s)
               Value function loss: 1.1371
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 50.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 8.84s
                        Total time: 27183.65s
                               ETA: 1021578.8s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.391s, learning 0.202s)
               Value function loss: 0.7898
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 49.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.59s
                        Total time: 27192.24s
                               ETA: 1021497.2s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.304s, learning 0.333s)
               Value function loss: 10.6563
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.93
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.64s
                        Total time: 27200.88s
                               ETA: 1021417.2s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.514s, learning 0.247s)
               Value function loss: 0.6454
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 48.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.76s
                        Total time: 27209.64s
                               ETA: 1021342.0s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.273s, learning 0.173s)
               Value function loss: 0.5101
                    Surrogate loss: -0.0263
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 50.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.45s
                        Total time: 27218.09s
                               ETA: 1021255.0s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.173s)
               Value function loss: 0.3874
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 49.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 8.81s
                        Total time: 27226.90s
                               ETA: 1021181.8s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.289s, learning 0.170s)
               Value function loss: 49.0671
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 6.08
               Mean episode length: 49.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.46s
                        Total time: 27235.36s
                               ETA: 1021095.4s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.229s, learning 0.279s)
               Value function loss: 0.4766
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.51s
                        Total time: 27243.87s
                               ETA: 1021010.9s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.587s, learning 0.172s)
               Value function loss: 63.8346
                    Surrogate loss: 0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 50.44
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 8.76s
                        Total time: 27252.63s
                               ETA: 1020935.9s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.134s, learning 0.166s)
               Value function loss: 91.5860
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 50.11
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.30s
                        Total time: 27260.93s
                               ETA: 1020843.7s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.347s, learning 0.157s)
               Value function loss: 93.4271
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 49.45
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 8.50s
                        Total time: 27269.43s
                               ETA: 1020759.2s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.599s, learning 0.160s)
               Value function loss: 0.8454
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 18.86
               Mean episode length: 49.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 8.76s
                        Total time: 27278.19s
                               ETA: 1020684.3s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.322s, learning 0.223s)
               Value function loss: 0.5850
                    Surrogate loss: -0.0186
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 48.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 8.54s
                        Total time: 27286.74s
                               ETA: 1020601.5s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.170s)
               Value function loss: 0.3991
                    Surrogate loss: -0.0223
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 50.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 8.48s
                        Total time: 27295.22s
                               ETA: 1020516.4s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.277s, learning 0.322s)
               Value function loss: 0.2319
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 50.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 8.60s
                        Total time: 27303.82s
                               ETA: 1020435.7s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.709s, learning 0.165s)
               Value function loss: 0.2585
                    Surrogate loss: -0.0258
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 49.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 8.87s
                        Total time: 27312.69s
                               ETA: 1020365.3s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.306s, learning 0.319s)
               Value function loss: 0.2148
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 49.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 8.63s
                        Total time: 27321.32s
                               ETA: 1020285.7s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.574s, learning 0.164s)
               Value function loss: 11.9713
                    Surrogate loss: 0.0061
             Mean action noise std: 0.71
                       Mean reward: 2.89
               Mean episode length: 48.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 8.74s
                        Total time: 27330.05s
                               ETA: 1020210.3s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.359s, learning 0.286s)
               Value function loss: 17.5974
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.96
               Mean episode length: 48.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 8.64s
                        Total time: 27338.70s
                               ETA: 1020131.5s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.659s, learning 0.179s)
               Value function loss: 4.1283
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 50.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 8.84s
                        Total time: 27347.54s
                               ETA: 1020060.0s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.629s, learning 0.166s)
               Value function loss: 0.2612
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 8.85
               Mean episode length: 50.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 8.79s
                        Total time: 27356.33s
                               ETA: 1019986.9s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.165s)
               Value function loss: 0.2111
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 50.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 8.53s
                        Total time: 27364.87s
                               ETA: 1019904.2s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.390s, learning 0.161s)
               Value function loss: 0.1956
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 50.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 8.55s
                        Total time: 27373.42s
                               ETA: 1019822.1s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.396s, learning 0.202s)
               Value function loss: 0.1504
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 3.17
               Mean episode length: 48.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 8.60s
                        Total time: 27382.02s
                               ETA: 1019741.9s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.587s, learning 0.162s)
               Value function loss: 0.1590
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 48.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 8.75s
                        Total time: 27390.76s
                               ETA: 1019667.3s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.564s, learning 0.182s)
               Value function loss: 0.1721
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 49.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 8.75s
                        Total time: 27399.51s
                               ETA: 1019592.6s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.457s, learning 0.163s)
               Value function loss: 0.1717
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 49.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 8.62s
                        Total time: 27408.13s
                               ETA: 1019513.4s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.018s, learning 0.220s)
               Value function loss: 134.1731
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 50.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 8.24s
                        Total time: 27416.37s
                               ETA: 1019419.9s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.873s, learning 0.164s)
               Value function loss: 482.3147
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 8.04s
                        Total time: 27424.41s
                               ETA: 1019319.1s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.105s, learning 0.186s)
               Value function loss: 330.4010
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 50.25
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 9.29s
                        Total time: 27433.70s
                               ETA: 1019264.9s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.163s)
               Value function loss: 19.3695
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 50.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 8.55s
                        Total time: 27442.24s
                               ETA: 1019183.1s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.702s, learning 0.196s)
               Value function loss: 215.6684
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 49.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 8.90s
                        Total time: 27451.14s
                               ETA: 1019114.4s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.166s)
               Value function loss: 33.9290
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 50.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 8.48s
                        Total time: 27459.62s
                               ETA: 1019030.3s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.570s, learning 0.354s)
               Value function loss: 2.8062
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 49.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 8.92s
                        Total time: 27468.54s
                               ETA: 1018962.6s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.688s, learning 0.296s)
               Value function loss: 1.6291
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 50.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 8.98s
                        Total time: 27477.53s
                               ETA: 1018897.3s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.411s, learning 0.177s)
               Value function loss: 302.2654
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 51.35
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 8.59s
                        Total time: 27486.12s
                               ETA: 1018817.3s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.284s, learning 0.253s)
               Value function loss: 485.6663
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 49.11
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.54s
                        Total time: 27494.65s
                               ETA: 1018735.5s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.596s, learning 0.250s)
               Value function loss: 22.8191
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 50.63
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 8.85s
                        Total time: 27503.50s
                               ETA: 1018665.1s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.496s, learning 0.167s)
               Value function loss: 6.7558
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 49.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.66s
                        Total time: 27512.16s
                               ETA: 1018588.1s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.412s, learning 0.166s)
               Value function loss: 9.6485
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 8.58s
                        Total time: 27520.74s
                               ETA: 1018507.9s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.907s, learning 0.162s)
               Value function loss: 1.3184
                    Surrogate loss: -0.0255
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.07s
                        Total time: 27528.81s
                               ETA: 1018409.0s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.303s, learning 0.213s)
               Value function loss: 121.3881
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 51.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 8.52s
                        Total time: 27537.32s
                               ETA: 1018326.7s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.613s, learning 0.162s)
               Value function loss: 1.0504
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 49.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.78s
                        Total time: 27546.10s
                               ETA: 1018254.0s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.358s, learning 0.164s)
               Value function loss: 0.7310
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 49.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.52s
                        Total time: 27554.62s
                               ETA: 1018172.0s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.268s, learning 0.403s)
               Value function loss: 0.5099
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 49.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 8.67s
                        Total time: 27563.29s
                               ETA: 1018095.5s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.440s, learning 0.160s)
               Value function loss: 21.7884
                    Surrogate loss: 0.0039
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 50.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.60s
                        Total time: 27571.89s
                               ETA: 1018016.5s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.668s, learning 0.368s)
               Value function loss: 0.4457
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 49.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 9.04s
                        Total time: 27580.93s
                               ETA: 1017953.7s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.702s, learning 0.170s)
               Value function loss: 0.3407
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 50.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.87s
                        Total time: 27589.80s
                               ETA: 1017884.8s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.436s, learning 0.182s)
               Value function loss: 0.4353
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 51.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.62s
                        Total time: 27598.42s
                               ETA: 1017806.6s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.706s, learning 0.170s)
               Value function loss: 0.2945
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 48.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.88s
                        Total time: 27607.29s
                               ETA: 1017738.0s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.263s, learning 0.363s)
               Value function loss: 0.2031
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 48.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.63s
                        Total time: 27615.92s
                               ETA: 1017660.2s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.318s, learning 0.238s)
               Value function loss: 0.1782
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.56s
                        Total time: 27624.47s
                               ETA: 1017579.9s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.600s, learning 0.374s)
               Value function loss: 0.1949
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 49.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.97s
                        Total time: 27633.45s
                               ETA: 1017515.0s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.551s, learning 0.363s)
               Value function loss: 0.1994
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.91s
                        Total time: 27642.36s
                               ETA: 1017448.0s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.647s, learning 0.160s)
               Value function loss: 0.1810
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.81s
                        Total time: 27651.17s
                               ETA: 1017377.0s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.198s, learning 0.358s)
               Value function loss: 0.1652
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 51.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.56s
                        Total time: 27659.73s
                               ETA: 1017296.9s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.407s, learning 0.206s)
               Value function loss: 0.1498
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 48.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.61s
                        Total time: 27668.34s
                               ETA: 1017219.0s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.410s, learning 0.354s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 50.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 8.76s
                        Total time: 27677.10s
                               ETA: 1017146.6s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.588s, learning 0.258s)
               Value function loss: 0.1450
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 49.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 8.85s
                        Total time: 27685.95s
                               ETA: 1017077.3s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.711s, learning 0.276s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 49.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 8.99s
                        Total time: 27694.94s
                               ETA: 1017013.2s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.476s, learning 0.301s)
               Value function loss: 0.1480
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 8.78s
                        Total time: 27703.71s
                               ETA: 1016941.5s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.701s, learning 0.368s)
               Value function loss: 0.1782
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 9.07s
                        Total time: 27712.78s
                               ETA: 1016880.5s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.517s, learning 0.193s)
               Value function loss: 0.1422
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 50.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 8.71s
                        Total time: 27721.49s
                               ETA: 1016806.4s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.766s, learning 0.224s)
               Value function loss: 0.1763
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 49.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 8.99s
                        Total time: 27730.48s
                               ETA: 1016742.6s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.957s, learning 0.176s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 9.13s
                        Total time: 27739.62s
                               ETA: 1016684.1s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.650s, learning 0.163s)
               Value function loss: 0.1322
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 48.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 8.81s
                        Total time: 27748.43s
                               ETA: 1016613.9s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.196s, learning 0.188s)
               Value function loss: 17.7954
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 49.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 8.38s
                        Total time: 27756.81s
                               ETA: 1016528.0s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.168s)
               Value function loss: 0.1853
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 47.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 8.53s
                        Total time: 27765.35s
                               ETA: 1016447.7s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.232s, learning 0.160s)
               Value function loss: 0.1685
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 50.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 8.39s
                        Total time: 27773.74s
                               ETA: 1016362.2s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.256s, learning 0.188s)
               Value function loss: 0.1472
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 3.17
               Mean episode length: 48.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 8.44s
                        Total time: 27782.18s
                               ETA: 1016278.8s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.554s, learning 0.192s)
               Value function loss: 0.1396
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 6.02
               Mean episode length: 49.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 8.75s
                        Total time: 27790.93s
                               ETA: 1016206.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.432s, learning 0.165s)
               Value function loss: 0.1571
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 49.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 8.60s
                        Total time: 27799.53s
                               ETA: 1016128.6s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.764s, learning 0.166s)
               Value function loss: 0.1646
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 3.32
               Mean episode length: 48.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 8.93s
                        Total time: 27808.46s
                               ETA: 1016063.0s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.517s, learning 0.169s)
               Value function loss: 0.1501
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 49.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 8.69s
                        Total time: 27817.14s
                               ETA: 1015988.6s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.660s, learning 0.162s)
               Value function loss: 46.5632
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 50.32
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 8.82s
                        Total time: 27825.97s
                               ETA: 1015919.1s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.722s, learning 0.159s)
               Value function loss: 0.1720
                    Surrogate loss: -0.0396
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 50.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 8.88s
                        Total time: 27834.85s
                               ETA: 1015851.9s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.751s, learning 0.332s)
               Value function loss: 0.1754
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 9.08s
                        Total time: 27843.93s
                               ETA: 1015792.1s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.555s, learning 0.189s)
               Value function loss: 0.1546
                    Surrogate loss: -0.0393
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 50.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 8.74s
                        Total time: 27852.67s
                               ETA: 1015719.9s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.617s, learning 0.237s)
               Value function loss: 0.1524
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 51.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 8.85s
                        Total time: 27861.53s
                               ETA: 1015651.8s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.560s, learning 0.184s)
               Value function loss: 0.1426
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 51.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 8.74s
                        Total time: 27870.27s
                               ETA: 1015579.8s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.436s, learning 0.285s)
               Value function loss: 0.1666
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 51.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 8.72s
                        Total time: 27878.99s
                               ETA: 1015506.9s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.719s, learning 0.167s)
               Value function loss: 0.1431
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 50.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 8.89s
                        Total time: 27887.88s
                               ETA: 1015440.1s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.154s, learning 0.268s)
               Value function loss: 0.1578
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 50.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 8.42s
                        Total time: 27896.30s
                               ETA: 1015356.5s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.642s, learning 0.162s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 50.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 8.80s
                        Total time: 27905.11s
                               ETA: 1015286.8s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.621s, learning 0.161s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 8.78s
                        Total time: 27913.89s
                               ETA: 1015216.4s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.668s, learning 0.265s)
               Value function loss: 0.1496
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 51.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 8.93s
                        Total time: 27922.82s
                               ETA: 1015151.5s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.426s, learning 0.177s)
               Value function loss: 29.6242
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 49.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 8.60s
                        Total time: 27931.42s
                               ETA: 1015074.6s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.401s, learning 0.166s)
               Value function loss: 130.5568
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.69
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 8.57s
                        Total time: 27939.99s
                               ETA: 1014996.5s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.169s)
               Value function loss: 4.2033
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 11.39
               Mean episode length: 50.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 8.80s
                        Total time: 27948.79s
                               ETA: 1014926.9s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.430s, learning 0.266s)
               Value function loss: 117.4651
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 50.94
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 8.70s
                        Total time: 27957.49s
                               ETA: 1014853.6s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.788s, learning 0.186s)
               Value function loss: 0.5632
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 24.32
               Mean episode length: 51.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 8.97s
                        Total time: 27966.46s
                               ETA: 1014790.4s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.689s, learning 0.163s)
               Value function loss: 0.3501
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 51.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 8.85s
                        Total time: 27975.31s
                               ETA: 1014722.8s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.358s, learning 0.178s)
               Value function loss: 0.3353
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 52.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 8.54s
                        Total time: 27983.85s
                               ETA: 1014643.9s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.555s, learning 0.183s)
               Value function loss: 0.2698
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 50.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.74s
                        Total time: 27992.59s
                               ETA: 1014572.3s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.258s, learning 0.283s)
               Value function loss: 0.2372
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 50.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 8.54s
                        Total time: 28001.13s
                               ETA: 1014493.6s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.380s, learning 0.165s)
               Value function loss: 0.2175
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 49.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.55s
                        Total time: 28009.67s
                               ETA: 1014415.1s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.623s, learning 0.281s)
               Value function loss: 0.1959
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 50.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.90s
                        Total time: 28018.58s
                               ETA: 1014349.6s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.432s, learning 0.181s)
               Value function loss: 0.1892
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 50.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.61s
                        Total time: 28027.19s
                               ETA: 1014273.7s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.517s, learning 0.183s)
               Value function loss: 39.3268
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 50.23
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.70s
                        Total time: 28035.89s
                               ETA: 1014201.0s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.597s, learning 0.160s)
               Value function loss: 0.2160
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 50.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.76s
                        Total time: 28044.65s
                               ETA: 1014130.3s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.720s, learning 0.163s)
               Value function loss: 0.1920
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 49.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 8.88s
                        Total time: 28053.53s
                               ETA: 1014064.3s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.066s, learning 0.157s)
               Value function loss: 0.1868
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 49.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.22s
                        Total time: 28061.75s
                               ETA: 1013974.4s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.471s, learning 0.164s)
               Value function loss: 268.6394
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 8.95
               Mean episode length: 50.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.63s
                        Total time: 28070.39s
                               ETA: 1013899.5s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1772 steps/s (collection: 8.910s, learning 0.332s)
               Value function loss: 17.4063
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 48.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 9.24s
                        Total time: 28079.63s
                               ETA: 1013846.6s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.236s, learning 0.164s)
               Value function loss: 0.4155
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.40s
                        Total time: 28088.03s
                               ETA: 1013763.3s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.410s, learning 0.320s)
               Value function loss: 0.2893
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 49.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.73s
                        Total time: 28096.76s
                               ETA: 1013692.0s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.336s, learning 0.162s)
               Value function loss: 0.2365
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 49.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.50s
                        Total time: 28105.26s
                               ETA: 1013612.3s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.254s, learning 0.268s)
               Value function loss: 88.9876
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 50.79
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.52s
                        Total time: 28113.78s
                               ETA: 1013533.6s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.159s)
               Value function loss: 0.3303
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 50.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.40s
                        Total time: 28122.18s
                               ETA: 1013450.5s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.220s, learning 0.287s)
               Value function loss: 0.2719
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.51s
                        Total time: 28130.69s
                               ETA: 1013371.3s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.625s, learning 0.184s)
               Value function loss: 0.2253
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 48.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.81s
                        Total time: 28139.50s
                               ETA: 1013303.1s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.554s, learning 0.164s)
               Value function loss: 304.1188
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.36
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.72s
                        Total time: 28148.21s
                               ETA: 1013231.6s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.421s, learning 0.165s)
               Value function loss: 357.8596
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 49.50
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.59s
                        Total time: 28156.80s
                               ETA: 1013155.4s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.616s, learning 0.216s)
               Value function loss: 1.9966
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.83s
                        Total time: 28165.63s
                               ETA: 1013088.1s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.623s, learning 0.162s)
               Value function loss: 0.9919
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 47.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.79s
                        Total time: 28174.42s
                               ETA: 1013019.2s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.206s, learning 0.167s)
               Value function loss: 0.6957
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 49.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.37s
                        Total time: 28182.79s
                               ETA: 1012935.5s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.155s, learning 0.291s)
               Value function loss: 0.4270
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 3.14
               Mean episode length: 47.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.45s
                        Total time: 28191.24s
                               ETA: 1012854.5s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.558s, learning 0.252s)
               Value function loss: 64.6173
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.24
               Mean episode length: 48.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.81s
                        Total time: 28200.05s
                               ETA: 1012786.7s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.462s, learning 0.255s)
               Value function loss: 17.2802
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 48.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.72s
                        Total time: 28208.76s
                               ETA: 1012715.5s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.565s, learning 0.309s)
               Value function loss: 0.4834
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 48.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.87s
                        Total time: 28217.64s
                               ETA: 1012649.9s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.731s, learning 0.168s)
               Value function loss: 0.3443
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 50.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.90s
                        Total time: 28226.54s
                               ETA: 1012585.4s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.140s, learning 0.318s)
               Value function loss: 0.2658
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 49.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.46s
                        Total time: 28234.99s
                               ETA: 1012505.0s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.652s, learning 0.188s)
               Value function loss: 0.1892
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 49.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.84s
                        Total time: 28243.83s
                               ETA: 1012438.4s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.454s, learning 0.252s)
               Value function loss: 0.1971
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 50.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.71s
                        Total time: 28252.54s
                               ETA: 1012367.1s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.518s, learning 0.163s)
               Value function loss: 0.1815
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 49.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 8.68s
                        Total time: 28261.22s
                               ETA: 1012294.9s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.797s, learning 0.287s)
               Value function loss: 0.1906
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 49.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 9.08s
                        Total time: 28270.30s
                               ETA: 1012237.2s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.390s, learning 0.187s)
               Value function loss: 0.1567
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.58s
                        Total time: 28278.88s
                               ETA: 1012161.3s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.831s, learning 0.175s)
               Value function loss: 0.1764
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.22
               Mean episode length: 48.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 9.01s
                        Total time: 28287.89s
                               ETA: 1012100.9s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.253s, learning 0.179s)
               Value function loss: 0.1794
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 50.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 8.43s
                        Total time: 28296.32s
                               ETA: 1012020.0s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.574s, learning 0.172s)
               Value function loss: 0.1367
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 48.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.75s
                        Total time: 28305.07s
                               ETA: 1011950.3s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.341s, learning 0.204s)
               Value function loss: 0.1363
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 3.32
               Mean episode length: 49.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 8.54s
                        Total time: 28313.61s
                               ETA: 1011873.5s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.260s, learning 0.174s)
               Value function loss: 0.1364
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 49.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.43s
                        Total time: 28322.04s
                               ETA: 1011792.8s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.582s, learning 0.312s)
               Value function loss: 0.1439
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 8.89s
                        Total time: 28330.94s
                               ETA: 1011728.6s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.466s, learning 0.349s)
               Value function loss: 0.1412
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 49.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 8.81s
                        Total time: 28339.75s
                               ETA: 1011661.6s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.662s, learning 0.169s)
               Value function loss: 0.1498
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 50.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 8.83s
                        Total time: 28348.58s
                               ETA: 1011595.2s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.511s, learning 0.197s)
               Value function loss: 0.1552
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 49.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.71s
                        Total time: 28357.29s
                               ETA: 1011524.4s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.137s, learning 0.221s)
               Value function loss: 0.1419
                    Surrogate loss: -0.0284
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 48.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 8.36s
                        Total time: 28365.65s
                               ETA: 1011441.3s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.804s, learning 0.348s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 4.21
               Mean episode length: 50.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 9.15s
                        Total time: 28374.80s
                               ETA: 1011386.5s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.301s, learning 0.178s)
               Value function loss: 0.1355
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 48.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.48s
                        Total time: 28383.28s
                               ETA: 1011307.7s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.432s, learning 0.277s)
               Value function loss: 7.2997
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 49.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.71s
                        Total time: 28391.99s
                               ETA: 1011237.2s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.390s, learning 0.176s)
               Value function loss: 0.1510
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 49.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.57s
                        Total time: 28400.55s
                               ETA: 1011161.6s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.165s, learning 0.202s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.37s
                        Total time: 28408.92s
                               ETA: 1011079.0s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.575s, learning 0.283s)
               Value function loss: 0.1343
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 49.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.86s
                        Total time: 28417.78s
                               ETA: 1011013.9s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.227s, learning 0.174s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 50.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 8.40s
                        Total time: 28426.18s
                               ETA: 1010932.6s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1090 steps/s (collection: 14.731s, learning 0.291s)
               Value function loss: 0.1318
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 51.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 15.02s
                        Total time: 28441.20s
                               ETA: 1011086.8s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.456s, learning 0.181s)
               Value function loss: 0.1511
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 50.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 16.64s
                        Total time: 28457.84s
                               ETA: 1011298.2s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.027s, learning 0.172s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 49.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 17.20s
                        Total time: 28475.04s
                               ETA: 1011529.4s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.940s, learning 0.163s)
               Value function loss: 15.5459
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 50.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 17.10s
                        Total time: 28492.14s
                               ETA: 1011757.0s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.598s, learning 0.171s)
               Value function loss: 0.1472
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 48.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 16.77s
                        Total time: 28508.91s
                               ETA: 1011972.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.957s, learning 0.169s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 49.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 17.13s
                        Total time: 28526.03s
                               ETA: 1012200.7s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.402s, learning 0.164s)
               Value function loss: 0.1439
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 6.47
               Mean episode length: 49.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 16.57s
                        Total time: 28542.60s
                               ETA: 1012408.7s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.432s, learning 0.222s)
               Value function loss: 0.1378
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 50.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 16.65s
                        Total time: 28559.25s
                               ETA: 1012619.7s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.446s, learning 0.209s)
               Value function loss: 0.1488
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 16.65s
                        Total time: 28575.91s
                               ETA: 1012830.6s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.459s, learning 0.172s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 4.96
               Mean episode length: 52.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 16.63s
                        Total time: 28592.54s
                               ETA: 1013040.4s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.625s, learning 0.168s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 49.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 16.79s
                        Total time: 28609.33s
                               ETA: 1013255.8s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.107s, learning 0.162s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 17.27s
                        Total time: 28626.60s
                               ETA: 1013487.9s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.983s, learning 0.170s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 17.15s
                        Total time: 28643.75s
                               ETA: 1013715.8s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.890s, learning 0.198s)
               Value function loss: 0.1408
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 17.09s
                        Total time: 28660.84s
                               ETA: 1013941.1s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.484s, learning 0.163s)
               Value function loss: 46.8943
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 8.84
               Mean episode length: 49.26
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 16.65s
                        Total time: 28677.49s
                               ETA: 1014150.7s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.855s, learning 0.313s)
               Value function loss: 29.7447
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 51.10
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 17.17s
                        Total time: 28694.66s
                               ETA: 1014378.5s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.735s, learning 0.180s)
               Value function loss: 0.2191
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 50.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 16.91s
                        Total time: 28711.57s
                               ETA: 1014597.2s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.164s, learning 0.177s)
               Value function loss: 12.5667
                    Surrogate loss: 0.0079
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 50.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 17.34s
                        Total time: 28728.91s
                               ETA: 1014830.8s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.986s, learning 0.207s)
               Value function loss: 0.1729
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 50.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 17.19s
                        Total time: 28746.10s
                               ETA: 1015059.0s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.115s, learning 0.184s)
               Value function loss: 0.1521
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 50.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 17.30s
                        Total time: 28763.40s
                               ETA: 1015290.7s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.294s, learning 0.169s)
               Value function loss: 0.1564
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 49.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 16.46s
                        Total time: 28779.87s
                               ETA: 1015492.8s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.729s, learning 0.186s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 50.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 16.91s
                        Total time: 28796.78s
                               ETA: 1015710.6s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.839s, learning 0.164s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 17.00s
                        Total time: 28813.78s
                               ETA: 1015931.4s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.409s, learning 0.273s)
               Value function loss: 283.0764
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.41
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 16.68s
                        Total time: 28830.47s
                               ETA: 1016140.7s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.657s, learning 0.198s)
               Value function loss: 0.2624
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 13.65
               Mean episode length: 49.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 16.85s
                        Total time: 28847.32s
                               ETA: 1016355.9s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.778s, learning 0.186s)
               Value function loss: 0.2174
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 50.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 16.96s
                        Total time: 28864.28s
                               ETA: 1016574.8s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.811s, learning 0.166s)
               Value function loss: 32.3913
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 49.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 16.98s
                        Total time: 28881.26s
                               ETA: 1016794.0s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.725s, learning 0.223s)
               Value function loss: 0.2399
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 16.95s
                        Total time: 28898.21s
                               ETA: 1017012.0s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.935s, learning 0.172s)
               Value function loss: 0.2176
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 6.29
               Mean episode length: 49.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 17.11s
                        Total time: 28915.32s
                               ETA: 1017235.4s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.569s, learning 0.213s)
               Value function loss: 0.1823
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 49.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 16.78s
                        Total time: 28932.10s
                               ETA: 1017447.2s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.760s, learning 0.167s)
               Value function loss: 0.1623
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 6.65
               Mean episode length: 50.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 16.93s
                        Total time: 28949.03s
                               ETA: 1017664.0s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.410s, learning 0.283s)
               Value function loss: 0.1453
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 16.69s
                        Total time: 28965.72s
                               ETA: 1017872.3s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.387s, learning 0.173s)
               Value function loss: 0.1482
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 50.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 16.56s
                        Total time: 28982.28s
                               ETA: 1018075.8s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.598s, learning 0.179s)
               Value function loss: 0.1547
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 49.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 16.78s
                        Total time: 28999.06s
                               ETA: 1018286.8s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 954 steps/s (collection: 17.005s, learning 0.163s)
               Value function loss: 0.1468
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 50.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 17.17s
                        Total time: 29016.22s
                               ETA: 1018511.3s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.820s, learning 0.182s)
               Value function loss: 16.5486
                    Surrogate loss: 0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 50.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 17.00s
                        Total time: 29033.22s
                               ETA: 1018729.9s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.621s, learning 0.262s)
               Value function loss: 0.1681
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 50.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 16.88s
                        Total time: 29050.11s
                               ETA: 1018944.0s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.142s, learning 0.161s)
               Value function loss: 17.5926
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 50.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 16.30s
                        Total time: 29066.41s
                               ETA: 1019137.7s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.535s, learning 0.192s)
               Value function loss: 13.2319
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 49.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.73s
                        Total time: 29075.14s
                               ETA: 1019065.7s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.503s, learning 0.160s)
               Value function loss: 0.3955
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 49.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 8.66s
                        Total time: 29083.80s
                               ETA: 1018991.6s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.330s, learning 0.251s)
               Value function loss: 0.2770
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 5.99
               Mean episode length: 48.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.58s
                        Total time: 29092.38s
                               ETA: 1018914.6s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.593s, learning 0.355s)
               Value function loss: 35.7346
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 48.56
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 8.95s
                        Total time: 29101.33s
                               ETA: 1018850.5s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.163s)
               Value function loss: 0.2640
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 49.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.66s
                        Total time: 29109.99s
                               ETA: 1018776.4s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.550s, learning 0.173s)
               Value function loss: 0.2399
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.72s
                        Total time: 29118.72s
                               ETA: 1018704.5s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.363s, learning 0.243s)
               Value function loss: 0.1952
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 9.44
               Mean episode length: 51.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.61s
                        Total time: 29127.32s
                               ETA: 1018628.5s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.480s, learning 0.162s)
               Value function loss: 0.1737
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 49.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.64s
                        Total time: 29135.96s
                               ETA: 1018553.9s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.504s, learning 0.188s)
               Value function loss: 17.6323
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 50.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 8.69s
                        Total time: 29144.66s
                               ETA: 1018481.1s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.211s, learning 0.161s)
               Value function loss: 0.2222
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 48.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.37s
                        Total time: 29153.03s
                               ETA: 1018397.1s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.469s, learning 0.157s)
               Value function loss: 0.2207
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 48.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.63s
                        Total time: 29161.65s
                               ETA: 1018322.0s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.497s, learning 0.161s)
               Value function loss: 0.1863
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 50.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.66s
                        Total time: 29170.31s
                               ETA: 1018248.1s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.609s, learning 0.234s)
               Value function loss: 64.3326
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.84s
                        Total time: 29179.15s
                               ETA: 1018180.7s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.456s, learning 0.158s)
               Value function loss: 46.0281
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 50.80
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.61s
                        Total time: 29187.77s
                               ETA: 1018105.4s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.165s)
               Value function loss: 0.2935
                    Surrogate loss: -0.0344
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 51.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.52s
                        Total time: 29196.29s
                               ETA: 1018026.9s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.378s, learning 0.159s)
               Value function loss: 0.2208
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.54s
                        Total time: 29204.83s
                               ETA: 1017949.0s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.164s)
               Value function loss: 29.4542
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 9.20
               Mean episode length: 50.31
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.67s
                        Total time: 29213.50s
                               ETA: 1017875.7s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.530s, learning 0.212s)
               Value function loss: 0.2127
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.74s
                        Total time: 29222.24s
                               ETA: 1017805.1s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.449s, learning 0.160s)
               Value function loss: 0.1800
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.61s
                        Total time: 29230.85s
                               ETA: 1017729.8s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.435s, learning 0.197s)
               Value function loss: 0.1669
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 49.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.63s
                        Total time: 29239.48s
                               ETA: 1017655.4s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.523s, learning 0.236s)
               Value function loss: 0.1526
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.76s
                        Total time: 29248.24s
                               ETA: 1017585.4s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.537s, learning 0.162s)
               Value function loss: 0.1417
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 49.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.70s
                        Total time: 29256.94s
                               ETA: 1017513.4s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.465s, learning 0.168s)
               Value function loss: 0.1360
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 49.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.63s
                        Total time: 29265.57s
                               ETA: 1017439.2s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.679s, learning 0.169s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 48.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.85s
                        Total time: 29274.42s
                               ETA: 1017372.4s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.508s, learning 0.163s)
               Value function loss: 15.2033
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 6.60
               Mean episode length: 50.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.67s
                        Total time: 29283.09s
                               ETA: 1017299.6s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.323s, learning 0.280s)
               Value function loss: 0.1482
                    Surrogate loss: -0.0421
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 48.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.60s
                        Total time: 29291.69s
                               ETA: 1017224.4s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.164s)
               Value function loss: 0.1660
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.53s
                        Total time: 29300.22s
                               ETA: 1017146.7s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.703s, learning 0.162s)
               Value function loss: 360.9108
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 49.17
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.86s
                        Total time: 29309.09s
                               ETA: 1017080.8s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.608s, learning 0.164s)
               Value function loss: 0.2650
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 50.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.77s
                        Total time: 29317.86s
                               ETA: 1017011.6s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.732s, learning 0.159s)
               Value function loss: 259.3425
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 16.56
               Mean episode length: 49.34
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.89s
                        Total time: 29326.75s
                               ETA: 1016946.6s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1981 steps/s (collection: 7.985s, learning 0.283s)
               Value function loss: 16.8706
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 49.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.27s
                        Total time: 29335.02s
                               ETA: 1016860.1s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.684s, learning 0.165s)
               Value function loss: 4.4707
                    Surrogate loss: -0.0113
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 48.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.85s
                        Total time: 29343.87s
                               ETA: 1016793.7s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.615s, learning 0.157s)
               Value function loss: 16.7692
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 50.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.77s
                        Total time: 29352.64s
                               ETA: 1016724.8s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.422s, learning 0.265s)
               Value function loss: 0.5568
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.69s
                        Total time: 29361.33s
                               ETA: 1016652.9s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.339s, learning 0.174s)
               Value function loss: 0.4131
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 47.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.51s
                        Total time: 29369.84s
                               ETA: 1016575.1s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.234s, learning 0.179s)
               Value function loss: 0.3052
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 48.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 8.41s
                        Total time: 29378.25s
                               ETA: 1016493.8s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.388s, learning 0.179s)
               Value function loss: 0.2614
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 50.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.57s
                        Total time: 29386.82s
                               ETA: 1016417.9s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.332s, learning 0.204s)
               Value function loss: 18.1333
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.99
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.54s
                        Total time: 29395.35s
                               ETA: 1016341.0s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.744s, learning 0.287s)
               Value function loss: 0.1901
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 9.03s
                        Total time: 29404.38s
                               ETA: 1016281.2s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.440s, learning 0.185s)
               Value function loss: 7.2926
                    Surrogate loss: 0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 50.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 8.62s
                        Total time: 29413.01s
                               ETA: 1016207.5s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.456s, learning 0.157s)
               Value function loss: 39.5887
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.21
               Mean episode length: 49.93
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.61s
                        Total time: 29421.62s
                               ETA: 1016133.4s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.464s, learning 0.276s)
               Value function loss: 0.1788
                    Surrogate loss: -0.0402
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 47.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.74s
                        Total time: 29430.36s
                               ETA: 1016063.7s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.168s)
               Value function loss: 0.1820
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 48.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.30s
                        Total time: 29438.66s
                               ETA: 1015978.8s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.583s, learning 0.178s)
               Value function loss: 47.2096
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.14
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 8.76s
                        Total time: 29447.42s
                               ETA: 1015909.9s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.053s, learning 0.166s)
               Value function loss: 0.1695
                    Surrogate loss: -0.0402
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 49.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.22s
                        Total time: 29455.64s
                               ETA: 1015822.4s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1771 steps/s (collection: 8.973s, learning 0.277s)
               Value function loss: 0.1847
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 49.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 9.25s
                        Total time: 29464.89s
                               ETA: 1015770.5s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.836s, learning 0.171s)
               Value function loss: 0.1586
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 48.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 9.01s
                        Total time: 29473.90s
                               ETA: 1015710.3s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.654s, learning 0.165s)
               Value function loss: 46.6848
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 8.82s
                        Total time: 29482.72s
                               ETA: 1015643.6s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.675s, learning 0.309s)
               Value function loss: 494.1549
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.70
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.98s
                        Total time: 29491.70s
                               ETA: 1015582.6s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.486s, learning 0.159s)
               Value function loss: 18.1256
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 8.64s
                        Total time: 29500.35s
                               ETA: 1015510.0s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.369s, learning 0.362s)
               Value function loss: 0.4968
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.73s
                        Total time: 29509.08s
                               ETA: 1015440.4s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.775s, learning 0.207s)
               Value function loss: 0.4389
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 47.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.98s
                        Total time: 29518.06s
                               ETA: 1015379.5s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.557s, learning 0.179s)
               Value function loss: 0.2888
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.74s
                        Total time: 29526.80s
                               ETA: 1015310.2s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.456s, learning 0.164s)
               Value function loss: 17.7326
                    Surrogate loss: 0.0049
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 47.93
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 8.62s
                        Total time: 29535.42s
                               ETA: 1015236.9s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.691s, learning 0.207s)
               Value function loss: 10.0058
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.90s
                        Total time: 29544.32s
                               ETA: 1015173.2s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.603s, learning 0.161s)
               Value function loss: 0.3731
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.76s
                        Total time: 29553.08s
                               ETA: 1015104.9s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.350s, learning 0.163s)
               Value function loss: 0.2955
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.51s
                        Total time: 29561.59s
                               ETA: 1015028.1s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.765s, learning 0.199s)
               Value function loss: 0.2187
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 5.95
               Mean episode length: 47.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.96s
                        Total time: 29570.56s
                               ETA: 1014966.8s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.675s, learning 0.180s)
               Value function loss: 0.1956
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 48.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.86s
                        Total time: 29579.41s
                               ETA: 1014901.8s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.407s, learning 0.281s)
               Value function loss: 0.1927
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 48.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.69s
                        Total time: 29588.10s
                               ETA: 1014831.1s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.789s, learning 0.178s)
               Value function loss: 0.1691
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.97s
                        Total time: 29597.07s
                               ETA: 1014770.0s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.887s, learning 0.169s)
               Value function loss: 0.1573
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 9.06s
                        Total time: 29606.12s
                               ETA: 1014712.1s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.277s, learning 0.188s)
               Value function loss: 9.9817
                    Surrogate loss: 0.0060
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 48.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.47s
                        Total time: 29614.59s
                               ETA: 1014633.9s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.892s, learning 0.162s)
               Value function loss: 0.1785
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 49.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 9.05s
                        Total time: 29623.64s
                               ETA: 1014575.9s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.828s, learning 0.175s)
               Value function loss: 0.1710
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 48.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 9.00s
                        Total time: 29632.65s
                               ETA: 1014516.2s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.771s, learning 0.184s)
               Value function loss: 0.1425
                    Surrogate loss: -0.0393
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 8.96s
                        Total time: 29641.60s
                               ETA: 1014454.9s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.694s, learning 0.167s)
               Value function loss: 191.7177
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 49.87
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 8.86s
                        Total time: 29650.46s
                               ETA: 1014390.4s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.480s, learning 0.218s)
               Value function loss: 0.2648
                    Surrogate loss: -0.0380
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 50.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 8.70s
                        Total time: 29659.16s
                               ETA: 1014320.3s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.702s, learning 0.186s)
               Value function loss: 0.2284
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 50.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 8.89s
                        Total time: 29668.05s
                               ETA: 1014256.9s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.497s, learning 0.168s)
               Value function loss: 0.1691
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 8.67s
                        Total time: 29676.71s
                               ETA: 1014185.8s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.549s, learning 0.160s)
               Value function loss: 93.6787
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 48.12
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 8.71s
                        Total time: 29685.42s
                               ETA: 1014116.3s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.876s, learning 0.160s)
               Value function loss: 0.2093
                    Surrogate loss: -0.0397
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 9.04s
                        Total time: 29694.46s
                               ETA: 1014058.0s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.431s, learning 0.186s)
               Value function loss: 388.6477
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 48.77
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 8.62s
                        Total time: 29703.08s
                               ETA: 1013985.4s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.620s, learning 0.227s)
               Value function loss: 53.9949
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 49.17
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 8.85s
                        Total time: 29711.92s
                               ETA: 1013920.7s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.662s, learning 0.172s)
               Value function loss: 4.4435
                    Surrogate loss: -0.0098
             Mean action noise std: 0.71
                       Mean reward: 6.51
               Mean episode length: 48.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 8.83s
                        Total time: 29720.76s
                               ETA: 1013855.6s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.843s, learning 0.236s)
               Value function loss: 0.4416
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 46.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 9.08s
                        Total time: 29729.84s
                               ETA: 1013798.9s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.597s, learning 0.188s)
               Value function loss: 70.2782
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 8.68
               Mean episode length: 48.29
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 8.79s
                        Total time: 29738.62s
                               ETA: 1013732.2s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.178s)
               Value function loss: 28.8709
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 6.98
               Mean episode length: 50.53
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 8.57s
                        Total time: 29747.19s
                               ETA: 1013658.3s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.398s, learning 0.159s)
               Value function loss: 106.2806
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 48.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 8.56s
                        Total time: 29755.75s
                               ETA: 1013584.0s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.691s, learning 0.158s)
               Value function loss: 1.8981
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 8.85s
                        Total time: 29764.60s
                               ETA: 1013519.6s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.506s, learning 0.177s)
               Value function loss: 0.9174
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 49.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 8.68s
                        Total time: 29773.28s
                               ETA: 1013449.6s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.656s, learning 0.229s)
               Value function loss: 31.7817
                    Surrogate loss: 0.0026
             Mean action noise std: 0.71
                       Mean reward: 6.45
               Mean episode length: 48.39
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 8.88s
                        Total time: 29782.17s
                               ETA: 1013386.5s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.809s, learning 0.181s)
               Value function loss: 35.6056
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 48.78
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 8.99s
                        Total time: 29791.16s
                               ETA: 1013327.0s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.171s)
               Value function loss: 0.8402
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 8.62s
                        Total time: 29799.78s
                               ETA: 1013255.1s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.590s, learning 0.187s)
               Value function loss: 50.4814
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 6.34
               Mean episode length: 48.29
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 8.78s
                        Total time: 29808.56s
                               ETA: 1013188.4s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.214s, learning 0.279s)
               Value function loss: 157.0371
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 6.40
               Mean episode length: 48.63
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 8.49s
                        Total time: 29817.05s
                               ETA: 1013112.2s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.898s, learning 0.184s)
               Value function loss: 0.9777
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 9.08s
                        Total time: 29826.13s
                               ETA: 1013056.0s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.779s, learning 0.167s)
               Value function loss: 16.5015
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 8.95s
                        Total time: 29835.08s
                               ETA: 1012995.2s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.737s, learning 0.196s)
               Value function loss: 0.6805
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 14.06
               Mean episode length: 49.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 8.93s
                        Total time: 29844.01s
                               ETA: 1012934.0s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.548s, learning 0.158s)
               Value function loss: 0.4908
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 49.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 8.71s
                        Total time: 29852.71s
                               ETA: 1012865.2s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.759s, learning 0.175s)
               Value function loss: 64.6395
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 47.43
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 8.93s
                        Total time: 29861.65s
                               ETA: 1012804.2s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.457s, learning 0.198s)
               Value function loss: 159.3759
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 49.47
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 8.65s
                        Total time: 29870.30s
                               ETA: 1012733.7s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.221s, learning 0.166s)
               Value function loss: 16.5593
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 47.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 8.39s
                        Total time: 29878.69s
                               ETA: 1012654.1s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.493s, learning 0.159s)
               Value function loss: 0.8017
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 48.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 8.65s
                        Total time: 29887.34s
                               ETA: 1012583.6s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.161s)
               Value function loss: 0.5072
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 8.74s
                        Total time: 29896.09s
                               ETA: 1012516.2s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.632s, learning 0.168s)
               Value function loss: 18.1917
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 46.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 8.80s
                        Total time: 29904.89s
                               ETA: 1012450.8s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.512s, learning 0.160s)
               Value function loss: 100.8365
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 6.21
               Mean episode length: 47.62
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 8.67s
                        Total time: 29913.56s
                               ETA: 1012381.1s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.165s)
               Value function loss: 117.0736
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.28
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 8.54s
                        Total time: 29922.09s
                               ETA: 1012306.8s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.716s, learning 0.162s)
               Value function loss: 46.0511
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 8.88s
                        Total time: 29930.97s
                               ETA: 1012244.2s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.705s, learning 0.194s)
               Value function loss: 244.1572
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 5.57
               Mean episode length: 46.10
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 8.90s
                        Total time: 29939.87s
                               ETA: 1012182.3s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.587s, learning 0.162s)
               Value function loss: 47.9233
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 47.93
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 8.75s
                        Total time: 29948.62s
                               ETA: 1012115.4s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.346s, learning 0.168s)
               Value function loss: 17.6354
                    Surrogate loss: 0.0077
             Mean action noise std: 0.71
                       Mean reward: 8.73
               Mean episode length: 47.96
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 8.51s
                        Total time: 29957.13s
                               ETA: 1012040.6s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.675s, learning 0.353s)
               Value function loss: 2.3407
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 8.99
               Mean episode length: 47.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 9.03s
                        Total time: 29966.16s
                               ETA: 1011983.2s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.561s, learning 0.168s)
               Value function loss: 1.0345
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 8.73s
                        Total time: 29974.89s
                               ETA: 1011915.7s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.531s, learning 0.227s)
               Value function loss: 162.9654
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.89
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 8.76s
                        Total time: 29983.65s
                               ETA: 1011849.2s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.646s, learning 0.265s)
               Value function loss: 14.8278
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 6.51
               Mean episode length: 49.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 8.91s
                        Total time: 29992.56s
                               ETA: 1011787.9s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.676s, learning 0.286s)
               Value function loss: 0.4995
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 48.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 8.96s
                        Total time: 30001.52s
                               ETA: 1011728.4s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.577s, learning 0.194s)
               Value function loss: 0.3849
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 11.21
               Mean episode length: 47.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 8.77s
                        Total time: 30010.29s
                               ETA: 1011662.5s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.655s, learning 0.181s)
               Value function loss: 361.5547
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.52
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 8.84s
                        Total time: 30019.13s
                               ETA: 1011598.8s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.706s, learning 0.178s)
               Value function loss: 0.3993
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 8.88s
                        Total time: 30028.01s
                               ETA: 1011536.8s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.172s)
               Value function loss: 231.5495
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 49.05
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.43s
                        Total time: 30036.45s
                               ETA: 1011459.6s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.409s, learning 0.208s)
               Value function loss: 4.1190
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 47.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.62s
                        Total time: 30045.06s
                               ETA: 1011388.6s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.546s, learning 0.404s)
               Value function loss: 27.7884
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 47.42
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.95s
                        Total time: 30054.01s
                               ETA: 1011329.0s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.737s, learning 0.260s)
               Value function loss: 18.1592
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 49.06
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 9.00s
                        Total time: 30063.01s
                               ETA: 1011270.9s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.776s, learning 0.255s)
               Value function loss: 407.6049
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 6.09
               Mean episode length: 48.03
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 9.03s
                        Total time: 30072.04s
                               ETA: 1011214.0s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.476s, learning 0.344s)
               Value function loss: 46.9454
                    Surrogate loss: -0.0080
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.82s
                        Total time: 30080.86s
                               ETA: 1011150.0s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.640s, learning 0.227s)
               Value function loss: 21.9911
                    Surrogate loss: -0.0093
             Mean action noise std: 0.71
                       Mean reward: 6.69
               Mean episode length: 48.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 8.87s
                        Total time: 30089.73s
                               ETA: 1011087.7s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.165s)
               Value function loss: 25.0147
                    Surrogate loss: -0.0072
             Mean action noise std: 0.71
                       Mean reward: 11.55
               Mean episode length: 49.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.80s
                        Total time: 30098.52s
                               ETA: 1011023.0s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.165s)
               Value function loss: 2.4109
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 47.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.42s
                        Total time: 30106.94s
                               ETA: 1010945.8s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.616s, learning 0.288s)
               Value function loss: 348.5448
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 48.10
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.90s
                        Total time: 30115.85s
                               ETA: 1010884.8s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.566s, learning 0.162s)
               Value function loss: 453.9535
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 6.41
               Mean episode length: 48.24
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.73s
                        Total time: 30124.57s
                               ETA: 1010817.9s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.546s, learning 0.191s)
               Value function loss: 154.4730
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 6.13
               Mean episode length: 47.95
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.74s
                        Total time: 30133.31s
                               ETA: 1010751.4s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.710s, learning 0.275s)
               Value function loss: 31.2653
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 8.36
               Mean episode length: 47.53
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.99s
                        Total time: 30142.30s
                               ETA: 1010693.3s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.163s)
               Value function loss: 20.1065
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.70s
                        Total time: 30150.99s
                               ETA: 1010625.5s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.468s, learning 0.304s)
               Value function loss: 1.9621
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.77s
                        Total time: 30159.77s
                               ETA: 1010560.3s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.548s, learning 0.217s)
               Value function loss: 1.1414
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 6.18
               Mean episode length: 47.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.77s
                        Total time: 30168.53s
                               ETA: 1010494.9s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.597s, learning 0.165s)
               Value function loss: 21.8517
                    Surrogate loss: 0.0073
             Mean action noise std: 0.71
                       Mean reward: 6.47
               Mean episode length: 49.15
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.76s
                        Total time: 30177.29s
                               ETA: 1010429.4s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.794s, learning 0.205s)
               Value function loss: 0.8521
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 47.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 9.00s
                        Total time: 30186.29s
                               ETA: 1010371.9s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.939s, learning 0.168s)
               Value function loss: 17.1500
                    Surrogate loss: 0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 9.11s
                        Total time: 30195.40s
                               ETA: 1010318.1s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.606s, learning 0.170s)
               Value function loss: 0.5883
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 48.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.78s
                        Total time: 30204.18s
                               ETA: 1010253.2s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.192s)
               Value function loss: 0.4917
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.79s
                        Total time: 30212.97s
                               ETA: 1010188.9s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.601s, learning 0.162s)
               Value function loss: 47.8606
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 49.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.76s
                        Total time: 30221.73s
                               ETA: 1010123.7s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.254s, learning 0.184s)
               Value function loss: 257.6784
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 47.89
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.44s
                        Total time: 30230.17s
                               ETA: 1010047.6s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.633s, learning 0.182s)
               Value function loss: 14.6705
                    Surrogate loss: -0.0068
             Mean action noise std: 0.71
                       Mean reward: 3.02
               Mean episode length: 47.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.82s
                        Total time: 30238.99s
                               ETA: 1009984.2s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.561s, learning 0.172s)
               Value function loss: 18.3425
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 8.73s
                        Total time: 30247.72s
                               ETA: 1009918.1s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.550s, learning 0.270s)
               Value function loss: 18.7128
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 16.03
               Mean episode length: 46.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.82s
                        Total time: 30256.54s
                               ETA: 1009854.9s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.593s, learning 0.202s)
               Value function loss: 73.4441
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 19.28
               Mean episode length: 48.46
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 8.80s
                        Total time: 30265.34s
                               ETA: 1009790.9s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.185s)
               Value function loss: 0.6686
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 8.66s
                        Total time: 30273.99s
                               ETA: 1009722.5s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.170s)
               Value function loss: 15.5877
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 48.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.58s
                        Total time: 30282.58s
                               ETA: 1009651.5s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.787s, learning 0.182s)
               Value function loss: 0.5588
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 48.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 8.97s
                        Total time: 30291.55s
                               ETA: 1009593.4s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.724s, learning 0.183s)
               Value function loss: 17.0901
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 47.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.91s
                        Total time: 30300.45s
                               ETA: 1009533.3s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.651s, learning 0.207s)
               Value function loss: 7.6318
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 47.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 8.86s
                        Total time: 30309.31s
                               ETA: 1009471.6s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.656s, learning 0.185s)
               Value function loss: 285.4617
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 47.26
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.84s
                        Total time: 30318.15s
                               ETA: 1009409.3s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.745s, learning 0.252s)
               Value function loss: 25.5034
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.24
               Mean episode length: 47.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 9.00s
                        Total time: 30327.15s
                               ETA: 1009352.3s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.706s, learning 0.216s)
               Value function loss: 4.3294
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 48.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 8.92s
                        Total time: 30336.07s
                               ETA: 1009292.8s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.164s, learning 0.160s)
               Value function loss: 0.6735
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 48.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.32s
                        Total time: 30344.39s
                               ETA: 1009213.5s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.711s, learning 0.188s)
               Value function loss: 0.6930
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.90s
                        Total time: 30353.29s
                               ETA: 1009153.3s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.635s, learning 0.178s)
               Value function loss: 0.3918
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 47.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.81s
                        Total time: 30362.10s
                               ETA: 1009090.4s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.921s, learning 0.160s)
               Value function loss: 9.5876
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 17.08s
                        Total time: 30379.18s
                               ETA: 1009302.1s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.421s, learning 0.182s)
               Value function loss: 84.8199
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 48.19
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 16.60s
                        Total time: 30395.79s
                               ETA: 1009497.9s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.352s, learning 0.202s)
               Value function loss: 0.4295
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 16.55s
                        Total time: 30412.34s
                               ETA: 1009691.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.147s, learning 0.191s)
               Value function loss: 0.3380
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 48.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 16.34s
                        Total time: 30428.68s
                               ETA: 1009878.5s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.062s, learning 0.161s)
               Value function loss: 191.9281
                    Surrogate loss: 0.0008
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.80
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 17.22s
                        Total time: 30445.90s
                               ETA: 1010094.4s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.079s, learning 0.170s)
               Value function loss: 0.4291
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 48.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 17.25s
                        Total time: 30463.15s
                               ETA: 1010310.9s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.295s, learning 0.331s)
               Value function loss: 0.3742
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 3.01
               Mean episode length: 46.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 16.63s
                        Total time: 30479.78s
                               ETA: 1010506.7s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 924 steps/s (collection: 17.500s, learning 0.227s)
               Value function loss: 0.3139
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 47.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 17.73s
                        Total time: 30497.50s
                               ETA: 1010738.7s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.039s, learning 0.224s)
               Value function loss: 19.2620
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 6.13
               Mean episode length: 48.46
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 17.26s
                        Total time: 30514.77s
                               ETA: 1010955.3s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.786s, learning 0.178s)
               Value function loss: 159.1914
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 47.39
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 16.96s
                        Total time: 30531.73s
                               ETA: 1011161.8s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.454s, learning 0.270s)
               Value function loss: 90.6688
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3.01
               Mean episode length: 46.86
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 16.72s
                        Total time: 30548.46s
                               ETA: 1011360.2s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.680s, learning 0.258s)
               Value function loss: 0.8148
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 48.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 16.94s
                        Total time: 30565.39s
                               ETA: 1011565.5s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.530s, learning 0.169s)
               Value function loss: 89.3343
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 6.06
               Mean episode length: 48.29
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 16.70s
                        Total time: 30582.09s
                               ETA: 1011762.8s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.691s, learning 0.164s)
               Value function loss: 69.0266
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 16.07
               Mean episode length: 49.18
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 16.86s
                        Total time: 30598.95s
                               ETA: 1011965.1s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.689s, learning 0.190s)
               Value function loss: 1.2301
                    Surrogate loss: -0.0307
             Mean action noise std: 0.71
                       Mean reward: 11.06
               Mean episode length: 49.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 16.88s
                        Total time: 30615.83s
                               ETA: 1012168.0s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.655s, learning 0.201s)
               Value function loss: 0.8278
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 10.82
               Mean episode length: 46.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 16.86s
                        Total time: 30632.68s
                               ETA: 1012370.1s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.223s, learning 0.176s)
               Value function loss: 15.4137
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 2.97
               Mean episode length: 46.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 16.40s
                        Total time: 30649.08s
                               ETA: 1012556.9s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.292s, learning 0.366s)
               Value function loss: 146.9821
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 2.99
               Mean episode length: 47.36
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 16.66s
                        Total time: 30665.74s
                               ETA: 1012752.1s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.581s, learning 0.170s)
               Value function loss: 0.7599
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 16.75s
                        Total time: 30682.49s
                               ETA: 1012950.2s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.566s, learning 0.169s)
               Value function loss: 0.6467
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 47.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 16.73s
                        Total time: 30699.23s
                               ETA: 1013147.6s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.221s, learning 0.301s)
               Value function loss: 0.4398
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 11.16
               Mean episode length: 48.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 16.52s
                        Total time: 30715.75s
                               ETA: 1013337.9s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.691s, learning 0.254s)
               Value function loss: 0.2999
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 48.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 16.94s
                        Total time: 30732.70s
                               ETA: 1013542.0s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.003s, learning 0.275s)
               Value function loss: 63.0331
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 47.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 17.28s
                        Total time: 30749.97s
                               ETA: 1013756.9s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.716s, learning 0.158s)
               Value function loss: 4.2754
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 3.00
               Mean episode length: 46.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 16.87s
                        Total time: 30766.85s
                               ETA: 1013958.3s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.445s, learning 0.243s)
               Value function loss: 0.3367
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 48.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 16.69s
                        Total time: 30783.54s
                               ETA: 1014153.4s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.815s, learning 0.202s)
               Value function loss: 4.0471
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 11.07
               Mean episode length: 47.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 17.02s
                        Total time: 30800.55s
                               ETA: 1014359.3s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.790s, learning 0.176s)
               Value function loss: 0.2607
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 3.02
               Mean episode length: 47.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 16.97s
                        Total time: 30817.52s
                               ETA: 1014563.3s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.899s, learning 0.206s)
               Value function loss: 7.2903
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.22
               Mean episode length: 48.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 17.11s
                        Total time: 30834.62s
                               ETA: 1014771.7s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.721s, learning 0.157s)
               Value function loss: 105.8880
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 47.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 16.88s
                        Total time: 30851.50s
                               ETA: 1014972.6s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.670s, learning 0.160s)
               Value function loss: 21.5410
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.27
               Mean episode length: 47.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 16.83s
                        Total time: 30868.33s
                               ETA: 1015171.7s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.995s, learning 0.184s)
               Value function loss: 4.2010
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 8.44
               Mean episode length: 47.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 17.18s
                        Total time: 30885.51s
                               ETA: 1015382.1s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.381s, learning 0.170s)
               Value function loss: 0.4888
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 47.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 16.55s
                        Total time: 30902.06s
                               ETA: 1015571.7s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.696s, learning 0.163s)
               Value function loss: 18.1134
                    Surrogate loss: 0.0038
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 48.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 16.86s
                        Total time: 30918.92s
                               ETA: 1015771.3s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.694s, learning 0.159s)
               Value function loss: 315.6608
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 2.90
               Mean episode length: 46.62
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 16.85s
                        Total time: 30935.77s
                               ETA: 1015970.6s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 950 steps/s (collection: 16.958s, learning 0.282s)
               Value function loss: 1.0541
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 48.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 17.24s
                        Total time: 30953.01s
                               ETA: 1016182.4s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.701s, learning 0.191s)
               Value function loss: 12.2183
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 5.68
               Mean episode length: 47.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 16.89s
                        Total time: 30969.90s
                               ETA: 1016382.6s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.111s, learning 0.198s)
               Value function loss: 0.6573
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 3.15
               Mean episode length: 48.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 17.31s
                        Total time: 30987.21s
                               ETA: 1016596.4s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1166 steps/s (collection: 13.861s, learning 0.182s)
               Value function loss: 0.4887
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 48.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 14.04s
                        Total time: 31001.26s
                               ETA: 1016702.9s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.741s, learning 0.164s)
               Value function loss: 0.5130
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.91s
                        Total time: 31010.16s
                               ETA: 1016640.9s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.894s, learning 0.173s)
               Value function loss: 0.2924
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 3.02
               Mean episode length: 47.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 9.07s
                        Total time: 31019.23s
                               ETA: 1016584.3s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.701s, learning 0.298s)
               Value function loss: 17.5816
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 48.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 9.00s
                        Total time: 31028.23s
                               ETA: 1016525.4s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.564s, learning 0.165s)
               Value function loss: 18.3483
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.73s
                        Total time: 31036.96s
                               ETA: 1016457.8s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.995s, learning 0.156s)
               Value function loss: 0.2979
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 3.23
               Mean episode length: 47.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 9.15s
                        Total time: 31046.11s
                               ETA: 1016404.0s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.670s, learning 0.243s)
               Value function loss: 0.2737
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.91s
                        Total time: 31055.02s
                               ETA: 1016342.4s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.925s, learning 0.189s)
               Value function loss: 0.2089
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 11.09
               Mean episode length: 48.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 9.11s
                        Total time: 31064.14s
                               ETA: 1016287.4s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.478s, learning 0.166s)
               Value function loss: 0.2182
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.64s
                        Total time: 31072.78s
                               ETA: 1016217.1s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.535s, learning 0.195s)
               Value function loss: 0.1748
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 48.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.73s
                        Total time: 31081.51s
                               ETA: 1016149.7s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.472s, learning 0.248s)
               Value function loss: 0.1518
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 49.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.72s
                        Total time: 31090.23s
                               ETA: 1016081.9s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.826s, learning 0.183s)
               Value function loss: 0.1463
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 47.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 9.01s
                        Total time: 31099.24s
                               ETA: 1016023.7s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.312s, learning 0.167s)
               Value function loss: 0.1719
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 8.48s
                        Total time: 31107.72s
                               ETA: 1015948.1s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.836s, learning 0.199s)
               Value function loss: 7.1497
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 5.81
               Mean episode length: 48.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 9.04s
                        Total time: 31116.75s
                               ETA: 1015890.8s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.784s, learning 0.166s)
               Value function loss: 0.1405
                    Surrogate loss: -0.0417
             Mean action noise std: 0.71
                       Mean reward: 2.95
               Mean episode length: 46.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.95s
                        Total time: 31125.70s
                               ETA: 1015830.7s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.493s, learning 0.160s)
               Value function loss: 17.4342
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.65s
                        Total time: 31134.36s
                               ETA: 1015761.0s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.689s, learning 0.160s)
               Value function loss: 3.9755
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.32
               Mean episode length: 47.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.85s
                        Total time: 31143.21s
                               ETA: 1015697.7s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.821s, learning 0.295s)
               Value function loss: 0.1768
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 46.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 9.12s
                        Total time: 31152.32s
                               ETA: 1015643.1s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.570s, learning 0.234s)
               Value function loss: 7.3252
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.80s
                        Total time: 31161.12s
                               ETA: 1015578.4s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.587s, learning 0.178s)
               Value function loss: 0.1654
                    Surrogate loss: -0.0410
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 48.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.77s
                        Total time: 31169.89s
                               ETA: 1015512.5s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.524s, learning 0.183s)
               Value function loss: 0.1424
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 48.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.71s
                        Total time: 31178.60s
                               ETA: 1015444.7s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.463s, learning 0.188s)
               Value function loss: 0.1354
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 3.31
               Mean episode length: 48.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.65s
                        Total time: 31187.25s
                               ETA: 1015375.1s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.465s, learning 0.186s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 5.75
               Mean episode length: 47.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.65s
                        Total time: 31195.90s
                               ETA: 1015305.6s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.380s, learning 0.261s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0400
             Mean action noise std: 0.71
                       Mean reward: 3.07
               Mean episode length: 47.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.64s
                        Total time: 31204.54s
                               ETA: 1015235.8s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.610s, learning 0.192s)
               Value function loss: 0.1361
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.80s
                        Total time: 31213.34s
                               ETA: 1015171.3s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.371s, learning 0.171s)
               Value function loss: 4.0032
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 47.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.54s
                        Total time: 31221.88s
                               ETA: 1015098.3s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.765s, learning 0.194s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 48.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.96s
                        Total time: 31230.84s
                               ETA: 1015039.0s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.798s, learning 0.209s)
               Value function loss: 9.9964
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 9.01s
                        Total time: 31239.85s
                               ETA: 1014981.2s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.115s, learning 0.199s)
               Value function loss: 39.0285
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 6.15
               Mean episode length: 48.71
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 9.31s
                        Total time: 31249.16s
                               ETA: 1014933.4s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.557s, learning 0.193s)
               Value function loss: 0.1624
                    Surrogate loss: -0.0409
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.75s
                        Total time: 31257.91s
                               ETA: 1014867.4s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.922s, learning 0.181s)
               Value function loss: 0.1553
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 48.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 9.10s
                        Total time: 31267.01s
                               ETA: 1014812.9s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.459s, learning 0.158s)
               Value function loss: 264.7138
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 48.38
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.62s
                        Total time: 31275.63s
                               ETA: 1014742.6s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.385s, learning 0.170s)
               Value function loss: 717.0107
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.89
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.56s
                        Total time: 31284.19s
                               ETA: 1014670.3s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.818s, learning 0.174s)
               Value function loss: 3.5038
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 3.39
               Mean episode length: 47.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.99s
                        Total time: 31293.18s
                               ETA: 1014612.3s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.759s, learning 0.185s)
               Value function loss: 61.1645
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 47.90
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.94s
                        Total time: 31302.12s
                               ETA: 1014552.7s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.555s, learning 0.159s)
               Value function loss: 213.2487
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 47.74
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.71s
                        Total time: 31310.84s
                               ETA: 1014485.8s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.540s, learning 0.194s)
               Value function loss: 2.8704
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 47.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.73s
                        Total time: 31319.57s
                               ETA: 1014419.5s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.501s, learning 0.315s)
               Value function loss: 1.1697
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 3.21
               Mean episode length: 47.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.82s
                        Total time: 31328.39s
                               ETA: 1014355.9s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.618s, learning 0.169s)
               Value function loss: 17.2905
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 48.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.79s
                        Total time: 31337.17s
                               ETA: 1014291.4s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.603s, learning 0.276s)
               Value function loss: 0.8253
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 47.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.88s
                        Total time: 31346.05s
                               ETA: 1014229.9s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.486s, learning 0.208s)
               Value function loss: 0.5096
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.69s
                        Total time: 31354.75s
                               ETA: 1014162.4s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.165s)
               Value function loss: 0.3682
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 3.06
               Mean episode length: 47.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.53s
                        Total time: 31363.27s
                               ETA: 1014089.6s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.161s)
               Value function loss: 0.2558
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 48.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.56s
                        Total time: 31371.83s
                               ETA: 1014017.9s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.094s, learning 0.294s)
               Value function loss: 0.2115
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 48.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 9.39s
                        Total time: 31381.22s
                               ETA: 1013973.0s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.896s, learning 0.160s)
               Value function loss: 0.1728
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 3.26
               Mean episode length: 47.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 9.06s
                        Total time: 31390.27s
                               ETA: 1013917.4s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.167s)
               Value function loss: 0.1729
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 2.98
               Mean episode length: 47.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.64s
                        Total time: 31398.91s
                               ETA: 1013848.4s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.719s, learning 0.211s)
               Value function loss: 0.1613
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 47.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.93s
                        Total time: 31407.84s
                               ETA: 1013788.8s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.163s)
               Value function loss: 12.5072
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.18
               Mean episode length: 47.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 8.48s
                        Total time: 31416.32s
                               ETA: 1013714.7s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.165s)
               Value function loss: 0.1457
                    Surrogate loss: -0.0408
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 47.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.60s
                        Total time: 31424.93s
                               ETA: 1013644.6s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.650s, learning 0.275s)
               Value function loss: 0.1409
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 46.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.93s
                        Total time: 31433.85s
                               ETA: 1013585.0s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.564s, learning 0.159s)
               Value function loss: 0.1429
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 47.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.72s
                        Total time: 31442.58s
                               ETA: 1013518.9s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.128s, learning 0.223s)
               Value function loss: 4.1052
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 6.18
               Mean episode length: 48.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.35s
                        Total time: 31450.93s
                               ETA: 1013440.8s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.828s, learning 0.165s)
               Value function loss: 0.1390
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 47.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.99s
                        Total time: 31459.92s
                               ETA: 1013383.5s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.787s, learning 0.168s)
               Value function loss: 15.3693
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.95s
                        Total time: 31468.87s
                               ETA: 1013324.9s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.284s, learning 0.185s)
               Value function loss: 0.1420
                    Surrogate loss: -0.0437
             Mean action noise std: 0.71
                       Mean reward: 3.41
               Mean episode length: 47.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.47s
                        Total time: 31477.34s
                               ETA: 1013250.8s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.762s, learning 0.395s)
               Value function loss: 0.1340
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 46.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 9.16s
                        Total time: 31486.50s
                               ETA: 1013198.8s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.799s, learning 0.164s)
               Value function loss: 0.1390
                    Surrogate loss: -0.0375
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 47.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.96s
                        Total time: 31495.46s
                               ETA: 1013140.6s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.685s, learning 0.158s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 2.94
               Mean episode length: 46.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.84s
                        Total time: 31504.31s
                               ETA: 1013078.6s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.640s, learning 0.162s)
               Value function loss: 254.0567
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.05
               Mean episode length: 47.37
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.80s
                        Total time: 31513.11s
                               ETA: 1013015.3s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.348s, learning 0.181s)
               Value function loss: 487.5336
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.76
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.53s
                        Total time: 31521.64s
                               ETA: 1012943.3s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.845s, learning 0.275s)
               Value function loss: 4.5346
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 49.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 9.12s
                        Total time: 31530.75s
                               ETA: 1012890.2s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.265s, learning 0.177s)
               Value function loss: 19.4895
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.70
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.44s
                        Total time: 31539.20s
                               ETA: 1012815.5s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.628s, learning 0.165s)
               Value function loss: 0.9966
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 31.20
               Mean episode length: 48.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.79s
                        Total time: 31547.99s
                               ETA: 1012752.1s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.678s, learning 0.162s)
               Value function loss: 0.6838
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.84s
                        Total time: 31556.83s
                               ETA: 1012690.2s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.467s, learning 0.163s)
               Value function loss: 0.4070
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 48.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.63s
                        Total time: 31565.46s
                               ETA: 1012621.6s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.212s, learning 0.234s)
               Value function loss: 73.5906
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 48.58
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 8.45s
                        Total time: 31573.91s
                               ETA: 1012547.2s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.729s, learning 0.159s)
               Value function loss: 21.6089
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.24
               Mean episode length: 47.79
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.89s
                        Total time: 31582.79s
                               ETA: 1012487.0s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.382s, learning 0.170s)
               Value function loss: 18.1469
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 46.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.55s
                        Total time: 31591.35s
                               ETA: 1012416.0s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.571s, learning 0.299s)
               Value function loss: 25.9178
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 49.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.87s
                        Total time: 31600.22s
                               ETA: 1012355.2s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.672s, learning 0.288s)
               Value function loss: 93.5235
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 11.02
               Mean episode length: 47.41
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.96s
                        Total time: 31609.18s
                               ETA: 1012297.4s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.304s, learning 0.159s)
               Value function loss: 0.6094
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 48.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.46s
                        Total time: 31617.64s
                               ETA: 1012223.7s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.313s, learning 0.202s)
               Value function loss: 39.5457
                    Surrogate loss: 0.0017
             Mean action noise std: 0.71
                       Mean reward: 6.25
               Mean episode length: 48.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 8.51s
                        Total time: 31626.15s
                               ETA: 1012151.7s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.379s, learning 0.158s)
               Value function loss: 0.4350
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 8.54s
                        Total time: 31634.69s
                               ETA: 1012080.5s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.664s, learning 0.187s)
               Value function loss: 0.2874
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.85s
                        Total time: 31643.54s
                               ETA: 1012019.3s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.773s, learning 0.170s)
               Value function loss: 0.2779
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 3.19
               Mean episode length: 47.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.94s
                        Total time: 31652.49s
                               ETA: 1011961.2s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.504s, learning 0.167s)
               Value function loss: 0.2662
                    Surrogate loss: -0.0351
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 46.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 8.67s
                        Total time: 31661.16s
                               ETA: 1011894.3s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.425s, learning 0.160s)
               Value function loss: 0.2289
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 49.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.58s
                        Total time: 31669.74s
                               ETA: 1011824.8s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.812s, learning 0.159s)
               Value function loss: 0.2141
                    Surrogate loss: -0.0362
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 8.97s
                        Total time: 31678.71s
                               ETA: 1011767.5s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.800s, learning 0.200s)
               Value function loss: 0.2027
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 3.23
               Mean episode length: 46.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 9.00s
                        Total time: 31687.71s
                               ETA: 1011711.3s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.692s, learning 0.296s)
               Value function loss: 0.1786
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 48.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.99s
                        Total time: 31696.70s
                               ETA: 1011654.7s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.782s, learning 0.167s)
               Value function loss: 0.1584
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 48.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 8.95s
                        Total time: 31705.65s
                               ETA: 1011596.9s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.566s, learning 0.264s)
               Value function loss: 0.1655
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 49.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.83s
                        Total time: 31714.48s
                               ETA: 1011535.3s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.401s, learning 0.168s)
               Value function loss: 0.1363
                    Surrogate loss: -0.0389
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 47.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 8.57s
                        Total time: 31723.05s
                               ETA: 1011465.5s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.417s, learning 0.160s)
               Value function loss: 0.1540
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 48.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.58s
                        Total time: 31731.62s
                               ETA: 1011395.9s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.528s, learning 0.160s)
               Value function loss: 0.1519
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 3.17
               Mean episode length: 46.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.69s
                        Total time: 31740.31s
                               ETA: 1011330.0s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.644s, learning 0.333s)
               Value function loss: 99.2616
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 46.82
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 8.98s
                        Total time: 31749.29s
                               ETA: 1011273.2s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.481s, learning 0.160s)
               Value function loss: 298.8628
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 3.35
               Mean episode length: 47.92
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 8.64s
                        Total time: 31757.93s
                               ETA: 1011205.9s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.613s, learning 0.266s)
               Value function loss: 6.1295
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 48.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 8.88s
                        Total time: 31766.81s
                               ETA: 1011146.1s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.421s, learning 0.163s)
               Value function loss: 1.0951
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 48.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 8.58s
                        Total time: 31775.39s
                               ETA: 1011077.0s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.635s, learning 0.216s)
               Value function loss: 0.7904
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 8.85
               Mean episode length: 48.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 8.85s
                        Total time: 31784.24s
                               ETA: 1011016.4s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.159s)
               Value function loss: 90.1835
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.13
               Mean episode length: 47.59
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 8.42s
                        Total time: 31792.67s
                               ETA: 1010942.2s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.161s)
               Value function loss: 0.4929
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 3.25
               Mean episode length: 47.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 8.69s
                        Total time: 31801.36s
                               ETA: 1010876.6s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.680s, learning 0.164s)
               Value function loss: 0.4168
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 48.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 8.84s
                        Total time: 31810.20s
                               ETA: 1010815.8s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.373s, learning 0.284s)
               Value function loss: 0.2517
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 3.23
               Mean episode length: 46.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 8.66s
                        Total time: 31818.86s
                               ETA: 1010749.2s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.002s, learning 0.168s)
               Value function loss: 47.0990
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 11.04
               Mean episode length: 47.11
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 9.17s
                        Total time: 31828.03s
                               ETA: 1010698.9s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.462s, learning 0.157s)
               Value function loss: 0.2542
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 8.14
               Mean episode length: 46.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 8.62s
                        Total time: 31836.65s
                               ETA: 1010631.2s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.633s, learning 0.348s)
               Value function loss: 0.2879
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 3.34
               Mean episode length: 48.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 8.98s
                        Total time: 31845.63s
                               ETA: 1010574.9s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.546s, learning 0.177s)
               Value function loss: 0.2251
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 49.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 8.72s
                        Total time: 31854.35s
                               ETA: 1010510.5s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.511s, learning 0.160s)
               Value function loss: 0.1879
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 47.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 8.67s
                        Total time: 31863.02s
                               ETA: 1010444.5s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.687s, learning 0.171s)
               Value function loss: 493.8105
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 47.79
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 8.86s
                        Total time: 31871.88s
                               ETA: 1010384.5s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.788s, learning 0.159s)
               Value function loss: 17.7091
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 8.95s
                        Total time: 31880.83s
                               ETA: 1010327.3s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.656s, learning 0.254s)
               Value function loss: 0.4443
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 23.51
               Mean episode length: 47.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 8.91s
                        Total time: 31889.74s
                               ETA: 1010269.0s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.671s, learning 0.162s)
               Value function loss: 452.4553
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 46.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 8.83s
                        Total time: 31898.57s
                               ETA: 1010208.3s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.169s)
               Value function loss: 4.1740
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 5.93
               Mean episode length: 47.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 8.42s
                        Total time: 31906.99s
                               ETA: 1010134.4s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.717s, learning 0.168s)
               Value function loss: 0.4945
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 23.76
               Mean episode length: 47.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 8.88s
                        Total time: 31915.87s
                               ETA: 1010075.4s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.460s, learning 0.251s)
               Value function loss: 4.1470
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 48.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 8.71s
                        Total time: 31924.58s
                               ETA: 1010010.9s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.450s, learning 0.167s)
               Value function loss: 0.3710
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 49.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 8.62s
                        Total time: 31933.20s
                               ETA: 1009943.5s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.571s, learning 0.159s)
               Value function loss: 0.2269
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 47.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 8.73s
                        Total time: 31941.93s
                               ETA: 1009879.6s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.563s, learning 0.192s)
               Value function loss: 0.2177
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 47.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 8.76s
                        Total time: 31950.69s
                               ETA: 1009816.7s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.682s, learning 0.165s)
               Value function loss: 0.1934
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 48.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 8.85s
                        Total time: 31959.53s
                               ETA: 1009756.6s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.682s, learning 0.163s)
               Value function loss: 0.1689
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 48.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 8.84s
                        Total time: 31968.38s
                               ETA: 1009696.6s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.171s)
               Value function loss: 0.2208
                    Surrogate loss: -0.0279
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 47.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 8.54s
                        Total time: 31976.91s
                               ETA: 1009626.8s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.574s, learning 0.209s)
               Value function loss: 0.1463
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 47.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 8.78s
                        Total time: 31985.70s
                               ETA: 1009564.8s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.656s, learning 0.195s)
               Value function loss: 324.3523
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.53
               Mean episode length: 47.31
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 8.85s
                        Total time: 31994.55s
                               ETA: 1009505.1s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.584s, learning 0.282s)
               Value function loss: 0.2127
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 8.87s
                        Total time: 32003.42s
                               ETA: 1009445.8s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.661s, learning 0.194s)
               Value function loss: 17.7879
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 8.85s
                        Total time: 32012.27s
                               ETA: 1009386.2s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.411s, learning 0.273s)
               Value function loss: 430.3365
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.37
               Mean episode length: 47.95
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 8.68s
                        Total time: 32020.95s
                               ETA: 1009321.3s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.462s, learning 0.179s)
               Value function loss: 0.5856
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 48.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 8.64s
                        Total time: 32029.59s
                               ETA: 1009255.0s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.547s, learning 0.271s)
               Value function loss: 47.0668
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.16
               Mean episode length: 46.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 8.82s
                        Total time: 32038.41s
                               ETA: 1009194.4s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.801s, learning 0.281s)
               Value function loss: 14.4226
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 46.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 9.08s
                        Total time: 32047.49s
                               ETA: 1009142.0s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.700s, learning 0.281s)
               Value function loss: 0.5341
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 8.98s
                        Total time: 32056.47s
                               ETA: 1009086.6s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.729s, learning 0.282s)
               Value function loss: 0.3272
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 3.34
               Mean episode length: 46.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 9.01s
                        Total time: 32065.48s
                               ETA: 1009032.1s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.341s, learning 0.156s)
               Value function loss: 17.9927
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 47.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 8.50s
                        Total time: 32073.98s
                               ETA: 1008961.5s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.162s)
               Value function loss: 94.6602
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 48.24
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 8.55s
                        Total time: 32082.53s
                               ETA: 1008892.5s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.818s, learning 0.173s)
               Value function loss: 0.4392
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 48.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 8.99s
                        Total time: 32091.52s
                               ETA: 1008837.5s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.794s, learning 0.167s)
               Value function loss: 47.6421
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.24
               Mean episode length: 46.55
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 8.96s
                        Total time: 32100.48s
                               ETA: 1008781.6s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.526s, learning 0.170s)
               Value function loss: 0.5072
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 8.70s
                        Total time: 32109.18s
                               ETA: 1008717.3s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.125s, learning 0.168s)
               Value function loss: 0.3324
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 48.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.29s
                        Total time: 32117.47s
                               ETA: 1008640.5s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.944s, learning 0.160s)
               Value function loss: 0.2448
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 9.10s
                        Total time: 32126.57s
                               ETA: 1008589.2s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.948s, learning 0.162s)
               Value function loss: 0.2188
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 9.11s
                        Total time: 32135.68s
                               ETA: 1008538.1s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.822s, learning 0.164s)
               Value function loss: 0.1868
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 48.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.99s
                        Total time: 32144.67s
                               ETA: 1008483.1s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.170s)
               Value function loss: 0.1725
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.63s
                        Total time: 32153.30s
                               ETA: 1008416.9s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.862s, learning 0.157s)
               Value function loss: 0.1514
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 9.02s
                        Total time: 32162.32s
                               ETA: 1008363.0s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.578s, learning 0.261s)
               Value function loss: 0.1604
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 47.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.84s
                        Total time: 32171.15s
                               ETA: 1008303.5s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.619s, learning 0.203s)
               Value function loss: 0.1348
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.82s
                        Total time: 32179.98s
                               ETA: 1008243.5s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1486 steps/s (collection: 10.856s, learning 0.163s)
               Value function loss: 0.1429
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 48.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 11.02s
                        Total time: 32191.00s
                               ETA: 1008252.4s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.301s, learning 0.264s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0383
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 47.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 16.56s
                        Total time: 32207.56s
                               ETA: 1008434.9s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 951 steps/s (collection: 16.976s, learning 0.243s)
               Value function loss: 0.1258
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 48.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 17.22s
                        Total time: 32224.78s
                               ETA: 1008637.7s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.815s, learning 0.168s)
               Value function loss: 0.1187
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 47.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 16.98s
                        Total time: 32241.76s
                               ETA: 1008833.0s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.633s, learning 0.164s)
               Value function loss: 0.1174
                    Surrogate loss: -0.0372
             Mean action noise std: 0.71
                       Mean reward: 3.20
               Mean episode length: 46.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 16.80s
                        Total time: 32258.56s
                               ETA: 1009022.3s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.818s, learning 0.159s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 16.98s
                        Total time: 32275.54s
                               ETA: 1009217.2s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 936 steps/s (collection: 17.339s, learning 0.162s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 17.50s
                        Total time: 32293.04s
                               ETA: 1009428.3s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.521s, learning 0.185s)
               Value function loss: 0.1199
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 47.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 16.71s
                        Total time: 32309.74s
                               ETA: 1009614.4s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.707s, learning 0.199s)
               Value function loss: 10.0768
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 48.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 16.91s
                        Total time: 32326.65s
                               ETA: 1009806.6s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.569s, learning 0.205s)
               Value function loss: 0.1197
                    Surrogate loss: -0.0449
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 47.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 16.77s
                        Total time: 32343.42s
                               ETA: 1009994.5s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.712s, learning 0.228s)
               Value function loss: 20.6769
                    Surrogate loss: 0.0042
             Mean action noise std: 0.71
                       Mean reward: 6.15
               Mean episode length: 47.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 16.94s
                        Total time: 32360.36s
                               ETA: 1010187.5s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.685s, learning 0.262s)
               Value function loss: 0.1431
                    Surrogate loss: -0.0447
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 47.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 16.95s
                        Total time: 32377.31s
                               ETA: 1010380.6s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.678s, learning 0.177s)
               Value function loss: 0.1575
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 48.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 16.85s
                        Total time: 32394.16s
                               ETA: 1010570.7s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.175s, learning 0.158s)
               Value function loss: 249.2727
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 48.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 17.33s
                        Total time: 32411.50s
                               ETA: 1010775.6s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.404s, learning 0.159s)
               Value function loss: 0.1978
                    Surrogate loss: -0.0419
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 16.56s
                        Total time: 32428.06s
                               ETA: 1010956.3s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.244s, learning 0.175s)
               Value function loss: 18.1212
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 16.42s
                        Total time: 32444.48s
                               ETA: 1011132.4s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.558s, learning 0.203s)
               Value function loss: 0.2956
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 18.65
               Mean episode length: 46.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 16.76s
                        Total time: 32461.24s
                               ETA: 1011319.0s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.698s, learning 0.164s)
               Value function loss: 0.2840
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 16.86s
                        Total time: 32478.10s
                               ETA: 1011508.6s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.831s, learning 0.161s)
               Value function loss: 59.7843
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 47.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 16.99s
                        Total time: 32495.09s
                               ETA: 1011702.2s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.539s, learning 0.173s)
               Value function loss: 0.2760
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 47.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 16.71s
                        Total time: 32511.81s
                               ETA: 1011886.9s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.547s, learning 0.253s)
               Value function loss: 0.2261
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 8.89
               Mean episode length: 46.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 16.80s
                        Total time: 32528.61s
                               ETA: 1012074.2s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.305s, learning 0.172s)
               Value function loss: 0.2082
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 16.48s
                        Total time: 32545.08s
                               ETA: 1012251.3s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.017s, learning 0.202s)
               Value function loss: 0.1948
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 47.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 16.22s
                        Total time: 32561.30s
                               ETA: 1012420.3s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.090s, learning 0.161s)
               Value function loss: 13.2731
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 47.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 17.25s
                        Total time: 32578.55s
                               ETA: 1012621.3s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.831s, learning 0.162s)
               Value function loss: 0.2451
                    Surrogate loss: -0.0397
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 46.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 16.99s
                        Total time: 32595.55s
                               ETA: 1012814.1s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.444s, learning 0.189s)
               Value function loss: 0.2113
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 3.60
               Mean episode length: 46.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 16.63s
                        Total time: 32612.18s
                               ETA: 1012995.5s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.859s, learning 0.184s)
               Value function loss: 0.2053
                    Surrogate loss: -0.0341
             Mean action noise std: 0.71
                       Mean reward: 3.46
               Mean episode length: 46.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 17.04s
                        Total time: 32629.22s
                               ETA: 1013189.6s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.762s, learning 0.284s)
               Value function loss: 0.1554
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 46.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 17.05s
                        Total time: 32646.27s
                               ETA: 1013383.6s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.763s, learning 0.234s)
               Value function loss: 0.1329
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 47.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 17.00s
                        Total time: 32663.26s
                               ETA: 1013576.0s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.072s, learning 0.165s)
               Value function loss: 0.1165
                    Surrogate loss: -0.0397
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 47.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 17.24s
                        Total time: 32680.50s
                               ETA: 1013775.7s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.450s, learning 0.232s)
               Value function loss: 0.1348
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 47.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 16.68s
                        Total time: 32697.18s
                               ETA: 1013958.1s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.625s, learning 0.173s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 46.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 16.80s
                        Total time: 32713.98s
                               ETA: 1014143.9s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.682s, learning 0.165s)
               Value function loss: 0.1335
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 16.85s
                        Total time: 32730.83s
                               ETA: 1014331.1s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.677s, learning 0.171s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0405
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 46.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 16.85s
                        Total time: 32747.68s
                               ETA: 1014518.2s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.647s, learning 0.208s)
               Value function loss: 0.1026
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 46.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 16.86s
                        Total time: 32764.53s
                               ETA: 1014705.4s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.029s, learning 0.164s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 47.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 17.19s
                        Total time: 32781.72s
                               ETA: 1014902.9s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.885s, learning 0.165s)
               Value function loss: 0.1061
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 47.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 17.05s
                        Total time: 32798.77s
                               ETA: 1015095.9s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.469s, learning 0.270s)
               Value function loss: 59.7994
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 46.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 16.74s
                        Total time: 32815.51s
                               ETA: 1015279.1s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.829s, learning 0.185s)
               Value function loss: 0.1660
                    Surrogate loss: -0.0438
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 46.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 12.01s
                        Total time: 32827.53s
                               ETA: 1015316.0s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.523s, learning 0.173s)
               Value function loss: 0.1673
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 3.47
               Mean episode length: 47.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.70s
                        Total time: 32836.22s
                               ETA: 1015250.3s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.073s, learning 0.162s)
               Value function loss: 0.1485
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 46.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.23s
                        Total time: 32844.46s
                               ETA: 1015170.4s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.449s, learning 0.360s)
               Value function loss: 0.1339
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 47.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.81s
                        Total time: 32853.27s
                               ETA: 1015108.3s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.703s, learning 0.206s)
               Value function loss: 0.1383
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 47.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 8.91s
                        Total time: 32862.18s
                               ETA: 1015049.3s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.501s, learning 0.165s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 45.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 8.67s
                        Total time: 32870.84s
                               ETA: 1014982.9s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.591s, learning 0.159s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0393
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 8.75s
                        Total time: 32879.59s
                               ETA: 1014919.0s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.576s, learning 0.198s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 45.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 8.77s
                        Total time: 32888.37s
                               ETA: 1014856.0s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.708s, learning 0.257s)
               Value function loss: 17.8594
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 46.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 8.97s
                        Total time: 32897.33s
                               ETA: 1014798.8s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.271s, learning 0.171s)
               Value function loss: 0.1490
                    Surrogate loss: -0.0420
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 46.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.44s
                        Total time: 32905.77s
                               ETA: 1014725.6s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.164s)
               Value function loss: 17.8338
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 46.07
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.58s
                        Total time: 32914.35s
                               ETA: 1014656.7s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.385s, learning 0.172s)
               Value function loss: 0.1340
                    Surrogate loss: -0.0416
             Mean action noise std: 0.71
                       Mean reward: 6.19
               Mean episode length: 46.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 8.56s
                        Total time: 32922.91s
                               ETA: 1014587.1s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.669s, learning 0.304s)
               Value function loss: 59.7177
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 46.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.97s
                        Total time: 32931.88s
                               ETA: 1014530.3s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.572s, learning 0.161s)
               Value function loss: 64.0900
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 47.42
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 8.73s
                        Total time: 32940.61s
                               ETA: 1014466.2s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.711s, learning 0.173s)
               Value function loss: 4.1555
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 45.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.88s
                        Total time: 32949.50s
                               ETA: 1014406.7s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.574s, learning 0.165s)
               Value function loss: 0.3663
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 46.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 8.74s
                        Total time: 32958.24s
                               ETA: 1014342.9s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.399s, learning 0.244s)
               Value function loss: 0.2647
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 8.64s
                        Total time: 32966.88s
                               ETA: 1014276.1s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.813s, learning 0.176s)
               Value function loss: 0.2027
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.99s
                        Total time: 32975.87s
                               ETA: 1014220.0s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.452s, learning 0.189s)
               Value function loss: 0.1917
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 3.45
               Mean episode length: 44.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.64s
                        Total time: 32984.51s
                               ETA: 1014153.2s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.859s, learning 0.235s)
               Value function loss: 70.4557
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 47.52
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 9.09s
                        Total time: 32993.60s
                               ETA: 1014100.4s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.617s, learning 0.171s)
               Value function loss: 0.1811
                    Surrogate loss: -0.0427
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 47.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.79s
                        Total time: 33002.39s
                               ETA: 1014038.3s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.513s, learning 0.364s)
               Value function loss: 9.7023
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 45.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.88s
                        Total time: 33011.27s
                               ETA: 1013978.9s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.285s, learning 0.189s)
               Value function loss: 0.2232
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 11.55
               Mean episode length: 47.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 8.47s
                        Total time: 33019.74s
                               ETA: 1013907.1s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.379s, learning 0.177s)
               Value function loss: 0.1977
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 46.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.56s
                        Total time: 33028.30s
                               ETA: 1013837.9s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.670s, learning 0.297s)
               Value function loss: 62.9512
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 47.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.97s
                        Total time: 33037.27s
                               ETA: 1013781.4s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.163s)
               Value function loss: 18.3290
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 4.45
               Mean episode length: 48.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.68s
                        Total time: 33045.94s
                               ETA: 1013715.9s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.663s, learning 0.230s)
               Value function loss: 73.6586
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 47.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.89s
                        Total time: 33054.84s
                               ETA: 1013657.2s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.738s, learning 0.172s)
               Value function loss: 4.5368
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 6.33
               Mean episode length: 47.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.91s
                        Total time: 33063.75s
                               ETA: 1013599.0s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.609s, learning 0.287s)
               Value function loss: 88.7982
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 47.54
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.90s
                        Total time: 33072.64s
                               ETA: 1013540.4s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.235s, learning 0.167s)
               Value function loss: 4.4871
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 5.79
               Mean episode length: 45.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.40s
                        Total time: 33081.04s
                               ETA: 1013466.7s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.479s, learning 0.182s)
               Value function loss: 0.5537
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 11.25
               Mean episode length: 46.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.66s
                        Total time: 33089.71s
                               ETA: 1013401.0s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.407s, learning 0.255s)
               Value function loss: 132.6791
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 46.91
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.66s
                        Total time: 33098.37s
                               ETA: 1013335.3s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.815s, learning 0.182s)
               Value function loss: 0.4848
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 3.44
               Mean episode length: 45.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 9.00s
                        Total time: 33107.36s
                               ETA: 1013280.0s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.645s, learning 0.178s)
               Value function loss: 30.0936
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 47.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.82s
                        Total time: 33116.19s
                               ETA: 1013219.3s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.595s, learning 0.156s)
               Value function loss: 0.4008
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 47.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.75s
                        Total time: 33124.94s
                               ETA: 1013156.4s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.582s, learning 0.259s)
               Value function loss: 0.3181
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 11.38
               Mean episode length: 48.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.84s
                        Total time: 33133.78s
                               ETA: 1013096.4s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.560s, learning 0.159s)
               Value function loss: 0.2405
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 47.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.72s
                        Total time: 33142.50s
                               ETA: 1013032.7s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.458s, learning 0.281s)
               Value function loss: 0.2196
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.74s
                        Total time: 33151.24s
                               ETA: 1012969.6s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.259s, learning 0.252s)
               Value function loss: 29.6151
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 3.58
               Mean episode length: 46.89
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.51s
                        Total time: 33159.75s
                               ETA: 1012899.6s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.161s)
               Value function loss: 0.1975
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 47.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.54s
                        Total time: 33168.29s
                               ETA: 1012830.6s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.509s, learning 0.184s)
               Value function loss: 53.5736
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.93
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.69s
                        Total time: 33176.99s
                               ETA: 1012766.2s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.483s, learning 0.159s)
               Value function loss: 0.3155
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 47.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.64s
                        Total time: 33185.63s
                               ETA: 1012700.3s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.407s, learning 0.176s)
               Value function loss: 0.2292
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 3.09
               Mean episode length: 46.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.58s
                        Total time: 33194.21s
                               ETA: 1012632.6s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.472s, learning 0.192s)
               Value function loss: 248.4049
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 46.65
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.66s
                        Total time: 33202.88s
                               ETA: 1012567.4s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.853s, learning 0.164s)
               Value function loss: 449.7899
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 49.51
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 9.02s
                        Total time: 33211.89s
                               ETA: 1012513.0s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.578s, learning 0.243s)
               Value function loss: 4.6120
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.82s
                        Total time: 33220.71s
                               ETA: 1012452.7s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.769s, learning 0.226s)
               Value function loss: 1.2707
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 48.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 9.00s
                        Total time: 33229.71s
                               ETA: 1012397.7s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.260s, learning 0.285s)
               Value function loss: 0.7008
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 47.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.55s
                        Total time: 33238.25s
                               ETA: 1012329.1s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.341s, learning 0.218s)
               Value function loss: 0.4923
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 21.72
               Mean episode length: 48.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.56s
                        Total time: 33246.81s
                               ETA: 1012260.8s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.738s, learning 0.209s)
               Value function loss: 149.2271
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.30
               Mean episode length: 46.48
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.95s
                        Total time: 33255.76s
                               ETA: 1012204.5s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.775s, learning 0.166s)
               Value function loss: 4.3066
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 47.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.94s
                        Total time: 33264.70s
                               ETA: 1012148.0s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.362s, learning 0.224s)
               Value function loss: 131.3793
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 49.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.59s
                        Total time: 33273.28s
                               ETA: 1012080.7s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.563s, learning 0.186s)
               Value function loss: 425.6602
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 9.05
               Mean episode length: 49.16
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.75s
                        Total time: 33282.03s
                               ETA: 1012018.4s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.556s, learning 0.306s)
               Value function loss: 5.2282
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 47.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.86s
                        Total time: 33290.90s
                               ETA: 1011959.6s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.607s, learning 0.161s)
               Value function loss: 2.3154
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.77s
                        Total time: 33299.66s
                               ETA: 1011897.9s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.496s, learning 0.280s)
               Value function loss: 1.4993
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.78s
                        Total time: 33308.44s
                               ETA: 1011836.6s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.589s, learning 0.239s)
               Value function loss: 159.3031
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 47.75
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.83s
                        Total time: 33317.27s
                               ETA: 1011776.8s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.708s, learning 0.173s)
               Value function loss: 283.3286
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 47.83
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.88s
                        Total time: 33326.15s
                               ETA: 1011718.7s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.316s, learning 0.161s)
               Value function loss: 8.8073
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 49.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.48s
                        Total time: 33334.62s
                               ETA: 1011648.4s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.404s, learning 0.277s)
               Value function loss: 37.9201
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.02
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.68s
                        Total time: 33343.31s
                               ETA: 1011584.3s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.557s, learning 0.180s)
               Value function loss: 19.2135
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.87
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 8.74s
                        Total time: 33352.04s
                               ETA: 1011521.9s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.469s, learning 0.217s)
               Value function loss: 18.9406
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.54
               Mean episode length: 47.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.69s
                        Total time: 33360.73s
                               ETA: 1011458.0s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.485s, learning 0.172s)
               Value function loss: 103.6692
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 49.40
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.66s
                        Total time: 33369.39s
                               ETA: 1011393.3s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.898s, learning 0.226s)
               Value function loss: 4.7928
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 6.24
               Mean episode length: 48.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 9.12s
                        Total time: 33378.51s
                               ETA: 1011342.8s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.495s, learning 0.180s)
               Value function loss: 0.8555
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 50.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.68s
                        Total time: 33387.19s
                               ETA: 1011278.6s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.683s, learning 0.334s)
               Value function loss: 258.2612
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.36
               Mean episode length: 47.26
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 9.02s
                        Total time: 33396.20s
                               ETA: 1011224.9s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.805s, learning 0.160s)
               Value function loss: 45.7655
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 21.91
               Mean episode length: 49.45
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.97s
                        Total time: 33405.17s
                               ETA: 1011169.6s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.491s, learning 0.201s)
               Value function loss: 1.2508
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.69s
                        Total time: 33413.86s
                               ETA: 1011106.1s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.754s, learning 0.172s)
               Value function loss: 70.5146
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 47.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.93s
                        Total time: 33422.79s
                               ETA: 1011049.7s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.383s, learning 0.370s)
               Value function loss: 0.6832
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 11.34
               Mean episode length: 48.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.75s
                        Total time: 33431.54s
                               ETA: 1010988.1s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.287s, learning 0.172s)
               Value function loss: 0.5510
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 3.69
               Mean episode length: 48.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.46s
                        Total time: 33440.00s
                               ETA: 1010917.6s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.458s, learning 0.168s)
               Value function loss: 0.4429
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 3.29
               Mean episode length: 47.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.63s
                        Total time: 33448.62s
                               ETA: 1010852.3s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.519s, learning 0.290s)
               Value function loss: 0.3889
                    Surrogate loss: -0.0255
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 49.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.81s
                        Total time: 33457.43s
                               ETA: 1010792.5s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.902s, learning 0.169s)
               Value function loss: 0.3747
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 9.07s
                        Total time: 33466.50s
                               ETA: 1010740.6s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.369s, learning 0.170s)
               Value function loss: 0.3101
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 47.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.54s
                        Total time: 33475.04s
                               ETA: 1010672.7s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.612s, learning 0.166s)
               Value function loss: 0.2575
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.78s
                        Total time: 33483.82s
                               ETA: 1010612.1s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.430s, learning 0.205s)
               Value function loss: 0.2292
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 3.38
               Mean episode length: 49.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.64s
                        Total time: 33492.46s
                               ETA: 1010547.1s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.552s, learning 0.160s)
               Value function loss: 177.2730
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 6.29
               Mean episode length: 48.07
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.71s
                        Total time: 33501.17s
                               ETA: 1010484.6s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.168s)
               Value function loss: 0.2927
                    Surrogate loss: -0.0395
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 49.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.53s
                        Total time: 33509.70s
                               ETA: 1010416.6s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.877s, learning 0.204s)
               Value function loss: 0.3067
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 48.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 9.08s
                        Total time: 33518.78s
                               ETA: 1010365.3s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.674s, learning 0.231s)
               Value function loss: 0.2534
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 47.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.90s
                        Total time: 33527.69s
                               ETA: 1010308.6s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.723s, learning 0.182s)
               Value function loss: 0.2084
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 13.91
               Mean episode length: 49.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.91s
                        Total time: 33536.59s
                               ETA: 1010252.0s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.408s, learning 0.183s)
               Value function loss: 0.2116
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 49.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.59s
                        Total time: 33545.18s
                               ETA: 1010185.9s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.397s, learning 0.228s)
               Value function loss: 0.1996
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 49.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.62s
                        Total time: 33553.81s
                               ETA: 1010120.9s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.447s, learning 0.171s)
               Value function loss: 0.2137
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 49.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.62s
                        Total time: 33562.43s
                               ETA: 1010055.8s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.688s, learning 0.285s)
               Value function loss: 17.7280
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 49.04
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.97s
                        Total time: 33571.40s
                               ETA: 1010001.3s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.764s, learning 0.187s)
               Value function loss: 12.1806
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.68
               Mean episode length: 48.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.95s
                        Total time: 33580.35s
                               ETA: 1009946.2s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.644s, learning 0.241s)
               Value function loss: 7.3808
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 49.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.88s
                        Total time: 33589.23s
                               ETA: 1009889.2s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.894s, learning 0.160s)
               Value function loss: 464.6083
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.88
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 9.05s
                        Total time: 33598.29s
                               ETA: 1009837.2s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.673s, learning 0.182s)
               Value function loss: 4.2601
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 7.06
               Mean episode length: 50.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.86s
                        Total time: 33607.14s
                               ETA: 1009779.4s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.684s, learning 0.191s)
               Value function loss: 0.3896
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.87s
                        Total time: 33616.02s
                               ETA: 1009722.1s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.384s, learning 0.275s)
               Value function loss: 0.3491
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 50.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.66s
                        Total time: 33624.68s
                               ETA: 1009658.4s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.735s, learning 0.167s)
               Value function loss: 0.3190
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 49.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 8.90s
                        Total time: 33633.58s
                               ETA: 1009602.0s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.846s, learning 0.168s)
               Value function loss: 0.2591
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 48.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 9.01s
                        Total time: 33642.59s
                               ETA: 1009549.0s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.780s, learning 0.233s)
               Value function loss: 39.4059
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 48.86
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 9.01s
                        Total time: 33651.61s
                               ETA: 1009496.0s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.558s, learning 0.162s)
               Value function loss: 0.3045
                    Surrogate loss: -0.0389
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 49.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 8.72s
                        Total time: 33660.33s
                               ETA: 1009434.3s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.512s, learning 0.344s)
               Value function loss: 0.2195
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 9.43
               Mean episode length: 49.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 8.86s
                        Total time: 33669.18s
                               ETA: 1009376.6s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.666s, learning 0.164s)
               Value function loss: 0.2248
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 48.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 8.83s
                        Total time: 33678.01s
                               ETA: 1009318.2s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.834s, learning 0.252s)
               Value function loss: 0.2407
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 49.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 9.09s
                        Total time: 33687.10s
                               ETA: 1009267.5s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.640s, learning 0.159s)
               Value function loss: 24.6933
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.52
               Mean episode length: 48.04
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 8.80s
                        Total time: 33695.90s
                               ETA: 1009208.3s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.493s, learning 0.157s)
               Value function loss: 310.3753
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.32
               Mean episode length: 49.61
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 8.65s
                        Total time: 33704.55s
                               ETA: 1009144.5s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.389s, learning 0.160s)
               Value function loss: 93.2988
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 4.27
               Mean episode length: 49.21
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 8.55s
                        Total time: 33713.09s
                               ETA: 1009077.9s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.332s, learning 0.200s)
               Value function loss: 82.3429
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 49.76
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 8.53s
                        Total time: 33721.63s
                               ETA: 1009010.7s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.517s, learning 0.260s)
               Value function loss: 13.8394
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 48.58
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 8.78s
                        Total time: 33730.40s
                               ETA: 1008950.9s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.337s, learning 0.162s)
               Value function loss: 389.8160
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 50.11
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 8.50s
                        Total time: 33738.90s
                               ETA: 1008882.9s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.627s, learning 0.169s)
               Value function loss: 23.4142
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 4.51
               Mean episode length: 50.57
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 8.80s
                        Total time: 33747.70s
                               ETA: 1008823.7s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.742s, learning 0.193s)
               Value function loss: 67.1213
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 50.53
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 8.94s
                        Total time: 33756.63s
                               ETA: 1008768.8s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.655s, learning 0.161s)
               Value function loss: 6.9881
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 48.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 8.82s
                        Total time: 33765.45s
                               ETA: 1008710.2s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.743s, learning 0.205s)
               Value function loss: 2.3016
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 6.43
               Mean episode length: 48.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 8.95s
                        Total time: 33774.40s
                               ETA: 1008655.7s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.837s, learning 0.157s)
               Value function loss: 207.4903
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 48.69
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 8.99s
                        Total time: 33783.39s
                               ETA: 1008602.6s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.625s, learning 0.167s)
               Value function loss: 33.1123
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 49.48
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 8.79s
                        Total time: 33792.18s
                               ETA: 1008543.5s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.809s, learning 0.212s)
               Value function loss: 1.5729
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 9.02s
                        Total time: 33801.20s
                               ETA: 1008491.2s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.507s, learning 0.173s)
               Value function loss: 16.9915
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.48
               Mean episode length: 49.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 8.68s
                        Total time: 33809.89s
                               ETA: 1008428.8s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.559s, learning 0.302s)
               Value function loss: 747.3486
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 17.03
               Mean episode length: 49.63
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 8.86s
                        Total time: 33818.75s
                               ETA: 1008371.9s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.368s, learning 0.230s)
               Value function loss: 12.7127
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 6.86
               Mean episode length: 49.27
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 8.60s
                        Total time: 33827.35s
                               ETA: 1008307.1s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.726s, learning 0.206s)
               Value function loss: 493.6272
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 26.49
               Mean episode length: 49.07
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 8.93s
                        Total time: 33836.28s
                               ETA: 1008252.3s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.897s, learning 0.189s)
               Value function loss: 202.8246
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 4.74
               Mean episode length: 51.28
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 9.09s
                        Total time: 33845.36s
                               ETA: 1008202.1s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.572s, learning 0.181s)
               Value function loss: 357.2506
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 9.65
               Mean episode length: 50.87
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 8.75s
                        Total time: 33854.12s
                               ETA: 1008142.0s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.936s, learning 0.254s)
               Value function loss: 31.8120
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 4.32
               Mean episode length: 49.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 9.19s
                        Total time: 33863.31s
                               ETA: 1008095.0s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.103s, learning 0.178s)
               Value function loss: 40.1729
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.31
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 9.28s
                        Total time: 33872.59s
                               ETA: 1008050.7s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.418s, learning 0.167s)
               Value function loss: 17.4475
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 48.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 8.58s
                        Total time: 33881.17s
                               ETA: 1007985.7s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.858s, learning 0.167s)
               Value function loss: 2.9675
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 9.52
               Mean episode length: 50.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 9.02s
                        Total time: 33890.20s
                               ETA: 1007933.8s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.496s, learning 0.191s)
               Value function loss: 2.1006
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 6.52
               Mean episode length: 49.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 8.69s
                        Total time: 33898.88s
                               ETA: 1007871.9s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1775 steps/s (collection: 8.943s, learning 0.284s)
               Value function loss: 308.4015
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 49.52
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 9.23s
                        Total time: 33908.11s
                               ETA: 1007826.1s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.607s, learning 0.173s)
               Value function loss: 64.2696
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 49.59
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 8.78s
                        Total time: 33916.89s
                               ETA: 1007767.1s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.428s, learning 0.166s)
               Value function loss: 2.1489
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 49.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 8.59s
                        Total time: 33925.48s
                               ETA: 1007702.5s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.914s, learning 0.214s)
               Value function loss: 1.6249
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 4.54
               Mean episode length: 50.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 9.13s
                        Total time: 33934.61s
                               ETA: 1007653.8s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.683s, learning 0.338s)
               Value function loss: 30.1949
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 3.42
               Mean episode length: 47.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 9.02s
                        Total time: 33943.63s
                               ETA: 1007602.0s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.205s, learning 0.179s)
               Value function loss: 120.3927
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 50.11
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 8.38s
                        Total time: 33952.02s
                               ETA: 1007531.3s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1783 steps/s (collection: 8.992s, learning 0.193s)
               Value function loss: 236.4053
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.79
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 9.18s
                        Total time: 33961.20s
                               ETA: 1007484.4s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.164s)
               Value function loss: 362.8229
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 49.96
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 8.53s
                        Total time: 33969.73s
                               ETA: 1007418.1s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.512s, learning 0.343s)
               Value function loss: 167.6602
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 48.88
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 8.86s
                        Total time: 33978.59s
                               ETA: 1007361.5s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1764 steps/s (collection: 9.108s, learning 0.178s)
               Value function loss: 87.0421
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 49.28
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 9.29s
                        Total time: 33987.87s
                               ETA: 1007317.7s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.685s, learning 0.169s)
               Value function loss: 3.5968
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 49.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.85s
                        Total time: 33996.73s
                               ETA: 1007261.0s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.415s, learning 0.281s)
               Value function loss: 68.5402
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 48.72
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 8.70s
                        Total time: 34005.42s
                               ETA: 1007199.8s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.325s, learning 0.156s)
               Value function loss: 5.5317
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 49.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.48s
                        Total time: 34013.90s
                               ETA: 1007132.2s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.571s, learning 0.186s)
               Value function loss: 1.2476
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 49.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 8.76s
                        Total time: 34022.66s
                               ETA: 1007072.8s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.507s, learning 0.157s)
               Value function loss: 1.0328
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 49.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.66s
                        Total time: 34031.32s
                               ETA: 1007010.7s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.629s, learning 0.163s)
               Value function loss: 0.7353
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 50.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.79s
                        Total time: 34040.12s
                               ETA: 1006952.4s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.615s, learning 0.207s)
               Value function loss: 0.5215
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 8.82s
                        Total time: 34048.94s
                               ETA: 1006895.1s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.128s, learning 0.202s)
               Value function loss: 0.4957
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 4.82
               Mean episode length: 51.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 17.33s
                        Total time: 34066.27s
                               ETA: 1007089.3s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.060s, learning 0.198s)
               Value function loss: 0.3793
                    Surrogate loss: -0.0284
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 17.26s
                        Total time: 34083.53s
                               ETA: 1007281.2s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.591s, learning 0.204s)
               Value function loss: 73.1450
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 48.13
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 16.79s
                        Total time: 34100.32s
                               ETA: 1007459.3s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.066s, learning 0.200s)
               Value function loss: 4.2637
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 49.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 17.27s
                        Total time: 34117.59s
                               ETA: 1007651.2s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.860s, learning 0.159s)
               Value function loss: 33.5647
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.62
               Mean episode length: 51.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 17.02s
                        Total time: 34134.61s
                               ETA: 1007835.7s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.927s, learning 0.169s)
               Value function loss: 364.6852
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 49.54
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 17.10s
                        Total time: 34151.70s
                               ETA: 1008022.3s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.774s, learning 0.170s)
               Value function loss: 4.6731
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 49.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 16.94s
                        Total time: 34168.64s
                               ETA: 1008204.3s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.783s, learning 0.317s)
               Value function loss: 120.3319
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.54
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 17.10s
                        Total time: 34185.74s
                               ETA: 1008390.9s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.985s, learning 0.172s)
               Value function loss: 262.3527
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 11.45
               Mean episode length: 49.03
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 17.16s
                        Total time: 34202.90s
                               ETA: 1008578.9s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.491s, learning 0.171s)
               Value function loss: 1.7788
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 11.46
               Mean episode length: 47.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 16.66s
                        Total time: 34219.56s
                               ETA: 1008752.3s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.419s, learning 0.200s)
               Value function loss: 325.9410
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 49.14
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 16.62s
                        Total time: 34236.18s
                               ETA: 1008924.3s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.898s, learning 0.228s)
               Value function loss: 63.9879
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 9.35
               Mean episode length: 50.37
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 17.13s
                        Total time: 34253.31s
                               ETA: 1009111.1s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.698s, learning 0.180s)
               Value function loss: 262.1755
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 49.87
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 16.88s
                        Total time: 34270.19s
                               ETA: 1009290.4s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.744s, learning 0.163s)
               Value function loss: 137.5286
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 50.81
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 16.91s
                        Total time: 34287.10s
                               ETA: 1009470.5s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.749s, learning 0.331s)
               Value function loss: 89.0934
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 26.95
               Mean episode length: 49.58
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 17.08s
                        Total time: 34304.18s
                               ETA: 1009655.6s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.481s, learning 0.165s)
               Value function loss: 10.1271
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 6.49
               Mean episode length: 48.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 16.65s
                        Total time: 34320.82s
                               ETA: 1009827.8s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.609s, learning 0.162s)
               Value function loss: 339.2919
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 50.01
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 16.77s
                        Total time: 34337.59s
                               ETA: 1010003.5s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.079s, learning 0.168s)
               Value function loss: 891.4890
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 48.80
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 17.25s
                        Total time: 34354.84s
                               ETA: 1010193.1s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.091s, learning 0.167s)
               Value function loss: 27.1664
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 9.60
               Mean episode length: 50.47
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 17.26s
                        Total time: 34372.10s
                               ETA: 1010382.9s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.959s, learning 0.160s)
               Value function loss: 282.6704
                    Surrogate loss: 0.0048
             Mean action noise std: 0.71
                       Mean reward: 12.17
               Mean episode length: 50.04
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 17.12s
                        Total time: 34389.22s
                               ETA: 1010568.5s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.820s, learning 0.162s)
               Value function loss: 520.6096
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 16.79
               Mean episode length: 48.69
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 16.98s
                        Total time: 34406.20s
                               ETA: 1010750.0s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.641s, learning 0.327s)
               Value function loss: 657.9215
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 49.59
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 16.97s
                        Total time: 34423.17s
                               ETA: 1010930.9s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.446s, learning 0.176s)
               Value function loss: 483.9460
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.47
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 16.62s
                        Total time: 34439.79s
                               ETA: 1011101.5s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.837s, learning 0.166s)
               Value function loss: 39.8760
                    Surrogate loss: -0.0194
             Mean action noise std: 0.71
                       Mean reward: 13.84
               Mean episode length: 47.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 17.00s
                        Total time: 34456.79s
                               ETA: 1011283.3s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.054s, learning 0.165s)
               Value function loss: 120.8637
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 49.94
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 17.22s
                        Total time: 34474.01s
                               ETA: 1011471.2s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.796s, learning 0.160s)
               Value function loss: 10.6363
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 4.94
               Mean episode length: 50.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 16.96s
                        Total time: 34490.97s
                               ETA: 1011651.3s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.790s, learning 0.172s)
               Value function loss: 3.8558
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 8.99
               Mean episode length: 49.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 16.96s
                        Total time: 34507.93s
                               ETA: 1011831.4s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.807s, learning 0.161s)
               Value function loss: 22.6460
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 50.02
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 16.97s
                        Total time: 34524.90s
                               ETA: 1012011.6s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.810s, learning 0.169s)
               Value function loss: 187.6251
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.84
               Mean episode length: 51.08
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 16.98s
                        Total time: 34541.87s
                               ETA: 1012192.1s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.793s, learning 0.307s)
               Value function loss: 203.8003
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 9.73
               Mean episode length: 50.06
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 17.10s
                        Total time: 34558.97s
                               ETA: 1012375.9s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.729s, learning 0.178s)
               Value function loss: 65.0649
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 4.82
               Mean episode length: 51.21
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 16.91s
                        Total time: 34575.88s
                               ETA: 1012553.9s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.790s, learning 0.286s)
               Value function loss: 403.9264
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 48.87
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 17.08s
                        Total time: 34592.96s
                               ETA: 1012736.8s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.650s, learning 0.170s)
               Value function loss: 544.7842
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 48.98
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 16.82s
                        Total time: 34609.78s
                               ETA: 1012912.1s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.672s, learning 0.268s)
               Value function loss: 89.5335
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.83
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 16.94s
                        Total time: 34626.72s
                               ETA: 1013090.7s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.130s, learning 0.169s)
               Value function loss: 16.5083
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 17.30s
                        Total time: 34644.01s
                               ETA: 1013279.8s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 935 steps/s (collection: 17.250s, learning 0.268s)
               Value function loss: 456.2717
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 6.42
               Mean episode length: 48.19
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 17.52s
                        Total time: 34661.53s
                               ETA: 1013475.1s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.075s, learning 0.158s)
               Value function loss: 49.9291
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 6.50
               Mean episode length: 47.49
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 17.23s
                        Total time: 34678.77s
                               ETA: 1013662.0s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1160 steps/s (collection: 13.954s, learning 0.163s)
               Value function loss: 20.5501
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 9.24
               Mean episode length: 49.06
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 14.12s
                        Total time: 34692.88s
                               ETA: 1013757.7s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.173s)
               Value function loss: 205.9170
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 49.82
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 8.54s
                        Total time: 34701.42s
                               ETA: 1013690.5s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.705s, learning 0.162s)
               Value function loss: 536.1389
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 6.96
               Mean episode length: 50.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.87s
                        Total time: 34710.29s
                               ETA: 1013632.8s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.749s, learning 0.172s)
               Value function loss: 177.1047
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 48.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.92s
                        Total time: 34719.21s
                               ETA: 1013576.7s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.416s, learning 0.171s)
               Value function loss: 79.7702
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 49.50
               Mean episode length: 50.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.59s
                        Total time: 34727.80s
                               ETA: 1013510.9s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.429s, learning 0.222s)
               Value function loss: 75.1768
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 4.43
               Mean episode length: 50.04
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.65s
                        Total time: 34736.45s
                               ETA: 1013447.0s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.589s, learning 0.206s)
               Value function loss: 12.2371
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 49.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.79s
                        Total time: 34745.25s
                               ETA: 1013387.3s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.579s, learning 0.160s)
               Value function loss: 24.6329
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 50.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.74s
                        Total time: 34753.99s
                               ETA: 1013326.0s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.574s, learning 0.323s)
               Value function loss: 26.7186
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 49.68
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.90s
                        Total time: 34762.88s
                               ETA: 1013269.4s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.319s, learning 0.161s)
               Value function loss: 144.7118
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 6.74
               Mean episode length: 49.19
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 8.48s
                        Total time: 34771.36s
                               ETA: 1013200.6s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.474s, learning 0.159s)
               Value function loss: 149.7540
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 51.39
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.63s
                        Total time: 34780.00s
                               ETA: 1013136.3s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.179s)
               Value function loss: 10.1975
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 9.10
               Mean episode length: 49.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.87s
                        Total time: 34788.87s
                               ETA: 1013079.1s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.439s, learning 0.370s)
               Value function loss: 57.5732
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.37
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.81s
                        Total time: 34797.68s
                               ETA: 1013020.0s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.621s, learning 0.161s)
               Value function loss: 79.5781
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 19.22
               Mean episode length: 49.45
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.78s
                        Total time: 34806.46s
                               ETA: 1012960.2s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.163s)
               Value function loss: 250.0542
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.69
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.53s
                        Total time: 34814.99s
                               ETA: 1012893.1s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.162s)
               Value function loss: 202.8783
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 4.99
               Mean episode length: 50.36
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.55s
                        Total time: 34823.54s
                               ETA: 1012826.6s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.437s, learning 0.165s)
               Value function loss: 294.2284
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 48.94
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.60s
                        Total time: 34832.15s
                               ETA: 1012761.7s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.693s, learning 0.205s)
               Value function loss: 3.1596
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 8.18
               Mean episode length: 52.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.90s
                        Total time: 34841.04s
                               ETA: 1012705.3s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.163s)
               Value function loss: 19.2342
                    Surrogate loss: 0.0008
             Mean action noise std: 0.71
                       Mean reward: 14.35
               Mean episode length: 50.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.61s
                        Total time: 34849.66s
                               ETA: 1012640.7s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.695s, learning 0.186s)
               Value function loss: 5.4278
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 14.43
               Mean episode length: 50.01
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.88s
                        Total time: 34858.54s
                               ETA: 1012584.0s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.679s, learning 0.160s)
               Value function loss: 1.3922
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 48.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.84s
                        Total time: 34867.38s
                               ETA: 1012526.0s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.472s, learning 0.163s)
               Value function loss: 33.0769
                    Surrogate loss: 0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 48.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.63s
                        Total time: 34876.01s
                               ETA: 1012462.1s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.164s)
               Value function loss: 308.3291
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.90
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.46s
                        Total time: 34884.47s
                               ETA: 1012393.2s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.765s, learning 0.210s)
               Value function loss: 2.9609
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 9.13
               Mean episode length: 50.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 8.97s
                        Total time: 34893.45s
                               ETA: 1012339.3s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.727s, learning 0.271s)
               Value function loss: 12.2326
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 4.62
               Mean episode length: 50.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 9.00s
                        Total time: 34902.44s
                               ETA: 1012286.1s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.545s, learning 0.293s)
               Value function loss: 1.3023
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 9.26
               Mean episode length: 48.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.84s
                        Total time: 34911.28s
                               ETA: 1012228.2s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.419s, learning 0.168s)
               Value function loss: 0.8840
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 11.84
               Mean episode length: 49.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.59s
                        Total time: 34919.87s
                               ETA: 1012163.2s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.881s, learning 0.187s)
               Value function loss: 204.9052
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 49.96
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 9.07s
                        Total time: 34928.94s
                               ETA: 1012112.0s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.613s, learning 0.163s)
               Value function loss: 489.6958
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 14.59
               Mean episode length: 50.65
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.78s
                        Total time: 34937.71s
                               ETA: 1012052.5s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.605s, learning 0.231s)
               Value function loss: 109.9507
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 50.07
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.84s
                        Total time: 34946.55s
                               ETA: 1011994.7s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.573s, learning 0.368s)
               Value function loss: 444.3592
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 48.65
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.94s
                        Total time: 34955.49s
                               ETA: 1011940.0s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.163s)
               Value function loss: 573.0991
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 19.42
               Mean episode length: 48.31
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.64s
                        Total time: 34964.13s
                               ETA: 1011876.5s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.347s, learning 0.164s)
               Value function loss: 21.9687
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 4.48
               Mean episode length: 49.41
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.51s
                        Total time: 34972.64s
                               ETA: 1011809.5s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.241s, learning 0.222s)
               Value function loss: 20.9110
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 12.48
               Mean episode length: 51.68
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.46s
                        Total time: 34981.10s
                               ETA: 1011741.0s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.503s, learning 0.188s)
               Value function loss: 2.1951
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: 6.82
               Mean episode length: 49.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.69s
                        Total time: 34989.79s
                               ETA: 1011679.2s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.914s, learning 0.166s)
               Value function loss: 1.6640
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 10.03
               Mean episode length: 50.88
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 9.08s
                        Total time: 34998.87s
                               ETA: 1011628.6s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.454s, learning 0.308s)
               Value function loss: 17.9987
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.58
               Mean episode length: 50.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.76s
                        Total time: 35007.64s
                               ETA: 1011568.9s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.690s, learning 0.169s)
               Value function loss: 486.6200
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 21.99
               Mean episode length: 50.66
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 8.86s
                        Total time: 35016.50s
                               ETA: 1011512.1s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.617s, learning 0.165s)
               Value function loss: 1.4760
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 48.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.78s
                        Total time: 35025.28s
                               ETA: 1011453.0s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.621s, learning 0.189s)
               Value function loss: 1.2800
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 49.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.81s
                        Total time: 35034.09s
                               ETA: 1011394.8s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.625s, learning 0.189s)
               Value function loss: 190.7409
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 48.86
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.81s
                        Total time: 35042.90s
                               ETA: 1011336.7s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.691s, learning 0.270s)
               Value function loss: 125.5055
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 50.26
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.96s
                        Total time: 35051.86s
                               ETA: 1011282.9s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.153s, learning 0.189s)
               Value function loss: 1.2807
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.34s
                        Total time: 35060.20s
                               ETA: 1011211.2s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.360s, learning 0.186s)
               Value function loss: 16.3578
                    Surrogate loss: 0.0053
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 48.88
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.55s
                        Total time: 35068.75s
                               ETA: 1011145.5s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.281s, learning 0.164s)
               Value function loss: 179.2304
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.66
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 8.44s
                        Total time: 35077.20s
                               ETA: 1011076.9s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.768s, learning 0.173s)
               Value function loss: 4.6939
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 24.32
               Mean episode length: 48.33
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.94s
                        Total time: 35086.14s
                               ETA: 1011022.6s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.479s, learning 0.212s)
               Value function loss: 0.8924
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 49.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.69s
                        Total time: 35094.83s
                               ETA: 1010961.2s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.629s, learning 0.184s)
               Value function loss: 119.3782
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.65
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 8.81s
                        Total time: 35103.64s
                               ETA: 1010903.3s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.734s, learning 0.332s)
               Value function loss: 144.9105
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.20
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 9.07s
                        Total time: 35112.71s
                               ETA: 1010852.7s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.276s, learning 0.160s)
               Value function loss: 201.4144
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 4.37
               Mean episode length: 49.98
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.44s
                        Total time: 35121.14s
                               ETA: 1010784.0s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.754s, learning 0.319s)
               Value function loss: 17.6175
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 49.95
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 9.07s
                        Total time: 35130.22s
                               ETA: 1010733.6s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.051s, learning 0.205s)
               Value function loss: 291.8621
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.65
               Mean episode length: 49.33
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 8.26s
                        Total time: 35138.47s
                               ETA: 1010659.8s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.850s, learning 0.161s)
               Value function loss: 8.0101
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 49.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 9.01s
                        Total time: 35147.48s
                               ETA: 1010607.8s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.733s, learning 0.160s)
               Value function loss: 55.8320
                    Surrogate loss: 0.0052
             Mean action noise std: 0.71
                       Mean reward: 7.04
               Mean episode length: 49.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.89s
                        Total time: 35156.38s
                               ETA: 1010552.3s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.437s, learning 0.230s)
               Value function loss: 3.4708
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 6.91
               Mean episode length: 50.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.67s
                        Total time: 35165.04s
                               ETA: 1010490.4s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.721s, learning 0.163s)
               Value function loss: 131.5816
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 47.51
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 8.88s
                        Total time: 35173.93s
                               ETA: 1010434.8s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.738s, learning 0.187s)
               Value function loss: 133.9365
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 50.09
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.92s
                        Total time: 35182.85s
                               ETA: 1010380.4s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.388s, learning 0.295s)
               Value function loss: 90.3080
                    Surrogate loss: -0.0044
             Mean action noise std: 0.71
                       Mean reward: 14.40
               Mean episode length: 49.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.68s
                        Total time: 35191.53s
                               ETA: 1010319.0s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.493s, learning 0.187s)
               Value function loss: 39.9895
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 48.47
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.68s
                        Total time: 35200.21s
                               ETA: 1010257.6s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.765s, learning 0.161s)
               Value function loss: 3.0142
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 48.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.93s
                        Total time: 35209.14s
                               ETA: 1010203.3s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.189s)
               Value function loss: 1.9041
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 48.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.53s
                        Total time: 35217.67s
                               ETA: 1010137.8s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.458s, learning 0.276s)
               Value function loss: 1.4096
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 49.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.73s
                        Total time: 35226.41s
                               ETA: 1010078.0s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.516s, learning 0.163s)
               Value function loss: 112.0302
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 9.40
               Mean episode length: 49.27
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.68s
                        Total time: 35235.09s
                               ETA: 1010016.7s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.413s, learning 0.303s)
               Value function loss: 17.5913
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 46.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.72s
                        Total time: 35243.80s
                               ETA: 1009956.5s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.776s, learning 0.165s)
               Value function loss: 0.9427
                    Surrogate loss: 0.0044
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 48.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.94s
                        Total time: 35252.74s
                               ETA: 1009902.8s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.560s, learning 0.187s)
               Value function loss: 0.7770
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 49.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.75s
                        Total time: 35261.49s
                               ETA: 1009843.5s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.596s, learning 0.185s)
               Value function loss: 48.8185
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 47.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.78s
                        Total time: 35270.27s
                               ETA: 1009785.3s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.278s, learning 0.221s)
               Value function loss: 49.9360
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 47.72
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.50s
                        Total time: 35278.77s
                               ETA: 1009719.0s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.867s, learning 0.348s)
               Value function loss: 0.6997
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 9.21s
                        Total time: 35287.99s
                               ETA: 1009673.2s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.592s, learning 0.185s)
               Value function loss: 180.4153
                    Surrogate loss: 0.0039
             Mean action noise std: 0.71
                       Mean reward: 4.21
               Mean episode length: 48.70
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.78s
                        Total time: 35296.76s
                               ETA: 1009614.9s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.340s, learning 0.161s)
               Value function loss: 596.3153
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 6.44
               Mean episode length: 47.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 8.50s
                        Total time: 35305.26s
                               ETA: 1009548.7s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.697s, learning 0.322s)
               Value function loss: 402.3134
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 47.87
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 9.02s
                        Total time: 35314.28s
                               ETA: 1009497.4s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.717s, learning 0.163s)
               Value function loss: 2.7807
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 47.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.88s
                        Total time: 35323.16s
                               ETA: 1009442.1s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.618s, learning 0.171s)
               Value function loss: 2.2835
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 49.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.79s
                        Total time: 35331.95s
                               ETA: 1009384.3s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1782 steps/s (collection: 9.029s, learning 0.161s)
               Value function loss: 65.5054
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.77
               Mean episode length: 48.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 9.19s
                        Total time: 35341.14s
                               ETA: 1009337.9s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.766s, learning 0.158s)
               Value function loss: 27.7535
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 48.17
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.92s
                        Total time: 35350.06s
                               ETA: 1009284.0s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.685s, learning 0.166s)
               Value function loss: 1.9081
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.85s
                        Total time: 35358.91s
                               ETA: 1009228.0s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.357s, learning 0.329s)
               Value function loss: 14.7065
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.43
               Mean episode length: 48.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.69s
                        Total time: 35367.60s
                               ETA: 1009167.4s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.160s)
               Value function loss: 87.7655
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 49.06
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.66s
                        Total time: 35376.26s
                               ETA: 1009106.1s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.728s, learning 0.214s)
               Value function loss: 1.0649
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 48.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.94s
                        Total time: 35385.21s
                               ETA: 1009052.8s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.820s, learning 0.157s)
               Value function loss: 0.7099
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 48.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.98s
                        Total time: 35394.18s
                               ETA: 1009000.6s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.403s, learning 0.325s)
               Value function loss: 0.6775
                    Surrogate loss: -0.0259
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 47.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 8.73s
                        Total time: 35402.91s
                               ETA: 1008941.2s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.404s, learning 0.190s)
               Value function loss: 193.8505
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 48.75
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 8.59s
                        Total time: 35411.51s
                               ETA: 1008878.1s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.630s, learning 0.162s)
               Value function loss: 0.7091
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 47.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.79s
                        Total time: 35420.30s
                               ETA: 1008820.6s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.596s, learning 0.160s)
               Value function loss: 0.8022
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 4.37
               Mean episode length: 49.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 8.76s
                        Total time: 35429.05s
                               ETA: 1008762.2s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.468s, learning 0.158s)
               Value function loss: 0.5957
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 4.62
               Mean episode length: 51.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 8.63s
                        Total time: 35437.68s
                               ETA: 1008700.0s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.866s, learning 0.162s)
               Value function loss: 0.4604
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 46.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 9.03s
                        Total time: 35446.71s
                               ETA: 1008649.4s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.647s, learning 0.240s)
               Value function loss: 0.5093
                    Surrogate loss: -0.0263
             Mean action noise std: 0.71
                       Mean reward: 4.70
               Mean episode length: 50.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 8.89s
                        Total time: 35455.59s
                               ETA: 1008594.7s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.761s, learning 0.166s)
               Value function loss: 0.4956
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 48.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 8.93s
                        Total time: 35464.52s
                               ETA: 1008541.2s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.456s, learning 0.160s)
               Value function loss: 0.4028
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 48.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 8.62s
                        Total time: 35473.14s
                               ETA: 1008479.0s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.708s, learning 0.165s)
               Value function loss: 216.9183
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 48.94
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 8.87s
                        Total time: 35482.01s
                               ETA: 1008424.0s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.574s, learning 0.201s)
               Value function loss: 39.0204
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.65
               Mean episode length: 47.70
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 8.77s
                        Total time: 35490.78s
                               ETA: 1008366.3s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.668s, learning 0.194s)
               Value function loss: 0.5889
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 4.45
               Mean episode length: 49.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 8.86s
                        Total time: 35499.65s
                               ETA: 1008311.1s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.530s, learning 0.282s)
               Value function loss: 352.6872
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 49.44
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 8.81s
                        Total time: 35508.46s
                               ETA: 1008254.5s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.730s, learning 0.179s)
               Value function loss: 61.1491
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 49.07
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 8.91s
                        Total time: 35517.37s
                               ETA: 1008200.6s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.671s, learning 0.181s)
               Value function loss: 26.1746
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 8.85s
                        Total time: 35526.22s
                               ETA: 1008145.2s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.406s, learning 0.229s)
               Value function loss: 215.1728
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 4.29
               Mean episode length: 48.85
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 8.63s
                        Total time: 35534.86s
                               ETA: 1008083.7s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.752s, learning 0.204s)
               Value function loss: 33.8637
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 17.15
               Mean episode length: 50.08
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 8.96s
                        Total time: 35543.81s
                               ETA: 1008031.3s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.656s, learning 0.203s)
               Value function loss: 17.1693
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 47.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 8.86s
                        Total time: 35552.67s
                               ETA: 1007976.1s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.687s, learning 0.161s)
               Value function loss: 73.9282
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 19.22
               Mean episode length: 48.55
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 8.85s
                        Total time: 35561.52s
                               ETA: 1007920.7s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.662s, learning 0.205s)
               Value function loss: 63.9556
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 49.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 8.87s
                        Total time: 35570.39s
                               ETA: 1007865.8s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.380s, learning 0.203s)
               Value function loss: 152.3242
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 11.95
               Mean episode length: 49.18
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 8.58s
                        Total time: 35578.97s
                               ETA: 1007803.0s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.621s, learning 0.247s)
               Value function loss: 2.5995
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 49.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 8.87s
                        Total time: 35587.84s
                               ETA: 1007748.2s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.698s, learning 0.319s)
               Value function loss: 201.9448
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 47.87
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 9.02s
                        Total time: 35596.85s
                               ETA: 1007697.7s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.873s, learning 0.242s)
               Value function loss: 122.6539
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 50.26
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 9.11s
                        Total time: 35605.97s
                               ETA: 1007649.9s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.728s, learning 0.233s)
               Value function loss: 213.1821
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 6.97
               Mean episode length: 49.52
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 8.96s
                        Total time: 35614.93s
                               ETA: 1007597.9s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.635s, learning 0.201s)
               Value function loss: 19.2429
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 26.78
               Mean episode length: 49.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 8.84s
                        Total time: 35623.76s
                               ETA: 1007542.3s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.661s, learning 0.393s)
               Value function loss: 64.4805
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 11.56
               Mean episode length: 50.16
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 9.05s
                        Total time: 35632.82s
                               ETA: 1007492.9s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.526s, learning 0.217s)
               Value function loss: 44.4287
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 49.69
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 8.74s
                        Total time: 35641.56s
                               ETA: 1007434.7s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.595s, learning 0.420s)
               Value function loss: 1.3940
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 49.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 9.01s
                        Total time: 35650.58s
                               ETA: 1007384.3s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.703s, learning 0.248s)
               Value function loss: 13.0147
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 12.01
               Mean episode length: 50.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 8.95s
                        Total time: 35659.53s
                               ETA: 1007332.1s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.760s, learning 0.216s)
               Value function loss: 253.7158
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 24.26
               Mean episode length: 48.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 8.98s
                        Total time: 35668.50s
                               ETA: 1007280.6s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.538s, learning 0.232s)
               Value function loss: 1.3600
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 8.77s
                        Total time: 35677.27s
                               ETA: 1007223.3s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.600s, learning 0.220s)
               Value function loss: 14.7689
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.63
               Mean episode length: 47.42
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 8.82s
                        Total time: 35686.09s
                               ETA: 1007167.5s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.685s, learning 0.297s)
               Value function loss: 0.9226
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 48.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 8.98s
                        Total time: 35695.07s
                               ETA: 1007116.3s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.703s, learning 0.321s)
               Value function loss: 4.6072
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 6.90
               Mean episode length: 49.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 9.02s
                        Total time: 35704.10s
                               ETA: 1007066.2s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.394s, learning 0.163s)
               Value function loss: 48.5554
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 48.92
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 8.56s
                        Total time: 35712.66s
                               ETA: 1007003.1s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.685s, learning 0.159s)
               Value function loss: 105.4612
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 4.33
               Mean episode length: 50.38
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 8.84s
                        Total time: 35721.50s
                               ETA: 1006948.0s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.976s, learning 0.159s)
               Value function loss: 269.7154
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 4.90
               Mean episode length: 50.42
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 9.13s
                        Total time: 35730.64s
                               ETA: 1006901.2s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.586s, learning 0.163s)
               Value function loss: 126.2158
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 49.14
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 8.75s
                        Total time: 35739.38s
                               ETA: 1006843.5s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.189s, learning 0.164s)
               Value function loss: 149.7543
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 50.57
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 8.35s
                        Total time: 35747.74s
                               ETA: 1006774.7s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.416s, learning 0.170s)
               Value function loss: 17.7315
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 17.29
               Mean episode length: 50.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 8.59s
                        Total time: 35756.32s
                               ETA: 1006712.5s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.623s, learning 0.189s)
               Value function loss: 238.0158
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 49.13
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 8.81s
                        Total time: 35765.14s
                               ETA: 1006656.7s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.828s, learning 0.165s)
               Value function loss: 28.9837
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 48.50
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 8.99s
                        Total time: 35774.13s
                               ETA: 1006606.0s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.406s, learning 0.165s)
               Value function loss: 94.9814
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 11.29
               Mean episode length: 49.14
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 8.57s
                        Total time: 35782.70s
                               ETA: 1006543.5s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.485s, learning 0.163s)
               Value function loss: 299.9582
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 6.94
               Mean episode length: 50.17
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 8.65s
                        Total time: 35791.35s
                               ETA: 1006483.2s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.305s, learning 0.183s)
               Value function loss: 8.8001
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 11.81
               Mean episode length: 49.99
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 8.49s
                        Total time: 35799.84s
                               ETA: 1006418.4s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.598s, learning 0.185s)
               Value function loss: 2.6606
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 47.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 8.78s
                        Total time: 35808.62s
                               ETA: 1006361.9s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.837s, learning 0.166s)
               Value function loss: 155.3886
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 8.94
               Mean episode length: 48.46
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 9.00s
                        Total time: 35817.62s
                               ETA: 1006311.6s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.311s, learning 0.167s)
               Value function loss: 18.7963
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 4.58
               Mean episode length: 50.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 8.48s
                        Total time: 35826.10s
                               ETA: 1006246.6s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.605s, learning 0.199s)
               Value function loss: 1.2715
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 49.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 8.80s
                        Total time: 35834.91s
                               ETA: 1006190.8s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.453s, learning 0.176s)
               Value function loss: 46.3520
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 49.55
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 8.63s
                        Total time: 35843.53s
                               ETA: 1006130.1s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.464s, learning 0.157s)
               Value function loss: 0.8700
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 6.90
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 8.62s
                        Total time: 35852.16s
                               ETA: 1006069.2s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.390s, learning 0.182s)
               Value function loss: 192.9256
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.29
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 8.57s
                        Total time: 35860.73s
                               ETA: 1006007.0s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.698s, learning 0.165s)
               Value function loss: 5.1860
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 4.45
               Mean episode length: 49.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 8.86s
                        Total time: 35869.59s
                               ETA: 1005953.0s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.740s, learning 0.189s)
               Value function loss: 13.2843
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 6.75
               Mean episode length: 49.82
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 8.93s
                        Total time: 35878.52s
                               ETA: 1005900.8s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.661s, learning 0.305s)
               Value function loss: 18.5373
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 49.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 8.97s
                        Total time: 35887.49s
                               ETA: 1005849.7s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.716s, learning 0.186s)
               Value function loss: 46.7446
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 14.11
               Mean episode length: 48.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.90s
                        Total time: 35896.39s
                               ETA: 1005796.8s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.763s, learning 0.173s)
               Value function loss: 63.5440
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.75
               Mean episode length: 50.16
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 8.94s
                        Total time: 35905.32s
                               ETA: 1005744.9s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.717s, learning 0.161s)
               Value function loss: 198.9089
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 12.07
               Mean episode length: 49.82
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.88s
                        Total time: 35914.20s
                               ETA: 1005691.4s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.491s, learning 0.169s)
               Value function loss: 1.5719
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 48.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 8.66s
                        Total time: 35922.86s
                               ETA: 1005631.8s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.716s, learning 0.321s)
               Value function loss: 19.3776
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 50.27
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 9.04s
                        Total time: 35931.90s
                               ETA: 1005582.8s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.590s, learning 0.194s)
               Value function loss: 0.8366
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 50.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.78s
                        Total time: 35940.68s
                               ETA: 1005526.8s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.688s, learning 0.159s)
               Value function loss: 181.7038
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 49.09
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.85s
                        Total time: 35949.53s
                               ETA: 1005472.5s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.877s, learning 0.167s)
               Value function loss: 0.9215
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 9.45
               Mean episode length: 49.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 9.04s
                        Total time: 35958.57s
                               ETA: 1005423.8s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.640s, learning 0.158s)
               Value function loss: 197.4196
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 21.83
               Mean episode length: 49.32
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.80s
                        Total time: 35967.37s
                               ETA: 1005368.2s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.751s, learning 0.160s)
               Value function loss: 269.8368
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 14.27
               Mean episode length: 48.88
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 16.91s
                        Total time: 35984.28s
                               ETA: 1005539.4s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 949 steps/s (collection: 16.977s, learning 0.273s)
               Value function loss: 248.5533
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 48.83
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 17.25s
                        Total time: 36001.53s
                               ETA: 1005719.9s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.742s, learning 0.208s)
               Value function loss: 3.1805
                    Surrogate loss: -0.0180
             Mean action noise std: 0.71
                       Mean reward: 22.36
               Mean episode length: 51.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 16.95s
                        Total time: 36018.48s
                               ETA: 1005891.9s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.097s, learning 0.245s)
               Value function loss: 1.8630
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 49.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 17.34s
                        Total time: 36035.82s
                               ETA: 1006074.8s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.698s, learning 0.166s)
               Value function loss: 525.6155
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.45
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 16.86s
                        Total time: 36052.69s
                               ETA: 1006244.2s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 944 steps/s (collection: 17.182s, learning 0.167s)
               Value function loss: 338.6134
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 4.56
               Mean episode length: 49.25
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 17.35s
                        Total time: 36070.04s
                               ETA: 1006427.0s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.625s, learning 0.172s)
               Value function loss: 246.5836
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 49.48
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 16.80s
                        Total time: 36086.84s
                               ETA: 1006594.4s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.722s, learning 0.219s)
               Value function loss: 23.7797
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 47.72
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 16.94s
                        Total time: 36103.78s
                               ETA: 1006765.6s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.750s, learning 0.163s)
               Value function loss: 95.1642
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.48
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 16.91s
                        Total time: 36120.69s
                               ETA: 1006935.9s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.724s, learning 0.170s)
               Value function loss: 129.0061
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.90
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 16.89s
                        Total time: 36137.58s
                               ETA: 1007105.6s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.953s, learning 0.156s)
               Value function loss: 150.2974
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 17.11
               Mean episode length: 50.83
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 17.11s
                        Total time: 36154.69s
                               ETA: 1007281.2s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.815s, learning 0.221s)
               Value function loss: 111.9495
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 17.17
               Mean episode length: 50.86
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 17.04s
                        Total time: 36171.73s
                               ETA: 1007454.6s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.779s, learning 0.206s)
               Value function loss: 15.5397
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 7.06
               Mean episode length: 50.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 16.98s
                        Total time: 36188.71s
                               ETA: 1007626.5s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.890s, learning 0.164s)
               Value function loss: 93.9732
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 12.06
               Mean episode length: 50.95
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 17.05s
                        Total time: 36205.77s
                               ETA: 1007800.3s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.963s, learning 0.170s)
               Value function loss: 1.9617
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 49.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 17.13s
                        Total time: 36222.90s
                               ETA: 1007976.1s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 954 steps/s (collection: 17.008s, learning 0.166s)
               Value function loss: 75.1059
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.72
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 17.17s
                        Total time: 36240.07s
                               ETA: 1008152.9s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.876s, learning 0.165s)
               Value function loss: 32.9202
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 6.71
               Mean episode length: 49.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 17.04s
                        Total time: 36257.11s
                               ETA: 1008325.9s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.839s, learning 0.160s)
               Value function loss: 63.0627
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 48.34
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 17.00s
                        Total time: 36274.11s
                               ETA: 1008497.7s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.729s, learning 0.183s)
               Value function loss: 228.4536
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 49.58
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 16.91s
                        Total time: 36291.03s
                               ETA: 1008666.9s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.621s, learning 0.158s)
               Value function loss: 2.7583
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 49.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 16.78s
                        Total time: 36307.80s
                               ETA: 1008832.3s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.887s, learning 0.160s)
               Value function loss: 2.0039
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 6.85
               Mean episode length: 49.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 17.05s
                        Total time: 36324.85s
                               ETA: 1009005.1s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.506s, learning 0.260s)
               Value function loss: 1.1733
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 7.09
               Mean episode length: 51.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 16.77s
                        Total time: 36341.62s
                               ETA: 1009169.9s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.797s, learning 0.219s)
               Value function loss: 0.7578
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 17.02s
                        Total time: 36358.63s
                               ETA: 1009341.6s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.204s, learning 0.161s)
               Value function loss: 132.4760
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 49.25
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 16.37s
                        Total time: 36375.00s
                               ETA: 1009495.1s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.636s, learning 0.176s)
               Value function loss: 234.1496
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.99
               Mean episode length: 51.36
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 16.81s
                        Total time: 36391.81s
                               ETA: 1009660.9s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.621s, learning 0.234s)
               Value function loss: 491.1191
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 48.40
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 16.85s
                        Total time: 36408.67s
                               ETA: 1009827.8s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.353s, learning 0.166s)
               Value function loss: 14.0844
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 16.52s
                        Total time: 36425.19s
                               ETA: 1009985.3s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.784s, learning 0.171s)
               Value function loss: 1.7458
                    Surrogate loss: -0.0258
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 48.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 16.95s
                        Total time: 36442.14s
                               ETA: 1010154.8s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 939 steps/s (collection: 17.282s, learning 0.162s)
               Value function loss: 180.6163
                    Surrogate loss: 0.0060
             Mean action noise std: 0.71
                       Mean reward: 4.50
               Mean episode length: 48.85
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 17.44s
                        Total time: 36459.58s
                               ETA: 1010337.7s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.262s, learning 0.159s)
               Value function loss: 5.4464
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 47.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 16.42s
                        Total time: 36476.00s
                               ETA: 1010492.1s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.004s, learning 0.203s)
               Value function loss: 535.1622
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 14.16
               Mean episode length: 49.06
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 17.21s
                        Total time: 36493.21s
                               ETA: 1010668.2s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.131s, learning 0.179s)
               Value function loss: 18.2331
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 9.17
               Mean episode length: 49.83
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 17.31s
                        Total time: 36510.52s
                               ETA: 1010847.1s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.622s, learning 0.168s)
               Value function loss: 133.2842
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 48.33
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 16.79s
                        Total time: 36527.31s
                               ETA: 1011011.5s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.701s, learning 0.164s)
               Value function loss: 1.5509
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 49.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 16.87s
                        Total time: 36544.18s
                               ETA: 1011177.8s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.108s, learning 0.164s)
               Value function loss: 1.3314
                    Surrogate loss: -0.0271
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 48.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 17.27s
                        Total time: 36561.45s
                               ETA: 1011355.3s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.072s, learning 0.186s)
               Value function loss: 1.0730
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 48.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 17.26s
                        Total time: 36578.71s
                               ETA: 1011532.3s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.878s, learning 0.158s)
               Value function loss: 0.9483
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 48.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 17.04s
                        Total time: 36595.74s
                               ETA: 1011703.0s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1176 steps/s (collection: 13.762s, learning 0.166s)
               Value function loss: 469.4591
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 47.60
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 13.93s
                        Total time: 36609.67s
                               ETA: 1011787.7s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.805s, learning 0.182s)
               Value function loss: 0.8850
                    Surrogate loss: -0.0294
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.99s
                        Total time: 36618.66s
                               ETA: 1011735.9s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.267s, learning 0.273s)
               Value function loss: 109.6554
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 21.81
               Mean episode length: 48.66
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.54s
                        Total time: 36627.20s
                               ETA: 1011671.7s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.926s, learning 0.211s)
               Value function loss: 0.6877
                    Surrogate loss: -0.0297
             Mean action noise std: 0.71
                       Mean reward: 14.43
               Mean episode length: 50.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 9.14s
                        Total time: 36636.33s
                               ETA: 1011624.1s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.462s, learning 0.198s)
               Value function loss: 30.0377
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 49.81
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.66s
                        Total time: 36645.00s
                               ETA: 1011563.3s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.323s, learning 0.157s)
               Value function loss: 172.3489
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.83
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.48s
                        Total time: 36653.47s
                               ETA: 1011497.5s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.386s, learning 0.234s)
               Value function loss: 90.3210
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 48.03
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.62s
                        Total time: 36662.10s
                               ETA: 1011435.7s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.533s, learning 0.160s)
               Value function loss: 14.1223
                    Surrogate loss: -0.0074
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 48.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.69s
                        Total time: 36670.79s
                               ETA: 1011375.9s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.168s)
               Value function loss: 1.1856
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 6.96
               Mean episode length: 49.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.42s
                        Total time: 36679.21s
                               ETA: 1011308.7s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.554s, learning 0.261s)
               Value function loss: 88.5979
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 49.99
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.81s
                        Total time: 36688.02s
                               ETA: 1011252.3s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.771s, learning 0.300s)
               Value function loss: 34.4349
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.26
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 9.07s
                        Total time: 36697.10s
                               ETA: 1011203.0s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.387s, learning 0.172s)
               Value function loss: 400.0240
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 48.49
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.56s
                        Total time: 36705.65s
                               ETA: 1011139.7s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.421s, learning 0.167s)
               Value function loss: 205.4235
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.92
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.59s
                        Total time: 36714.24s
                               ETA: 1011077.2s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.301s, learning 0.168s)
               Value function loss: 3.3770
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 48.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.47s
                        Total time: 36722.71s
                               ETA: 1011011.4s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.469s, learning 0.210s)
               Value function loss: 19.3056
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 47.73
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.68s
                        Total time: 36731.39s
                               ETA: 1010951.4s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.653s, learning 0.177s)
               Value function loss: 1.0102
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 48.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.83s
                        Total time: 36740.22s
                               ETA: 1010895.6s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.744s, learning 0.167s)
               Value function loss: 16.4426
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.87
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.91s
                        Total time: 36749.13s
                               ETA: 1010842.1s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.385s, learning 0.234s)
               Value function loss: 31.4798
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.21
               Mean episode length: 47.77
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.62s
                        Total time: 36757.75s
                               ETA: 1010780.5s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.228s, learning 0.188s)
               Value function loss: 93.1944
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 4.72
               Mean episode length: 49.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.42s
                        Total time: 36766.17s
                               ETA: 1010713.5s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.435s, learning 0.342s)
               Value function loss: 111.2836
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 13.89
               Mean episode length: 47.45
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.78s
                        Total time: 36774.94s
                               ETA: 1010656.3s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.627s, learning 0.163s)
               Value function loss: 121.9363
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.37
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.79s
                        Total time: 36783.73s
                               ETA: 1010599.6s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.589s, learning 0.162s)
               Value function loss: 166.0603
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 6.37
               Mean episode length: 48.12
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.75s
                        Total time: 36792.49s
                               ETA: 1010541.8s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.701s, learning 0.169s)
               Value function loss: 222.1941
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 47.88
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.87s
                        Total time: 36801.35s
                               ETA: 1010487.3s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.332s, learning 0.173s)
               Value function loss: 19.3789
                    Surrogate loss: -0.0067
             Mean action noise std: 0.71
                       Mean reward: 9.45
               Mean episode length: 49.12
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.51s
                        Total time: 36809.86s
                               ETA: 1010422.8s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.537s, learning 0.165s)
               Value function loss: 113.0317
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.90
               Mean episode length: 51.24
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.70s
                        Total time: 36818.56s
                               ETA: 1010363.7s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.400s, learning 0.161s)
               Value function loss: 18.7936
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 16.58
               Mean episode length: 48.88
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.56s
                        Total time: 36827.12s
                               ETA: 1010300.8s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.593s, learning 0.161s)
               Value function loss: 156.9862
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 48.99
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.75s
                        Total time: 36835.88s
                               ETA: 1010243.3s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.163s)
               Value function loss: 18.4206
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.12
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.55s
                        Total time: 36844.42s
                               ETA: 1010180.1s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.880s, learning 0.263s)
               Value function loss: 18.4441
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 11.82
               Mean episode length: 48.75
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 9.14s
                        Total time: 36853.57s
                               ETA: 1010133.2s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.825s, learning 0.231s)
               Value function loss: 1.1843
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 48.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 9.06s
                        Total time: 36862.62s
                               ETA: 1010084.0s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.504s, learning 0.171s)
               Value function loss: 80.0587
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.62
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.68s
                        Total time: 36871.30s
                               ETA: 1010024.4s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.648s, learning 0.167s)
               Value function loss: 47.2530
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 47.88
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.82s
                        Total time: 36880.11s
                               ETA: 1009968.7s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.620s, learning 0.172s)
               Value function loss: 1.1663
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 49.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.79s
                        Total time: 36888.91s
                               ETA: 1009912.3s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.934s, learning 0.268s)
               Value function loss: 258.5721
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 9.06
               Mean episode length: 48.73
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 9.20s
                        Total time: 36898.11s
                               ETA: 1009867.2s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.276s, learning 0.160s)
               Value function loss: 49.1608
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 47.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.44s
                        Total time: 36906.54s
                               ETA: 1009801.2s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.756s, learning 0.173s)
               Value function loss: 1.0278
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.93s
                        Total time: 36915.47s
                               ETA: 1009748.6s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.734s, learning 0.157s)
               Value function loss: 25.3763
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.89s
                        Total time: 36924.37s
                               ETA: 1009695.1s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.823s, learning 0.155s)
               Value function loss: 0.9027
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 23.83
               Mean episode length: 46.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.98s
                        Total time: 36933.34s
                               ETA: 1009643.9s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.368s, learning 0.160s)
               Value function loss: 120.1217
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 48.10
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.53s
                        Total time: 36941.87s
                               ETA: 1009580.5s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.417s, learning 0.352s)
               Value function loss: 88.8451
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 48.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.77s
                        Total time: 36950.64s
                               ETA: 1009523.7s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.508s, learning 0.161s)
               Value function loss: 1.8870
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 46.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.67s
                        Total time: 36959.31s
                               ETA: 1009464.2s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.361s, learning 0.161s)
               Value function loss: 40.8569
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 46.85
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.52s
                        Total time: 36967.83s
                               ETA: 1009400.7s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.166s)
               Value function loss: 128.7289
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 47.33
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.63s
                        Total time: 36976.46s
                               ETA: 1009340.1s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.546s, learning 0.158s)
               Value function loss: 23.1644
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 48.10
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.70s
                        Total time: 36985.16s
                               ETA: 1009281.7s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.323s, learning 0.176s)
               Value function loss: 53.1022
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 47.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.50s
                        Total time: 36993.66s
                               ETA: 1009217.6s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.336s, learning 0.174s)
               Value function loss: 46.2846
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 46.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.51s
                        Total time: 37002.17s
                               ETA: 1009153.9s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.688s, learning 0.161s)
               Value function loss: 80.1964
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.85s
                        Total time: 37011.02s
                               ETA: 1009099.5s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.644s, learning 0.160s)
               Value function loss: 15.9940
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 11.66
               Mean episode length: 48.87
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.80s
                        Total time: 37019.82s
                               ETA: 1009043.9s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.628s, learning 0.160s)
               Value function loss: 1.0559
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 47.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.79s
                        Total time: 37028.61s
                               ETA: 1008987.8s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.172s)
               Value function loss: 64.4562
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 14.26
               Mean episode length: 48.13
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.82s
                        Total time: 37037.44s
                               ETA: 1008932.8s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.646s, learning 0.223s)
               Value function loss: 46.5716
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 47.32
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.87s
                        Total time: 37046.31s
                               ETA: 1008879.0s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.441s, learning 0.171s)
               Value function loss: 508.7903
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 4.36
               Mean episode length: 48.54
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.61s
                        Total time: 37054.92s
                               ETA: 1008818.3s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.551s, learning 0.349s)
               Value function loss: 49.9854
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: 3.90
               Mean episode length: 47.83
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.90s
                        Total time: 37063.82s
                               ETA: 1008765.4s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.358s, learning 0.165s)
               Value function loss: 255.4019
                    Surrogate loss: 0.0063
             Mean action noise std: 0.71
                       Mean reward: 6.08
               Mean episode length: 47.05
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.52s
                        Total time: 37072.34s
                               ETA: 1008702.3s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.420s, learning 0.175s)
               Value function loss: 2.3817
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 48.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.60s
                        Total time: 37080.93s
                               ETA: 1008641.2s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.468s, learning 0.174s)
               Value function loss: 1.7238
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 13.94
               Mean episode length: 48.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.64s
                        Total time: 37089.58s
                               ETA: 1008581.3s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.650s, learning 0.181s)
               Value function loss: 1.3902
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 3.56
               Mean episode length: 45.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.83s
                        Total time: 37098.41s
                               ETA: 1008526.7s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.164s)
               Value function loss: 125.7325
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 13.75
               Mean episode length: 46.73
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.66s
                        Total time: 37107.07s
                               ETA: 1008467.5s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.739s, learning 0.316s)
               Value function loss: 1.2856
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 47.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 9.06s
                        Total time: 37116.13s
                               ETA: 1008419.0s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.060s, learning 0.355s)
               Value function loss: 64.3296
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 48.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 9.41s
                        Total time: 37125.54s
                               ETA: 1008380.3s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.605s, learning 0.202s)
               Value function loss: 22.6415
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.29
               Mean episode length: 50.04
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.81s
                        Total time: 37134.35s
                               ETA: 1008325.1s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.486s, learning 0.203s)
               Value function loss: 17.6265
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.69s
                        Total time: 37143.04s
                               ETA: 1008266.7s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.603s, learning 0.160s)
               Value function loss: 1.2279
                    Surrogate loss: 0.0058
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.76s
                        Total time: 37151.80s
                               ETA: 1008210.4s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.854s, learning 0.179s)
               Value function loss: 66.4581
                    Surrogate loss: 0.0026
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 48.48
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 9.03s
                        Total time: 37160.83s
                               ETA: 1008161.4s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.467s, learning 0.284s)
               Value function loss: 4.3108
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 50.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 8.75s
                        Total time: 37169.58s
                               ETA: 1008104.8s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.705s, learning 0.176s)
               Value function loss: 18.5505
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 48.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.88s
                        Total time: 37178.47s
                               ETA: 1008051.7s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.457s, learning 0.291s)
               Value function loss: 0.4028
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 48.03
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.75s
                        Total time: 37187.21s
                               ETA: 1007995.1s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1777 steps/s (collection: 8.968s, learning 0.251s)
               Value function loss: 0.5046
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 48.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 9.22s
                        Total time: 37196.43s
                               ETA: 1007951.2s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.824s, learning 0.162s)
               Value function loss: 0.4090
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 49.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.99s
                        Total time: 37205.42s
                               ETA: 1007901.1s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.418s, learning 0.157s)
               Value function loss: 0.3360
                    Surrogate loss: -0.0317
             Mean action noise std: 0.71
                       Mean reward: 3.49
               Mean episode length: 46.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 8.57s
                        Total time: 37213.99s
                               ETA: 1007839.8s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.601s, learning 0.173s)
               Value function loss: 0.2994
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 47.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.77s
                        Total time: 37222.77s
                               ETA: 1007784.0s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.510s, learning 0.190s)
               Value function loss: 0.2971
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 8.70s
                        Total time: 37231.47s
                               ETA: 1007726.2s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.157s)
               Value function loss: 0.2556
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 48.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.74s
                        Total time: 37240.21s
                               ETA: 1007669.4s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.627s, learning 0.161s)
               Value function loss: 0.2829
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 47.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.79s
                        Total time: 37248.99s
                               ETA: 1007614.0s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.472s, learning 0.235s)
               Value function loss: 93.1406
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 14.09
               Mean episode length: 47.91
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.71s
                        Total time: 37257.70s
                               ETA: 1007556.5s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.308s, learning 0.225s)
               Value function loss: 46.0177
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 4.21
               Mean episode length: 49.37
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.53s
                        Total time: 37266.23s
                               ETA: 1007494.3s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.715s, learning 0.158s)
               Value function loss: 70.5929
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.72
               Mean episode length: 46.98
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 8.87s
                        Total time: 37275.11s
                               ETA: 1007441.3s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.630s, learning 0.162s)
               Value function loss: 0.4942
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 6.63
               Mean episode length: 48.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 8.79s
                        Total time: 37283.90s
                               ETA: 1007386.1s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.474s, learning 0.161s)
               Value function loss: 0.4231
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 8.64s
                        Total time: 37292.53s
                               ETA: 1007326.7s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.623s, learning 0.273s)
               Value function loss: 133.5842
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 11.78
               Mean episode length: 48.56
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.90s
                        Total time: 37301.43s
                               ETA: 1007274.4s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.691s, learning 0.197s)
               Value function loss: 0.5310
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 49.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 8.89s
                        Total time: 37310.32s
                               ETA: 1007221.9s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.545s, learning 0.170s)
               Value function loss: 0.4754
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 46.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.72s
                        Total time: 37319.03s
                               ETA: 1007164.8s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.792s, learning 0.341s)
               Value function loss: 0.3783
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 9.13s
                        Total time: 37328.17s
                               ETA: 1007118.9s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.536s, learning 0.164s)
               Value function loss: 0.3187
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 47.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.70s
                        Total time: 37336.87s
                               ETA: 1007061.5s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.488s, learning 0.192s)
               Value function loss: 0.3040
                    Surrogate loss: -0.0263
             Mean action noise std: 0.71
                       Mean reward: 3.57
               Mean episode length: 47.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.68s
                        Total time: 37345.55s
                               ETA: 1007003.5s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.748s, learning 0.192s)
               Value function loss: 48.0323
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 48.96
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 8.94s
                        Total time: 37354.49s
                               ETA: 1006952.5s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.878s, learning 0.189s)
               Value function loss: 0.3104
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 46.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 9.07s
                        Total time: 37363.55s
                               ETA: 1006904.9s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.164s)
               Value function loss: 59.7394
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 46.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.72s
                        Total time: 37372.27s
                               ETA: 1006848.0s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.587s, learning 0.389s)
               Value function loss: 0.3664
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 48.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 8.98s
                        Total time: 37381.24s
                               ETA: 1006798.0s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.658s, learning 0.158s)
               Value function loss: 0.3737
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 48.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.82s
                        Total time: 37390.06s
                               ETA: 1006743.9s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.619s, learning 0.157s)
               Value function loss: 132.2659
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.59
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 8.78s
                        Total time: 37398.84s
                               ETA: 1006688.6s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.539s, learning 0.326s)
               Value function loss: 0.3017
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 3.71
               Mean episode length: 46.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 8.86s
                        Total time: 37407.70s
                               ETA: 1006635.8s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.444s, learning 0.159s)
               Value function loss: 146.9577
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 46.93
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.60s
                        Total time: 37416.31s
                               ETA: 1006575.9s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.414s, learning 0.209s)
               Value function loss: 17.7937
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 47.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.62s
                        Total time: 37424.93s
                               ETA: 1006516.6s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.350s, learning 0.180s)
               Value function loss: 0.4795
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 48.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.53s
                        Total time: 37433.46s
                               ETA: 1006454.8s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.921s, learning 0.166s)
               Value function loss: 0.4087
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 9.09s
                        Total time: 37442.55s
                               ETA: 1006408.0s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.706s, learning 0.159s)
               Value function loss: 0.2915
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 47.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 8.87s
                        Total time: 37451.41s
                               ETA: 1006355.3s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.073s, learning 0.253s)
               Value function loss: 18.0301
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 3.91
               Mean episode length: 48.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 9.33s
                        Total time: 37460.74s
                               ETA: 1006315.0s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.899s, learning 0.265s)
               Value function loss: 160.4479
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.84
               Mean episode length: 47.53
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 9.16s
                        Total time: 37469.90s
                               ETA: 1006270.4s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.584s, learning 0.266s)
               Value function loss: 0.3936
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 47.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.85s
                        Total time: 37478.75s
                               ETA: 1006217.3s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.757s, learning 0.179s)
               Value function loss: 0.3514
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 48.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.94s
                        Total time: 37487.69s
                               ETA: 1006166.6s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.610s, learning 0.187s)
               Value function loss: 0.2846
                    Surrogate loss: -0.0263
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 48.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 8.80s
                        Total time: 37496.48s
                               ETA: 1006112.2s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.641s, learning 0.185s)
               Value function loss: 0.2691
                    Surrogate loss: -0.0284
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 8.83s
                        Total time: 37505.31s
                               ETA: 1006058.6s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.203s, learning 0.161s)
               Value function loss: 0.2318
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 48.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 9.36s
                        Total time: 37514.68s
                               ETA: 1006019.4s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.542s, learning 0.221s)
               Value function loss: 0.2682
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 4.32
               Mean episode length: 49.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.76s
                        Total time: 37523.44s
                               ETA: 1005964.2s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.580s, learning 0.304s)
               Value function loss: 64.6028
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 47.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.88s
                        Total time: 37532.32s
                               ETA: 1005912.2s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.166s)
               Value function loss: 3.6086
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 48.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.54s
                        Total time: 37540.87s
                               ETA: 1005851.1s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.647s, learning 0.162s)
               Value function loss: 0.3613
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 48.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.81s
                        Total time: 37549.68s
                               ETA: 1005797.1s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.687s, learning 0.246s)
               Value function loss: 12.3553
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 3.66
               Mean episode length: 46.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.93s
                        Total time: 37558.61s
                               ETA: 1005746.5s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.667s, learning 0.291s)
               Value function loss: 52.3276
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 9.00
               Mean episode length: 47.48
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.96s
                        Total time: 37567.57s
                               ETA: 1005696.6s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.671s, learning 0.253s)
               Value function loss: 0.2683
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 48.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.92s
                        Total time: 37576.49s
                               ETA: 1005645.8s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.543s, learning 0.162s)
               Value function loss: 0.2957
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.70s
                        Total time: 37585.20s
                               ETA: 1005589.2s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.588s, learning 0.176s)
               Value function loss: 0.3385
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 47.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.76s
                        Total time: 37593.96s
                               ETA: 1005534.2s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.433s, learning 0.245s)
               Value function loss: 0.2926
                    Surrogate loss: -0.0297
             Mean action noise std: 0.71
                       Mean reward: 4.31
               Mean episode length: 49.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.68s
                        Total time: 37602.64s
                               ETA: 1005476.8s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.872s, learning 0.172s)
               Value function loss: 314.2515
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.62
               Mean episode length: 46.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 9.04s
                        Total time: 37611.68s
                               ETA: 1005429.3s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.693s, learning 0.165s)
               Value function loss: 0.3806
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 47.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.86s
                        Total time: 37620.54s
                               ETA: 1005376.9s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.650s, learning 0.168s)
               Value function loss: 0.3874
                    Surrogate loss: -0.0258
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 47.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.82s
                        Total time: 37629.36s
                               ETA: 1005323.4s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.461s, learning 0.159s)
               Value function loss: 0.3489
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 3.33
               Mean episode length: 46.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.62s
                        Total time: 37637.98s
                               ETA: 1005264.6s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.962s, learning 0.157s)
               Value function loss: 0.3181
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 48.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 9.12s
                        Total time: 37647.10s
                               ETA: 1005219.2s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.694s, learning 0.229s)
               Value function loss: 0.2847
                    Surrogate loss: -0.0264
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 48.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 8.92s
                        Total time: 37656.02s
                               ETA: 1005168.6s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.686s, learning 0.247s)
               Value function loss: 0.2650
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 47.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 8.93s
                        Total time: 37664.95s
                               ETA: 1005118.3s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.311s, learning 0.261s)
               Value function loss: 0.2841
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 4.94
               Mean episode length: 50.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.57s
                        Total time: 37673.53s
                               ETA: 1005058.4s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.804s, learning 0.169s)
               Value function loss: 0.2237
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 47.38
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 8.97s
                        Total time: 37682.50s
                               ETA: 1005009.1s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.725s, learning 0.166s)
               Value function loss: 0.2404
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 48.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 8.89s
                        Total time: 37691.39s
                               ETA: 1004957.8s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.449s, learning 0.165s)
               Value function loss: 0.2235
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 48.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.61s
                        Total time: 37700.00s
                               ETA: 1004899.0s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.469s, learning 0.159s)
               Value function loss: 0.2338
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 48.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.63s
                        Total time: 37708.63s
                               ETA: 1004840.7s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.309s, learning 0.252s)
               Value function loss: 0.2101
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 47.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.56s
                        Total time: 37717.19s
                               ETA: 1004780.6s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.167s)
               Value function loss: 0.2101
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 47.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.38s
                        Total time: 37725.57s
                               ETA: 1004715.7s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.628s, learning 0.251s)
               Value function loss: 40.0472
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 47.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.88s
                        Total time: 37734.45s
                               ETA: 1004664.2s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.715s, learning 0.188s)
               Value function loss: 17.9499
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 48.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.90s
                        Total time: 37743.36s
                               ETA: 1004613.3s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.610s, learning 0.171s)
               Value function loss: 18.0124
                    Surrogate loss: 0.0030
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 49.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.78s
                        Total time: 37752.14s
                               ETA: 1004559.1s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.696s, learning 0.295s)
               Value function loss: 0.2193
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.52
               Mean episode length: 49.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.99s
                        Total time: 37761.13s
                               ETA: 1004510.6s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.689s, learning 0.164s)
               Value function loss: 11.8342
                    Surrogate loss: 0.0036
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 49.41
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.85s
                        Total time: 37769.98s
                               ETA: 1004458.4s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.496s, learning 0.162s)
               Value function loss: 0.2307
                    Surrogate loss: -0.0395
             Mean action noise std: 0.71
                       Mean reward: 4.70
               Mean episode length: 49.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.66s
                        Total time: 37778.64s
                               ETA: 1004401.1s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.709s, learning 0.320s)
               Value function loss: 0.2271
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 3.81
               Mean episode length: 48.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 9.03s
                        Total time: 37787.67s
                               ETA: 1004353.6s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.489s, learning 0.169s)
               Value function loss: 0.2064
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 46.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.66s
                        Total time: 37796.32s
                               ETA: 1004296.3s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1592 steps/s (collection: 10.002s, learning 0.288s)
               Value function loss: 59.4651
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 48.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 10.29s
                        Total time: 37806.61s
                               ETA: 1004282.5s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.557s, learning 0.172s)
               Value function loss: 140.6992
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.58
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 16.73s
                        Total time: 37823.34s
                               ETA: 1004439.6s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 937 steps/s (collection: 17.219s, learning 0.251s)
               Value function loss: 158.4620
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 4.66
               Mean episode length: 49.78
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 17.47s
                        Total time: 37840.81s
                               ETA: 1004616.2s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.586s, learning 0.155s)
               Value function loss: 63.0369
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 48.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 16.74s
                        Total time: 37857.55s
                               ETA: 1004773.5s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.022s, learning 0.187s)
               Value function loss: 2.5093
                    Surrogate loss: -0.0278
             Mean action noise std: 0.71
                       Mean reward: 9.31
               Mean episode length: 48.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 17.21s
                        Total time: 37874.76s
                               ETA: 1004943.0s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.084s, learning 0.165s)
               Value function loss: 1.1528
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 17.25s
                        Total time: 37892.01s
                               ETA: 1005113.5s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.542s, learning 0.167s)
               Value function loss: 106.5542
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 11.77
               Mean episode length: 49.67
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 16.71s
                        Total time: 37908.72s
                               ETA: 1005269.5s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.672s, learning 0.164s)
               Value function loss: 1.0204
                    Surrogate loss: -0.0332
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 16.84s
                        Total time: 37925.56s
                               ETA: 1005428.9s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.053s, learning 0.247s)
               Value function loss: 0.6727
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 48.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 17.30s
                        Total time: 37942.85s
                               ETA: 1005600.4s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.820s, learning 0.221s)
               Value function loss: 0.5278
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 4.17
               Mean episode length: 48.64
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 17.04s
                        Total time: 37959.90s
                               ETA: 1005765.0s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.231s, learning 0.164s)
               Value function loss: 0.4326
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 48.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 17.39s
                        Total time: 37977.29s
                               ETA: 1005938.9s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.629s, learning 0.354s)
               Value function loss: 0.3672
                    Surrogate loss: -0.0245
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 49.58
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 16.98s
                        Total time: 37994.27s
                               ETA: 1006101.7s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.913s, learning 0.276s)
               Value function loss: 0.3163
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 48.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 17.19s
                        Total time: 38011.46s
                               ETA: 1006269.9s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.891s, learning 0.184s)
               Value function loss: 0.2985
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 49.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 17.07s
                        Total time: 38028.54s
                               ETA: 1006435.0s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.530s, learning 0.156s)
               Value function loss: 333.8866
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 48.47
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 16.69s
                        Total time: 38045.22s
                               ETA: 1006589.7s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.806s, learning 0.159s)
               Value function loss: 0.5020
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 49.58
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 16.96s
                        Total time: 38062.19s
                               ETA: 1006751.7s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.862s, learning 0.171s)
               Value function loss: 0.4700
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 6.86
               Mean episode length: 49.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 17.03s
                        Total time: 38079.22s
                               ETA: 1006915.3s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.348s, learning 0.185s)
               Value function loss: 0.3175
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 50.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 16.53s
                        Total time: 38095.75s
                               ETA: 1007065.7s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.896s, learning 0.199s)
               Value function loss: 0.2493
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 48.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 17.09s
                        Total time: 38112.85s
                               ETA: 1007230.8s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.956s, learning 0.159s)
               Value function loss: 0.2164
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 3.64
               Mean episode length: 48.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 17.12s
                        Total time: 38129.96s
                               ETA: 1007396.4s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.809s, learning 0.313s)
               Value function loss: 53.9643
                    Surrogate loss: 0.0041
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 48.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 17.12s
                        Total time: 38147.08s
                               ETA: 1007562.0s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.501s, learning 0.165s)
               Value function loss: 0.2982
                    Surrogate loss: -0.0404
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 16.67s
                        Total time: 38163.75s
                               ETA: 1007715.5s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.794s, learning 0.160s)
               Value function loss: 0.2874
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 4.48
               Mean episode length: 49.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 16.95s
                        Total time: 38180.70s
                               ETA: 1007876.5s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.699s, learning 0.392s)
               Value function loss: 0.2436
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 50.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 17.09s
                        Total time: 38197.80s
                               ETA: 1008041.0s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.844s, learning 0.234s)
               Value function loss: 214.1383
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 49.29
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 17.08s
                        Total time: 38214.87s
                               ETA: 1008205.0s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.509s, learning 0.158s)
               Value function loss: 17.6577
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.62
               Mean episode length: 51.23
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 16.67s
                        Total time: 38231.54s
                               ETA: 1008358.2s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 950 steps/s (collection: 17.052s, learning 0.184s)
               Value function loss: 18.2625
                    Surrogate loss: 0.0036
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 49.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 17.24s
                        Total time: 38248.78s
                               ETA: 1008526.2s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.489s, learning 0.164s)
               Value function loss: 73.7911
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 3.89
               Mean episode length: 48.69
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 16.65s
                        Total time: 38265.43s
                               ETA: 1008678.8s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.956s, learning 0.183s)
               Value function loss: 0.5556
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 4.40
               Mean episode length: 50.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 17.14s
                        Total time: 38282.57s
                               ETA: 1008844.1s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.046s, learning 0.254s)
               Value function loss: 18.3728
                    Surrogate loss: 0.0033
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.70
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 17.30s
                        Total time: 38299.87s
                               ETA: 1009013.5s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.544s, learning 0.158s)
               Value function loss: 0.5025
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 16.70s
                        Total time: 38316.57s
                               ETA: 1009167.1s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.488s, learning 0.163s)
               Value function loss: 64.2388
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 6.87
               Mean episode length: 50.02
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 16.65s
                        Total time: 38333.22s
                               ETA: 1009319.3s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.115s, learning 0.185s)
               Value function loss: 0.4457
                    Surrogate loss: -0.0399
             Mean action noise std: 0.71
                       Mean reward: 4.65
               Mean episode length: 50.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 17.30s
                        Total time: 38350.52s
                               ETA: 1009488.4s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.103s, learning 0.166s)
               Value function loss: 138.7106
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 49.18
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 17.27s
                        Total time: 38367.79s
                               ETA: 1009656.7s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.858s, learning 0.184s)
               Value function loss: 4.2048
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 4.27
               Mean episode length: 50.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 17.04s
                        Total time: 38384.83s
                               ETA: 1009818.8s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.777s, learning 0.240s)
               Value function loss: 0.5135
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 17.02s
                        Total time: 38401.85s
                               ETA: 1009980.2s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.848s, learning 0.160s)
               Value function loss: 4.2622
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.56
               Mean episode length: 49.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 17.01s
                        Total time: 38418.86s
                               ETA: 1010141.2s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.683s, learning 0.164s)
               Value function loss: 12.1873
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 5.12
               Mean episode length: 50.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 16.85s
                        Total time: 38435.70s
                               ETA: 1010298.0s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.460s, learning 0.166s)
               Value function loss: 0.4190
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 6.46
               Mean episode length: 48.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 12.63s
                        Total time: 38448.33s
                               ETA: 1010343.7s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.804s, learning 0.207s)
               Value function loss: 0.3462
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 49.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 9.01s
                        Total time: 38457.34s
                               ETA: 1010294.4s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.776s, learning 0.220s)
               Value function loss: 0.2822
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 4.54
               Mean episode length: 49.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 9.00s
                        Total time: 38466.34s
                               ETA: 1010244.7s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.745s, learning 0.163s)
               Value function loss: 0.2929
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 8.91s
                        Total time: 38475.25s
                               ETA: 1010192.8s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.617s, learning 0.165s)
               Value function loss: 16.8037
                    Surrogate loss: 0.0026
             Mean action noise std: 0.71
                       Mean reward: 4.35
               Mean episode length: 49.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 8.78s
                        Total time: 38484.03s
                               ETA: 1010137.6s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.348s, learning 0.162s)
               Value function loss: 0.3108
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 49.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 8.51s
                        Total time: 38492.54s
                               ETA: 1010075.2s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.462s, learning 0.166s)
               Value function loss: 7.1137
                    Surrogate loss: 0.0043
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 47.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 8.63s
                        Total time: 38501.17s
                               ETA: 1010016.0s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.137s, learning 0.169s)
               Value function loss: 131.9607
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 4.63
               Mean episode length: 49.44
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 8.31s
                        Total time: 38509.47s
                               ETA: 1009948.4s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.613s, learning 0.244s)
               Value function loss: 0.3253
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 4.37
               Mean episode length: 47.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 8.86s
                        Total time: 38518.33s
                               ETA: 1009895.2s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.850s, learning 0.185s)
               Value function loss: 9.2058
                    Surrogate loss: 0.0037
             Mean action noise std: 0.71
                       Mean reward: 4.03
               Mean episode length: 48.35
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 9.03s
                        Total time: 38527.36s
                               ETA: 1009846.7s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.812s, learning 0.227s)
               Value function loss: 0.2791
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 3.92
               Mean episode length: 48.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 9.04s
                        Total time: 38536.40s
                               ETA: 1009798.4s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.730s, learning 0.224s)
               Value function loss: 81.9990
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 14.24
               Mean episode length: 48.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.95s
                        Total time: 38545.36s
                               ETA: 1009747.9s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.680s, learning 0.249s)
               Value function loss: 33.6607
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 6.78
               Mean episode length: 49.75
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 8.93s
                        Total time: 38554.29s
                               ETA: 1009696.7s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.470s, learning 0.197s)
               Value function loss: 298.1269
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 3.80
               Mean episode length: 46.57
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 8.67s
                        Total time: 38562.95s
                               ETA: 1009638.7s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.340s, learning 0.325s)
               Value function loss: 8.6376
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 49.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.67s
                        Total time: 38571.62s
                               ETA: 1009580.7s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.533s, learning 0.244s)
               Value function loss: 1.0010
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 6.51
               Mean episode length: 48.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.78s
                        Total time: 38580.40s
                               ETA: 1009525.6s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.684s, learning 0.216s)
               Value function loss: 0.6728
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 6.84
               Mean episode length: 50.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.90s
                        Total time: 38589.30s
                               ETA: 1009473.7s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.730s, learning 0.158s)
               Value function loss: 0.4223
                    Surrogate loss: -0.0278
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 49.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.89s
                        Total time: 38598.18s
                               ETA: 1009421.6s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.824s, learning 0.164s)
               Value function loss: 0.3457
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 48.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.99s
                        Total time: 38607.17s
                               ETA: 1009372.1s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.796s, learning 0.165s)
               Value function loss: 0.2549
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.96s
                        Total time: 38616.13s
                               ETA: 1009322.0s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.349s, learning 0.169s)
               Value function loss: 0.2886
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 49.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.52s
                        Total time: 38624.65s
                               ETA: 1009260.2s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1783 steps/s (collection: 8.946s, learning 0.242s)
               Value function loss: 0.2667
                    Surrogate loss: -0.0248
             Mean action noise std: 0.71
                       Mean reward: 3.82
               Mean episode length: 48.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 9.19s
                        Total time: 38633.84s
                               ETA: 1009216.0s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.637s, learning 0.258s)
               Value function loss: 46.4841
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3.28
               Mean episode length: 46.77
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.90s
                        Total time: 38642.73s
                               ETA: 1009164.2s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.443s, learning 0.298s)
               Value function loss: 3.8449
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.74s
                        Total time: 38651.47s
                               ETA: 1009108.4s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.639s, learning 0.191s)
               Value function loss: 0.3943
                    Surrogate loss: -0.0272
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 49.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.83s
                        Total time: 38660.30s
                               ETA: 1009054.9s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.493s, learning 0.164s)
               Value function loss: 0.3967
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 9.25
               Mean episode length: 49.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.66s
                        Total time: 38668.96s
                               ETA: 1008996.9s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.528s, learning 0.269s)
               Value function loss: 0.3054
                    Surrogate loss: -0.0276
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 50.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.80s
                        Total time: 38677.76s
                               ETA: 1008942.6s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.560s, learning 0.171s)
               Value function loss: 0.2348
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 49.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.73s
                        Total time: 38686.49s
                               ETA: 1008886.6s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.779s, learning 0.297s)
               Value function loss: 0.2247
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 49.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 9.08s
                        Total time: 38695.56s
                               ETA: 1008839.7s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.982s, learning 0.169s)
               Value function loss: 0.2636
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 4.60
               Mean episode length: 51.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 9.15s
                        Total time: 38704.72s
                               ETA: 1008794.7s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.722s, learning 0.216s)
               Value function loss: 0.2321
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 50.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.94s
                        Total time: 38713.65s
                               ETA: 1008744.2s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.812s, learning 0.222s)
               Value function loss: 0.1884
                    Surrogate loss: -0.0336
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 48.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 9.03s
                        Total time: 38722.69s
                               ETA: 1008696.2s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.649s, learning 0.191s)
               Value function loss: 0.1761
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 49.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 8.84s
                        Total time: 38731.53s
                               ETA: 1008643.1s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.479s, learning 0.168s)
               Value function loss: 11.3185
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 50.51
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.65s
                        Total time: 38740.17s
                               ETA: 1008585.1s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.551s, learning 0.191s)
               Value function loss: 213.3825
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 48.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.74s
                        Total time: 38748.92s
                               ETA: 1008529.5s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.557s, learning 0.172s)
               Value function loss: 0.5031
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 49.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.73s
                        Total time: 38757.64s
                               ETA: 1008473.7s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.563s, learning 0.166s)
               Value function loss: 0.3075
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 48.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.73s
                        Total time: 38766.37s
                               ETA: 1008417.9s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.688s, learning 0.186s)
               Value function loss: 0.2937
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.87s
                        Total time: 38775.25s
                               ETA: 1008365.9s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.412s, learning 0.189s)
               Value function loss: 0.2393
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 50.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 8.60s
                        Total time: 38783.85s
                               ETA: 1008306.8s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.680s, learning 0.185s)
               Value function loss: 7.3693
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 48.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.87s
                        Total time: 38792.71s
                               ETA: 1008254.6s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.518s, learning 0.185s)
               Value function loss: 68.0732
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.42
               Mean episode length: 50.23
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.70s
                        Total time: 38801.42s
                               ETA: 1008198.2s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.644s, learning 0.349s)
               Value function loss: 4.0528
                    Surrogate loss: -0.0070
             Mean action noise std: 0.71
                       Mean reward: 12.15
               Mean episode length: 50.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.99s
                        Total time: 38810.41s
                               ETA: 1008149.4s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.280s, learning 0.184s)
               Value function loss: 29.4669
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 7.22
               Mean episode length: 50.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.46s
                        Total time: 38818.88s
                               ETA: 1008086.8s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.735s, learning 0.183s)
               Value function loss: 9.9587
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 49.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.92s
                        Total time: 38827.79s
                               ETA: 1008036.1s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.618s, learning 0.187s)
               Value function loss: 17.5907
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 50.72
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.80s
                        Total time: 38836.60s
                               ETA: 1007982.4s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1787 steps/s (collection: 9.006s, learning 0.161s)
               Value function loss: 29.1541
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 9.22
               Mean episode length: 48.87
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 9.17s
                        Total time: 38845.77s
                               ETA: 1007938.2s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.450s, learning 0.166s)
               Value function loss: 10.9749
                    Surrogate loss: -0.0029
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 49.71
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.62s
                        Total time: 38854.38s
                               ETA: 1007879.7s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.565s, learning 0.373s)
               Value function loss: 0.2800
                    Surrogate loss: -0.0390
             Mean action noise std: 0.71
                       Mean reward: 6.54
               Mean episode length: 48.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.94s
                        Total time: 38863.32s
                               ETA: 1007829.6s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.413s, learning 0.160s)
               Value function loss: 0.2307
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 4.66
               Mean episode length: 49.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.57s
                        Total time: 38871.89s
                               ETA: 1007770.1s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.182s, learning 0.199s)
               Value function loss: 0.2856
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 4.69
               Mean episode length: 50.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.38s
                        Total time: 38880.27s
                               ETA: 1007705.6s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.169s, learning 0.182s)
               Value function loss: 0.2777
                    Surrogate loss: -0.0259
             Mean action noise std: 0.71
                       Mean reward: 4.13
               Mean episode length: 49.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.35s
                        Total time: 38888.63s
                               ETA: 1007640.3s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.492s, learning 0.159s)
               Value function loss: 16.3840
                    Surrogate loss: 0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 48.18
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.65s
                        Total time: 38897.28s
                               ETA: 1007582.8s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.526s, learning 0.162s)
               Value function loss: 20.4112
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 4.01
               Mean episode length: 48.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 8.69s
                        Total time: 38905.96s
                               ETA: 1007526.4s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.913s, learning 0.183s)
               Value function loss: 0.3245
                    Surrogate loss: -0.0376
             Mean action noise std: 0.71
                       Mean reward: 6.74
               Mean episode length: 50.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 9.10s
                        Total time: 38915.06s
                               ETA: 1007480.5s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.732s, learning 0.177s)
               Value function loss: 17.1782
                    Surrogate loss: 0.0056
             Mean action noise std: 0.71
                       Mean reward: 6.49
               Mean episode length: 49.27
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 8.91s
                        Total time: 38923.97s
                               ETA: 1007429.8s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.771s, learning 0.199s)
               Value function loss: 0.3506
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 49.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.97s
                        Total time: 38932.94s
                               ETA: 1007380.7s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.648s, learning 0.158s)
               Value function loss: 0.2847
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 4.34
               Mean episode length: 49.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 8.81s
                        Total time: 38941.75s
                               ETA: 1007327.3s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.893s, learning 0.161s)
               Value function loss: 53.6147
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 48.97
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 9.05s
                        Total time: 38950.80s
                               ETA: 1007280.4s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.043s, learning 0.159s)
               Value function loss: 89.4555
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4.15
               Mean episode length: 49.45
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.20s
                        Total time: 38959.00s
                               ETA: 1007211.6s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.709s, learning 0.186s)
               Value function loss: 0.7868
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 49.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.90s
                        Total time: 38967.90s
                               ETA: 1007160.6s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.590s, learning 0.160s)
               Value function loss: 16.5086
                    Surrogate loss: 0.0083
             Mean action noise std: 0.71
                       Mean reward: 3.74
               Mean episode length: 48.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.75s
                        Total time: 38976.65s
                               ETA: 1007105.9s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.466s, learning 0.186s)
               Value function loss: 0.3590
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 50.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.65s
                        Total time: 38985.30s
                               ETA: 1007048.7s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.782s, learning 0.162s)
               Value function loss: 12.1594
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 3.75
               Mean episode length: 49.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.94s
                        Total time: 38994.24s
                               ETA: 1006999.1s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.686s, learning 0.207s)
               Value function loss: 0.2584
                    Surrogate loss: -0.0392
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 48.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 8.89s
                        Total time: 39003.13s
                               ETA: 1006948.2s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.767s, learning 0.169s)
               Value function loss: 0.2726
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 48.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.94s
                        Total time: 39012.07s
                               ETA: 1006898.4s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.575s, learning 0.183s)
               Value function loss: 0.2247
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 8.76s
                        Total time: 39020.83s
                               ETA: 1006844.1s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1780 steps/s (collection: 9.035s, learning 0.164s)
               Value function loss: 0.2339
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 3.40
               Mean episode length: 47.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 9.20s
                        Total time: 39030.03s
                               ETA: 1006801.1s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.860s, learning 0.189s)
               Value function loss: 0.2029
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 48.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 9.05s
                        Total time: 39039.08s
                               ETA: 1006754.3s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.589s, learning 0.167s)
               Value function loss: 0.1918
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.51
               Mean episode length: 48.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.76s
                        Total time: 39047.83s
                               ETA: 1006700.0s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.626s, learning 0.162s)
               Value function loss: 0.1618
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 3.85
               Mean episode length: 49.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.79s
                        Total time: 39056.62s
                               ETA: 1006646.5s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.174s)
               Value function loss: 0.1703
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 3.70
               Mean episode length: 49.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 8.83s
                        Total time: 39065.45s
                               ETA: 1006594.1s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.259s, learning 0.216s)
               Value function loss: 0.2026
                    Surrogate loss: -0.0279
             Mean action noise std: 0.71
                       Mean reward: 3.43
               Mean episode length: 47.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 8.47s
                        Total time: 39073.92s
                               ETA: 1006532.5s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.490s, learning 0.160s)
               Value function loss: 0.1776
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 50.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.65s
                        Total time: 39082.57s
                               ETA: 1006475.6s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.752s, learning 0.233s)
               Value function loss: 0.1705
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 4.11
               Mean episode length: 49.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 8.98s
                        Total time: 39091.56s
                               ETA: 1006427.3s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.828s, learning 0.252s)
               Value function loss: 11.9286
                    Surrogate loss: 0.0054
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 49.90
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 9.08s
                        Total time: 39100.64s
                               ETA: 1006381.4s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.623s, learning 0.242s)
               Value function loss: 0.1825
                    Surrogate loss: -0.0403
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 49.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.86s
                        Total time: 39109.50s
                               ETA: 1006330.1s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.521s, learning 0.275s)
               Value function loss: 0.1907
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 4.08
               Mean episode length: 47.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.80s
                        Total time: 39118.30s
                               ETA: 1006277.0s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.673s, learning 0.163s)
               Value function loss: 15.5294
                    Surrogate loss: 0.0053
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 48.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.84s
                        Total time: 39127.14s
                               ETA: 1006224.9s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1784 steps/s (collection: 8.997s, learning 0.184s)
               Value function loss: 0.2018
                    Surrogate loss: -0.0380
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 48.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 9.18s
                        Total time: 39136.32s
                               ETA: 1006181.7s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.852s, learning 0.156s)
               Value function loss: 0.2032
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 49.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 9.01s
                        Total time: 39145.32s
                               ETA: 1006134.1s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.719s, learning 0.182s)
               Value function loss: 0.1743
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 4.41
               Mean episode length: 49.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 8.90s
                        Total time: 39154.22s
                               ETA: 1006083.8s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.745s, learning 0.162s)
               Value function loss: 0.1603
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 48.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.91s
                        Total time: 39163.13s
                               ETA: 1006033.6s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.816s, learning 0.195s)
               Value function loss: 16.7742
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 4.12
               Mean episode length: 48.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 9.01s
                        Total time: 39172.14s
                               ETA: 1005986.2s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.754s, learning 0.196s)
               Value function loss: 0.1991
                    Surrogate loss: -0.0402
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 49.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 8.95s
                        Total time: 39181.09s
                               ETA: 1005937.2s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1776 steps/s (collection: 8.882s, learning 0.341s)
               Value function loss: 0.2075
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 3.76
               Mean episode length: 47.89
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 9.22s
                        Total time: 39190.32s
                               ETA: 1005895.2s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.468s, learning 0.186s)
               Value function loss: 17.5871
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 6.46
               Mean episode length: 48.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 8.65s
                        Total time: 39198.97s
                               ETA: 1005838.6s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.890s, learning 0.171s)
               Value function loss: 10.9357
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 47.66
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 9.06s
                        Total time: 39208.03s
                               ETA: 1005792.5s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.132s, learning 0.166s)
               Value function loss: 0.2947
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 4.36
               Mean episode length: 49.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 8.30s
                        Total time: 39216.33s
                               ETA: 1005726.9s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.337s, learning 0.307s)
               Value function loss: 0.2565
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 9.24
               Mean episode length: 48.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 8.64s
                        Total time: 39224.97s
                               ETA: 1005670.1s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1757 steps/s (collection: 8.982s, learning 0.337s)
               Value function loss: 29.9600
                    Surrogate loss: 0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.36
               Mean episode length: 48.99
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 9.32s
                        Total time: 39234.29s
                               ETA: 1005630.8s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.352s, learning 0.169s)
               Value function loss: 0.2126
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 49.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 8.52s
                        Total time: 39242.81s
                               ETA: 1005570.9s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.755s, learning 0.184s)
               Value function loss: 0.2267
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 9.60
               Mean episode length: 49.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 8.94s
                        Total time: 39251.75s
                               ETA: 1005521.8s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.619s, learning 0.158s)
               Value function loss: 0.2179
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 4.19
               Mean episode length: 48.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.78s
                        Total time: 39260.53s
                               ETA: 1005468.5s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.503s, learning 0.171s)
               Value function loss: 0.1845
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 3.93
               Mean episode length: 47.88
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.67s
                        Total time: 39269.20s
                               ETA: 1005412.7s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.166s)
               Value function loss: 0.1713
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 4.22
               Mean episode length: 49.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 8.66s
                        Total time: 39277.86s
                               ETA: 1005356.6s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.533s, learning 0.166s)
               Value function loss: 0.1534
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 3.97
               Mean episode length: 49.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 8.70s
                        Total time: 39286.56s
                               ETA: 1005301.5s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.692s, learning 0.167s)
               Value function loss: 0.1725
                    Surrogate loss: -0.0331
             Mean action noise std: 0.71
                       Mean reward: 4.47
               Mean episode length: 48.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.86s
                        Total time: 39295.42s
                               ETA: 1005250.4s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.507s, learning 0.367s)
               Value function loss: 0.1530
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 46.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 8.87s
                        Total time: 39304.30s
                               ETA: 1005199.8s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.162s)
               Value function loss: 0.1540
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 3.94
               Mean episode length: 48.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.61s
                        Total time: 39312.91s
                               ETA: 1005142.5s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.325s, learning 0.283s)
               Value function loss: 0.1549
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 4.07
               Mean episode length: 46.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.61s
                        Total time: 39321.51s
                               ETA: 1005085.1s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.336s, learning 0.162s)
               Value function loss: 0.1579
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 4.63
               Mean episode length: 49.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 8.50s
                        Total time: 39330.01s
                               ETA: 1005024.9s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.647s, learning 0.309s)
               Value function loss: 0.1406
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 3.73
               Mean episode length: 47.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.96s
                        Total time: 39338.97s
                               ETA: 1004976.5s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.639s, learning 0.170s)
               Value function loss: 13.6653
                    Surrogate loss: 0.0014
             Mean action noise std: 0.71
                       Mean reward: 4.81
               Mean episode length: 48.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.81s
                        Total time: 39347.78s
                               ETA: 1004924.3s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.627s, learning 0.206s)
               Value function loss: 0.1775
                    Surrogate loss: -0.0405
             Mean action noise std: 0.71
                       Mean reward: 4.28
               Mean episode length: 47.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.83s
                        Total time: 39356.61s
                               ETA: 1004872.8s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.696s, learning 0.296s)
               Value function loss: 0.1548
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 3.50
               Mean episode length: 46.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.99s
                        Total time: 39365.60s
                               ETA: 1004825.3s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.912s, learning 0.189s)
               Value function loss: 0.1592
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 4.04
               Mean episode length: 47.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 9.10s
                        Total time: 39374.71s
                               ETA: 1004780.7s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.767s, learning 0.241s)
               Value function loss: 0.1558
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 3.61
               Mean episode length: 46.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 9.01s
                        Total time: 39383.71s
                               ETA: 1004733.6s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.682s, learning 0.168s)
               Value function loss: 0.1430
                    Surrogate loss: -0.0343
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 48.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 8.85s
                        Total time: 39392.56s
                               ETA: 1004682.6s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.173s)
               Value function loss: 0.1652
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 4.92
               Mean episode length: 50.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 8.68s
                        Total time: 39401.24s
                               ETA: 1004627.3s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.903s, learning 0.172s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 49.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 9.07s
                        Total time: 39410.32s
                               ETA: 1004582.1s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.458s, learning 0.167s)
               Value function loss: 120.1691
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 4.14
               Mean episode length: 48.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 8.63s
                        Total time: 39418.94s
                               ETA: 1004525.4s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.319s, learning 0.165s)
               Value function loss: 0.2366
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 47.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 8.48s
                        Total time: 39427.43s
                               ETA: 1004465.2s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.793s, learning 0.173s)
               Value function loss: 0.2239
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 47.72
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 8.97s
                        Total time: 39436.39s
                               ETA: 1004417.2s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.734s, learning 0.235s)
               Value function loss: 0.1966
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 48.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 8.97s
                        Total time: 39445.36s
                               ETA: 1004369.4s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.596s, learning 0.160s)
               Value function loss: 0.1812
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 4.18
               Mean episode length: 48.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 8.76s
                        Total time: 39454.12s
                               ETA: 1004316.1s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.480s, learning 0.171s)
               Value function loss: 15.6143
                    Surrogate loss: 0.0060
             Mean action noise std: 0.71
                       Mean reward: 4.02
               Mean episode length: 48.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 8.65s
                        Total time: 39462.77s
                               ETA: 1004260.2s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.645s, learning 0.162s)
               Value function loss: 0.2363
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 4.60
               Mean episode length: 50.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 8.81s
                        Total time: 39471.58s
                               ETA: 1004208.3s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.497s, learning 0.163s)
               Value function loss: 59.2663
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 47.07
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 8.66s
                        Total time: 39480.24s
                               ETA: 1004152.7s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.524s, learning 0.173s)
               Value function loss: 0.2842
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 49.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 8.70s
                        Total time: 39488.94s
                               ETA: 1004098.0s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.809s, learning 0.162s)
               Value function loss: 0.2569
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 49.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 8.97s
                        Total time: 39497.91s
                               ETA: 1004050.4s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.590s, learning 0.167s)
               Value function loss: 0.1957
                    Surrogate loss: -0.0324
             Mean action noise std: 0.71
                       Mean reward: 4.42
               Mean episode length: 48.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 8.76s
                        Total time: 39506.66s
                               ETA: 1003997.3s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.749s, learning 0.245s)
               Value function loss: 0.1890
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 3.78
               Mean episode length: 47.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 8.99s
                        Total time: 39515.66s
                               ETA: 1003950.2s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.687s, learning 0.157s)
               Value function loss: 0.1880
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 49.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 8.84s
                        Total time: 39524.50s
                               ETA: 1003899.4s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.407s, learning 0.197s)
               Value function loss: 13.7348
                    Surrogate loss: 0.0049
             Mean action noise std: 0.71
                       Mean reward: 4.48
               Mean episode length: 49.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.60s
                        Total time: 39533.11s
                               ETA: 1003842.5s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.834s, learning 0.273s)
               Value function loss: 0.2294
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 3.67
               Mean episode length: 47.13
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 9.11s
                        Total time: 39542.21s
                               ETA: 1003798.3s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.851s, learning 0.161s)
               Value function loss: 0.1872
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 6.99
               Mean episode length: 48.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 9.01s
                        Total time: 39551.22s
                               ETA: 1003751.8s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.539s, learning 0.331s)
               Value function loss: 0.1785
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 4.47
               Mean episode length: 49.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 8.87s
                        Total time: 39560.09s
                               ETA: 1003701.7s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.512s, learning 0.172s)
               Value function loss: 0.1742
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 3.87
               Mean episode length: 47.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.68s
                        Total time: 39568.78s
                               ETA: 1003647.0s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.784s, learning 0.250s)
               Value function loss: 17.4890
                    Surrogate loss: 0.0048
             Mean action noise std: 0.71
                       Mean reward: 3.86
               Mean episode length: 47.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 9.03s
                        Total time: 39577.81s
                               ETA: 1003601.1s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.519s, learning 0.172s)
               Value function loss: 107.1319
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 3.59
               Mean episode length: 47.50
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 8.69s
                        Total time: 39586.50s
                               ETA: 1003546.5s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.258s, learning 0.156s)
               Value function loss: 16.2894
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 4.47
               Mean episode length: 48.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.41s
                        Total time: 39594.92s
                               ETA: 1003485.0s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.273s, learning 0.163s)
               Value function loss: 4.2256
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 9.21
               Mean episode length: 48.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.44s
                        Total time: 39603.35s
                               ETA: 1003424.0s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.474s, learning 0.168s)
               Value function loss: 0.4434
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 4.20
               Mean episode length: 49.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 8.64s
                        Total time: 39611.99s
                               ETA: 1003368.3s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.631s, learning 0.179s)
               Value function loss: 0.3399
                    Surrogate loss: -0.0269
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 48.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.81s
                        Total time: 39620.80s
                               ETA: 1003316.8s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.722s, learning 0.162s)
               Value function loss: 0.3133
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 4.25
               Mean episode length: 48.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.88s
                        Total time: 39629.69s
                               ETA: 1003267.3s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.619s, learning 0.239s)
               Value function loss: 0.2882
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 3.48
               Mean episode length: 46.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.86s
                        Total time: 39638.55s
                               ETA: 1003217.1s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.553s, learning 0.177s)
               Value function loss: 0.2415
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 4.06
               Mean episode length: 49.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 8.73s
                        Total time: 39647.28s
                               ETA: 1003163.7s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.804s, learning 0.184s)
               Value function loss: 0.2327
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 3.88
               Mean episode length: 48.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 9.99s
                        Total time: 39657.26s
                               ETA: 1003142.1s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 935 steps/s (collection: 17.345s, learning 0.170s)
               Value function loss: 0.2109
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 3.79
               Mean episode length: 47.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 17.52s
                        Total time: 39674.78s
                               ETA: 1003310.9s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.660s, learning 0.168s)
               Value function loss: 11.9138
                    Surrogate loss: 0.0050
             Mean action noise std: 0.71
                       Mean reward: 3.99
               Mean episode length: 48.11
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 16.83s
                        Total time: 39691.61s
                               ETA: 1003462.3s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.116s, learning 0.214s)
               Value function loss: 0.2103
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 4.50
               Mean episode length: 48.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 17.33s
                        Total time: 39708.94s
                               ETA: 1003626.2s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.785s, learning 0.168s)
               Value function loss: 0.2158
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 47.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 16.95s
                        Total time: 39725.89s
                               ETA: 1003780.5s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 926 steps/s (collection: 17.359s, learning 0.329s)
               Value function loss: 11.9620
                    Surrogate loss: 0.0028
             Mean action noise std: 0.71
                       Mean reward: 4.50
               Mean episode length: 49.46
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 17.69s
                        Total time: 39743.58s
                               ETA: 1003953.3s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.688s, learning 0.268s)
               Value function loss: 123.8091
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.56
               Mean episode length: 49.18
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 16.96s
                        Total time: 39760.53s
                               ETA: 1004107.5s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.562s, learning 0.173s)
               Value function loss: 0.3110
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 4.49
               Mean episode length: 49.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 16.73s
                        Total time: 39777.27s
                               ETA: 1004256.0s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.912s, learning 0.283s)
               Value function loss: 0.2446
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 12.34
               Mean episode length: 49.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 17.20s
                        Total time: 39794.47s
                               ETA: 1004416.1s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 937 steps/s (collection: 17.300s, learning 0.177s)
               Value function loss: 15.0827
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 3.83
               Mean episode length: 47.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 17.48s
                        Total time: 39811.94s
                               ETA: 1004583.1s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 917 steps/s (collection: 17.572s, learning 0.288s)
               Value function loss: 0.3078
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 4.86
               Mean episode length: 49.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 17.86s
                        Total time: 39829.80s
                               ETA: 1004759.8s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 921 steps/s (collection: 17.537s, learning 0.249s)
               Value function loss: 16.5963
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 3.96
               Mean episode length: 48.74
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 17.79s
                        Total time: 39847.59s
                               ETA: 1004934.4s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.878s, learning 0.320s)
               Value function loss: 18.2209
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 6.45
               Mean episode length: 47.86
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 17.20s
                        Total time: 39864.79s
                               ETA: 1005094.2s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.787s, learning 0.158s)
               Value function loss: 78.9883
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 4.39
               Mean episode length: 49.22
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 16.95s
                        Total time: 39881.73s
                               ETA: 1005247.5s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 938 steps/s (collection: 17.291s, learning 0.170s)
               Value function loss: 104.5996
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 49.08
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 17.46s
                        Total time: 39899.19s
                               ETA: 1005413.7s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 934 steps/s (collection: 17.375s, learning 0.159s)
               Value function loss: 58.1315
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 24.27
               Mean episode length: 48.01
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 17.53s
                        Total time: 39916.73s
                               ETA: 1005581.6s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.133s, learning 0.175s)
               Value function loss: 0.8376
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 4.10
               Mean episode length: 47.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 17.31s
                        Total time: 39934.04s
                               ETA: 1005743.8s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.686s, learning 0.170s)
               Value function loss: 0.5615
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 6.96
               Mean episode length: 48.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 16.86s
                        Total time: 39950.89s
                               ETA: 1005894.4s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.840s, learning 0.166s)
               Value function loss: 0.4130
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 4.26
               Mean episode length: 48.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 17.01s
                        Total time: 39967.90s
                               ETA: 1006048.8s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.420s, learning 0.169s)
               Value function loss: 0.3203
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 4.05
               Mean episode length: 47.04
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 16.59s
                        Total time: 39984.49s
                               ETA: 1006192.6s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.298s, learning 0.160s)
               Value function loss: 0.3189
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 3.95
               Mean episode length: 47.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 16.46s
                        Total time: 40000.94s
                               ETA: 1006333.0s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.893s, learning 0.158s)
               Value function loss: 0.2578
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 3.98
               Mean episode length: 47.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 17.05s
                        Total time: 40018.00s
                               ETA: 1006488.2s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.358s, learning 0.216s)
               Value function loss: 0.2534
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 4.00
               Mean episode length: 47.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 16.57s
                        Total time: 40034.57s
                               ETA: 1006631.3s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 906 steps/s (collection: 17.749s, learning 0.318s)
               Value function loss: 0.2277
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 48.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 18.07s
                        Total time: 40052.64s
                               ETA: 1006811.9s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.019s, learning 0.229s)
               Value function loss: 17.5492
                    Surrogate loss: 0.0051
             Mean action noise std: 0.71
                       Mean reward: 4.24
               Mean episode length: 49.27
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 17.25s
                        Total time: 40069.89s
                               ETA: 1006971.8s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 947 steps/s (collection: 16.985s, learning 0.298s)
               Value function loss: 0.2858
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 47.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 17.28s
                        Total time: 40087.17s
                               ETA: 1007132.5s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 947 steps/s (collection: 16.946s, learning 0.345s)
               Value function loss: 0.2830
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 4.23
               Mean episode length: 47.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 17.29s
                        Total time: 40104.46s
                               ETA: 1007293.3s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.046s, learning 0.201s)
               Value function loss: 0.2528
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 6.75
               Mean episode length: 46.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 17.25s
                        Total time: 40121.71s
                               ETA: 1007452.9s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.847s, learning 0.286s)
               Value function loss: 4.0310
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4.09
               Mean episode length: 48.12
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 17.13s
                        Total time: 40138.84s
                               ETA: 1007609.6s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.662s, learning 0.256s)
               Value function loss: 131.7664
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 7.02
               Mean episode length: 48.47
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 16.92s
                        Total time: 40155.76s
                               ETA: 1007760.7s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.901s, learning 0.184s)
               Value function loss: 33.1758
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 4.44
               Mean episode length: 48.90
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 17.09s
                        Total time: 40172.84s
                               ETA: 1007916.0s
